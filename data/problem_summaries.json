{
  "135": {
    "problem_id": "135",
    "title": "Click-Through Rate Prediction for Mobile Ads",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that a mobile ad will be clicked (CTR prediction) based on anonymized ad impression data. The goal is to develop models that outperform standard classification algorithms in predicting ad clicks.",
    "evaluation_metric": null,
    "full_content": "# Click-Through Rate Prediction for Mobile Ads\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a mobile ad will be clicked (CTR prediction) based on anonymized ad impression data. The goal is to develop models that outperform standard classification algorithms in predicting ad clicks.\n    * **Key Points:**\n        * Focus on sponsored search and real-time bidding applications\n        * Winning models will be released under open-source license\n        * Data spans 11 days of mobile ad impressions\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing anonymized mobile ad impressions with categorical and temporal features\n* **Data Files:**\n    * `train.gz` - 10 days of chronologically ordered click-through data (subsampled non-clicks and clicks)\n    * `test.gz` - 1 day of ads for testing predictions\n    * `sampleSubmission.csv` - Example submission file\n* **Key Features:**\n    * Temporal feature: `hour` (YYMMDDHH format)\n    * Target variable: `click` (0/1)\n    * Multiple anonymized categorical variables (C1-C21)\n    * Ad placement features (`banner_pos`)\n    * Contextual features (`site_id`, `site_domain`, `site_category`)\n    * App-related features (`app_id`, `app_domain`, `app_category`)\n    * Device characteristics (`device_id`, `device_type`, `device_model`, `device_conn_type`)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (Log Loss)\n    * **Metric Characteristics:**\n        * Measures the accuracy of probabilistic predictions\n        * Penalizes confident wrong predictions more heavily\n        * Smaller values indicate better performance\n        * Calculation: -1/N * Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)] where p_i is predicted probability and y_i is actual label",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a mobile ad will be clicked (CTR prediction) based on anonymized ad impression data. The goal is to develop models that outperform standard classification algorithms in predicting ad clicks.\n    * **Key Points:**\n        * Focus on sponsored search and real-time bidding applications\n        * Winning models will be released under open-source license\n        * Data spans 11 days of mobile ad impressions",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing anonymized mobile ad impressions with categorical and temporal features\n* **Data Files:**\n    * `train.gz` - 10 days of chronologically ordered click-through data (subsampled non-clicks and clicks)\n    * `test.gz` - 1 day of ads for testing predictions\n    * `sampleSubmission.csv` - Example submission file\n* **Key Features:**\n    * Temporal feature: `hour` (YYMMDDHH format)\n    * Target variable: `click` (0/1)\n    * Multiple anonymized categorical variables (C1-C21)\n    * Ad placement features (`banner_pos`)\n    * Contextual features (`site_id`, `site_domain`, `site_category`)\n    * App-related features (`app_id`, `app_domain`, `app_category`)\n    * Device characteristics (`device_id`, `device_type`, `device_model`, `device_conn_type`)",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (Log Loss)\n    * **Metric Characteristics:**\n        * Measures the accuracy of probabilistic predictions\n        * Penalizes confident wrong predictions more heavily\n        * Smaller values indicate better performance\n        * Calculation: -1/N * Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)] where p_i is predicted probability and y_i is actual label"
    },
    "file_path": "kaggle_datasets/135/problem_summary.md"
  },
  "307": {
    "problem_id": "307",
    "title": "Binary Classification of Malware Infection Risk",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Malware Infection Risk\n\n## Problem Description\n- **Problem Type:** Binary Classification  \n- **Objective:** Predict the probability that a Windows machine will be infected by malware based on its properties and telemetry data. The goal is to proactively identify machines at risk before malware attacks occur.  \n- **Key Points:**  \n  - Focuses on real-world telemetry data from Microsoft's endpoint protection solution (Windows Defender).  \n  - Dataset includes machines sampled with a higher proportion of malware-infected instances than typical in the wild.  \n  - Time-series complications (e.g., new machines, patches, OS updates) may cause discrepancies between cross-validation and leaderboard scores.  \n\n## Dataset Overview  \n- **Data Type:** Tabular data (machine-level telemetry and configuration attributes).  \n- **Context:** Data combines heartbeat and threat reports from Windows Defender, with each row representing a unique machine.  \n- **Data Files:**  \n  - `train.csv` (labeled data with `HasDetections` as the target).  \n  - `test.csv` (unlabeled data for prediction).  \n  - `sample_submission.csv` (example submission format).  \n- **Key Features:**  \n  - **Machine ID:** `MachineIdentifier` (unique key).  \n  - **Defender State:** `EngineVersion`, `AppVersion`, `AvSigVersion`, etc.  \n  - **Hardware/OS Properties:** `Processor`, `OsVer`, `OsBuild`, `Census_*` (e.g., RAM, disk type, display specs).  \n  - **Security Configurations:** `IsProtected`, `Firewall`, `SmartScreen`, `UacLuaenable`.  \n  - **Geographic/Org Context:** `CountryIdentifier`, `OrganizationIdentifier`.  \n\n## Evaluation Metrics  \n- **Primary Metric:** Area Under the ROC Curve (AUC).  \n- **Components:**  \n  - Predictions are probabilities (`HasDetections` = 1 for malware presence).  \n  - AUC measures the model's ability to rank infected machines higher than non-infected ones.  \n  - Submission format: `MachineIdentifier` paired with predicted probability (e.g., `1,0.5`).",
    "sections": {},
    "file_path": "kaggle_datasets/307/problem_summary.md"
  },
  "551": {
    "problem_id": "551",
    "title": "Predicting Parkinson's Disease Progression with Protein and Peptide Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Parkinson's Disease Progression with Protein and Peptide Data\n\n## Problem Description\n- **Problem Type**: Time Series Regression with Multivariate Output\n- **Objective**: Predict MDS-UPDRS scores (measuring Parkinson's disease progression) using longitudinal protein/peptide data. The goal is to:\n  - Forecast current visit scores\n  - Predict scores for potential future visits (6, 12, and 24 months ahead)\n- **Key Points**:\n  - Focus on identifying biomarkers correlated with disease progression\n  - Must handle irregular time intervals between patient visits\n  - Model cannot \"peek forward\" in time (enforced via Kaggle's time-series API)\n  - Includes medication status as a confounding variable (particularly for motor function scores)\n\n## Dataset Overview\n- **Data Type**: Multimodal time-series data combining:\n  - Tabular clinical records (UPDRS scores, medication status)\n  - Mass spectrometry measurements (protein/peptide abundances)\n- **Data Files**:\n  - `train_peptides.csv`: Peptide-level mass spectrometry data\n  - `train_proteins.csv`: Aggregated protein expression data\n  - `train_clinical_data.csv`: UPDRS scores and medication status\n  - `supplemental_clinical_data.csv`: Additional clinical records without CSF samples\n- **Key Features**:\n  - Temporal visit data (`visit_month` relative to baseline)\n  - Protein/peptide abundances (`NPX`, `PeptideAbundance`)\n  - 4 UPDRS sub-scores (parts 1-4) measuring different symptom categories\n  - Medication status during assessment (`upd23b_clinical_state_on_medication`)\n\n## Evaluation Metrics\n- **Primary Metric**: Symmetric Mean Absolute Percentage Error (SMAPE)\n  - SMAPE = 0 when both actual and predicted values are 0\n- **Implementation Details**:\n  - Evaluated on predictions for:\n    1. Current visit UPDRS scores\n    2. Future visits (6/12/24 month projections)\n  - Predictions for non-occurring visits are ignored\n  - Uses Kaggle's time-series API to prevent data leakage",
    "sections": {},
    "file_path": "kaggle_datasets/551/problem_summary.md"
  },
  "61": {
    "problem_id": "61",
    "title": "Prescription Volume Prediction",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict future prescription volumes for Pfizer Oncology products. The competition focuses on forecasting demand for cancer treatments, aiding in supply chain optimization and resource allocation.",
    "evaluation_metric": null,
    "full_content": "# Prescription Volume Prediction\n\n**Problem Description:**\n*   **Problem Type:** Time Series Forecasting\n*   **Objective:** Predict future prescription volumes for Pfizer Oncology products. The competition focuses on forecasting demand for cancer treatments, aiding in supply chain optimization and resource allocation.\n    *   **Key Points:**\n        *   Private, invitation-only competition (limited public details available).\n        *   Sponsored by Pfizer Oncology, suggesting a real-world pharmaceutical industry application.\n        *   Focus on oncology drugs, implying potential complexities like treatment regimens or disease-specific demand patterns.\n\n**Dataset Overview:**\n*   **Data Type & Context:** Likely tabular time series data (though specifics are private), potentially including:\n    *   Historical prescription volumes.\n    *   Time-based features (e.g., dates, months, quarters).\n    *   Possible external factors (e.g., marketing campaigns, drug approvals).\n*   **Data Files:** Not publicly disclosed (private competition).\n*   **Features:** Undisclosed, but likely involve temporal patterns and potentially anonymized product/demographic identifiers.\n\n**Evaluation Metrics:**\n*   **Evaluation Metric:** Undisclosed (private competition), but likely a time series forecasting metric such as:\n    *   RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error) for continuous volume predictions.\n    *   Possibly a custom metric aligning with business objectives (e.g., weighted accuracy for high-volume drugs).",
    "sections": {
      "Problem Description": "*   **Problem Type:** Time Series Forecasting\n*   **Objective:** Predict future prescription volumes for Pfizer Oncology products. The competition focuses on forecasting demand for cancer treatments, aiding in supply chain optimization and resource allocation.\n    *   **Key Points:**\n        *   Private, invitation-only competition (limited public details available).\n        *   Sponsored by Pfizer Oncology, suggesting a real-world pharmaceutical industry application.\n        *   Focus on oncology drugs, implying potential complexities like treatment regimens or disease-specific demand patterns.",
      "Dataset Overview": "*   **Data Type & Context:** Likely tabular time series data (though specifics are private), potentially including:\n    *   Historical prescription volumes.\n    *   Time-based features (e.g., dates, months, quarters).\n    *   Possible external factors (e.g., marketing campaigns, drug approvals).\n*   **Data Files:** Not publicly disclosed (private competition).\n*   **Features:** Undisclosed, but likely involve temporal patterns and potentially anonymized product/demographic identifiers.",
      "Evaluation Metrics": "*   **Evaluation Metric:** Undisclosed (private competition), but likely a time series forecasting metric such as:\n    *   RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error) for continuous volume predictions.\n    *   Possibly a custom metric aligning with business objectives (e.g., weighted accuracy for high-volume drugs)."
    },
    "file_path": "kaggle_datasets/61/problem_summary.md"
  },
  "95": {
    "problem_id": "95",
    "title": "Solar Energy Production Forecasting with Weather Models",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Solar Energy Production Forecasting with Weather Models\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting (Regression)\n* **Objective:** Predict daily total incoming solar energy (in Joules/m²) at 98 Oklahoma Mesonet sites using ensemble weather model data. The goal is to improve short-term solar energy production forecasts for utility planning.\n* **Key Points:**\n  * Uses an 11-member ensemble weather forecast system (GEFS Reforecast Version 2)\n  * Must handle spatial interpolation from weather model grid points to station locations\n  * Practical application for renewable energy grid integration\n  * Training period: 1994-2007, with separate test periods (2008-2009 public, more recent private)\n\n## Dataset Overview\n* **Data Type:** Multimodal (Spatio-temporal weather model outputs + station measurements)\n* **Primary Files:**\n  * `gefs_train.tar.gz/gefs_train.zip`: NetCDF4 files containing 15 weather variables across ensemble members\n  * `train.csv`: Daily solar energy measurements at 98 stations (1994-2007)\n  * `station_info.csv`: Latitude/longitude/elevation of Mesonet stations\n  * `gefs_elevations.nc`: Model terrain elevation data\n* **Key Features:**\n  * Weather variables include radiation fluxes, temperature, humidity, cloud cover, precipitation\n  * 3D spatio-temporal structure (latitude × longitude × forecast hour × ensemble member)\n  * Solar energy measured by ground pyranometers at 5-minute intervals\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE)\n* **Calculation:**\n  * Sum of absolute errors across all stations and days\n  * Normalized by total number of station-days (S×E)\n  * Formula: MAE = (1/SE) × Σ|Fₛₑ - Oₛₑ| where F=forecast, O=observation\n* **Key Properties:**\n  * Less sensitive to outliers than RMSE\n  * Standard metric in renewable energy forecasting\n  * Predictions required in Joules/m² units",
    "sections": {},
    "file_path": "kaggle_datasets/95/problem_summary.md"
  },
  "338": {
    "problem_id": "338",
    "title": "Multiclass Classification of Diabetic Retinopathy Severity from Retinal Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of Diabetic Retinopathy Severity from Retinal Images\n\n## Problem Description\n* **Problem Type:** Multiclass Classification (Medical Image Analysis)\n* **Objective:** Develop a machine learning model to classify the severity of diabetic retinopathy (DR) in retinal fundus images on a 5-point scale (0-4), where:\n    * 0 = No DR\n    * 1 = Mild\n    * 2 = Moderate\n    * 3 = Severe\n    * 4 = Proliferative DR\n* **Key Points:**\n    * Real-world clinical application for early blindness prevention in rural areas\n    * Images contain significant noise/variation (artifacts, focus issues, exposure differences)\n    * Data collected from multiple clinics with different cameras over time\n    * Kernels-only competition with strict submission requirements\n\n## Dataset Overview\n* **Data Type:** Retinal fundus photography images (PNG) with tabular labels\n* **Context:** Medical screening for diabetic retinopathy in rural Indian populations\n* **Data Files:**\n    * `train.csv` - image IDs and labeled diagnoses (0-4)\n    * `test.csv` - image IDs for prediction\n    * `sample_submission.csv` - submission format template\n    * `train.zip` - training set images (~3,660 images)\n    * `test.zip` - public test set images (~1,933 images)\n* **Features:**\n    * High-resolution color fundus photographs\n    * Labels represent clinician-assessed DR severity\n    * Significant real-world variation in image quality\n\n## Evaluation Metrics\n* **Primary Metric:** Quadratic Weighted Kappa (QWK)\n    * Measures agreement between human ratings and model predictions\n    * Range: 0 (random) to 1 (perfect agreement), can be negative\n* **Calculation Components:**\n    1. Construct N×N histogram matrix O of rating pairs (human vs predicted)\n    2. Compute weight matrix w based on score differences\n    3. Calculate expected rating matrix E assuming no correlation\n    4. Compare observed (O) and expected (E) matrices\n* **Submission Format:** CSV with `id_code` and predicted `diagnosis` (0-4) for each test image",
    "sections": {},
    "file_path": "kaggle_datasets/338/problem_summary.md"
  },
  "556": {
    "problem_id": "556",
    "title": "3D Scene Reconstruction from 2D Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 3D Scene Reconstruction from 2D Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - 3D Reconstruction (Structure from Motion)\n* **Objective**: Reconstruct accurate 3D scenes from unstructured collections of 2D images by estimating camera poses (rotation and translation) for each image.\n    * Key Points:\n        * Extends beyond two-view matching (from previous year's challenge) to multi-view reconstruction\n        * Aims to handle real-world challenges like varying viewpoints, lighting, and weather conditions\n        * Potential applications in mapping, photography, and cultural heritage preservation\n\n## Dataset Overview\n* **Data Type**: Image datasets with associated 3D reconstruction data\n* **Context**: Collections of images taken around various scenes, with some datasets created using specialized capture apps\n* **Data Files**:\n    * `train/*/*/images/` - Training images grouped by scene\n    * `train/*/*/sfm/` - 3D reconstruction files (COLMAP format)\n    * `train/train_labels.csv` - Ground truth poses (rotation matrices and translation vectors)\n    * `sample_submission.csv` - Submission format template\n* **Key Features**:\n    * Images vary from <10 to ~250 per scene\n    * Hidden test set contains ~1,100 images\n    * Each image requires estimation of a 3×3 rotation matrix and 3D translation vector\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Average Accuracy (mAA)\n    * Components:\n        1. Compute relative error for all possible image pairs (rotation in degrees, translation in meters)\n        2. Apply ten pairs of thresholds (e.g., 1°-10° rotation, 20cm-5m translation)\n        3. Calculate percentage of accurate samples at each threshold level\n        4. Average results across all thresholds (rewarding more accurate poses)\n        5. Compute mAA by averaging across scenes and datasets\n* **Submission Format**:\n    * CSV with image path, dataset/scene identifiers, and pose parameters\n    * Rotation matrix (9 values) and translation vector (3 values) as semicolon-separated strings",
    "sections": {},
    "file_path": "kaggle_datasets/556/problem_summary.md"
  },
  "300": {
    "problem_id": "300",
    "title": "20 Newsgroups Ciphertext Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 20 Newsgroups Ciphertext Challenge\n\n## Problem Description\n* **Problem Type:**  \n  *Multi-class Classification* (with encrypted text data)\n* **Objective:**  \n  Predict the originating newsgroup (1 of 20 classes) for ciphertext segments derived from the 20 Newsgroups dataset. The challenge involves decrypting or analyzing text encrypted with layered classical ciphers before classification.\n* **Key Points:**  \n  * Ciphertext is generated by applying **1–4 layered classical ciphers** (e.g., substitution, transposition) per document, with `difficulty` indicating the number/order of ciphers used.  \n  * Each document is split into **300-character chunks**, encrypted separately.  \n  * Participants may need to reverse-engineer encryption or develop models robust to ciphertext.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n  * **Text data** (encrypted segments from newsgroup posts).  \n  * Original dataset: ~20,000 documents evenly distributed across 20 newsgroups (unencrypted version available separately).  \n* **Data Files:**  \n  * `train.csv`: Contains `Id`, encrypted `ciphertext`, `difficulty` level, and `target` (newsgroup label).  \n  * `test.csv`: Contains `Id`, `ciphertext`, and `difficulty` (no labels).  \n  * `sample_submission.csv`: Submission template with `Id` and predicted label.  \n* **Key Features:**  \n  * `ciphertext`: Encrypted 300-character chunks (line breaks preserved).  \n  * `difficulty`: Integer (1–4) indicating cipher layers applied.  \n\n## Evaluation Metrics\n* **Primary Metric:**  \n  * **Macro F1-Score**: Average F1-score across all 20 classes, treating each class equally regardless of frequency.  \n* **Components:**  \n  * For each class:  \n    * Precision = TP / (TP + FP)  \n    * Recall = TP / (TP + FN)  \n    * F1 = 2 * (Precision * Recall) / (Precision + Recall)  \n  * Final score: Mean of per-class F1-scores.",
    "sections": {},
    "file_path": "kaggle_datasets/300/problem_summary.md"
  },
  "132": {
    "problem_id": "132",
    "title": "Seizure Prediction from Intracranial EEG Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Seizure Prediction from Intracranial EEG Data\n\n## Problem Description\n- **Problem Type:** Binary Classification (Preictal vs. Interictal EEG states)\n- **Objective:**  \n  - Develop an algorithm to distinguish between pre-seizure (preictal) and non-seizure (interictal) brain states using intracranial EEG recordings.  \n  - The goal is to enable seizure forecasting systems that could improve quality of life for epilepsy patients by providing warnings before seizures occur.\n- **Key Points:**\n  - Focus on differentiating preictal (1-hour before seizure with 5-minute buffer) from interictal states.\n  - Data includes both canine and human subjects with naturally occurring epilepsy.\n  - Special considerations:\n    - Only \"lead seizures\" (occurring ≥4 hours after another seizure) are included.\n    - Interictal segments are carefully selected to avoid contamination:\n      - Canine data: ≥1 week from any seizure\n      - Human data: ≥4 hours from any seizure (due to shorter monitoring periods).\n\n## Dataset Overview\n- **Data Type & Context:**  \n  - Time-series intracranial EEG recordings from dogs and humans with epilepsy.\n  - Data collected via ambulatory monitoring systems (dogs) and clinical intracranial monitoring (humans).\n- **Data Files:**\n  - Organized by subject (Dog_1, Patient_1, etc.) with separate folders for each.\n  - Three file types per subject:\n    - `preictal_segment_N.mat` (pre-seizure training data)\n    - `interictal_segment_N.mat` (non-seizure training data)\n    - `test_segment_N.mat` (testing data)\n- **Key Features:**\n  - Each .mat file contains:\n    - `data`: Matrix of EEG values (electrodes × time)\n    - `sampling_frequency`: 400Hz (dogs) or 5000Hz (humans)\n    - `channels`: Electrode names\n    - `sequence`: Temporal position within 1-hour clip series\n  - 10-minute data clips for both preictal and interictal states.\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n- **Submission Format:**\n  - CSV file with predicted probabilities for each test clip:\n    ```csv\n    clip,preictal\n    Dog_1_test_segment_0001",
    "sections": {},
    "file_path": "kaggle_datasets/132/problem_summary.md"
  },
  "59": {
    "problem_id": "59",
    "title": "Traveling Santa Problem with Disjoint Paths",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Traveling Santa Problem with Disjoint Paths\n\n## Problem Description\n* **Problem Type**: Combinatorial Optimization (Traveling Salesman Problem variant)\n* **Objective**: \n    * Find two disjoint paths through a set of cities (chimneys) that Santa must visit to deliver presents.\n    * The key constraint is that the paths must have completely disjoint edges - if one path contains edge A→B, the other cannot contain A→B or B→A.\n    * The score is determined by the longer of the two path distances.\n* **Key Points**:\n    * This is a modified TSP where Santa wants unpredictable routes each year\n    * Must provide two complete paths (not just one optimal path)\n    * Edge disjointness is strictly enforced (both directions count as same edge)\n    * Primary goal is to minimize the maximum of the two path lengths\n\n## Dataset Overview\n* **Data Type**: Tabular data with geographic coordinates\n* **Context**: Represents cities/chimneys Santa must visit\n* **Data Files**:\n    * `santa_cities.csv`: Contains city IDs and coordinates\n    * `random_paths_benchmark.csv`: Example submission file\n* **Features**:\n    * `id`: Unique identifier for each city\n    * `x`, `y`: Coordinate positions of each city\n    * Euclidean distance between points is used for path length calculations\n\n## Evaluation Metrics\n* **Primary Metric**: Maximum path length (longer of the two submitted paths)\n* **Scoring Details**:\n    * Each path's length is calculated as sum of Euclidean distances between consecutive cities\n    * The larger of the two path lengths becomes the final score\n    * Submissions violating the edge-disjointness constraint are rejected\n    * Ties are broken by submission time (earlier submissions win)",
    "sections": {},
    "file_path": "kaggle_datasets/59/problem_summary.md"
  },
  "569": {
    "problem_id": "569",
    "title": "Predicting CO2 Emissions in Rwanda Using Satellite Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting CO2 Emissions in Rwanda Using Satellite Data\n\n## Problem Description\n* **Problem Type:** Time Series Regression\n* **Objective:** Predict weekly CO2 emissions across 497 unique locations in Rwanda using satellite-derived atmospheric data. The goal is to enable accurate carbon emission monitoring in regions lacking ground-based systems.\n    * **Key Points:**\n        * Focus on Africa, where ground monitoring is limited.\n        * Uses open-source Sentinel-5P satellite data.\n        * Predictions cover weekly emissions for 2022 (test period) based on 2019-2021 training data.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular time series data of atmospheric measurements from satellite observations across Rwanda (farmlands, cities, power plants).\n* **Data Files:**\n    * `train.csv` (2019-2021 data)\n    * `test.csv` (2022 prediction targets)\n    * `sample_submission.csv`\n* **Key Features:**\n    * 7 main satellite-derived atmospheric features, each with sub-features (e.g., column density measurements):\n        * Sulphur Dioxide (SO₂)\n        * Carbon Monoxide (CO)\n        * Nitrogen Dioxide (NO₂)\n        * Formaldehyde (HCHO)\n        * UV Aerosol Index\n        * Ozone (O₃)\n        * Cloud cover\n    * Spatiotemporal identifiers: `ID_LAT_LON_YEAR_WEEK` combining location coordinates and timestamps.\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**  \n        RMSE = √(1/N Σ(yᵢ - ŷᵢ)²)  \n        Where:  \n        - yᵢ = true emission value  \n        - ŷᵢ = predicted value  \n        - N = total predictions",
    "sections": {},
    "file_path": "kaggle_datasets/569/problem_summary.md"
  },
  "92": {
    "problem_id": "92",
    "title": "Webpage Evergreen Classification Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Webpage Evergreen Classification Challenge\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Build a classifier to categorize webpages as either \"evergreen\" (timeless quality, relevant long-term) or \"ephemeral\" (short-term relevance). The goal is to improve content recommendation systems by predicting content longevity without human intervention.\n- **Key Points:**\n  - Focuses on web content quality and longevity prediction\n  - Uses both structured webpage metadata and raw content\n  - Successful models could be integrated into a real-world recommendation engine\n\n## Dataset Overview\n- **Data Type & Context:** Mixed dataset containing:\n  - Tabular data with webpage metadata and features\n  - Raw HTML content of webpages\n- **Data Files:**\n  - `train.tsv`: 7,395 labeled URLs (evergreen=1, non-evergreen=0)\n  - `test.tsv`: 3,171 unlabeled URLs for evaluation\n  - `raw_content.zip`: Raw HTML content for all URLs\n  - `sampleSubmission.csv`: Example submission file\n- **Key Features:**\n  - URL identifiers and boilerplate text (JSON format)\n  - AlchemyAPI categorization and scores\n  - Various webpage characteristics:\n    - Link statistics (avglinksize, commonLinkRatios)\n    - Content ratios (html_ratio, image_ratio)\n    - Structural features (frameBased, hasDomainLink)\n    - Text quality metrics (spelling_errors_ratio)\n    - News classification flags (is_news, news_front_page)\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n- **Metric Details:**\n  - Measures ranking quality of predictions (higher values indicate better separation between classes)\n  - Implementation available in multiple languages:\n    - MATLAB: `perfcurve` from stats toolbox\n    - R: `roc.area` from verification package\n    - Python: `metrics.roc_curve` and `metrics.auc` from scikit-learn\n  - Submissions can use real-valued predictions (only relative ranking matters)",
    "sections": {},
    "file_path": "kaggle_datasets/92/problem_summary.md"
  },
  "66": {
    "problem_id": "66",
    "title": "Predicting Parkinson's Disease Progression with Smartphone Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Parkinson's Disease Progression with Smartphone Data\n\n## Problem Description\n* **Problem Type:**  \n  - Binary Classification (PD patients vs. control subjects)  \n  - Regression/Time Series Analysis (quantifying symptom progression)  \n\n* **Objective:**  \n  - Develop a method to objectively measure Parkinson’s disease (PD) symptoms using passively collected smartphone sensor data.  \n  - Distinguish PD patients from healthy controls and/or quantify symptom severity/progression.  \n\n* **Key Points:**  \n  - Focus on **passive data collection** (no active user input required).  \n  - Emphasis on **clinical utility**: Solutions should inform treatment, care, or quality of life.  \n  - **Innovation encouraged**: Submissions can combine provided data with external sources or novel methodologies.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n  - **Time-series sensor data** from Android smartphones (6,000+ hours across 16 subjects: 9 PD patients, 7 controls).  \n  - Includes **multi-modal streams** (audio, accelerometry, compass, ambient light, proximity, battery, GPS).  \n\n* **Data Files:**  \n  - **Primary Data**:  \n    - `mjff_text_files.zip` (pre-processed CSV) or `mjff_binary_files.zip` (raw binary).  \n    - Supporting files: `HDL Data Documentation.pdf`, `Study Overview.pdf`, participant metadata.  \n  - **Clinical Labels**:  \n    - UPDRS (Unified Parkinson’s Disease Rating Scale) questionnaires for PD patients.  \n\n* **Key Features:**  \n  - **Audio**: Spectral norms, MFCCs.  \n  - **Accelerometry/Compass**: Axis-wise statistics (mean, deviation, spectral density).  \n  - **Contextual Sensors**: Ambient light, proximity, battery, GPS.  \n\n## Evaluation Metrics\n* **Evaluation Criteria:**  \n  Submissions were judged holistically based on:  \n  1. **Clinical Relevance**:  \n     - Ability to distinguish PD vs. controls.  \n     - Correlation with symptom progression/variability.  \n  2. **Innovation**:  \n     - Novelty of methods (e.g., fusion of sensor streams).  \n  3. **Potential Impact**:  \n     - Feasibility for real-world patient monitoring.  \n\n* **Submission Format:**  \n  - **Narrative report** (≤5 pages) + appendix (charts, code).",
    "sections": {},
    "file_path": "kaggle_datasets/66/problem_summary.md"
  },
  "336": {
    "problem_id": "336",
    "title": "Pneumothorax Segmentation in Chest X-rays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Pneumothorax Segmentation in Chest X-rays  \n\n## Problem Description  \n- **Problem Type:** Computer Vision - Medical Image Segmentation  \n- **Objective:** Develop a model to classify and segment pneumothorax (collapsed lung) in chest radiographic images. The goal is to:  \n  - Detect the presence/absence of pneumothorax.  \n  - If present, generate pixel-wise binary masks indicating the affected regions.  \n- **Key Points:**  \n  - The task involves both classification (detection) and segmentation (localization).  \n  - Annotations are provided as run-length encoded (RLE) masks for training images.  \n  - Some training images have multiple annotations (overlapping masks).  \n\n## Dataset Overview  \n- **Data Type & Context:** DICOM-format chest X-ray images with associated RLE-encoded segmentation masks.  \n- **Data Files:**  \n  - `stage_2_train.csv`: Contains image IDs and RLE-encoded masks (or `-1` if no pneumothorax).  \n  - `stage_2_images.zip`: Test set images in DICOM format.  \n  - `stage_2_sample_submission.csv`: Sample submission file with test IDs.  \n- **Features:**  \n  - Images are grayscale medical scans with varying resolutions.  \n  - Masks are binary (1 = pneumothorax pixel, 0 = background).  \n\n## Evaluation Metrics  \n- **Primary Metric:** Mean Dice coefficient (Sørensen–Dice score).  \n  - **Formula:** \\( \\frac{2 \\times |X \\cap Y|}{|X| + |Y|} \\), where:  \n    - \\( X \\) = predicted mask pixels.  \n    - \\( Y \\) = ground truth mask pixels.  \n    - Score is 1 if both masks are empty.  \n- **Submission Format:**  \n  - RLE-encoded masks with relative pixel offsets (e.g., `start_pixel run_length` pairs).  \n  - Each test image must have exactly one combined mask (merged if multiple predictions exist).",
    "sections": {},
    "file_path": "kaggle_datasets/336/problem_summary.md"
  },
  "104": {
    "problem_id": "104",
    "title": "Binary Image Classification of Dogs vs. Cats",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Image Classification of Dogs vs. Cats\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Image Recognition)\n* **Objective:** Develop an algorithm to automatically classify images as containing either a dog (label = 1) or a cat (label = 0). The task originates from breaking the Asirra CAPTCHA system, which was designed to be easy for humans but difficult for machines.\n* **Key Points:**\n  * Originally considered a challenging computer vision problem (experts estimated >60% accuracy would require major advances)\n  * Serves as a benchmark for state-of-the-art computer vision/deep learning approaches\n  * Focuses on real-world diversity in images (varying backgrounds, angles, lighting conditions)\n  * Strict prohibition against manual labeling of test data\n\n## Dataset Overview\n* **Data Type & Context:** RGB images of dogs and cats in diverse real-world settings (from Petfinder.com shelters)\n* **Data Files:**\n  * train.zip - 25,000 labeled training images\n  * test1.zip - Unlabeled test images for prediction\n  * sampleSubmission.csv - Example submission file with correct format\n* **Features:**\n  * Images contain varied poses, lighting conditions, and backgrounds\n  * No additional metadata provided (pure image classification task)\n\n## Evaluation Metrics\n* **Primary Metric:** Classification Accuracy (percentage of correctly labeled images)\n* **Scoring Notes:**\n  * Simple binary accuracy metric (no probabilistic scoring accepted)\n  * For CAPTCHA context: Final score raised to the 12th power to estimate HIP-breaking probability\n  * Emphasis on clean, interpretable evaluation despite potential limitations of accuracy metric",
    "sections": {},
    "file_path": "kaggle_datasets/104/problem_summary.md"
  },
  "560": {
    "problem_id": "560",
    "title": "Extracting Tabular Data from Scientific Graphs for Accessibility",
    "problem_type": "Computer Vision - Image-to-Structured Data Conversion",
    "objective": "Develop an ML model to extract structured tabular data from four types of scientific graphs (bar graphs, dot plots, line graphs, and scatter plots) to make STEM educational materials accessible to students with disabilities.",
    "evaluation_metric": null,
    "full_content": "# Extracting Tabular Data from Scientific Graphs for Accessibility\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Image-to-Structured Data Conversion\n* **Objective:** Develop an ML model to extract structured tabular data from four types of scientific graphs (bar graphs, dot plots, line graphs, and scatter plots) to make STEM educational materials accessible to students with disabilities.\n* **Key Points:**\n  * Must handle multiple chart types with different data conventions\n  * Requires accurate extraction of both categorical and numerical data\n  * Solution will be integrated into real-world accessibility product (PageAI)\n  * Special considerations for histograms (x-axis has one more value than y-axis)\n  * Must handle percentage values and shared origin labels in line graphs\n\n**Dataset Overview:**\n* **Data Type:** Image data (scientific graphs) with JSON annotations\n* **Context:** ~65,000 annotated scientific figures from STEM materials (mix of synthetic and professionally-produced sources)\n* **Data Files:**\n  * `train/images/` - Training set images\n  * `train/annotations/` - JSON files with detailed annotations\n  * `test/images/` - Test set images (hidden in actual competition)\n  * `sample_submission.csv` - Submission format example\n* **Key Features:**\n  * Images contain various scientific graph types\n  * JSON annotations include:\n    * Chart type identification\n    * Bounding boxes for plot elements\n    * Text elements with roles (titles, labels, etc.)\n    * Axis information (tick positions, value types)\n    * Ground truth data series\n\n**Evaluation Metrics:**\n* **Primary Metric:** Custom similarity score combining:\n  * Exact match check for chart type and number of values\n  * For numerical data: Normalized RMSE (NRMSE) with sigmoid transform\n  * For categorical data: Normalized Levenshtein distance (NLev) with sigmoid transform\n* **Scoring Components:**\n  1. Initial check for correct chart type and value count (0 score if mismatch)\n  2. Numerical series evaluated by: \n     * NRMSE = σ(RMSE(y,ŷ)/RMSE(y,ȳ))\n  3. Categorical series evaluated by:\n     * NLev = σ(∑Lev(yi,ŷi)/∑length(yi))\n  4. Final score is mean of all instance scores\n  *",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Image-to-Structured Data Conversion\n* **Objective:** Develop an ML model to extract structured tabular data from four types of scientific graphs (bar graphs, dot plots, line graphs, and scatter plots) to make STEM educational materials accessible to students with disabilities.\n* **Key Points:**\n  * Must handle multiple chart types with different data conventions\n  * Requires accurate extraction of both categorical and numerical data\n  * Solution will be integrated into real-world accessibility product (PageAI)\n  * Special considerations for histograms (x-axis has one more value than y-axis)\n  * Must handle percentage values and shared origin labels in line graphs",
      "Dataset Overview": "* **Data Type:** Image data (scientific graphs) with JSON annotations\n* **Context:** ~65,000 annotated scientific figures from STEM materials (mix of synthetic and professionally-produced sources)\n* **Data Files:**\n  * `train/images/` - Training set images\n  * `train/annotations/` - JSON files with detailed annotations\n  * `test/images/` - Test set images (hidden in actual competition)\n  * `sample_submission.csv` - Submission format example\n* **Key Features:**\n  * Images contain various scientific graph types\n  * JSON annotations include:\n    * Chart type identification\n    * Bounding boxes for plot elements\n    * Text elements with roles (titles, labels, etc.)\n    * Axis information (tick positions, value types)\n    * Ground truth data series",
      "Evaluation Metrics": "* **Primary Metric:** Custom similarity score combining:\n  * Exact match check for chart type and number of values\n  * For numerical data: Normalized RMSE (NRMSE) with sigmoid transform\n  * For categorical data: Normalized Levenshtein distance (NLev) with sigmoid transform\n* **Scoring Components:**\n  1. Initial check for correct chart type and value count (0 score if mismatch)\n  2. Numerical series evaluated by: \n     * NRMSE = σ(RMSE(y,ŷ)/RMSE(y,ȳ))\n  3. Categorical series evaluated by:\n     * NLev = σ(∑Lev(yi,ŷi)/∑length(yi))\n  4. Final score is mean of all instance scores\n  *"
    },
    "file_path": "kaggle_datasets/560/problem_summary.md"
  },
  "594": {
    "problem_id": "594",
    "title": "Solving Geometric Permutation Puzzles with Minimal Moves",
    "problem_type": "Optimization (Permutation Puzzle Solving)",
    "objective": "Solve twisty, cube-like puzzles (polytopes) in the fewest possible moves. The goal is to transform each puzzle's `initial_state` to its `solution_state` by applying a sequence of allowed moves or their inverses, while optionally utilizing wildcards for partial correctness.",
    "evaluation_metric": null,
    "full_content": "# Solving Geometric Permutation Puzzles with Minimal Moves\n\n**Problem Description:**\n* **Problem Type:** Optimization (Permutation Puzzle Solving)\n* **Objective:** Solve twisty, cube-like puzzles (polytopes) in the fewest possible moves. The goal is to transform each puzzle's `initial_state` to its `solution_state` by applying a sequence of allowed moves or their inverses, while optionally utilizing wildcards for partial correctness.\n* **Key Points:**\n  * Puzzles involve geometric shapes (e.g., hyper-dodecahedrons) with colored sides, analogous to Rubik's cubes but with varied structures.\n  * Solutions must account for `allowed_moves` (permutations) and their inverses, specified for each puzzle type.\n  * `num_wildcards` allows a limited number of mismatches in the final state, adding flexibility to solutions.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data describing permutation puzzles, their states, and allowed moves.\n* **Data Files:**\n  * `puzzle_info.csv`: Contains puzzle types and their `allowed_moves` (permutations in array form).\n  * `puzzles.csv`: Includes `id`, `puzzle_type`, `solution_state`, `initial_state`, and `num_wildcards` for each puzzle.\n  * `sample_submission.csv`: Example submission file with `id` and `moves` columns.\n* **Features:**\n  * `solution_state` and `initial_state`: Semicolon-delimited color arrangements representing puzzle configurations.\n  * `allowed_moves`: Permutations defining valid transformations for each puzzle type.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Total number of moves across all puzzle solutions. The goal is to minimize this sum.\n* **Components:**\n  * Solutions must transform `initial_state` to `solution_state` (or within `num_wildcards` mismatches).\n  * Moves must be from `allowed_moves` or their inverses (denoted with a `-` prefix).\n  * Invalid solutions (incorrect transformations) are disqualified.",
    "sections": {
      "Problem Description": "* **Problem Type:** Optimization (Permutation Puzzle Solving)\n* **Objective:** Solve twisty, cube-like puzzles (polytopes) in the fewest possible moves. The goal is to transform each puzzle's `initial_state` to its `solution_state` by applying a sequence of allowed moves or their inverses, while optionally utilizing wildcards for partial correctness.\n* **Key Points:**\n  * Puzzles involve geometric shapes (e.g., hyper-dodecahedrons) with colored sides, analogous to Rubik's cubes but with varied structures.\n  * Solutions must account for `allowed_moves` (permutations) and their inverses, specified for each puzzle type.\n  * `num_wildcards` allows a limited number of mismatches in the final state, adding flexibility to solutions.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data describing permutation puzzles, their states, and allowed moves.\n* **Data Files:**\n  * `puzzle_info.csv`: Contains puzzle types and their `allowed_moves` (permutations in array form).\n  * `puzzles.csv`: Includes `id`, `puzzle_type`, `solution_state`, `initial_state`, and `num_wildcards` for each puzzle.\n  * `sample_submission.csv`: Example submission file with `id` and `moves` columns.\n* **Features:**\n  * `solution_state` and `initial_state`: Semicolon-delimited color arrangements representing puzzle configurations.\n  * `allowed_moves`: Permutations defining valid transformations for each puzzle type.",
      "Evaluation Metrics": "* **Primary Metric:** Total number of moves across all puzzle solutions. The goal is to minimize this sum.\n* **Components:**\n  * Solutions must transform `initial_state` to `solution_state` (or within `num_wildcards` mismatches).\n  * Moves must be from `allowed_moves` or their inverses (denoted with a `-` prefix).\n  * Invalid solutions (incorrect transformations) are disqualified."
    },
    "file_path": "kaggle_datasets/594/problem_summary.md"
  },
  "309": {
    "problem_id": "309",
    "title": "Histopathologic Cancer Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Histopathologic Cancer Detection\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Medical Image Analysis)\n* **Objective:**  \n    * Predict whether the center 32x32px region of a histopathologic image patch contains metastatic tumor tissue.  \n    * The task mimics clinical metastasis detection in lymph node scans, framed as a binary image classification problem.  \n* **Key Points:**  \n    * Focus on the **center region** of each patch (outer regions do not influence labels).  \n    * Dataset is derived from the PatchCamelyon (PCam) benchmark, optimized for ML research (no duplicates in Kaggle version).  \n    * Designed for accessibility: Models can be trained on a single GPU within hours.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * High-resolution histopathology images (TIFF format) of lymph node tissue patches.  \n    * Simplified version of PCam dataset, curated for binary classification.  \n* **Data Files:**  \n    * `train/`: Folder containing training images.  \n    * `test/`: Folder containing test images.  \n    * `train_labels.csv`: Maps training image IDs to binary labels (0 = no tumor, 1 = tumor).  \n    * `sample_submission.csv`: Submission template with test image IDs.  \n* **Key Features:**  \n    * Each image is a small patch from larger pathology scans.  \n    * Labels are binary and determined by tumor presence in the **center 32x32px area**.  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n    * Measures the model’s ability to distinguish between tumor-positive and tumor-negative patches.  \n    * Submissions require predicted probabilities for the positive class (tumor present).  \n* **Submission Format:**  \n    * CSV file with `id` (image filename) and `label` (predicted probability of tumor).",
    "sections": {},
    "file_path": "kaggle_datasets/309/problem_summary.md"
  },
  "50": {
    "problem_id": "50",
    "title": "Predicting Mobile Visitor Product Interest from Search Behavior",
    "problem_type": "Recommendation System / Multi-class Classification",
    "objective": "Predict which BestBuy product (SKU) a mobile web visitor will click on based on their search query or browsing behavior over a 2-year period.",
    "evaluation_metric": null,
    "full_content": "# Predicting Mobile Visitor Product Interest from Search Behavior\n\n**Problem Description:**\n* **Problem Type:** Recommendation System / Multi-class Classification\n* **Objective:** Predict which BestBuy product (SKU) a mobile web visitor will click on based on their search query or browsing behavior over a 2-year period.\n    * **Key Points:**\n        * Focuses on modeling user-product interactions from mobile web behavior.\n        * Involves handling large-scale data (7GB) with 1.8 million clicks from 1.2 million users.\n        * Participants can choose between a cloud-scale problem or a smaller PC-scale version.\n        * The relationship between search queries and resulting clicks isn't guaranteed to be direct (clicks may not always come from search results).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing user interaction logs from BestBuy's mobile website.\n* **Key Data Files:**\n    * `train.csv`: Contains historical user clicks with timestamps, search queries, and product information.\n    * `test.csv`: Contains user queries for which predictions need to be made (without the clicked SKU).\n    * `product_data.tar.gz`: Additional product metadata including categories and reviews.\n* **Important Features:**\n    * User IDs, product SKUs, product categories\n    * Search queries (text data)\n    * Timestamps for both queries and clicks (with max 5-minute difference)\n    * Product metadata from supplementary files\n\n**Evaluation Metrics:**\n* **Primary Metric:** MAP@5 (Mean Average Precision at 5)\n    * **Components:**\n        * For each user query, the system predicts up to 5 relevant products (SKUs).\n        * Precision is calculated at each position in the ranked list of recommendations.\n        * Average Precision is computed for each query by averaging these precision values.\n        * The final score is the mean of Average Precision values across all test queries.\n    * Focuses on both the relevance and ranking order of recommendations.",
    "sections": {
      "Problem Description": "* **Problem Type:** Recommendation System / Multi-class Classification\n* **Objective:** Predict which BestBuy product (SKU) a mobile web visitor will click on based on their search query or browsing behavior over a 2-year period.\n    * **Key Points:**\n        * Focuses on modeling user-product interactions from mobile web behavior.\n        * Involves handling large-scale data (7GB) with 1.8 million clicks from 1.2 million users.\n        * Participants can choose between a cloud-scale problem or a smaller PC-scale version.\n        * The relationship between search queries and resulting clicks isn't guaranteed to be direct (clicks may not always come from search results).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing user interaction logs from BestBuy's mobile website.\n* **Key Data Files:**\n    * `train.csv`: Contains historical user clicks with timestamps, search queries, and product information.\n    * `test.csv`: Contains user queries for which predictions need to be made (without the clicked SKU).\n    * `product_data.tar.gz`: Additional product metadata including categories and reviews.\n* **Important Features:**\n    * User IDs, product SKUs, product categories\n    * Search queries (text data)\n    * Timestamps for both queries and clicks (with max 5-minute difference)\n    * Product metadata from supplementary files",
      "Evaluation Metrics": "* **Primary Metric:** MAP@5 (Mean Average Precision at 5)\n    * **Components:**\n        * For each user query, the system predicts up to 5 relevant products (SKUs).\n        * Precision is calculated at each position in the ranked list of recommendations.\n        * Average Precision is computed for each query by averaging these precision values.\n        * The final score is the mean of Average Precision values across all test queries.\n    * Focuses on both the relevance and ranking order of recommendations."
    },
    "file_path": "kaggle_datasets/50/problem_summary.md"
  },
  "68": {
    "problem_id": "68",
    "title": "Predicting Hospital Admissions Using Historical Claims Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Hospital Admissions Using Historical Claims Data\n\n## Problem Description\n* **Problem Type:** Regression (with elements of time series forecasting)\n* **Objective:** Predict the number of days a patient will spend in a hospital (including inpatient admissions and emergency room visits) within the next year using historical healthcare claims data. The goal is to enable early intervention strategies to prevent unnecessary hospitalizations.\n* **Key Points:**\n  * Focus on reducing unnecessary hospital admissions (estimated $30 billion annual cost in the US)\n  * Uses multi-year patient history to forecast future hospitalization needs\n  * Privacy-protected data with suppressed/extreme values (e.g., hospital stays >2 weeks grouped as 15 days)\n  * Long-term competition (2-year duration) with significant real-world healthcare implications\n\n## Dataset Overview\n* **Data Type:** Tabular healthcare claims data with temporal components\n* **Context:** De-identified member data collected over 48 months from Heritage Provider Network\n* **Data Files:**\n  * Members Table (demographics)\n  * Claims Table (detailed service records)\n  * Labs Table (test results)\n  * Rx Table (prescriptions)\n  * DaysInHospital Tables (Y2/Y3 for training)\n  * Target Table (Y4 for prediction)\n* **Key Features:**\n  * Member demographics (age, sex)\n  * Claim details (provider, specialty, place of service, diagnosis codes, procedures)\n  * Temporal features (year of claim, days since first service)\n  * Medical indicators (Charlson comorbidity index)\n  * Lab tests and prescription records\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n  * Formula: sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))^2)\n  * Where:\n    * p_i = predicted days in hospital for member i\n    * a_i = actual days in hospital for member i\n    * n = total number of members\n* **Key Aspects:**\n  * Uses natural logarithm to mitigate impact of extreme values\n  * Predictions evaluated to 6 decimal places\n  * Accuracy Threshold of 0.4 set as grand prize qualification benchmark\n  * Long-term evaluation with periodic milestone assessments",
    "sections": {},
    "file_path": "kaggle_datasets/68/problem_summary.md"
  },
  "593": {
    "problem_id": "593",
    "title": "Binary Classification of AI-Generated vs. Student-Written Essays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of AI-Generated vs. Student-Written Essays\n\n## Problem Description\n* **Problem Type:** Binary Classification (NLP - Text Classification)\n* **Objective:**  \n  Develop a machine learning model to distinguish between essays written by middle/high school students and those generated by large language models (LLMs). The goal is to advance detection techniques for real-world educational scenarios where LLM-generated plagiarism is a concern.\n* **Key Points:**\n  * Focus on moderate-length essays across diverse subjects/prompts\n  * Generalization across multiple unknown LLM sources (not just one model)\n  * Educational context: Essays are responses to specific reading prompts with source text references\n\n## Dataset Overview\n* **Data Type & Context:**  \n  Text data (essays) with metadata about writing prompts. Contains:\n  * Student-written essays (majority in training set)\n  * LLM-generated essays (minority in training set, balanced in test set)\n* **Data Files:**\n  * `train_essays.csv` - Labeled training essays (student/LLM)\n  * `test_essays.csv` - Unlabeled test essays (dummy data replaced during scoring)\n  * `train_prompts.csv` - Source texts and instructions for each essay prompt\n  * `sample_submission.csv` - Submission template\n* **Key Features:**\n  * Essay text content (`text` field)\n  * Prompt metadata (instructions, source texts with paragraph numbering)\n  * Prompt IDs linking essays to their source material\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n* **Efficiency Prize Metric (Optional):**  \n  Combined score considering both AUC and runtime:\n  ```\n  Efficiency = (AUC_Benchmark - max_AUC) + (Runtime_Seconds / 32400)\n  ```\n  Where:\n  * `AUC_Benchmark` = Performance of sample submission\n  * `max_AUC` = Best AUC on private leaderboard\n  * Runtime measured in seconds (CPU-only for efficiency track)",
    "sections": {},
    "file_path": "kaggle_datasets/593/problem_summary.md"
  },
  "567": {
    "problem_id": "567",
    "title": "Identifying Contrails in Satellite Images to Mitigate Global Warming",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Identifying Contrails in Satellite Images to Mitigate Global Warming\n\n## Problem Description\n* **Problem Type**: Computer Vision - Binary Segmentation  \n* **Objective**: Train machine learning models to accurately identify contrails (condensation trails from aircraft) in geostationary satellite imagery. The goal is to help researchers validate climate models and enable airlines to avoid contrail formation, thereby reducing their contribution to global warming.  \n* **Key Points**:  \n  * Contrails contribute ~1% of human-caused global warming by trapping heat.  \n  * Models must segment contrails from temporal sequences of infrared satellite images.  \n  * Focus on empirical validation of existing climate models through pixel-wise detection.  \n\n## Dataset Overview  \n* **Data Type**: Multispectral satellite image sequences (time series) with binary segmentation masks  \n* **Context**: NOAA GOES-16 Advanced Baseline Imager (ABI) data reprojected to local scenes  \n* **Data Files**:  \n  * `train/`, `validation/`, `test/` folders containing per-record:  \n    * `band_{08-16}.npy`: H × W × T arrays of brightness temperatures (8 infrared bands)  \n    * `human_pixel_masks.npy`: Aggregated binary ground truth masks  \n    * `human_individual_masks.npy`: (Train only) Per-labeler annotations  \n  * JSON metadata with timestamps/projection parameters  \n* **Key Features**:  \n  * 4 pre-label and 3 post-label frames per sequence (10-min intervals)  \n  * 8 spectral bands (Band 08-16) at different infrared wavelengths  \n  * Masks require >50% annotator agreement (minimum 10-pixel contrails)  \n\n## Evaluation Metrics  \n* **Primary Metric**: Global Dice Coefficient (Sørensen–Dice index)  \n  * Formula: 2∗|𝑋∩𝑌| / (|𝑋|+|𝑌|)  \n  * Where:  \n    * X = All predicted contrail pixels in test set  \n    * Y = All ground truth contrail pixels in test set  \n* **Submission Format**: Run-length encoded binary masks:  \n  * Space-delimited pairs (start_position, run_length)  \n  * Empty predictions marked with '-'  \n  * Pixels numbered top-to-bottom, then left-to-right",
    "sections": {},
    "file_path": "kaggle_datasets/567/problem_summary.md"
  },
  "103": {
    "problem_id": "103",
    "title": "3D Bin Packing Optimization for Santa's Sleigh",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 3D Bin Packing Optimization for Santa's Sleigh\n\n## Problem Description\n* **Problem Type**: 3D Bin Packing Optimization with Order Constraints\n* **Objective**: \n    * Pack 1 million presents into Santa's sleigh as compactly as possible while maintaining optimal delivery order.\n    * The sleigh has fixed dimensions (1000×1000 units) with infinite vertical space.\n    * Presents must be packed parallel to x-y-z axes (90° rotations allowed).\n    * Present delivery order is determined by PresentId (should ideally be delivered in ascending order).\n* **Key Points**:\n    * Presents can be rotated in any 90° increment (24 possible orientations).\n    * Solution must specify exact coordinates for all 8 vertices of each present.\n    * The challenge combines spatial optimization with sequencing constraints.\n\n## Dataset Overview\n* **Data Type**: Tabular data containing present dimensions and IDs\n* **Data Files**:\n    * `presents.csv`/`presents.mat`: Contains 1M presents with:\n        * PresentId (delivery order)\n        * Dimension1, Dimension2, Dimension3 (integer length units)\n* **Features**:\n    * All presents are rectangular prisms with varying dimensions\n    * Coordinate system uses integer multiples of fundamental length unit (ℓ)\n    * Sleigh grid coordinates range from (1,1,1) to (1000,1000,∞)\n\n## Evaluation Metrics\n* **Primary Metric**: Custom compound score (M) combining:\n    * Compactness: `2 × max(z_i)` (double the height of tallest present)\n    * Ordering: `σ(Γ) = Σ|i - Γ_i|` (sum of absolute deviations from ideal order)\n* **Metric Calculation**:\n    1. For each z-layer (height level):\n        * Extract all presents intersecting that layer\n        * Sort presents by PresentId to get best possible order for that layer\n    2. Concatenate all layers' orders to form Γ\n    3. Calculate σ(Γ) as sum of position deviations\n    4. Final score: `M = 2×max_height + σ(Γ)`\n* **Ideal Case**: \n    * Perfect order (Γ = 1,2,3,...,N) → σ(Γ) = 0\n    * Minimum height → lowest possible M",
    "sections": {},
    "file_path": "kaggle_datasets/103/problem_summary.md"
  },
  "331": {
    "problem_id": "331",
    "title": "Toxicity Classification with Unintended Bias Mitigation",
    "problem_type": "Binary Classification (with bias mitigation focus)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Toxicity Classification with Unintended Bias Mitigation\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with bias mitigation focus)\n* **Objective:**  \n    * Build a model to detect toxic comments (defined as rude, disrespectful, or likely to make someone leave a discussion)  \n    * **Key Challenge:** Minimize unintended model bias against frequently attacked identity groups (e.g., incorrectly flagging non-toxic comments mentioning \"gay\" as toxic due to training data imbalances).\n    * **Key Points:**  \n        * Focus on fairness: Models must perform well across diverse identity subgroups.  \n        * Toxicity subtypes (e.g., threats, insults) are provided but not required for prediction.  \n        * Dataset contains explicit content (profanity, offensive language).  \n\n**Dataset Overview:**\n* **Data Type:** Text data (online comments from Civil Comments platform)  \n* **Context:** Comments labeled for toxicity and identity mentions (2015-2017, English-language news sites).  \n* **Key Files:**  \n    * `train.csv`: Comment text (`comment_text`), toxicity labels (`target`), identity labels (e.g., `female`, `muslim`), and metadata.  \n    * `test.csv`: Comment text only (no labels).  \n    * Post-competition additions: Expanded test sets with labels (`test_public_expanded.csv`, `test_private_expanded.csv`) and individual rater annotations.  \n* **Important Features:**  \n    * Toxicity labels are fractional (0.0–1.0), representing rater consensus.  \n    * Identity labels indicate mentions of specific groups (e.g., `homosexual_gay_or_lesbian`, `black`).  \n    * Metadata includes timestamps, user ratings (e.g., \"likes\"), and annotator counts.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom composite score balancing:  \n    * **Overall AUC** (ROC-AUC for full test set).  \n    * **Bias AUCs** (3 per identity subgroup):  \n        * **Subgroup AUC:** AUC for comments mentioning the identity.  \n        * **BPSN AUC:** AUC for non-toxic identity mentions vs. toxic non-identity comments.  \n        * **BNSP AUC:** AUC for toxic identity mentions vs. non-toxic non-identity comments.  \n    * **Generalized Mean of Bias AUCs** (power",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with bias mitigation focus)\n* **Objective:**  \n    * Build a model to detect toxic comments (defined as rude, disrespectful, or likely to make someone leave a discussion)  \n    * **Key Challenge:** Minimize unintended model bias against frequently attacked identity groups (e.g., incorrectly flagging non-toxic comments mentioning \"gay\" as toxic due to training data imbalances).\n    * **Key Points:**  \n        * Focus on fairness: Models must perform well across diverse identity subgroups.  \n        * Toxicity subtypes (e.g., threats, insults) are provided but not required for prediction.  \n        * Dataset contains explicit content (profanity, offensive language).",
      "Dataset Overview": "* **Data Type:** Text data (online comments from Civil Comments platform)  \n* **Context:** Comments labeled for toxicity and identity mentions (2015-2017, English-language news sites).  \n* **Key Files:**  \n    * `train.csv`: Comment text (`comment_text`), toxicity labels (`target`), identity labels (e.g., `female`, `muslim`), and metadata.  \n    * `test.csv`: Comment text only (no labels).  \n    * Post-competition additions: Expanded test sets with labels (`test_public_expanded.csv`, `test_private_expanded.csv`) and individual rater annotations.  \n* **Important Features:**  \n    * Toxicity labels are fractional (0.0–1.0), representing rater consensus.  \n    * Identity labels indicate mentions of specific groups (e.g., `homosexual_gay_or_lesbian`, `black`).  \n    * Metadata includes timestamps, user ratings (e.g., \"likes\"), and annotator counts.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom composite score balancing:  \n    * **Overall AUC** (ROC-AUC for full test set).  \n    * **Bias AUCs** (3 per identity subgroup):  \n        * **Subgroup AUC:** AUC for comments mentioning the identity.  \n        * **BPSN AUC:** AUC for non-toxic identity mentions vs. toxic non-identity comments.  \n        * **BNSP AUC:** AUC for toxic identity mentions vs. non-toxic non-identity comments.  \n    * **Generalized Mean of Bias AUCs** (power"
    },
    "file_path": "kaggle_datasets/331/problem_summary.md"
  },
  "57": {
    "problem_id": "57",
    "title": "Predicting Customer Retention in Insurance Policies",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Customer Retention in Insurance Policies\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether current insurance customers will remain with the company for an entire 6-month policy term based on their characteristics.\n    * Key Points:\n        * Focuses on customer retention risk assessment for an insurance company\n        * Predicts binary outcome: customer stays (1) or leaves (0) within 6 months\n        * Time-based evaluation split: training data (2008-2010) vs test data (2011)\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing insurance policy records with customer characteristics\n    * **Data Files:**\n        * Dataset #1: One record per policy term (simplified format)\n        * Dataset #2: Multiple records per policy term (detailed change history)\n        * Rolled-up versions provided as: train_rolled_up, train_rolled_up_sample, test_rolled_up\n    * **Features:**\n        * Policy characteristics (duration, changes, etc.)\n        * Customer interaction history\n        * Temporal information about policy changes\n\n## Evaluation Metrics\n* **Primary Metric:** Bernoulli log likelihood (Log Loss)\n    * **Calculation:**\n        * y*log(P) + (1-y)*log(1-P)\n        * Where:\n            * y = actual binary response (0 or 1)\n            * P = predicted probability of retention\n        * Summed across all observations\n        * Higher values indicate better performance",
    "sections": {},
    "file_path": "kaggle_datasets/57/problem_summary.md"
  },
  "558": {
    "problem_id": "558",
    "title": "Ink Detection in 3D X-ray Scans of Ancient Papyrus Scrolls",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Ink Detection in 3D X-ray Scans of Ancient Papyrus Scrolls\n\n## Problem Description\n* **Problem Type**: Image Segmentation (Binary Classification)\n* **Objective**: Detect ink presence in 3D X-ray scans of carbonized papyrus fragments from ancient Roman scrolls buried by the Vesuvius eruption. The goal is to create binary masks identifying ink locations to enable reading the scroll contents without physical unwrapping.\n* **Key Points**:\n  * Focuses on a sub-problem of the larger Vesuvius Challenge to digitally \"unwrap\" scrolls\n  * Uses aligned 3D X-ray scans and infrared photographs with ground truth ink labels\n  * Ink is not readily visible in raw X-ray scans, requiring ML detection\n  * Success could enable reading other unopened scrolls from the same library\n\n## Dataset Overview\n* **Data Type**: 3D volumetric imaging data (X-ray scans) + 2D infrared images\n* **Context**: Scans of four detached fragments from Herculaneum scrolls (carbonized by volcanic eruption)\n* **Data Files**:\n  * `[train|test]/[fragment_id]/surface_volume/[image_id].tif` - 65 Z-slice TIFFs per fragment (4µm resolution)\n  * `[train|test]/[fragment_id]/mask.png` - Binary data mask\n  * `train/[fragment_id]/inklabels.png` - Ground truth ink masks\n  * `train/[fragment_id]/ir.png` - Infrared reference photos\n  * `inklabels_rle.csv` - Run-length encoded labels\n* **Key Features**:\n  * High-resolution 3D X-ray scans (width × height × 65 slices)\n  * Aligned infrared images showing visible ink patterns\n  * Binary masks indicating valid data regions\n\n## Evaluation Metrics\n* **Primary Metric**: Modified Sørensen-Dice coefficient (F0.5 score)\n  * Weighted to prioritize precision over recall: \n    ```\n    (1+β²)pr/(β²p+r) where β=0.5\n    p = precision (tp/(tp+fp))\n    r = recall (tp/(tp+fn))\n    ```\n  * Encourages coherent character formation from detected ink\n* **Submission Format**: Run-length encoded binary masks\n  * Space-delimited pairs of start position and run length",
    "sections": {},
    "file_path": "kaggle_datasets/558/problem_summary.md"
  },
  "168": {
    "problem_id": "168",
    "title": "Coupon Purchase Recommendation for E-Commerce",
    "problem_type": "Recommender System (Multi-label Ranking)",
    "objective": "Predict which coupons a customer will purchase during a specific time period to improve recommendation systems on an e-commerce platform.",
    "evaluation_metric": null,
    "full_content": "# Coupon Purchase Recommendation for E-Commerce\n\n**Problem Description:**\n* **Problem Type:** Recommender System (Multi-label Ranking)\n* **Objective:** Predict which coupons a customer will purchase during a specific time period to improve recommendation systems on an e-commerce platform.\n    * Key Points:\n        * Focuses on generating ranked lists of coupons for each user\n        * Uses historical browsing and purchase behavior\n        * Limited to predicting purchases from a predefined set of 310 test coupons\n        * Aims to model user preferences based on demographic and transactional data\n\n**Dataset Overview:**\n* **Data Type & Context:** Relational tabular data from a Japanese coupon site (ponpare.jp), containing:\n    * User demographics\n    * Coupon attributes and validity periods\n    * Browsing and purchase histories\n    * Geographic information\n* **Key Files:**\n    * `user_list.csv` - User profiles (demographics, registration dates)\n    * `coupon_list_train/test.csv` - Coupon details (discounts, categories, validity)\n    * `coupon_visit_train.csv` - Browsing logs\n    * `coupon_detail_train.csv` - Purchase records\n    * `coupon_area_train/test.csv` - Coupon geographic availability\n* **Notable Features:**\n    * Temporal features (registration dates, coupon validity periods)\n    * User demographics (age, gender, location)\n    * Coupon characteristics (discount rates, categories, usable days)\n    * Behavioral data (browsing patterns with purchase flags)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision at 10 (MAP@10)\n    * Components:\n        * For each user, calculates precision at each position in the top-10 recommended coupons\n        * Averages these precision values across all positions\n        * Final score is the mean of these averages across all users\n        * Special cases:\n            * If user purchased 0 coupons: contributes 0 to the average\n            * Only considers up to min(10, actual_purchases) positions",
    "sections": {
      "Problem Description": "* **Problem Type:** Recommender System (Multi-label Ranking)\n* **Objective:** Predict which coupons a customer will purchase during a specific time period to improve recommendation systems on an e-commerce platform.\n    * Key Points:\n        * Focuses on generating ranked lists of coupons for each user\n        * Uses historical browsing and purchase behavior\n        * Limited to predicting purchases from a predefined set of 310 test coupons\n        * Aims to model user preferences based on demographic and transactional data",
      "Dataset Overview": "* **Data Type & Context:** Relational tabular data from a Japanese coupon site (ponpare.jp), containing:\n    * User demographics\n    * Coupon attributes and validity periods\n    * Browsing and purchase histories\n    * Geographic information\n* **Key Files:**\n    * `user_list.csv` - User profiles (demographics, registration dates)\n    * `coupon_list_train/test.csv` - Coupon details (discounts, categories, validity)\n    * `coupon_visit_train.csv` - Browsing logs\n    * `coupon_detail_train.csv` - Purchase records\n    * `coupon_area_train/test.csv` - Coupon geographic availability\n* **Notable Features:**\n    * Temporal features (registration dates, coupon validity periods)\n    * User demographics (age, gender, location)\n    * Coupon characteristics (discount rates, categories, usable days)\n    * Behavioral data (browsing patterns with purchase flags)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision at 10 (MAP@10)\n    * Components:\n        * For each user, calculates precision at each position in the top-10 recommended coupons\n        * Averages these precision values across all positions\n        * Final score is the mean of these averages across all users\n        * Special cases:\n            * If user purchased 0 coupons: contributes 0 to the average\n            * Only considers up to min(10, actual_purchases) positions"
    },
    "file_path": "kaggle_datasets/168/problem_summary.md"
  },
  "391": {
    "problem_id": "391",
    "title": "ALASKA2 Image Steganalysis",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# ALASKA2 Image Steganalysis\n\n## Problem Description\n* **Problem Type:** Binary Classification (Image Steganalysis)\n* **Objective:** Detect the presence of hidden messages (steganography) within digital images. Participants must develop a robust model to distinguish between:\n    * \"Cover\" images (unaltered)\n    * Images with hidden data using one of three steganography algorithms (JMiPOD, JUNIWARD, UERD)\n* **Key Points:**\n    * Focus on **low false-alarm rates** to avoid misleading law enforcement investigations.\n    * Dataset mimics real-world conditions with:\n        * Images from **50+ different cameras** (smartphones to high-end devices).\n        * Varied JPEG compression quality factors (95, 90, or 75).\n        * Adaptive payload lengths (shorter messages in smooth images, longer in textured ones).\n\n## Dataset Overview\n* **Data Type:** JPEG images (digital photos) with/without hidden steganographic data.\n* **Context:** Simulates forensic investigation scenarios for law enforcement.\n* **Data Files:**\n    * `Cover/`: 75,000 unaltered training images.\n    * `JMiPOD/`, `JUNIWARD/`, `UERD/`: 75,000 training images each, processed with respective steganography algorithms.\n    * `Test/`: 5,000 test images (mixed cover/stego).\n    * `sample_submission.csv`: Submission template.\n* **Features:**\n    * Images vary in device source, content texture, and compression.\n    * **No payload length metadata** provided (only average: 0.4 bits per non-zero AC DCT coefficient).\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Area Under the Curve (AUC)  \n* **Weighting Scheme:**\n    * ROC curve regions are weighted to prioritize low false-positive rates:\n        * **TPR [0.0, 0.4]:** 2× weight (emphasizes low-FPR performance).\n        * **TPR [0.4, 1.0]:** 1× weight.\n    * Final score normalized to [0, 1].  \n* **Rationale:** Prioritizes models that minimize false alarms (critical for law enforcement applications).",
    "sections": {},
    "file_path": "kaggle_datasets/391/problem_summary.md"
  },
  "533": {
    "problem_id": "533",
    "title": "Ordinal Regression for Wine Quality Prediction",
    "problem_type": "Ordinal Regression (Predicting ordered categorical outcomes)",
    "objective": "Predict the quality rating of wines based on their physicochemical properties. The target variable (`quality`) is an ordinal integer representing wine quality levels.",
    "evaluation_metric": null,
    "full_content": "# Ordinal Regression for Wine Quality Prediction\n\n**Problem Description:**\n* **Problem Type:** Ordinal Regression (Predicting ordered categorical outcomes)\n* **Objective:** Predict the quality rating of wines based on their physicochemical properties. The target variable (`quality`) is an ordinal integer representing wine quality levels.\n* **Key Points:**\n  * The task focuses on predicting ordered categories rather than continuous values or unordered classes.\n  * Dataset is synthetically generated from the original Wine Quality dataset to preserve real-world characteristics while preventing label leakage.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing physicochemical measurements of wines (e.g., acidity, pH, alcohol content) with associated quality ratings.\n* **Data Files:**\n  * `train.csv`: Contains features and target (`quality`) for model training\n  * `test.csv`: Contains features for making predictions\n  * `sample_submission.csv`: Demonstrates submission format with `Id` and predicted `quality`\n* **Features:** Multiple numerical features describing wine composition (exact features not specified, but similar to original dataset's acidity, sugar, pH, alcohol measurements).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Quadratic Weighted Kappa\n  * Measures agreement between predicted and actual ordinal ratings\n  * Ranges from 0 (random agreement) to 1 (perfect agreement), can be negative for worse-than-random performance\n* **Calculation Components:**\n  1. Constructs NxN histogram matrix O of actual vs predicted counts\n  2. Computes weight matrix w where wᵢⱼ = (i-j)²/(N-1)²\n  3. Creates expected matrix E as outer product of actual/predicted histograms\n  4. Final score: κ = 1 - (∑wᵢⱼOᵢⱼ)/(∑wᵢⱼEᵢ�)",
    "sections": {
      "Problem Description": "* **Problem Type:** Ordinal Regression (Predicting ordered categorical outcomes)\n* **Objective:** Predict the quality rating of wines based on their physicochemical properties. The target variable (`quality`) is an ordinal integer representing wine quality levels.\n* **Key Points:**\n  * The task focuses on predicting ordered categories rather than continuous values or unordered classes.\n  * Dataset is synthetically generated from the original Wine Quality dataset to preserve real-world characteristics while preventing label leakage.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing physicochemical measurements of wines (e.g., acidity, pH, alcohol content) with associated quality ratings.\n* **Data Files:**\n  * `train.csv`: Contains features and target (`quality`) for model training\n  * `test.csv`: Contains features for making predictions\n  * `sample_submission.csv`: Demonstrates submission format with `Id` and predicted `quality`\n* **Features:** Multiple numerical features describing wine composition (exact features not specified, but similar to original dataset's acidity, sugar, pH, alcohol measurements).",
      "Evaluation Metrics": "* **Primary Metric:** Quadratic Weighted Kappa\n  * Measures agreement between predicted and actual ordinal ratings\n  * Ranges from 0 (random agreement) to 1 (perfect agreement), can be negative for worse-than-random performance\n* **Calculation Components:**\n  1. Constructs NxN histogram matrix O of actual vs predicted counts\n  2. Computes weight matrix w where wᵢⱼ = (i-j)²/(N-1)²\n  3. Creates expected matrix E as outer product of actual/predicted histograms\n  4. Final score: κ = 1 - (∑wᵢⱼOᵢⱼ)/(∑wᵢⱼEᵢ�)"
    },
    "file_path": "kaggle_datasets/533/problem_summary.md"
  },
  "365": {
    "problem_id": "365",
    "title": "NCAA Women's Basketball Tournament Outcome Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NCAA Women's Basketball Tournament Outcome Prediction\n\n## Problem Description\n* **Problem Type**: Binary Classification (with probabilistic outputs)\n* **Objective**: Predict the probability of one team defeating another in all possible matchups during the NCAA Division I Women's Basketball Tournament (March Madness). The competition has two stages:\n  * Stage 1: Historical model building using past tournament data (2015-2019 seasons)\n  * Stage 2: Forecasting outcomes for all possible matchups in the 2020 tournament\n* **Key Points**:\n  * Predictions must cover all possible pairwise matchups (64 teams → 2,016 predictions per season)\n  * Team1 vs Team2 is treated identically to Team2 vs Team1 (only lower TeamID vs higher TeamID pairs are submitted)\n  * Model evaluation occurs on historical data first, then on actual 2020 tournament results\n\n## Dataset Overview\n* **Data Type**: Tabular data with extensive basketball statistics and game outcomes\n* **Context**: NCAA Division I Women's Basketball games (regular season and tournament) from 1998-2020\n* **Key Data Files**:\n  * Core files:\n    * `WTeams.csv` - Team IDs and names\n    * `WSeasons.csv` - Season metadata\n    * `WNCAATourneySeeds.csv` - Tournament seeds\n    * `WRegularSeasonCompactResults.csv` - Game outcomes (1998-2019)\n    * `WNCAATourneyCompactResults.csv` - Tournament outcomes (1998-2019)\n  * Advanced files:\n    * Detailed results with team box scores (2010-2019)\n    * Play-by-play data (2015-2019)\n    * Game location data\n    * Conference affiliations\n* **Important Features**:\n  * Team identifiers and historical performance\n  * Game outcomes (scores, locations, overtime periods)\n  * Team statistics (field goals, rebounds, assists, turnovers, etc.)\n  * Tournament seeding information\n  * Play-by-play event logs (recent seasons)\n\n## Evaluation Metrics\n* **Primary Metric**: Logarithmic Loss (LogLoss)\n* **Calculation**:\n  ```math\n  LogLoss = −1/n ∑[y_i log(ŷ_i) + (1−y_i) log(1−ŷ_i)]\n  ```\n  Where:\n  * n = number of games played\n  *",
    "sections": {},
    "file_path": "kaggle_datasets/365/problem_summary.md"
  },
  "157": {
    "problem_id": "157",
    "title": "Taxi Trip Time Prediction Based on Partial Trajectories",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Taxi Trip Time Prediction Based on Partial Trajectories\n\n## Problem Description\n* **Problem Type**: Regression (Time Series Forecasting)\n* **Objective**: Predict the total travel time of taxi trips in Porto, Portugal using only their initial partial trajectories. The goal is to improve taxi dispatch efficiency by estimating when drivers will become available.\n* **Key Points**:\n  * Uses GPS trajectory data sampled every 15 seconds\n  * Must handle trips with missing GPS data (MISSING_DATA flag)\n  * Predicts complete trip duration when only partial trajectory is available\n  * Companion competition predicts final destination (separate task)\n\n## Dataset Overview\n* **Data Type**: Tabular data with GPS trajectory strings\n* **Context**: 1 year of taxi trip data (442 taxis) from Porto, Portugal\n* **Data Files**:\n  * train.csv (complete trips with 9 features)\n  * test.csv (partial trips for prediction)\n  * sampleSubmission.csv\n  * metaData_taxistandsID_name_GPSlocation.csv\n* **Key Features**:\n  * TRIP_ID: Unique trip identifier\n  * CALL_TYPE: Dispatch method (A=central, B=stand, C=street)\n  * POLYLINE: GPS coordinates (WGS84) as string (15-second intervals)\n  * TIMESTAMP: Trip start time (Unix seconds)\n  * MISSING_DATA: Flag for incomplete GPS streams\n  * TAXI_ID: Driver identifier\n  * DAYTYPE: Day classification (normal, pre-holiday, holiday)\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Logarithmic Error (RMSLE)\n* **Calculation**:\n  * RMSLE = √(1/n Σ(log(p_i + 1) - log(a_i + 1))²)\n  * Where:\n    * n = number of test trips\n    * p_i = predicted travel time (seconds)\n    * a_i = actual travel time (seconds)\n    * log = natural logarithm\n* **Key Characteristics**:\n  * Penalizes underestimates more than overestimates\n  * Logarithmic scale reduces impact of large errors\n  * +1 terms handle zero values",
    "sections": {},
    "file_path": "kaggle_datasets/157/problem_summary.md"
  },
  "150": {
    "problem_id": "150",
    "title": "Predicting Sales of Weather-Sensitive Products During Extreme Weather Events",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Sales of Weather-Sensitive Products During Extreme Weather Events\n\n## Problem Description\n* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the sales of 111 weather-sensitive products (e.g., umbrellas, bread, milk) at 45 Walmart stores during and around major weather events (defined as days with >1 inch of rain or >2 inches of snow). The goal is to forecast sales for a ±3-day window surrounding each storm.\n* **Key Points:**\n  * Focus on weather-sensitive product sales during extreme weather conditions.\n  * Predictions are required for specific event windows, not continuous time series.\n  * Sales data does not distinguish between stockouts and zero demand.\n  * Perfect weather forecasts are provided (no need to predict weather).\n\n## Dataset Overview\n* **Data Type:** Tabular data (sales and weather records)\n* **Context:** Sales data for 111 products across 45 Walmart stores, paired with weather station data for 20 locations.\n* **Data Files:**\n  * `train.csv`: Historical sales data (store_nbr, item_nbr, date, units).\n  * `test.csv`: Store-item-date combinations requiring sales predictions (encrypted).\n  * `weather.csv`: NOAA weather data per station and date.\n  * `key.csv`: Mapping between stores and weather stations.\n  * `sampleSubmission.csv`: Submission format example.\n* **Important Features:**\n  * Sales data: store_nbr, item_nbr, date, units sold.\n  * Weather data: Precipitation, snowfall, and other NOAA metrics.\n  * Composite ID: store_nbr_item_nbr_date for submissions.\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n* **Calculation:**\n  * RMSLE = √(1/n Σ(log(p_i + 1) - log(a_i + 1))²)\n  * Where:\n    * n = number of test rows\n    * p_i = predicted units sold\n    * a_i = actual units sold\n    * log = natural logarithm\n* **Key Properties:**\n  * Penalizes underestimates more than overestimates.\n  * Logarithmic transformation reduces impact of large errors.",
    "sections": {},
    "file_path": "kaggle_datasets/150/problem_summary.md"
  },
  "362": {
    "problem_id": "362",
    "title": "Multi-label Classification for Subjective Q&A Quality Scoring",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification for Subjective Q&A Quality Scoring\n\n## Problem Description\n* **Problem Type:** Multi-label Classification (with continuous-valued targets)\n* **Objective:** Predict 30 subjective quality scores for question-answer pairs, where each score represents a different dimension of quality (e.g., question clarity, answer helpfulness). The goal is to automate the assessment of complex, subjective aspects of Q&A content that typically require human judgment.\n* **Key Points:**\n  * Targets are not binary but continuous values in the range [0,1], representing aggregated human ratings\n  * Focuses on subjective aspects of Q&A that go beyond factual correctness\n  * Data collected from ~70 different websites with minimal rater guidance to maximize \"common-sense\" judgments\n  * Intended to advance NLP systems' ability to handle nuanced, human-like Q&A interactions\n\n## Dataset Overview\n* **Data Type:** Text data (question-answer pairs) with associated metadata\n* **Context:** Questions and answers from various StackExchange properties, covering diverse topics\n* **Data Files:**\n  * `train.csv` - Contains question-answer pairs with 30 target labels\n  * `test.csv` - Question-answer pairs for which predictions must be made\n  * `sample_submission.csv` - Example submission format with all 30 target columns\n* **Features:**\n  * Text features: `question_title`, `question_body`, `answer`\n  * 30 target labels (e.g., `question_asker_intent_understanding`, `answer_well_written`)\n  * Some questions appear multiple times with different answers in training data\n\n## Evaluation Metrics\n* **Evaluation Metric:** Mean column-wise Spearman's rank correlation coefficient\n* **Components:**\n  * Spearman's correlation is computed separately for each of the 30 target columns\n  * The final score is the mean of these 30 correlation values\n  * Measures how well the predicted rankings match the true rankings across all dimensions\n  * Particularly suitable for evaluating ordinal relationships in continuous predictions",
    "sections": {},
    "file_path": "kaggle_datasets/362/problem_summary.md"
  },
  "534": {
    "problem_id": "534",
    "title": "Regression with a Tabular Paris Housing Price Dataset",
    "problem_type": "Regression",
    "objective": "Predict housing prices in Paris based on various features. The goal is to build a model that accurately estimates the `price` of houses using the provided tabular dataset.",
    "evaluation_metric": null,
    "full_content": "# Regression with a Tabular Paris Housing Price Dataset\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict housing prices in Paris based on various features. The goal is to build a model that accurately estimates the `price` of houses using the provided tabular dataset.\n* **Key Points:**\n  * Dataset is synthetically generated from a deep learning model trained on real-world Paris housing data.\n  * Participants are encouraged to explore differences between synthetic and original data, and potentially incorporate the original dataset to improve performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing housing-related features for properties in Paris.\n* **Data Files:**\n  * `train.csv`: Training dataset with the target variable `price`.\n  * `test.csv`: Test dataset without the target variable.\n  * `sample_submission.csv`: Example submission file showing the required format.\n* **Features:** The dataset contains 37 columns (features) related to housing characteristics (specific features not listed in description, but implied to be housing-related attributes).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Root Mean Squared Error (RMSE)\n* **Components:**\n  * RMSE is calculated as the square root of the average of squared differences between predicted and actual values.\n  * Formula: \n    ```\n    RMSE = sqrt(1/N * sum((y_i - y_hat_i)^2))\n    ```\n    where `y_i` is the actual value, `y_hat_i` is the predicted value, and `N` is the number of observations.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict housing prices in Paris based on various features. The goal is to build a model that accurately estimates the `price` of houses using the provided tabular dataset.\n* **Key Points:**\n  * Dataset is synthetically generated from a deep learning model trained on real-world Paris housing data.\n  * Participants are encouraged to explore differences between synthetic and original data, and potentially incorporate the original dataset to improve performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing housing-related features for properties in Paris.\n* **Data Files:**\n  * `train.csv`: Training dataset with the target variable `price`.\n  * `test.csv`: Test dataset without the target variable.\n  * `sample_submission.csv`: Example submission file showing the required format.\n* **Features:** The dataset contains 37 columns (features) related to housing characteristics (specific features not listed in description, but implied to be housing-related attributes).",
      "Evaluation Metrics": "* **Evaluation Metric:** Root Mean Squared Error (RMSE)\n* **Components:**\n  * RMSE is calculated as the square root of the average of squared differences between predicted and actual values.\n  * Formula: \n    ```\n    RMSE = sqrt(1/N * sum((y_i - y_hat_i)^2))\n    ```\n    where `y_i` is the actual value, `y_hat_i` is the predicted value, and `N` is the number of observations."
    },
    "file_path": "kaggle_datasets/534/problem_summary.md"
  },
  "396": {
    "problem_id": "396",
    "title": "SIIM-ISIC Melanoma Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# SIIM-ISIC Melanoma Classification\n\n## Problem Description\n* **Problem Type**: Binary Classification (Medical Image Analysis)\n* **Objective**:  \n  * Predict the probability that a skin lesion image contains malignant melanoma (versus benign)  \n  * Key clinical context: Models should leverage \"patient-level contextual information\" (e.g., comparing multiple lesions from the same patient to identify outliers) to mimic dermatologists' \"ugly duckling\" diagnostic approach.\n* **Key Points**:  \n  * Early detection of melanoma is critical, as it accounts for 75% of skin cancer deaths despite being less common.  \n  * Special competition prizes were awarded for best models using/not using patient-level context (e.g., `patient_id` clustering).  \n\n## Dataset Overview\n* **Data Type**:  \n  * Medical images (dermoscopic skin lesion photos) in DICOM, JPEG, and TFRecord formats (1024x102px resized)  \n  * Tabular metadata with patient demographics and lesion characteristics  \n* **Data Files**:  \n  * `train.csv` (image metadata + binary labels)  \n  * `test.csv` (image metadata for prediction)  \n  * `sample_submission.csv` (prediction template)  \n* **Key Features**:  \n  * Image-level: DICOM files with pixel data and metadata  \n  * Tabular: `patient_id`, `sex`, `age_approx`, `anatom_site_general_challenge` (lesion location), `diagnosis` (train only), `benign_malignant` (malignancy indicator)  \n\n## Evaluation Metrics\n* **Primary Metric**: Area Under the ROC Curve (AUC)  \n  * Measures model performance in ranking malignant lesions higher than benign ones across probability thresholds.  \n  * Interpretation: Higher AUC (closer to 1.0) indicates better discrimination between classes.",
    "sections": {},
    "file_path": "kaggle_datasets/396/problem_summary.md"
  },
  "159": {
    "problem_id": "159",
    "title": "Predicting Search Results Relevance for eCommerce",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Search Results Relevance for eCommerce\n\n## Problem Description\n- **Problem Type:** Multi-class Classification (Ordinal)\n- **Objective:** Predict the relevance score (1-4) of eCommerce search results based on query-product description pairs, where:\n  - 4 = Completely satisfies the search query\n  - 1 = Doesn't match the search term\n- **Key Points:**\n  - Goal is to create an open-source model for evaluating search algorithm performance\n  - Targets small businesses needing to improve search experiences\n  - Data contains raw HTML snippets in product descriptions (includes noisy/irrelevant info)\n  - Test set contains unlabeled data to discourage hand-labeling attempts\n\n## Dataset Overview\n- **Data Type:** Text data (search queries + product descriptions with HTML formatting)\n- **Context:** eCommerce search results from multiple websites, human-rated for relevance\n- **Data Files:**\n  - train.csv (contains labeled relevance scores)\n  - test.csv\n  - sampleSubmission.csv\n- **Key Features:**\n  - `query`: Search term used\n  - `product_description`: Raw HTML-formatted product descriptions\n  - `median_relevance`: Target variable (1-4, median of 3 human ratings)\n  - `relevance_variance`: Variance among human raters\n\n## Evaluation Metrics\n- **Primary Metric:** Quadratic Weighted Kappa\n- **Metric Components:**\n  - Measures agreement between human ratings (Rater A) and predicted scores (Rater B)\n  - Range: 0 (random agreement) to 1 (perfect agreement), can be negative\n  - Calculation process:\n    1. Construct N×N histogram matrix O of rating pairs\n    2. Compute weight matrix w where wᵢⱼ = (i-j)²/(N-1)²\n    3. Create expected ratings matrix E (assuming no correlation)\n    4. Final score: κ = 1 - (∑wᵢⱼOᵢⱼ)/(∑wᵢⱼEᵢⱼ)",
    "sections": {},
    "file_path": "kaggle_datasets/159/problem_summary.md"
  },
  "32": {
    "problem_id": "32",
    "title": "Predicting US Corporate Bond Trade Prices",
    "problem_type": "Regression (Time Series Forecasting)",
    "objective": "Predict the next trade price of a US corporate bond using historical trade data and bond characteristics.",
    "evaluation_metric": null,
    "full_content": "# Predicting US Corporate Bond Trade Prices\n\n**Problem Description:**\n* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the next trade price of a US corporate bond using historical trade data and bond characteristics.\n    * **Key Points:**\n        * Focuses on trading dynamics and microstructure of individual bonds.\n        * Participants are given a reference price (`curve_based_price`) as an intermediate calculation from Benchmark Solutions.\n        * Time series reconstruction is possible for training data (using `bond_id`), but test data is designed to prevent this.\n\n**Dataset Overview:**\n* **Data Type:** Tabular time series data of US corporate bond trades.\n* **Data Files:**\n    * `train.csv` (contains `bond_id` for time series reconstruction)\n    * `test.csv`\n    * `random_forest_sample_submission.csv` (example submission format)\n* **Key Features:**\n    * Bond characteristics: `current_coupon`, `time_to_maturity`, `is_callable`\n    * Trade details: `trade_size`, `trade_type`, `reporting_delay`\n    * Reference price: `curve_based_price`\n    * Historical trade data: Last 10 trades' prices, sizes, types, and time differences (`trade_price_last{1-10}`, `trade_size_last{1-10}`, etc.)\n    * Evaluation weight: `weight` column (square root of time since last observation, scaled to mean=1)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Weighted Mean Absolute Error (wMAE)\n    * **Components:**\n        * Each observation's error is weighted by the `weight` column\n        * Weights represent square root of time since last trade (scaled to mean=1)\n        * Emphasizes accuracy for more recent trades",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the next trade price of a US corporate bond using historical trade data and bond characteristics.\n    * **Key Points:**\n        * Focuses on trading dynamics and microstructure of individual bonds.\n        * Participants are given a reference price (`curve_based_price`) as an intermediate calculation from Benchmark Solutions.\n        * Time series reconstruction is possible for training data (using `bond_id`), but test data is designed to prevent this.",
      "Dataset Overview": "* **Data Type:** Tabular time series data of US corporate bond trades.\n* **Data Files:**\n    * `train.csv` (contains `bond_id` for time series reconstruction)\n    * `test.csv`\n    * `random_forest_sample_submission.csv` (example submission format)\n* **Key Features:**\n    * Bond characteristics: `current_coupon`, `time_to_maturity`, `is_callable`\n    * Trade details: `trade_size`, `trade_type`, `reporting_delay`\n    * Reference price: `curve_based_price`\n    * Historical trade data: Last 10 trades' prices, sizes, types, and time differences (`trade_price_last{1-10}`, `trade_size_last{1-10}`, etc.)\n    * Evaluation weight: `weight` column (square root of time since last observation, scaled to mean=1)",
      "Evaluation Metrics": "* **Primary Metric:** Weighted Mean Absolute Error (wMAE)\n    * **Components:**\n        * Each observation's error is weighted by the `weight` column\n        * Weights represent square root of time since last trade (scaled to mean=1)\n        * Emphasizes accuracy for more recent trades"
    },
    "file_path": "kaggle_datasets/32/problem_summary.md"
  },
  "502": {
    "problem_id": "502",
    "title": "Smartphone Decimeter-Level GNSS Positioning Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Smartphone Decimeter-Level GNSS Positioning Challenge\n\n## Problem Description\n- **Problem Type**: Geospatial Regression / Signal Processing\n- **Objective**: Improve smartphone GNSS positioning accuracy from meter-level (3-5m) to decimeter/centimeter-level precision using raw sensor data. The goal is to enable lane-level navigation and other high-precision location-based services.\n- **Key Points**:\n  - Focuses on post-processing applications (future route data available for predictions)\n  - Uses only opensky and light urban road environments (no deep urban data)\n  - Requires handling multi-modal sensor fusion (GNSS, IMU, etc.)\n  - Builds upon previous year's competition with expanded dataset\n\n## Dataset Overview\n- **Data Type**: Multi-modal time-series sensor data:\n  - Raw GNSS measurements (pseudoranges, carrier phase, satellite ephemeris)\n  - IMU data (accelerometer, gyroscope, magnetometer readings)\n  - Ground truth positions from reference GNSS receivers\n- **Data Files**:\n  - `device_gnss.csv`: Raw GNSS measurements with derived values\n  - `device_imu.csv`: IMU sensor readings\n  - `ground_truth.csv`: Reference positions\n  - Supplemental files in RINEX/NMEA formats\n- **Key Features**:\n  - Satellite signal measurements (CN0, pseudorange, Doppler)\n  - Receiver clock states and biases\n  - Satellite positions/velocities in ECEF coordinates\n  - IMU triaxial measurements\n  - Millisecond-precision timestamps\n\n## Evaluation Metrics\n- **Primary Metric**: Mean of 50th and 95th percentile distance errors\n- **Calculation Process**:\n  1. Compute horizontal distance errors (meters) between predictions and ground truth\n  2. For each phone and second:\n     - Calculate 50th percentile (median) error\n     - Calculate 95th percentile error\n  3. Average these two percentiles per phone\n  4. Final score: Mean of averaged values across all test phones\n- **Key Aspects**:\n  - Evaluates both typical performance (50th %ile) and outlier handling (95th %ile)\n  - Requires precise timestamp alignment\n  - Uses WGS84 coordinate system for distance calculations",
    "sections": {},
    "file_path": "kaggle_datasets/502/problem_summary.md"
  },
  "166": {
    "problem_id": "166",
    "title": "Predicting Industrial Tube Assembly Prices",
    "problem_type": "Regression",
    "objective": "Predict the quoted price of industrial tube assemblies based on their specifications and manufacturing details.",
    "evaluation_metric": null,
    "full_content": "# Predicting Industrial Tube Assembly Prices\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the quoted price of industrial tube assemblies based on their specifications and manufacturing details.\n    * Key Points:\n        * Tubes vary across multiple dimensions (materials, bends, bolt patterns, etc.)\n        * Must account for different manufacturers' unique pricing models\n        * Pricing predictions must be accurate across various tube configurations\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing industrial tube specifications and pricing information\n    * **Data Files:** (Dataset removed, but likely included):\n        * Training set with tube features and actual prices\n        * Test set with tube features (without prices)\n        * Sample submission file\n    * **Features:** Likely included:\n        * Base materials\n        * Number of bends\n        * Bend radius\n        * Bolt patterns\n        * End types\n        * Manufacturer information\n        * Annual volume data\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * **Calculation:**\n        * Computes the squared differences between log-transformed predictions and actual values\n        * Formula: √(1/n Σ(log(p_i+1) - log(a_i+1))²)\n        * Where:\n            * n = number of price quotes\n            * p_i = predicted price\n            * a_i = actual price\n            * log = natural logarithm\n    * Benefits:\n        * Penalizes underestimates more than overestimates\n        * Handles large price ranges effectively",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the quoted price of industrial tube assemblies based on their specifications and manufacturing details.\n    * Key Points:\n        * Tubes vary across multiple dimensions (materials, bends, bolt patterns, etc.)\n        * Must account for different manufacturers' unique pricing models\n        * Pricing predictions must be accurate across various tube configurations",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing industrial tube specifications and pricing information\n    * **Data Files:** (Dataset removed, but likely included):\n        * Training set with tube features and actual prices\n        * Test set with tube features (without prices)\n        * Sample submission file\n    * **Features:** Likely included:\n        * Base materials\n        * Number of bends\n        * Bend radius\n        * Bolt patterns\n        * End types\n        * Manufacturer information\n        * Annual volume data",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * **Calculation:**\n        * Computes the squared differences between log-transformed predictions and actual values\n        * Formula: √(1/n Σ(log(p_i+1) - log(a_i+1))²)\n        * Where:\n            * n = number of price quotes\n            * p_i = predicted price\n            * a_i = actual price\n            * log = natural logarithm\n    * Benefits:\n        * Penalizes underestimates more than overestimates\n        * Handles large price ranges effectively"
    },
    "file_path": "kaggle_datasets/166/problem_summary.md"
  },
  "354": {
    "problem_id": "354",
    "title": "Energy Consumption Prediction for Buildings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Energy Consumption Prediction for Buildings\n\n## Problem Description\n* **Problem Type**: Regression (Time Series Forecasting)\n* **Objective**: Develop accurate models to predict building energy usage across four meter types (electricity, chilled water, steam, hot water) based on historical usage and weather data. The goal is to create counterfactual models that estimate what energy consumption would have been without efficiency improvements.\n* **Key Points**:\n  * Focus on scaling energy efficiency estimation methods across different building types and meter types\n  * Address challenges in modeling real-world measurement errors in energy data\n  * Support pay-for-performance financing by providing reliable savings estimates\n\n## Dataset Overview\n* **Data Type**: Tabular time-series data with building metadata and weather data\n* **Context**: Three years of hourly meter readings from 1,000+ buildings across multiple global sites\n* **Data Files**:\n  * `train.csv`: Hourly meter readings (building_id, meter type, timestamp, meter_reading)\n  * `building_meta.csv`: Building characteristics (site_id, primary_use, square_feet, year_built, floor_count)\n  * `weather_[train/test].csv`: Meteorological data (temperature, cloud coverage, precipitation, etc.)\n  * `test.csv`: Submission format with row_id, building_id, meter, timestamp\n* **Key Features**:\n  * Temporal features (hourly timestamps)\n  * Building characteristics (size, age, primary use)\n  * Weather conditions at each site\n  * Four meter types with different measurement units\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Logarithmic Error (RMSLE)\n* **Metric Components**:\n  * Calculated as: √(1/n Σ(log(p_i + 1) - log(a_i + 1))²)\n  * Where:\n    * p_i = predicted energy consumption\n    * a_i = actual energy consumption\n    * n = number of observations\n  * Uses natural logarithm to normalize scale across different meter types\n  * Penalizes underestimates more than overestimates",
    "sections": {},
    "file_path": "kaggle_datasets/354/problem_summary.md"
  },
  "192": {
    "problem_id": "192",
    "title": "Predicting Customer Satisfaction for Santander Bank",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Customer Satisfaction for Santander Bank\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a customer is satisfied (0) or dissatisfied (1) with their banking experience based on anonymized features. The goal is to identify dissatisfied customers early to enable proactive retention measures.\n    * **Key Points:**\n        * Focus on early detection of dissatisfaction before customers churn\n        * Uses anonymized customer data with hundreds of features\n        * Predictive task framed as a probability estimation problem\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing anonymized numerical features representing customer attributes and banking behavior\n* **Data Files:**\n    * `train.csv`: Training data including the binary TARGET column (0=satisfied, 1=dissatisfied)\n    * `test.csv`: Test data without the TARGET column\n    * `sample_submission.csv`: Example submission file showing required format\n* **Features:**\n    * 743 total columns (including target)\n    * All features anonymized and numerical\n    * Primary prediction target: TARGET column (binary satisfaction indicator)\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between satisfied/dissatisfied customers\n        * Evaluates predicted probabilities against actual binary outcomes\n        * Higher AUC indicates better discrimination performance\n        * Metric ranges from 0.5 (random) to 1.0 (perfect separation)",
    "sections": {},
    "file_path": "kaggle_datasets/192/problem_summary.md"
  },
  "398": {
    "problem_id": "398",
    "title": "Global Wheat Detection via Image Analysis",
    "problem_type": "Computer Vision - Object Detection (Bounding Box Prediction)",
    "objective": "Detect wheat heads in outdoor field images by predicting bounding boxes around each wheat head. The goal is to create a generalized solution that works across different wheat varieties, growing conditions, and global regions.",
    "evaluation_metric": null,
    "full_content": "# Global Wheat Detection via Image Analysis\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Detection (Bounding Box Prediction)\n* **Objective:** Detect wheat heads in outdoor field images by predicting bounding boxes around each wheat head. The goal is to create a generalized solution that works across different wheat varieties, growing conditions, and global regions.\n    * **Key Points:**\n        * Must handle challenging visual conditions: overlapping plants, wind-blurred images, and varying appearances due to maturity, color, genotype, and orientation.\n        * Must generalize across different global growing environments (training data from Europe/North America, test data from Australia/Japan/China).\n        * Some images may contain no wheat heads (requiring no predictions).\n\n**Dataset Overview:**\n* **Data Type & Context:** RGB images of wheat fields with annotated bounding boxes for wheat heads. Images were collected from multiple global locations under varying field conditions.\n* **Data Files:**\n    * `train.csv` (contains image IDs, dimensions, and bounding box coordinates)\n    * `train.zip` (training images)\n    * `test.zip` (test images)\n    * `sample_submission.csv` (example submission format)\n* **Key Features:**\n    * Each bounding box is formatted as `[xmin, ymin, width, height]`\n    * Images vary in resolution (width/height provided for each)\n    * Dataset includes >3,000 training images (Europe/North America) and ~1,000 test images (Australia/Japan/China)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision (mAP) across Intersection over Union (IoU) thresholds from 0.5 to 0.75 (step size 0.05)\n    * **Components:**\n        * IoU calculation: `Area of Overlap / Area of Union` between predicted and ground truth boxes\n        * At each threshold (t):\n            * True Positive (TP): Predicted box matches ground truth with IoU > t\n            * False Positive (FP): Predicted box has no matching ground truth\n            * False Negative (FN): Ground truth has no matching prediction\n        * Precision at threshold t: `TP(t) / [TP(t) + FP(t) + FN(t)]`\n        * Final score: Mean of average precisions across all test images\n    * **Special Cases:**\n        * Images with no",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Detection (Bounding Box Prediction)\n* **Objective:** Detect wheat heads in outdoor field images by predicting bounding boxes around each wheat head. The goal is to create a generalized solution that works across different wheat varieties, growing conditions, and global regions.\n    * **Key Points:**\n        * Must handle challenging visual conditions: overlapping plants, wind-blurred images, and varying appearances due to maturity, color, genotype, and orientation.\n        * Must generalize across different global growing environments (training data from Europe/North America, test data from Australia/Japan/China).\n        * Some images may contain no wheat heads (requiring no predictions).",
      "Dataset Overview": "* **Data Type & Context:** RGB images of wheat fields with annotated bounding boxes for wheat heads. Images were collected from multiple global locations under varying field conditions.\n* **Data Files:**\n    * `train.csv` (contains image IDs, dimensions, and bounding box coordinates)\n    * `train.zip` (training images)\n    * `test.zip` (test images)\n    * `sample_submission.csv` (example submission format)\n* **Key Features:**\n    * Each bounding box is formatted as `[xmin, ymin, width, height]`\n    * Images vary in resolution (width/height provided for each)\n    * Dataset includes >3,000 training images (Europe/North America) and ~1,000 test images (Australia/Japan/China)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision (mAP) across Intersection over Union (IoU) thresholds from 0.5 to 0.75 (step size 0.05)\n    * **Components:**\n        * IoU calculation: `Area of Overlap / Area of Union` between predicted and ground truth boxes\n        * At each threshold (t):\n            * True Positive (TP): Predicted box matches ground truth with IoU > t\n            * False Positive (FP): Predicted box has no matching ground truth\n            * False Negative (FN): Ground truth has no matching prediction\n        * Precision at threshold t: `TP(t) / [TP(t) + FP(t) + FN(t)]`\n        * Final score: Mean of average precisions across all test images\n    * **Special Cases:**\n        * Images with no"
    },
    "file_path": "kaggle_datasets/398/problem_summary.md"
  },
  "35": {
    "problem_id": "35",
    "title": "Predicting User Follow Recommendations in Tencent Weibo",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting User Follow Recommendations in Tencent Weibo\n\n## Problem Description\n* **Problem Type:** Binary Classification (with Ranking Focus)\n* **Objective:** Predict whether a user will follow a recommended item (person, organization, or group) in Tencent Weibo's social network. The task involves ranking recommended items by likelihood of being followed.\n* **Key Points:**\n  * Focuses on reducing information overload by improving recommendation quality\n  * Items belong to hierarchical categories (e.g., \"science-and-technology.internet.mobile\")\n  * Must handle implicit rejections (when users ignore recommendations)\n  * Privacy-protected data with anonymized user/item IDs and encoded Chinese content\n\n## Dataset Overview\n* **Data Type:** Tabular data with social network interactions, user profiles, and item metadata\n* **Data Files:**\n  * `rec_log_train.txt`: User-item interactions with follow/reject labels (1/-1)\n  * `rec_log_test.txt`: User-item interactions without labels\n  * `user_profile.txt`: Demographic data and user tags\n  * `item.txt`: Item categories and keywords\n  * `user_action.txt`: User interaction statistics (@mentions, retweets, comments)\n  * `user_sns.txt`: User follow history (social graph)\n  * `user_key_word.txt`: Keywords extracted from user content\n* **Key Features:**\n  * User features: Birth year, gender, tweet count, interest tags\n  * Item features: Hierarchical categories, profile keywords\n  * Interaction features: Timestamps, action counts, social connections\n  * All text features are encoded as numeric IDs for privacy\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Average Precision at 3 (MAP@3)\n* **Metric Components:**\n  * For each user, calculates precision at each position k in the top 3 recommendations\n  * Averages these precision values, weighted by whether the item was followed\n  * Formula: `ap@n = Σ (P(k) for k=1..n) / (number of followed items)`\n    * Where P(k) = (number of followed items up to position k) / k\n  * Final score is the average across all users' ap@3 values",
    "sections": {},
    "file_path": "kaggle_datasets/35/problem_summary.md"
  },
  "195": {
    "problem_id": "195",
    "title": "Binary Classification of Kobe Bryant's Shot Success",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that a given field goal attempt by Kobe Bryant during his 20-year NBA career was successful (i.e., whether the shot went in).",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Kobe Bryant's Shot Success\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a given field goal attempt by Kobe Bryant during his 20-year NBA career was successful (i.e., whether the shot went in).\n    * **Key Points:**\n        * The competition focuses on analyzing shot location and game circumstances to predict success.\n        * Participants must avoid temporal leakage by only training on events that occurred prior to the shot being predicted.\n        * The task combines classification basics with potential feature engineering and time series analysis.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing every field goal attempt by Kobe Bryant during his NBA career, with spatial, temporal, and game-context features.\n* **Data Files:**\n    * `data.csv` (contains both training and test shots, with test set shot_made_flags removed)\n    * `sample_submission.csv` (contains required shot_ids for submission)\n* **Key Features:**\n    * Spatial features: loc_x, loc_y, shot_distance, shot_zone_area, shot_zone_range\n    * Temporal features: game_date, season, minutes_remaining, seconds_remaining\n    * Game context: playoffs, opponent, matchup, period\n    * Shot characteristics: action_type, combined_shot_type, shot_type\n    * Target variable: shot_made_flag (binary outcome to predict)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (Log Loss)\n    * **Components:**\n        * Measures the accuracy of a classifier by penalizing false classifications.\n        * For binary classification: Log Loss = -(y*log(p) + (1-y)*log(1-p)), where y is the binary outcome and p is the predicted probability.\n        * Lower values indicate better performance, with perfect predictions yielding a log loss of 0.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a given field goal attempt by Kobe Bryant during his 20-year NBA career was successful (i.e., whether the shot went in).\n    * **Key Points:**\n        * The competition focuses on analyzing shot location and game circumstances to predict success.\n        * Participants must avoid temporal leakage by only training on events that occurred prior to the shot being predicted.\n        * The task combines classification basics with potential feature engineering and time series analysis.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing every field goal attempt by Kobe Bryant during his NBA career, with spatial, temporal, and game-context features.\n* **Data Files:**\n    * `data.csv` (contains both training and test shots, with test set shot_made_flags removed)\n    * `sample_submission.csv` (contains required shot_ids for submission)\n* **Key Features:**\n    * Spatial features: loc_x, loc_y, shot_distance, shot_zone_area, shot_zone_range\n    * Temporal features: game_date, season, minutes_remaining, seconds_remaining\n    * Game context: playoffs, opponent, matchup, period\n    * Shot characteristics: action_type, combined_shot_type, shot_type\n    * Target variable: shot_made_flag (binary outcome to predict)",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (Log Loss)\n    * **Components:**\n        * Measures the accuracy of a classifier by penalizing false classifications.\n        * For binary classification: Log Loss = -(y*log(p) + (1-y)*log(1-p)), where y is the binary outcome and p is the predicted probability.\n        * Lower values indicate better performance, with perfect predictions yielding a log loss of 0."
    },
    "file_path": "kaggle_datasets/195/problem_summary.md"
  },
  "353": {
    "problem_id": "353",
    "title": "Kannada Handwritten Digit Classification",
    "problem_type": "Multi-class Classification (Image Recognition)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Kannada Handwritten Digit Classification\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Image Recognition)\n* **Objective:**  \n    * Predict the correct Kannada digit (0-9) from handwritten grayscale images.  \n    * The competition serves as an extension of the classic MNIST challenge, using Kannada script digits instead of Arabic numerals.\n* **Key Points:**  \n    * Focuses on the Kannada language script (spoken in southwestern India).  \n    * Includes an additional challenging test set ('Dig-MNIST') with out-of-domain handwritten samples for validation.  \n    * Kernels-only competition (submissions via Kaggle Notebooks with internet disabled).  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Grayscale images (28x28 pixels) of handwritten Kannada digits (0-9).  \n    * Each pixel value ranges from 0-255 (darker pixels = higher values).  \n* **Data Files:**  \n    * `train.csv`: Labeled images (785 columns: `label` + 784 pixel columns).  \n    * `test.csv`: Unlabeled images (784 pixel columns).  \n    * `Dig-MNIST.csv`: Additional labeled dataset for validation (harder out-of-domain samples).  \n    * `sample_submission.csv`: Submission template.  \n* **Features:**  \n    * Pixel columns named `pixel{x}` (x = 0-783), mapped to a 28x28 grid (row-major order).  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy (percentage of correctly classified test images).  \n    * Example: 0.97 accuracy = 97% correct predictions.  \n* **Note:** The 'Dig-MNIST' dataset provides a secondary benchmark (reported baseline: 76.1% accuracy).",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Image Recognition)\n* **Objective:**  \n    * Predict the correct Kannada digit (0-9) from handwritten grayscale images.  \n    * The competition serves as an extension of the classic MNIST challenge, using Kannada script digits instead of Arabic numerals.\n* **Key Points:**  \n    * Focuses on the Kannada language script (spoken in southwestern India).  \n    * Includes an additional challenging test set ('Dig-MNIST') with out-of-domain handwritten samples for validation.  \n    * Kernels-only competition (submissions via Kaggle Notebooks with internet disabled).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Grayscale images (28x28 pixels) of handwritten Kannada digits (0-9).  \n    * Each pixel value ranges from 0-255 (darker pixels = higher values).  \n* **Data Files:**  \n    * `train.csv`: Labeled images (785 columns: `label` + 784 pixel columns).  \n    * `test.csv`: Unlabeled images (784 pixel columns).  \n    * `Dig-MNIST.csv`: Additional labeled dataset for validation (harder out-of-domain samples).  \n    * `sample_submission.csv`: Submission template.  \n* **Features:**  \n    * Pixel columns named `pixel{x}` (x = 0-783), mapped to a 28x28 grid (row-major order).",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy (percentage of correctly classified test images).  \n    * Example: 0.97 accuracy = 97% correct predictions.  \n* **Note:** The 'Dig-MNIST' dataset provides a secondary benchmark (reported baseline: 76.1% accuracy)."
    },
    "file_path": "kaggle_datasets/353/problem_summary.md"
  },
  "161": {
    "problem_id": "161",
    "title": "Predicting Context Ad Click-Through Rates",
    "problem_type": "Binary Classification",
    "objective": "Predict whether individual users will click on context ads displayed on Avito.ru (Russia's largest classifieds website). The goal is to improve Avito's existing model by incorporating individual user behavior rather than relying solely on general ad performance statistics.",
    "evaluation_metric": null,
    "full_content": "# Predicting Context Ad Click-Through Rates\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether individual users will click on context ads displayed on Avito.ru (Russia's largest classifieds website). The goal is to improve Avito's existing model by incorporating individual user behavior rather than relying solely on general ad performance statistics.\n    * **Key Points:**\n        * Focuses specifically on contextual ads (pay-per-click model)\n        * Requires modeling user-ad interaction patterns\n        * Uses relational data spanning user behavior, ad characteristics, and search context\n\n**Dataset Overview:**\n* **Data Type & Context:** Relational tabular data capturing user interactions with classified ads platform\n* **Key Data Files:**\n    * `trainSearchStream.tsv`/`testSearchStream.tsv` (main datasets with impression records)\n    * `AdsInfo.tsv` (ad characteristics)\n    * `UserInfo.tsv` (user device/browser details)\n    * `SearchInfo.tsv` (search session context)\n    * `VisitsStream.tsv`/`PhoneRequestsStream.tsv` (user engagement signals)\n    * Supporting files for locations and categories\n* **Important Features:**\n    * User behavior signals (clicks, phone requests, visits)\n    * Ad characteristics (type, position, historical CTR, price, category)\n    * Search context (query, location, filters)\n    * User device/browser information\n    * Temporal features across multiple interaction points\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss (Binary Cross-Entropy)\n    * **Calculation:**\n        * Logarithmic penalty for confident incorrect predictions\n        * Formula: `-(1/n) * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n        * Predictions clipped to [1e-15, 1-1e-15] to avoid infinite penalties\n    * **Properties:**\n        * Stronger penalty for confident wrong predictions\n        * Directly evaluates predicted probabilities\n        * Balanced evaluation for imbalanced classes (common in CTR prediction)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether individual users will click on context ads displayed on Avito.ru (Russia's largest classifieds website). The goal is to improve Avito's existing model by incorporating individual user behavior rather than relying solely on general ad performance statistics.\n    * **Key Points:**\n        * Focuses specifically on contextual ads (pay-per-click model)\n        * Requires modeling user-ad interaction patterns\n        * Uses relational data spanning user behavior, ad characteristics, and search context",
      "Dataset Overview": "* **Data Type & Context:** Relational tabular data capturing user interactions with classified ads platform\n* **Key Data Files:**\n    * `trainSearchStream.tsv`/`testSearchStream.tsv` (main datasets with impression records)\n    * `AdsInfo.tsv` (ad characteristics)\n    * `UserInfo.tsv` (user device/browser details)\n    * `SearchInfo.tsv` (search session context)\n    * `VisitsStream.tsv`/`PhoneRequestsStream.tsv` (user engagement signals)\n    * Supporting files for locations and categories\n* **Important Features:**\n    * User behavior signals (clicks, phone requests, visits)\n    * Ad characteristics (type, position, historical CTR, price, category)\n    * Search context (query, location, filters)\n    * User device/browser information\n    * Temporal features across multiple interaction points",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss (Binary Cross-Entropy)\n    * **Calculation:**\n        * Logarithmic penalty for confident incorrect predictions\n        * Formula: `-(1/n) * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n        * Predictions clipped to [1e-15, 1-1e-15] to avoid infinite penalties\n    * **Properties:**\n        * Stronger penalty for confident wrong predictions\n        * Directly evaluates predicted probabilities\n        * Balanced evaluation for imbalanced classes (common in CTR prediction)"
    },
    "file_path": "kaggle_datasets/161/problem_summary.md"
  },
  "505": {
    "problem_id": "505",
    "title": "Credit Default Prediction with Time-Series Behavioral Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Credit Default Prediction with Time-Series Behavioral Data\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a credit card customer will default (fail to pay their credit card balance within 120 days after their latest statement date) based on their monthly customer profile.\n    * **Key Points:**\n        * Focuses on credit risk management for a payment card issuer.\n        * Uses time-series behavioral data and anonymized customer profiles.\n        * Negative class (non-default) is subsampled at 5%, requiring weighting in evaluation.\n        * Aims to improve upon existing production models for credit default prediction.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing anonymized, normalized time-series behavioral data of credit card customers.\n    * **Data Files:**\n        * `train_data.csv`: Training data with multiple statement dates per customer.\n        * `train_labels.csv`: Target labels (default=1) for each customer in training set.\n        * `test_data.csv`: Test data for which predictions must be made.\n        * `sample_submission.csv`: Example submission file format.\n    * **Key Features:**\n        * Features are aggregated profile metrics per customer per statement date.\n        * Categorized into: Delinquency (D_*), Spend (S_*), Payment (P_*), Balance (B_*), and Risk (R_*) variables.\n        * 11 categorical features including B_30, B_38, D_114, etc.\n\n## Evaluation Metrics\n* **Primary Metric:** Custom metric (M) combining:\n    * Normalized Gini Coefficient (G)\n    * Default rate captured at 4% (D) - percentage of true defaults captured in top 4% of predictions\n    * Formula: M = 0.5 × (G + D)\n* **Key Details:**\n    * Negative labels receive 20× weight to adjust for downsampling.\n    * Metric ranges from 0 to 1.0, with higher values indicating better performance.\n    * Default rate at 4% represents a sensitivity/recall statistic.",
    "sections": {},
    "file_path": "kaggle_datasets/505/problem_summary.md"
  },
  "566": {
    "problem_id": "566",
    "title": "Forecasting Mini-Course Sales",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict a full year's worth of sales (2022) for various fictitious learning modules sold in different Kaggle-branded stores across real countries. The dataset captures real-world effects like seasonality, weekends, and holidays.",
    "evaluation_metric": null,
    "full_content": "# Forecasting Mini-Course Sales\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict a full year's worth of sales (2022) for various fictitious learning modules sold in different Kaggle-branded stores across real countries. The dataset captures real-world effects like seasonality, weekends, and holidays.\n    * **Key Points:**\n        * Synthetic dataset designed to mimic real-world sales patterns.\n        * Requires forecasting at the date-country-store-item combination level.\n        * Public leaderboard scored on Q1 2022 test data; Private leaderboard on remaining quarters.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data representing daily sales of learning modules across multiple stores and countries.\n* **Data Files:**\n    * `train.csv`: Contains historical sales data (date-country-store-item combinations).\n    * `test.csv`: Requires predicting sales for date-country-store-item combinations in 2022.\n    * `sample_submission.csv`: Example submission file with `id` and `num_sold` columns.\n* **Features:** Likely includes:\n    * Temporal features (date, day of week, holidays).\n    * Categorical features (country, store, item).\n    * Target variable: `num_sold` (sales count).\n\n**Evaluation Metrics:**\n* **Primary Metric:** SMAPE (Symmetric Mean Absolute Percentage Error).\n    * **SMAPE Definition:** \n        * Standard formula used, except SMAPE = 0 when both actual and predicted values are 0.\n        * Penalizes over- and under-predictions symmetrically.\n        * Suitable for percentage-based error measurement in time series forecasting.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict a full year's worth of sales (2022) for various fictitious learning modules sold in different Kaggle-branded stores across real countries. The dataset captures real-world effects like seasonality, weekends, and holidays.\n    * **Key Points:**\n        * Synthetic dataset designed to mimic real-world sales patterns.\n        * Requires forecasting at the date-country-store-item combination level.\n        * Public leaderboard scored on Q1 2022 test data; Private leaderboard on remaining quarters.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data representing daily sales of learning modules across multiple stores and countries.\n* **Data Files:**\n    * `train.csv`: Contains historical sales data (date-country-store-item combinations).\n    * `test.csv`: Requires predicting sales for date-country-store-item combinations in 2022.\n    * `sample_submission.csv`: Example submission file with `id` and `num_sold` columns.\n* **Features:** Likely includes:\n    * Temporal features (date, day of week, holidays).\n    * Categorical features (country, store, item).\n    * Target variable: `num_sold` (sales count).",
      "Evaluation Metrics": "* **Primary Metric:** SMAPE (Symmetric Mean Absolute Percentage Error).\n    * **SMAPE Definition:** \n        * Standard formula used, except SMAPE = 0 when both actual and predicted values are 0.\n        * Penalizes over- and under-predictions symmetrically.\n        * Suitable for percentage-based error measurement in time series forecasting."
    },
    "file_path": "kaggle_datasets/566/problem_summary.md"
  },
  "330": {
    "problem_id": "330",
    "title": "Aerial Cactus Identification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Aerial Cactus Identification\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Image Classification)\n* **Objective:**  \n    * Create a classifier to determine whether a 32x32 aerial image contains a columnar cactus (Neobuxbaumia tetetzo).  \n    * The task supports ecological surveillance efforts by automating vegetation recognition in protected areas.  \n* **Key Points:**  \n    * Part of the VIGIA project for autonomous surveillance of protected natural areas in Mexico.  \n    * Kernels-only competition (submissions required via Kaggle Kernels).  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Aerial thumbnail images (32x32 pixels) of desert terrain, some containing columnar cacti.  \n* **Data Files:**  \n    * `train/` - Training set images.  \n    * `test/` - Test set images (labels to be predicted).  \n    * `train.csv` - Labels for training images (`has_cactus`: 1 = present, 0 = absent).  \n    * `sample_submission.csv` - Submission template with `id` and predicted probability.  \n* **Features:**  \n    * Image filenames serve as unique IDs.  \n    * Binary label (`has_cactus`) indicates cactus presence/absence.  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n    * Measures the classifier’s ability to distinguish between cactus/non-cactus images.  \n    * Predictions require probabilities (`has_cactus` between 0 and 1) for each test image.",
    "sections": {},
    "file_path": "kaggle_datasets/330/problem_summary.md"
  },
  "102": {
    "problem_id": "102",
    "title": "Personalized Web Search Ranking Challenge",
    "problem_type": "Learning to Rank (LTR) / Personalized Search Re-ranking",
    "objective": "Re-rank top-10 URLs in search engine results pages (SERPs) based on user-specific context to improve personalized search experience. Participants must leverage:",
    "evaluation_metric": null,
    "full_content": "# Personalized Web Search Ranking Challenge\n\n**Problem Description:**\n* **Problem Type:** Learning to Rank (LTR) / Personalized Search Re-ranking\n* **Objective:** Re-rank top-10 URLs in search engine results pages (SERPs) based on user-specific context to improve personalized search experience. Participants must leverage:\n    * Long-term user history (behavior across sessions)\n    * Short-term session context (current search session behavior)\n* **Key Points:**\n    * Focus on personalization using anonymized search logs\n    * URLs have graded relevance labels (0-2) derived from dwell time\n    * Test queries are sampled to ensure sufficient user context exists\n\n**Dataset Overview:**\n* **Data Type:** Search engine log data (tabular/sequential) with:\n    * User actions (queries, clicks)\n    * Temporal session information\n    * URL rankings and domains\n* **Data Files:**\n    * `train.gz` (~16GB uncompressed) - 27 days of training sessions\n    * `test.gz` - 3 days of test sessions\n    * Baseline submission files (`random-baseline.gz`, `non-personalised-baseline.gz`)\n* **Key Features:**\n    * Anonymized IDs for users, queries, terms, URLs, and domains\n    * Session metadata with timestamps\n    * Click logs with dwell times\n    * Pre-filtered to remove commercial queries and overly popular queries\n\n**Evaluation Metrics:**\n* **Primary Metric:** Normalized Discounted Cumulative Gain (NDCG)\n* **Relevance Grading:**\n    * **0 (Irrelevant):** No clicks OR dwell time <50 units\n    * **1 (Relevant):** Dwell time 50-399 units\n    * **2 (Highly Relevant):** Dwell time ≥400 units OR last click in session\n* **Submission Format:**\n    * Ranked list of SessionID-URLID pairs per test query\n    * Only URLs from test queries (marked TypeOfRecord=T) are evaluated",
    "sections": {
      "Problem Description": "* **Problem Type:** Learning to Rank (LTR) / Personalized Search Re-ranking\n* **Objective:** Re-rank top-10 URLs in search engine results pages (SERPs) based on user-specific context to improve personalized search experience. Participants must leverage:\n    * Long-term user history (behavior across sessions)\n    * Short-term session context (current search session behavior)\n* **Key Points:**\n    * Focus on personalization using anonymized search logs\n    * URLs have graded relevance labels (0-2) derived from dwell time\n    * Test queries are sampled to ensure sufficient user context exists",
      "Dataset Overview": "* **Data Type:** Search engine log data (tabular/sequential) with:\n    * User actions (queries, clicks)\n    * Temporal session information\n    * URL rankings and domains\n* **Data Files:**\n    * `train.gz` (~16GB uncompressed) - 27 days of training sessions\n    * `test.gz` - 3 days of test sessions\n    * Baseline submission files (`random-baseline.gz`, `non-personalised-baseline.gz`)\n* **Key Features:**\n    * Anonymized IDs for users, queries, terms, URLs, and domains\n    * Session metadata with timestamps\n    * Click logs with dwell times\n    * Pre-filtered to remove commercial queries and overly popular queries",
      "Evaluation Metrics": "* **Primary Metric:** Normalized Discounted Cumulative Gain (NDCG)\n* **Relevance Grading:**\n    * **0 (Irrelevant):** No clicks OR dwell time <50 units\n    * **1 (Relevant):** Dwell time 50-399 units\n    * **2 (Highly Relevant):** Dwell time ≥400 units OR last click in session\n* **Submission Format:**\n    * Ranked list of SessionID-URLID pairs per test query\n    * Only URLs from test queries (marked TypeOfRecord=T) are evaluated"
    },
    "file_path": "kaggle_datasets/102/problem_summary.md"
  },
  "592": {
    "problem_id": "592",
    "title": "Predicting Essay Quality from Keystroke Logs",
    "problem_type": "Regression (with potential multi-output aspects if considering multiple quality dimensions)",
    "objective": "Predict the quality score (0-6 scale) of an essay based solely on behavioral features extracted from keystroke logs during its composition. The goal is to uncover relationships between writing processes (typing patterns, revisions, pauses) and final writing quality.",
    "evaluation_metric": null,
    "full_content": "# Predicting Essay Quality from Keystroke Logs\n\n**Problem Description:**\n* **Problem Type:** Regression (with potential multi-output aspects if considering multiple quality dimensions)\n* **Objective:** Predict the quality score (0-6 scale) of an essay based solely on behavioral features extracted from keystroke logs during its composition. The goal is to uncover relationships between writing processes (typing patterns, revisions, pauses) and final writing quality.\n* **Key Points:**\n  * Focus on **process features** (e.g., pause duration, revision frequency) rather than textual content (all alphanumeric characters are anonymized).\n  * Must handle **temporal sequences** of keystroke events with variable lengths.\n  * Potential applications in automated writing evaluation and intelligent tutoring systems.\n\n**Dataset Overview:**\n* **Data Type:** Time-series log data of keystroke/mouse events with tabular metadata.\n* **Context:** Logs from ~5000 essays written under controlled conditions (30-minute argumentative essays).\n* **Data Files:**\n  * `train_logs.csv`: Chronological keystroke/mouse events (anonymized text, timestamps, activity types).\n  * `test_logs.csv`: Same structure as training logs (full test set hidden in code competition).\n  * `train_scores.csv`: Essay IDs and corresponding quality scores (0-6).\n  * `sample_submission.csv`: Submission template with ID-score pairs.\n* **Key Features:**\n  * Temporal metrics: `down_time`, `up_time`, `action_time`.\n  * Behavioral categories: `activity` (Input, Remove/Cut, Paste, etc.).\n  * Contextual state: `cursor_position`, `word_count` after each event.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE) between predicted and actual scores.\n  * Formula:  \n    ```RMSE = sqrt(mean((y_true - y_pred)^2))```\n  * Lower values indicate better performance (perfect prediction: 0.0).\n* **Efficiency Prize:** Additional CPU-only track evaluating:\n  * Model runtime (seconds).\n  * Normalized RMSE relative to baseline and best-performing model.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (with potential multi-output aspects if considering multiple quality dimensions)\n* **Objective:** Predict the quality score (0-6 scale) of an essay based solely on behavioral features extracted from keystroke logs during its composition. The goal is to uncover relationships between writing processes (typing patterns, revisions, pauses) and final writing quality.\n* **Key Points:**\n  * Focus on **process features** (e.g., pause duration, revision frequency) rather than textual content (all alphanumeric characters are anonymized).\n  * Must handle **temporal sequences** of keystroke events with variable lengths.\n  * Potential applications in automated writing evaluation and intelligent tutoring systems.",
      "Dataset Overview": "* **Data Type:** Time-series log data of keystroke/mouse events with tabular metadata.\n* **Context:** Logs from ~5000 essays written under controlled conditions (30-minute argumentative essays).\n* **Data Files:**\n  * `train_logs.csv`: Chronological keystroke/mouse events (anonymized text, timestamps, activity types).\n  * `test_logs.csv`: Same structure as training logs (full test set hidden in code competition).\n  * `train_scores.csv`: Essay IDs and corresponding quality scores (0-6).\n  * `sample_submission.csv`: Submission template with ID-score pairs.\n* **Key Features:**\n  * Temporal metrics: `down_time`, `up_time`, `action_time`.\n  * Behavioral categories: `activity` (Input, Remove/Cut, Paste, etc.).\n  * Contextual state: `cursor_position`, `word_count` after each event.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE) between predicted and actual scores.\n  * Formula:  \n    ```RMSE = sqrt(mean((y_true - y_pred)^2))```\n  * Lower values indicate better performance (perfect prediction: 0.0).\n* **Efficiency Prize:** Additional CPU-only track evaluating:\n  * Model runtime (seconds).\n  * Normalized RMSE relative to baseline and best-performing model."
    },
    "file_path": "kaggle_datasets/592/problem_summary.md"
  },
  "69": {
    "problem_id": "69",
    "title": "Binary Detection of North Atlantic Right Whale Calls from Audio Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Detection of North Atlantic Right Whale Calls from Audio Recordings\n\n## Problem Description\n* **Problem Type:** Binary Classification (Audio Signal Processing)\n* **Objective:** Develop an algorithm to detect the presence of North Atlantic right whale \"up-calls\" in 2-second audio clips, with the goal of improving ship routing decisions to prevent whale-ship collisions.\n    * **Key Points:**\n        * Focus on identifying a specific whale vocalization (rising \"whoop\" sound lasting ~1 second)\n        * Real-world conservation application: Protecting endangered North Atlantic right whales (population ~400)\n        * Must distinguish calls from background noise, other whale sounds, and non-biological ocean noise\n        * Performance improvements could directly impact marine conservation efforts\n\n## Dataset Overview\n* **Data Type:** Audio time series data (.aiff format) with binary labels\n* **Context:** Underwater acoustic recordings from Cornell's bioacoustic buoy network\n* **Data Files:**\n    * `whale_data.zip` (main dataset)\n    * `small_data_sample_revised.zip` (10 example clips)\n    * `train.csv` (labels for training set)\n    * `sample_submission.csv`\n* **Key Features:**\n    * 30,000 labeled training samples (2-second clips at 2kHz sample rate)\n    * 54,503 unlabeled test samples\n    * Each clip may contain: right whale calls, other whale sounds, or noise\n    * Suggested preprocessing: Frequency domain transformation (FFT) may be beneficial\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Implementation Details:**\n        * Predictions evaluated against expert-annotated ground truth\n        * Requires probability scores for positive class (whale call present)\n        * Standard implementations provided for:\n            * MATLAB (`perfcurve`)\n            * R (`ROCR` package)\n            * Python (`scikit-learn` metrics)",
    "sections": {},
    "file_path": "kaggle_datasets/69/problem_summary.md"
  },
  "559": {
    "problem_id": "559",
    "title": "Forecasting Microbusiness Density at US County Level",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict monthly microbusiness density (microbusinesses per 100 people over 18) for US counties. The goal is to help policymakers understand trends in very small businesses that are often missed by traditional economic data sources.",
    "evaluation_metric": null,
    "full_content": "# Forecasting Microbusiness Density at US County Level\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict monthly microbusiness density (microbusinesses per 100 people over 18) for US counties. The goal is to help policymakers understand trends in very small businesses that are often missed by traditional economic data sources.\n    * **Key Points:**\n        * Focus on county-level predictions across the United States\n        * Microbusinesses defined as entities with ≤10 employees and online presence\n        * Population figures used for density calculation have a 2-year lag (e.g., 2021 density uses 2019 population data)\n        * Encouraged to use external data sources to enhance predictions\n\n**Dataset Overview:**\n* **Data Type:** Tabular time series data with geographic and temporal components\n* **Context:** County-level economic data with monthly granularity\n* **Data Files:**\n    * `train.csv`: Historical microbusiness density data (2019-2022)\n    * `test.csv`: Metadata for submission rows\n    * `revealed_test.csv`: Older test data released during competition\n    * `census_starter.csv`: Example census features (broadband access, education, income etc.)\n* **Key Features:**\n    * `cfips`: County FIPS code\n    * `first_day_of_month`: Temporal indicator\n    * `microbusiness_density`: Target variable\n    * `active`: Raw microbusiness count (training only)\n    * Census-derived features (education, income, broadband access etc.)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE)\n    * **Special Handling:** SMAPE = 0 when both actual and predicted values are 0\n    * **Calculation:** \n        * SMAPE = (200% × |actual - predicted|) / (|actual| + |predicted|)\n        * Averaged across all predictions\n* **Scoring Approach:**\n    * Public leaderboard uses most recent month's data\n    * Final evaluation uses future data not available during submission period\n    * Static forecasts only (no notebook reruns during evaluation phase)",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict monthly microbusiness density (microbusinesses per 100 people over 18) for US counties. The goal is to help policymakers understand trends in very small businesses that are often missed by traditional economic data sources.\n    * **Key Points:**\n        * Focus on county-level predictions across the United States\n        * Microbusinesses defined as entities with ≤10 employees and online presence\n        * Population figures used for density calculation have a 2-year lag (e.g., 2021 density uses 2019 population data)\n        * Encouraged to use external data sources to enhance predictions",
      "Dataset Overview": "* **Data Type:** Tabular time series data with geographic and temporal components\n* **Context:** County-level economic data with monthly granularity\n* **Data Files:**\n    * `train.csv`: Historical microbusiness density data (2019-2022)\n    * `test.csv`: Metadata for submission rows\n    * `revealed_test.csv`: Older test data released during competition\n    * `census_starter.csv`: Example census features (broadband access, education, income etc.)\n* **Key Features:**\n    * `cfips`: County FIPS code\n    * `first_day_of_month`: Temporal indicator\n    * `microbusiness_density`: Target variable\n    * `active`: Raw microbusiness count (training only)\n    * Census-derived features (education, income, broadband access etc.)",
      "Evaluation Metrics": "* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE)\n    * **Special Handling:** SMAPE = 0 when both actual and predicted values are 0\n    * **Calculation:** \n        * SMAPE = (200% × |actual - predicted|) / (|actual| + |predicted|)\n        * Averaged across all predictions\n* **Scoring Approach:**\n    * Public leaderboard uses most recent month's data\n    * Final evaluation uses future data not available during submission period\n    * Static forecasts only (no notebook reruns during evaluation phase)"
    },
    "file_path": "kaggle_datasets/559/problem_summary.md"
  },
  "56": {
    "problem_id": "56",
    "title": "Predicting Optimal Paths in Dynamic Internet Topology Graphs",
    "problem_type": "Binary Classification (with probabilistic outputs)",
    "objective": "Predict the probability that a given path in a dynamic internet topology graph remains an optimal (shortest) path across 5 future time steps.",
    "evaluation_metric": null,
    "full_content": "# Predicting Optimal Paths in Dynamic Internet Topology Graphs\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability that a given path in a dynamic internet topology graph remains an optimal (shortest) path across 5 future time steps. \n    * **Key Points:**\n        * Focuses on dynamic graph structures representing Autonomous Systems (AS) in internet routing\n        * Must account for peer relationships (zero-weight edges) and regular edges (weight=1)\n        * Paths may have varying lengths and multiple optimal routes may exist\n        * Graphs evolve over time with edges appearing/disappearing\n        * Data is anonymized with distorted AS names to prevent reverse engineering\n\n**Dataset Overview:**\n* **Data Type:** Time-series of directed graphs representing internet topology\n* **Context:** 15 training graphs and 5 test graphs (not provided) showing AS connections at sequential time intervals\n* **Data Files:**\n    * `train.zip`: Contains 15 training graphs with edge lists (format: `AS_Name1 | AS_Name2 | Edge_weight`)\n    * `paths.txt`: Contains 10,000 paths (sequences of AS names) to evaluate\n* **Key Features:**\n    * Nodes represent anonymized Autonomous Systems\n    * Edges represent either peer relationships (weight=0) or paid routes (weight=1)\n    * Temporal evolution of graph structure between time steps\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Implementation Notes:**\n        * Submissions require probabilistic predictions (real values between 0-1)\n        * Format: 50,000 predictions (10,000 paths × 5 test times) as single column vector\n        * Predictions evaluated against binary ground truth (path optimal/not optimal)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability that a given path in a dynamic internet topology graph remains an optimal (shortest) path across 5 future time steps. \n    * **Key Points:**\n        * Focuses on dynamic graph structures representing Autonomous Systems (AS) in internet routing\n        * Must account for peer relationships (zero-weight edges) and regular edges (weight=1)\n        * Paths may have varying lengths and multiple optimal routes may exist\n        * Graphs evolve over time with edges appearing/disappearing\n        * Data is anonymized with distorted AS names to prevent reverse engineering",
      "Dataset Overview": "* **Data Type:** Time-series of directed graphs representing internet topology\n* **Context:** 15 training graphs and 5 test graphs (not provided) showing AS connections at sequential time intervals\n* **Data Files:**\n    * `train.zip`: Contains 15 training graphs with edge lists (format: `AS_Name1 | AS_Name2 | Edge_weight`)\n    * `paths.txt`: Contains 10,000 paths (sequences of AS names) to evaluate\n* **Key Features:**\n    * Nodes represent anonymized Autonomous Systems\n    * Edges represent either peer relationships (weight=0) or paid routes (weight=1)\n    * Temporal evolution of graph structure between time steps",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Implementation Notes:**\n        * Submissions require probabilistic predictions (real values between 0-1)\n        * Format: 50,000 predictions (10,000 paths × 5 test times) as single column vector\n        * Predictions evaluated against binary ground truth (path optimal/not optimal)"
    },
    "file_path": "kaggle_datasets/56/problem_summary.md"
  },
  "595": {
    "problem_id": "595",
    "title": "Binary Classification with a Bank Churn Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Bank Churn Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict whether a bank customer will churn (close their account) or continue with their account.  \n  * **Key Points:**  \n    * The task involves predicting the probability of churn (`Exited`).  \n    * The dataset is synthetically generated but based on real-world bank customer churn data.  \n    * Participants are encouraged to explore differences between the synthetic and original datasets.  \n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing bank customer attributes and churn status.  \n* **Data Files:**  \n  * `train.csv` - Contains features and the binary target (`Exited`).  \n  * `test.csv` - Contains features for which predictions must be made.  \n  * `sample_submission.csv` - Example submission file in the required format.  \n* **Features:**  \n  * Likely includes customer demographics, account details, and transaction history (exact features not listed but inferred from context).  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).  \n  * **Components:**  \n    * Measures the model's ability to distinguish between churned and non-churned customers.  \n    * Higher AUC-ROC indicates better classification performance.",
    "sections": {},
    "file_path": "kaggle_datasets/595/problem_summary.md"
  },
  "105": {
    "problem_id": "105",
    "title": "Predicting Starting States in Conway's Reverse Game of Life",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Starting States in Conway's Reverse Game of Life\n\n## Problem Description\n* **Problem Type:** Binary Classification (per cell) / Binary Matrix Prediction\n* **Objective:** Predict the starting configuration of a 20x20 Game of Life board given its final state after a known number of evolution steps (delta). The challenge involves reversing the deterministic but information-lossy evolution rules of Conway's Game of Life.\n* **Key Points:**\n  * The competition explores whether machine learning can reverse-engineer the initial state from a final state in this cellular automaton.\n  * Each cell must be predicted as either alive (1) or dead (0).\n  * The problem is inherently many-to-one (multiple starting states can lead to the same final state).\n  * Time reversal is limited to short durations (delta between 1-5 steps) to mitigate degeneracy issues.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing 20x20 binary grids (400 cells per board) from Conway's Game of Life simulations.\n* **Data Files:**\n  * train.csv - 50,000 games with starting and stopping boards\n  * test.csv - 50,000 games with only stopping boards (require starting board prediction)\n  * sampleSubmission.csv - Submission format example\n* **Features:**\n  * `id` - Unique game identifier\n  * `delta` - Number of evolution steps between start and stop boards (1-5)\n  * `start.1` to `start.400` - Binary values of starting board (column-wise order)\n  * `stop.1` to `stop.400` - Binary values of final/stopping board\n* **Data Generation:**\n  * Boards initialized with random density (1%-99% alive)\n  * 5 \"warmup\" evolution steps before recording start state\n  * Additional delta steps (1-5) applied to reach stop state\n  * Empty stop boards were discarded\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE)\n  * Equivalent to 1 - classification accuracy for binary predictions\n  * Calculated as average of absolute differences between predicted and true binary values across all cells\n* **Submission Format:**\n  * Predictions must be in column-wise order (start.1 = row 1 column 1, start.2 = row 2 column 1, etc.)\n  * Each row",
    "sections": {},
    "file_path": "kaggle_datasets/105/problem_summary.md"
  },
  "337": {
    "problem_id": "337",
    "title": "Decrypting Shakespearean Ciphertext with Classic and Modern Ciphers",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Decrypting Shakespearean Ciphertext with Classic and Modern Ciphers\n\n## Problem Description\n* **Problem Type**:  \n  - Text Decryption / Matching (Multi-class Classification)\n* **Objective**:  \n  - Match encrypted Shakespearean text snippets (`ciphertext`) to their corresponding original plaintext lines by predicting the correct `index` identifier.  \n  - Decrypt text transformed by up to 4 layered classic/modern ciphers, where `difficulty` level indicates the number and order of ciphers applied.\n* **Key Points**:  \n  - Ciphers range from simple historical methods to a \"slightly less simple\" modern cipher.  \n  - Each ciphertext is padded to the next 100-character length with random characters before encryption.  \n  - **Meta-challenge**: Deduce the cipher types and keys (especially for the 4th cipher) to solve the decryption puzzle.\n\n## Dataset Overview\n* **Data Type & Context**:  \n  - **Text data**: Encrypted (`ciphertext`) and decrypted (`plaintext`) lines from Shakespearean plays.  \n  - **Tabular format**: CSV files linking ciphertext IDs to plaintext indices.\n* **Data Files**:  \n  - `training.csv`: Contains `plaintext_id`, original `text`, and `index` (target label).  \n  - `test.csv`: Contains `ciphertext_id`, encrypted `ciphertext`, and `difficulty` (1–4, indicating cipher layers applied).  \n  - `sample_submission.csv`: Submission template with `ciphertext_id` and predicted `index`.\n* **Key Features**:  \n  - `difficulty`: Critical for understanding cipher layers (e.g., `difficulty=2` means cipher #1 → cipher #2).  \n  - `ciphertext`: Encrypted text with variable lengths (padded to nearest 100 characters).  \n  - `index`: Unique identifier for plaintext lines in training data.\n\n## Evaluation Metrics\n* **Primary Metric**:  \n  - **Accuracy**: Proportion of correctly matched `ciphertext_id` to `index` pairs.  \n* **Submission Format**:  \n  - CSV with `ciphertext_id` and predicted `index` (e.g., `ID_0827e580b,10`).  \n* **Special Prizes**:  \n  - **Phil Prize**: Awarded for correctly identifying the",
    "sections": {},
    "file_path": "kaggle_datasets/337/problem_summary.md"
  },
  "561": {
    "problem_id": "561",
    "title": "Binary Classification of Machine Failures",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of machine failure based on given features. The goal is to classify whether a machine will fail (binary outcome) using synthetic data derived from real-world machine failure predictions.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Machine Failures\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of machine failure based on given features. The goal is to classify whether a machine will fail (binary outcome) using synthetic data derived from real-world machine failure predictions.\n* **Key Points:**\n  * The dataset is synthetically generated to mimic real-world data while ensuring test labels are not publicly available.\n  * Participants are encouraged to explore differences between the synthetic and original datasets and test whether incorporating the original data improves model performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing machine failure predictions, with features likely including operational parameters and sensor readings (exact features not specified but inferred from context).\n* **Data Files:**\n  * `train.csv`: Contains training data with a binary target column `Machine failure`.\n  * `test.csv`: Contains test data for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:** The dataset includes 29 columns (exact features not listed, but the target `Machine failure` is binary and not in the last column position).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Area Under the ROC Curve (AUC-ROC).\n* **Components:**\n  * Submissions must predict probabilities of machine failure for each `id` in the test set.\n  * The ROC curve measures the trade-off between true positive rate and false positive rate, with AUC summarizing the model's ability to distinguish between classes. Higher AUC values indicate better performance.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of machine failure based on given features. The goal is to classify whether a machine will fail (binary outcome) using synthetic data derived from real-world machine failure predictions.\n* **Key Points:**\n  * The dataset is synthetically generated to mimic real-world data while ensuring test labels are not publicly available.\n  * Participants are encouraged to explore differences between the synthetic and original datasets and test whether incorporating the original data improves model performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing machine failure predictions, with features likely including operational parameters and sensor readings (exact features not specified but inferred from context).\n* **Data Files:**\n  * `train.csv`: Contains training data with a binary target column `Machine failure`.\n  * `test.csv`: Contains test data for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:** The dataset includes 29 columns (exact features not listed, but the target `Machine failure` is binary and not in the last column position).",
      "Evaluation Metrics": "* **Evaluation Metric:** Area Under the ROC Curve (AUC-ROC).\n* **Components:**\n  * Submissions must predict probabilities of machine failure for each `id` in the test set.\n  * The ROC curve measures the trade-off between true positive rate and false positive rate, with AUC summarizing the model's ability to distinguish between classes. Higher AUC values indicate better performance."
    },
    "file_path": "kaggle_datasets/561/problem_summary.md"
  },
  "51": {
    "problem_id": "51",
    "title": "Predicting Molecular Activity for Drug Development",
    "problem_type": "Regression (Predicting Biological Activity Values)",
    "objective": "Predict the biological activity values of molecules based on their chemical structure descriptors, focusing on both on-target and off-target effects to aid in developing safe and effective medicines.",
    "evaluation_metric": null,
    "full_content": "# Predicting Molecular Activity for Drug Development\n\n**Problem Description:**\n* **Problem Type:** Regression (Predicting Biological Activity Values)\n* **Objective:** Predict the biological activity values of molecules based on their chemical structure descriptors, focusing on both on-target and off-target effects to aid in developing safe and effective medicines.\n    * **Key Points:**\n        * The competition involves 15 distinct biological activity datasets, each representing a different target.\n        * Molecules may appear in multiple datasets, requiring models to generalize across different biological contexts.\n        * The training/test split is based on time of testing (not random), simulating real-world drug discovery challenges where future compounds may differ from past ones.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing molecular descriptors derived from chemical structures, with associated biological activity values.\n* **Data Files:**\n    * TrainingSet.zip/7z: Contains 15 CSV files (one per activity dataset) with MoleculeID, Activity, and molecular descriptors.\n    * TestSet.zip/7z: Contains 15 CSV files in same format but without Activity values.\n    * ntree20_basicBenchmark.csv: Example submission file.\n    * Additional R scripts for evaluation and benchmarking.\n* **Features:**\n    * Each molecule is represented by numerous numerical descriptors derived from its chemical structure.\n    * Molecule IDs are prefixed with dataset identifiers (e.g., \"ACT1_\") to maintain uniqueness across datasets.\n    * Activity values are in varying units across different datasets.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Average R² (coefficient of determination) across all 15 datasets.\n    * **Calculation:**\n        * For each dataset s: Compute r²ₛ = [Σ(xᵢ - x̄)(yᵢ - ȳ)]² / [Σ(xᵢ - x̄)² Σ(yᵢ - ȳ)²]\n            * x = known activity, x̄ = mean known activity\n            * y = predicted activity, ȳ = mean predicted activity\n            * Nₛ = number of molecules in dataset s\n        * Final score: R² = (1/15) Σ r²ₛ (average across all 15 datasets)\n    * **Key Aspect:** The time-based train/test split makes this more challenging than random splits, better simulating real-world drug discovery scenarios.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Predicting Biological Activity Values)\n* **Objective:** Predict the biological activity values of molecules based on their chemical structure descriptors, focusing on both on-target and off-target effects to aid in developing safe and effective medicines.\n    * **Key Points:**\n        * The competition involves 15 distinct biological activity datasets, each representing a different target.\n        * Molecules may appear in multiple datasets, requiring models to generalize across different biological contexts.\n        * The training/test split is based on time of testing (not random), simulating real-world drug discovery challenges where future compounds may differ from past ones.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing molecular descriptors derived from chemical structures, with associated biological activity values.\n* **Data Files:**\n    * TrainingSet.zip/7z: Contains 15 CSV files (one per activity dataset) with MoleculeID, Activity, and molecular descriptors.\n    * TestSet.zip/7z: Contains 15 CSV files in same format but without Activity values.\n    * ntree20_basicBenchmark.csv: Example submission file.\n    * Additional R scripts for evaluation and benchmarking.\n* **Features:**\n    * Each molecule is represented by numerous numerical descriptors derived from its chemical structure.\n    * Molecule IDs are prefixed with dataset identifiers (e.g., \"ACT1_\") to maintain uniqueness across datasets.\n    * Activity values are in varying units across different datasets.",
      "Evaluation Metrics": "* **Primary Metric:** Average R² (coefficient of determination) across all 15 datasets.\n    * **Calculation:**\n        * For each dataset s: Compute r²ₛ = [Σ(xᵢ - x̄)(yᵢ - ȳ)]² / [Σ(xᵢ - x̄)² Σ(yᵢ - ȳ)²]\n            * x = known activity, x̄ = mean known activity\n            * y = predicted activity, ȳ = mean predicted activity\n            * Nₛ = number of molecules in dataset s\n        * Final score: R² = (1/15) Σ r²ₛ (average across all 15 datasets)\n    * **Key Aspect:** The time-based train/test split makes this more challenging than random splits, better simulating real-world drug discovery scenarios."
    },
    "file_path": "kaggle_datasets/51/problem_summary.md"
  },
  "308": {
    "problem_id": "308",
    "title": "VSB Power Line Fault Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# VSB Power Line Fault Detection\n\n## Problem Description\n* **Problem Type:** Binary Classification (Signal Processing)\n* **Objective:** Detect partial discharge patterns in signals from medium-voltage overhead power lines to identify faults before they cause outages or fires.\n    * **Key Points:**\n        * Focus on identifying subtle damage (e.g., tree branch contact, insulator flaws) that doesn't immediately disrupt power.\n        * Data represents voltage measurements from a 3-phase power scheme.\n        * Goal is to enable continuous monitoring and reduce manual inspection costs.\n\n## Dataset Overview\n* **Data Type & Context:** Time-series signal data (tabular format) of voltage measurements from power lines.\n    * **Data Files:**\n        * `metadata_train.csv` / `metadata_test.csv`: Contain signal IDs, measurement groups, phase IDs, and labels (0=undamaged, 1=fault).\n        * `train.parquet` / `test.parquet`: Signal data (800,000 int8 measurements per signal, stored column-wise for efficiency).\n        * `sample_submission.csv`: Submission template.\n    * **Features:**\n        * Each signal is a 20-millisecond snapshot (800k measurements) of voltage fluctuations at 50 Hz.\n        * Metadata links signals to their 3-phase group and target label.\n\n## Evaluation Metrics\n* **Primary Metric:** Matthews Correlation Coefficient (MCC)\n    * **Components:**  \n        MCC = (TP×TN − FP×FN) / √[(TP+FP)(TP+FN)(TN+FP)(TN+FN)]  \n        Where:\n        * TP = True Positives  \n        * TN = True Negatives  \n        * FP = False Positives  \n        * FN = False Negatives  \n    * **Why MCC?** Suitable for imbalanced binary classification as it accounts for all confusion matrix categories.",
    "sections": {},
    "file_path": "kaggle_datasets/308/problem_summary.md"
  },
  "58": {
    "problem_id": "58",
    "title": "Predicting Dark Matter Halo Positions from Galaxy Ellipticity Data",
    "problem_type": "Regression (Coordinate Prediction)",
    "objective": "Predict the (x, y) coordinates of dark matter halos in simulated astronomical skies based on the observed ellipticity (e1, e2) and positions of background galaxies. Each sky may contain 1-3 halos.",
    "evaluation_metric": null,
    "full_content": "# Predicting Dark Matter Halo Positions from Galaxy Ellipticity Data\n\n**Problem Description:**\n* **Problem Type:** Regression (Coordinate Prediction)\n* **Objective:** Predict the (x, y) coordinates of dark matter halos in simulated astronomical skies based on the observed ellipticity (e1, e2) and positions of background galaxies. Each sky may contain 1-3 halos.\n* **Key Points:**\n  * Dark matter halos distort the shapes of background galaxies via gravitational lensing, creating measurable ellipticity patterns.\n  * The challenge requires predicting both positional accuracy (minimizing radial distance) and directional unbiasedness (avoiding systematic angular bias).\n  * Solutions must handle variable counts of halos per sky (1-3), with unused halo predictions zero-padded.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing simulated astronomical observations:\n  * **Training Data:** 300 skies (each as separate CSV) with:\n    * Galaxy-level features: `id`, `x-coordinate`, `y-coordinate`, `e1`, `e2` (ellipticity components)\n    * Halo labels: Provided in `Training_halos.csv` with reference points and 1-3 halo coordinates per sky.\n  * **Test Data:** 120 skies with galaxy data (same format as training) and `Test_haloCounts.csv` specifying halo counts (1-3) per sky.\n* **Key Features:**\n  * Spatial coordinates (x,y) of galaxies (0-4200 pixel range)\n  * Ellipticity components (e1, e2) quantifying galaxy shape distortions\n  * Reference points for angular bias calculation in evaluation\n\n**Evaluation Metrics:**\n* **Primary Metric:** Custom composite score 𝑚 = 𝐹/1000 + 𝐺\n  * **Component F:** Average radial distance (in pixels) between predicted and true halo positions, optimally matched per sky.\n  * **Component G:** Directional bias metric calculated as:\n    ```math\n    G = \\sqrt{\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\cos(\\phi_i)\\right)^2 + \\left(\\frac{1}{N}\\sum_{j=1}^{N}\\sin(\\phi_j)\\right)^2}\n    ```\n    where 𝜙 is the angle between predicted position and reference line (lower G indicates less bias).\n* **",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Coordinate Prediction)\n* **Objective:** Predict the (x, y) coordinates of dark matter halos in simulated astronomical skies based on the observed ellipticity (e1, e2) and positions of background galaxies. Each sky may contain 1-3 halos.\n* **Key Points:**\n  * Dark matter halos distort the shapes of background galaxies via gravitational lensing, creating measurable ellipticity patterns.\n  * The challenge requires predicting both positional accuracy (minimizing radial distance) and directional unbiasedness (avoiding systematic angular bias).\n  * Solutions must handle variable counts of halos per sky (1-3), with unused halo predictions zero-padded.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing simulated astronomical observations:\n  * **Training Data:** 300 skies (each as separate CSV) with:\n    * Galaxy-level features: `id`, `x-coordinate`, `y-coordinate`, `e1`, `e2` (ellipticity components)\n    * Halo labels: Provided in `Training_halos.csv` with reference points and 1-3 halo coordinates per sky.\n  * **Test Data:** 120 skies with galaxy data (same format as training) and `Test_haloCounts.csv` specifying halo counts (1-3) per sky.\n* **Key Features:**\n  * Spatial coordinates (x,y) of galaxies (0-4200 pixel range)\n  * Ellipticity components (e1, e2) quantifying galaxy shape distortions\n  * Reference points for angular bias calculation in evaluation",
      "Evaluation Metrics": "* **Primary Metric:** Custom composite score 𝑚 = 𝐹/1000 + 𝐺\n  * **Component F:** Average radial distance (in pixels) between predicted and true halo positions, optimally matched per sky.\n  * **Component G:** Directional bias metric calculated as:\n    ```math\n    G = \\sqrt{\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\cos(\\phi_i)\\right)^2 + \\left(\\frac{1}{N}\\sum_{j=1}^{N}\\sin(\\phi_j)\\right)^2}\n    ```\n    where 𝜙 is the angle between predicted position and reference line (lower G indicates less bias).\n* **"
    },
    "file_path": "kaggle_datasets/58/problem_summary.md"
  },
  "557": {
    "problem_id": "557",
    "title": "Predicting Crab Age with Physical Attributes",
    "problem_type": "Regression",
    "objective": "Predict the age of crabs based on their physical attributes. The goal is to develop a regression model that accurately estimates crab age using provided features.",
    "evaluation_metric": null,
    "full_content": "# Predicting Crab Age with Physical Attributes\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the age of crabs based on their physical attributes. The goal is to develop a regression model that accurately estimates crab age using provided features.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world crab age prediction data.\n  * Participants are encouraged to explore using the original dataset to potentially improve model performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing physical measurements of crabs.\n* **Data Files:**\n  * `train.csv` - Training data with target variable `Age`\n  * `test.csv` - Test data for which predictions must be made\n  * `sample_submission.csv` - Example submission file format\n* **Features:** Physical attributes of crabs (exact features not specified, but similar to original dataset which likely includes measurements like shell dimensions, weight, etc.)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)\n* **Metric Components:**\n  * Calculated as the average absolute difference between predicted and actual ages\n  * Formula: MAE = (1/n) * Σ|y_i - x_i| where:\n    * y_i = ground truth age\n    * x_i = predicted age\n    * n = number of samples in test set",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the age of crabs based on their physical attributes. The goal is to develop a regression model that accurately estimates crab age using provided features.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world crab age prediction data.\n  * Participants are encouraged to explore using the original dataset to potentially improve model performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing physical measurements of crabs.\n* **Data Files:**\n  * `train.csv` - Training data with target variable `Age`\n  * `test.csv` - Test data for which predictions must be made\n  * `sample_submission.csv` - Example submission file format\n* **Features:** Physical attributes of crabs (exact features not specified, but similar to original dataset which likely includes measurements like shell dimensions, weight, etc.)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)\n* **Metric Components:**\n  * Calculated as the average absolute difference between predicted and actual ages\n  * Formula: MAE = (1/n) * Σ|y_i - x_i| where:\n    * y_i = ground truth age\n    * x_i = predicted age\n    * n = number of samples in test set"
    },
    "file_path": "kaggle_datasets/557/problem_summary.md"
  },
  "133": {
    "problem_id": "133",
    "title": "Binary Classification with Synthetic Dataset using Scikit-learn",
    "problem_type": "Binary Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Synthetic Dataset using Scikit-learn\n\n**Problem Description:**\n* **Problem Type:** Binary Classification  \n* **Objective:**  \n  * Predict binary class labels (0 or 1) for a synthetic dataset using Scikit-learn.  \n  * The competition serves as a practice ground for exploring Scikit-learn's classification capabilities, encouraging knowledge sharing and experimentation.  \n* **Key Points:**  \n  * Focus on learning/experimenting with Scikit-learn rather than achieving high accuracy.  \n  * Participants are encouraged to share code and approaches via tutorials.  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  * Synthetic tabular data with 40 anonymized numerical features representing objects from two classes.  \n* **Data Files:**  \n  * `train.csv` (1000 samples)  \n  * `test.csv` (9000 samples)  \n  * `trainLabels.csv` (labels for training set)  \n* **Features:**  \n  * 40 numerical features (no additional context provided).  \n  * Labels: Binary (0 or 1).  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Classification Accuracy  \n  * Percentage of correctly predicted labels on the test set.  \n* **Submission Format:**  \n  * CSV file with columns: `Id` (1 to 9000) and `Solution` (0 or 1).  \n  * Example:  \n    ```csv\n    Id,Solution\n    1,0\n    2,1\n    ...\n    9000,0\n    ",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification  \n* **Objective:**  \n  * Predict binary class labels (0 or 1) for a synthetic dataset using Scikit-learn.  \n  * The competition serves as a practice ground for exploring Scikit-learn's classification capabilities, encouraging knowledge sharing and experimentation.  \n* **Key Points:**  \n  * Focus on learning/experimenting with Scikit-learn rather than achieving high accuracy.  \n  * Participants are encouraged to share code and approaches via tutorials.",
      "Dataset Overview": "* **Data Type & Context:**  \n  * Synthetic tabular data with 40 anonymized numerical features representing objects from two classes.  \n* **Data Files:**  \n  * `train.csv` (1000 samples)  \n  * `test.csv` (9000 samples)  \n  * `trainLabels.csv` (labels for training set)  \n* **Features:**  \n  * 40 numerical features (no additional context provided).  \n  * Labels: Binary (0 or 1).",
      "Evaluation Metrics": "* **Primary Metric:** Classification Accuracy  \n  * Percentage of correctly predicted labels on the test set.  \n* **Submission Format:**  \n  * CSV file with columns: `Id` (1 to 9000) and `Solution` (0 or 1).  \n  * Example:  \n    ```csv\n    Id,Solution\n    1,0\n    2,1\n    ...\n    9000,0"
    },
    "file_path": "kaggle_datasets/133/problem_summary.md"
  },
  "301": {
    "problem_id": "301",
    "title": "PUBG Finish Placement Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# PUBG Finish Placement Prediction\n\n## Problem Description\n* **Problem Type:** Regression\n* **Objective:** Predict the final placement percentage (`winPlacePerc`) of players in PUBG matches, where 1 represents first place and 0 represents last place.\n    * **Key Points:**\n        * Predictions must be made based on anonymized in-game statistics (e.g., kills, distance traveled, items used).\n        * The dataset includes various match types (solos, duos, squads, etc.), with no fixed player count per match.\n        * The target variable is a normalized placement derived from `maxPlace` (worst recorded placement in the match).\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing post-game statistics for individual players in PUBG matches.\n* **Data Files:**\n    * `train_V2.csv` - Training set with target variable (`winPlacePerc`).\n    * `test_V2.csv` - Test set for submission.\n    * `sample_submission_V2.csv` - Example submission format.\n* **Key Features:**\n    * In-game actions: `kills`, `DBNOs` (enemies knocked), `assists`, `boosts`, `heals`, `damageDealt`.\n    * Movement metrics: `walkDistance`, `rideDistance`, `swimDistance`.\n    * Match context: `matchType`, `matchDuration`, `numGroups`, `maxPlace`.\n    * Rankings: `killPlace`, `killPoints`, `winPoints`.\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE) between predicted and actual `winPlacePerc`.\n    * **Calculation:** \n        * MAE = average of absolute differences between predicted and true values.\n        * Lower values indicate better performance (closer to true placements).",
    "sections": {},
    "file_path": "kaggle_datasets/301/problem_summary.md"
  },
  "67": {
    "problem_id": "67",
    "title": "Job Salary Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Job Salary Prediction\n\n## Problem Description\n* **Problem Type**: Regression\n* **Objective**: Predict the normalized annual salary of UK job ads based on their textual content and structured metadata. The goal is to improve transparency in the job market by estimating salaries for ads where they are not publicly disclosed.\n* **Key Points**:\n  * The dataset is mostly unstructured text (job titles, descriptions) with some structured fields (location, contract type, etc.).\n  * Data is noisy due to real-world variations (e.g., incorrect salaries, duplicate ads, non-UK ads).\n  * Models should analyze the impact of keywords/phrases and leverage structured fields like location and company.\n\n## Dataset Overview\n* **Data Type**: Tabular data with mixed text and categorical features.\n* **Context**: UK job ads scraped from multiple sources, with salary information and metadata.\n* **Data Files**:\n  * `Train_rev1.zip` (training data with salary labels)\n  * `Valid_rev1.csv` (validation set for public leaderboard)\n  * `Test_rev1.zip` (final test set without salary labels)\n  * `Location_Tree.csv` (supplemental hierarchical location data)\n* **Features**:\n  * Text fields: `Title`, `FullDescription` (salary-related numbers redacted)\n  * Categorical/metadata: `LocationRaw`, `LocationNormalized`, `ContractType`, `ContractTime`, `Company`, `Category`, `SourceName`\n  * Target: `SalaryNormalized` (annualized midpoint salary)\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE)\n  * Measures the average absolute difference between predicted and actual salaries.\n  * Lower values indicate better performance (closer to true salaries).\n* **Submission Format**:\n  * CSV with columns: `Id` (job ad ID), `SalaryNormalized` (predicted salary).",
    "sections": {},
    "file_path": "kaggle_datasets/67/problem_summary.md"
  },
  "93": {
    "problem_id": "93",
    "title": "Personalized Hotel Ranking for Expedia Searches",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Personalized Hotel Ranking for Expedia Searches\n\n## Problem Description\n* **Problem Type**: Learning to Rank (LTR) - Recommendation System\n* **Objective**: Develop a model to rank hotels in Expedia's search results to maximize user purchases and clicks. The goal is to personalize the order of hotels displayed to users based on their likelihood to book or click.\n    * **Key Points**:\n        * Must handle position bias (some results were randomly sorted for unbiased training).\n        * Integrate price competitiveness data from competitors.\n        * Consider user history, hotel characteristics, and location attractiveness.\n\n## Dataset Overview\n* **Data Type**: Tabular data with search impressions, user interactions, and hotel attributes.\n* **Context**: Expedia hotel search logs with user behavior (clicks/purchases) and competitive pricing data.\n* **Data Files**:\n    * `train.csv`: Contains search impressions with user actions (clicks/purchases).\n    * `test.csv`: Search impressions without user actions (for prediction).\n* **Key Features**:\n    * User/hotel metadata (`visitor_hist_starrating`, `prop_starrating`).\n    * Location scores (`prop_location_score1/2`).\n    * Price/promotion data (`price_usd`, `promotion_flag`).\n    * Competitor pricing (`comp1_rate` to `comp8_rate_percent_diff`).\n    * Search context (`srch_length_of_stay`, `srch_children_count`).\n\n## Evaluation Metrics\n* **Evaluation Metric**: Normalized Discounted Cumulative Gain (NDCG@38).\n    * **Components**:\n        * Relevance grades:\n            * **5**: Hotel purchased.\n            * **1**: Hotel clicked (no purchase).\n            * **0**: No interaction.\n        * NDCG is calculated per search query (`srch_id`) and averaged across all queries.\n        * Focuses on top-38 ranked results (position-aware metric).",
    "sections": {},
    "file_path": "kaggle_datasets/93/problem_summary.md"
  },
  "568": {
    "problem_id": "568",
    "title": "Binary Classification of Age-Related Medical Conditions",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Age-Related Medical Conditions\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a person has one or more of three age-related medical conditions (Class 1) or none (Class 0) based on anonymized health characteristics. The goal is to improve upon existing models (like XGBoost and random forest) for reliable medical predictions.\n    * **Key Points:**\n        * Focus on bioinformatics applications for aging-related condition detection.\n        * Requires handling of small sample sizes and diverse data.\n        * Privacy-preserving approach by using encoded health characteristics instead of intrusive patient data collection.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing anonymized health measurements linked to age-related conditions.\n    * **Data Files:**\n        * `train.csv`: Contains `Id`, 56 anonymized features (55 numeric + 1 categorical `EJ`), and binary `Class` target.\n        * `test.csv`: Test set with the same features (excluding `Class`).\n        * `greeks.csv`: Supplemental training metadata, including condition type (`Alpha`), experimental characteristics (`Beta`, `Gamma`, `Delta`), and collection date (`Epsilon`).\n        * `sample_submission.csv`: Example submission file with `Id` and predicted probabilities for both classes.\n    * **Features:** \n        * Primary features are anonymized (e.g., `AB`-`GL`), mostly numerical except for `EJ` (categorical).\n        * Metadata in `greeks.csv` provides additional context but is only available for training data.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Balanced Logarithmic Loss\n    * **Components:**\n        * Logarithmic loss is calculated separately for each class (Class 0 and Class 1) and averaged to ensure balanced importance.\n        * Formula:  \n          ```\n          Log Loss = −(1/N0 * Σ[y0i log(p0i)] + 1/N1 * Σ[y1i log(p1i)]) / 2\n          ```\n          Where:\n          * `Nc` = number of observations in class `c`.\n          * `yci` = 1 if observation `i` is in class `c`, else 0.\n          * `pci` = predicted probability for observation `i` in class `c`.\n        * Probabilities are clipped to `[10^-15,",
    "sections": {},
    "file_path": "kaggle_datasets/568/problem_summary.md"
  },
  "306": {
    "problem_id": "306",
    "title": "Humpback Whale Identification from Tail Images",
    "problem_type": "Multi-class Image Classification (with an open-set recognition component)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Humpback Whale Identification from Tail Images\n\n**Problem Description:**\n* **Problem Type:** Multi-class Image Classification (with an open-set recognition component)\n* **Objective:**  \n    * Build an algorithm to identify individual humpback whales from images of their tails (flukes). \n    * The model must classify test images into one of the known whale IDs from the training set or predict \"new_whale\" for previously unseen individuals.\n* **Key Points:**\n    * The challenge involves fine-grained recognition with high intra-class variation (same whale's tail in different poses/conditions) and low inter-class variation (similar markings across different whales).\n    * The dataset exhibits a long-tail distribution with few examples per whale ID (3,000+ classes with limited samples each).\n    * Conservation application: Automating manual identification work done by marine researchers for population tracking.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Image dataset containing photographs of humpback whale flukes (tails) collected by research institutions and public contributors.\n    * Each image is labeled with a unique whale ID when known.\n* **Data Files:**\n    * `train.zip`: Folder containing training images (~25,000 images)\n    * `train.csv`: Mapping of training image filenames to whale IDs (with \"new_whale\" for unknown individuals)\n    * `test.zip`: Folder containing test images for prediction\n    * `sample_submission.csv`: Example submission file format\n* **Key Features:**\n    * Images show flukes with natural variations in angle, lighting, water conditions, and partial visibility.\n    * Primary identifying features are natural markings/scar patterns on flukes.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision @ 5 (MAP@5)\n* **Metric Components:**\n    * For each test image, predictions are evaluated based on the top 5 predicted whale IDs.\n    * Precision is calculated at each cutoff rank k (from 1 to 5), considering only correct unique predictions.\n    * A correct prediction at rank k gives full credit for that prediction, and subsequent predictions of the same ID are ignored.\n    * The final score averages this precision across all test images.\n    * Example: Predicting the correct ID in any of the top 5 positions (without duplicate predictions) yields perfect precision for that image.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Image Classification (with an open-set recognition component)\n* **Objective:**  \n    * Build an algorithm to identify individual humpback whales from images of their tails (flukes). \n    * The model must classify test images into one of the known whale IDs from the training set or predict \"new_whale\" for previously unseen individuals.\n* **Key Points:**\n    * The challenge involves fine-grained recognition with high intra-class variation (same whale's tail in different poses/conditions) and low inter-class variation (similar markings across different whales).\n    * The dataset exhibits a long-tail distribution with few examples per whale ID (3,000+ classes with limited samples each).\n    * Conservation application: Automating manual identification work done by marine researchers for population tracking.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Image dataset containing photographs of humpback whale flukes (tails) collected by research institutions and public contributors.\n    * Each image is labeled with a unique whale ID when known.\n* **Data Files:**\n    * `train.zip`: Folder containing training images (~25,000 images)\n    * `train.csv`: Mapping of training image filenames to whale IDs (with \"new_whale\" for unknown individuals)\n    * `test.zip`: Folder containing test images for prediction\n    * `sample_submission.csv`: Example submission file format\n* **Key Features:**\n    * Images show flukes with natural variations in angle, lighting, water conditions, and partial visibility.\n    * Primary identifying features are natural markings/scar patterns on flukes.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision @ 5 (MAP@5)\n* **Metric Components:**\n    * For each test image, predictions are evaluated based on the top 5 predicted whale IDs.\n    * Precision is calculated at each cutoff rank k (from 1 to 5), considering only correct unique predictions.\n    * A correct prediction at rank k gives full credit for that prediction, and subsequent predictions of the same ID are ignored.\n    * The final score averages this precision across all test images.\n    * Example: Predicting the correct ID in any of the top 5 positions (without duplicate predictions) yields perfect precision for that image."
    },
    "file_path": "kaggle_datasets/306/problem_summary.md"
  },
  "134": {
    "problem_id": "134",
    "title": "Toy Production Scheduling for Santa's Workshop",
    "problem_type": "Time Series Forecasting / Optimization",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Toy Production Scheduling for Santa's Workshop\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting / Optimization\n* **Objective:**  \n    * Participants are tasked with scheduling the production of toys in Santa's workshop to minimize the total time taken to complete all toy orders. The challenge involves efficiently assigning elves to build toys based on arrival times and durations while considering workshop constraints.\n* **Key Points:**\n    * Must handle 10 million toy orders with varying arrival times and production durations\n    * Requires optimization of elf workforce scheduling\n    * Time-based constraints must be considered (arrival times and production durations)\n\n**Dataset Overview:**\n* **Data Type:** Tabular time-series data\n* **Context:** Santa's workshop toy production orders\n* **Data Files:**\n    * `toys_rev2`: Contains list of 10 million toys to be built\n    * `sampleSubmission_rev2`: Example submission file\n* **Features:**\n    * `ToyId`: Unique identifier for each toy (1-10,000,000)\n    * `Arrival_time`: When toy order arrives at workshop (YYYY MM DD HH MM format)\n    * `Duration`: Production time required for each toy (in minutes)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Custom optimization score\n    * The evaluation metric is designed to minimize the total time taken to complete all toy production\n    * The exact calculation involves:\n        * Tracking when each toy is completed\n        * Considering elf productivity and scheduling efficiency\n        * Penalizing late completions or inefficient scheduling\n* **Note:** The competition provides sample code for the evaluation metric to help participants understand the scoring mechanism",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting / Optimization\n* **Objective:**  \n    * Participants are tasked with scheduling the production of toys in Santa's workshop to minimize the total time taken to complete all toy orders. The challenge involves efficiently assigning elves to build toys based on arrival times and durations while considering workshop constraints.\n* **Key Points:**\n    * Must handle 10 million toy orders with varying arrival times and production durations\n    * Requires optimization of elf workforce scheduling\n    * Time-based constraints must be considered (arrival times and production durations)",
      "Dataset Overview": "* **Data Type:** Tabular time-series data\n* **Context:** Santa's workshop toy production orders\n* **Data Files:**\n    * `toys_rev2`: Contains list of 10 million toys to be built\n    * `sampleSubmission_rev2`: Example submission file\n* **Features:**\n    * `ToyId`: Unique identifier for each toy (1-10,000,000)\n    * `Arrival_time`: When toy order arrives at workshop (YYYY MM DD HH MM format)\n    * `Duration`: Production time required for each toy (in minutes)",
      "Evaluation Metrics": "* **Primary Metric:** Custom optimization score\n    * The evaluation metric is designed to minimize the total time taken to complete all toy production\n    * The exact calculation involves:\n        * Tracking when each toy is completed\n        * Considering elf productivity and scheduling efficiency\n        * Penalizing late completions or inefficient scheduling\n* **Note:** The competition provides sample code for the evaluation metric to help participants understand the scoring mechanism"
    },
    "file_path": "kaggle_datasets/134/problem_summary.md"
  },
  "550": {
    "problem_id": "550",
    "title": "Predicting Wild Blueberry Yield with Regression",
    "problem_type": "Regression",
    "objective": "Predict the yield of wild blueberries based on various features. The task involves building a regression model that accurately estimates the yield quantity.",
    "evaluation_metric": null,
    "full_content": "# Predicting Wild Blueberry Yield with Regression\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the yield of wild blueberries based on various features. The task involves building a regression model that accurately estimates the yield quantity.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world wild blueberry yield data.\n  * Participants are encouraged to explore differences between the synthetic and original datasets.\n  * Incorporating the original dataset in training may improve model performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing features related to wild blueberry yield prediction.\n* **Data Files:**\n  * `train.csv` - Training dataset with target variable `yield`\n  * `test.csv` - Test dataset for making predictions\n  * `sample_submission.csv` - Example submission file format\n* **Features:** The dataset contains 37 columns (features) related to blueberry yield prediction (specific features not detailed in provided context).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)\n* **Metric Components:**\n  * Calculated as the average of absolute differences between predicted values and actual observations\n  * Formula: MAE = (1/n) * Σ|y_i - x_i| where:\n    * y_i = ground truth value\n    * x_i = predicted value\n    * n = number of observations in test set",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the yield of wild blueberries based on various features. The task involves building a regression model that accurately estimates the yield quantity.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world wild blueberry yield data.\n  * Participants are encouraged to explore differences between the synthetic and original datasets.\n  * Incorporating the original dataset in training may improve model performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing features related to wild blueberry yield prediction.\n* **Data Files:**\n  * `train.csv` - Training dataset with target variable `yield`\n  * `test.csv` - Test dataset for making predictions\n  * `sample_submission.csv` - Example submission file format\n* **Features:** The dataset contains 37 columns (features) related to blueberry yield prediction (specific features not detailed in provided context).",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)\n* **Metric Components:**\n  * Calculated as the average of absolute differences between predicted values and actual observations\n  * Formula: MAE = (1/n) * Σ|y_i - x_i| where:\n    * y_i = ground truth value\n    * x_i = predicted value\n    * n = number of observations in test set"
    },
    "file_path": "kaggle_datasets/550/problem_summary.md"
  },
  "94": {
    "problem_id": "94",
    "title": "Stock Price Direction Prediction with Time Series Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Stock Price Direction Prediction with Time Series Data\n\n## Problem Description\n* **Problem Type:** Binary Classification (Directional Prediction)\n* **Objective:** Predict the probability that a stock's price will increase from its opening price on day 10 to its closing price on day 10, based on historical price and volume data.\n    * Key Points:\n        * Predictions must be probabilities between 0 (certain decrease) and 1 (certain increase)\n        * Test set is sampled from a non-overlapping time period following the training data\n        * Cases where opening and closing prices are equal are excluded from evaluation\n        * Competition emphasizes outperforming random chance (\"monkey with a dart\")\n\n## Dataset Overview\n* **Data Type & Context:** Time series data of stock prices and trading volumes\n    * **Data Files:**\n        * train.csv - Contains 94 stocks with 500 days of historical data per stock\n        * test.csv - Contains 25 segments of data for the same 94 stocks, each with 9 days of history plus day 10 opening price\n    * **Key Features:**\n        * Normalized price data (relative to first day opening price)\n        * Daily features: Opening price, Maximum price, Minimum price, Closing price, Trading volume\n        * Data excludes non-trading days (holidays, weekends)\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Implementation details provided for:\n        * MATLAB (stats toolbox)\n        * R (verification package)\n        * Python (scikit-learn metrics module)\n* **Leaderboard Composition:**\n    * Public leaderboard based on 10 segments of test data\n    * Private leaderboard based on 15 additional segments\n    * Final prize awards determined by private leaderboard performance",
    "sections": {},
    "file_path": "kaggle_datasets/94/problem_summary.md"
  },
  "339": {
    "problem_id": "339",
    "title": "Cellular Image Classification for Genetic Perturbation Identification",
    "problem_type": "Multi-class Classification (Computer Vision)",
    "objective": "Classify microscope images of cells under genetic perturbations (siRNAs) into one of 1,108 possible classes. The core challenge is distinguishing true biological signals from experimental noise introduced by technical variations across batches, plates, and imaging conditions.",
    "evaluation_metric": null,
    "full_content": "# Cellular Image Classification for Genetic Perturbation Identification\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Computer Vision)\n* **Objective:** Classify microscope images of cells under genetic perturbations (siRNAs) into one of 1,108 possible classes. The core challenge is distinguishing true biological signals from experimental noise introduced by technical variations across batches, plates, and imaging conditions.\n    * **Key Points:**\n        * Focus on robustness to experimental noise (batch effects, plate variations, imaging conditions)\n        * Predict siRNA targets from cellular images across multiple cell lines\n        * Part of NeurIPS 2019 competition track with real-world drug discovery applications\n\n**Dataset Overview:**\n* **Data Type:** Microscopy images of human cells (6-channel) with tabular metadata\n* **Context:** Pharmaceutical research dataset generated by Recursion Pharmaceuticals to model drug-cell interactions\n* **Data Files:**\n    * `train.zip`/`test.zip` - Image files organized by experiment/plate/well\n    * `train.csv`/`test.csv` - Metadata including experiment, plate, well, and siRNA target\n    * `*_controls.csv` - Control samples with well_type annotations\n    * `pixel_stats.csv` - Per-channel image statistics\n* **Features:**\n    * Images captured at 2 sites per well across 6 microscope channels\n    * 51 experimental batches across multiple cell lines\n    * Each plate contains 308 wells with various siRNA perturbations\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multiclass Accuracy\n    * Simple average of correct siRNA predictions across all test samples\n    * Directly measures model's ability to identify genetic perturbations despite experimental noise",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Computer Vision)\n* **Objective:** Classify microscope images of cells under genetic perturbations (siRNAs) into one of 1,108 possible classes. The core challenge is distinguishing true biological signals from experimental noise introduced by technical variations across batches, plates, and imaging conditions.\n    * **Key Points:**\n        * Focus on robustness to experimental noise (batch effects, plate variations, imaging conditions)\n        * Predict siRNA targets from cellular images across multiple cell lines\n        * Part of NeurIPS 2019 competition track with real-world drug discovery applications",
      "Dataset Overview": "* **Data Type:** Microscopy images of human cells (6-channel) with tabular metadata\n* **Context:** Pharmaceutical research dataset generated by Recursion Pharmaceuticals to model drug-cell interactions\n* **Data Files:**\n    * `train.zip`/`test.zip` - Image files organized by experiment/plate/well\n    * `train.csv`/`test.csv` - Metadata including experiment, plate, well, and siRNA target\n    * `*_controls.csv` - Control samples with well_type annotations\n    * `pixel_stats.csv` - Per-channel image statistics\n* **Features:**\n    * Images captured at 2 sites per well across 6 microscope channels\n    * 51 experimental batches across multiple cell lines\n    * Each plate contains 308 wells with various siRNA perturbations",
      "Evaluation Metrics": "* **Primary Metric:** Multiclass Accuracy\n    * Simple average of correct siRNA predictions across all test samples\n    * Directly measures model's ability to identify genetic perturbations despite experimental noise"
    },
    "file_path": "kaggle_datasets/339/problem_summary.md"
  },
  "60": {
    "problem_id": "60",
    "title": "Visualizing Trends in Colorado Public School Performance",
    "problem_type": "Data Visualization & Exploratory Analysis",
    "objective": "Create compelling visualizations to uncover trends and insights in Colorado public school performance data across three academic years (2009-2012). The competition focuses on translating complex educational metrics into accessible visual narratives.",
    "evaluation_metric": null,
    "full_content": "# Visualizing Trends in Colorado Public School Performance\n\n**Problem Description:**\n* **Problem Type:** Data Visualization & Exploratory Analysis\n* **Objective:** Create compelling visualizations to uncover trends and insights in Colorado public school performance data across three academic years (2009-2012). The competition focuses on translating complex educational metrics into accessible visual narratives.\n* **Key Points:**\n  * Analyze school grading trends over time (A-F scale)\n  * Investigate equity questions regarding access to high-performing schools\n  * Explore correlations between school performance and student demographics\n  * Identify schools excelling in academic growth metrics\n  * Compare performance across districts and school types (Elementary/Middle/High)\n  * Examine college/career readiness indicators\n\n**Dataset Overview:**\n* **Data Type:** Tabular educational performance data with geographic components\n* **Context:** Three years of school grading data from Colorado Department of Education, including:\n  * Academic achievement metrics\n  * Student growth percentiles\n  * Demographic breakdowns\n  * College readiness indicators\n* **Data Files:**\n  * Annual files (2010, 2011, 2012) containing:\n    * Final grades (main dataset)\n    * Year-over-year changes\n    * Enrollment demographics\n    * Free/reduced lunch eligibility\n    * College readiness indicators\n    * School location data (addresses and GPS coordinates)\n* **Key Features:**\n  * School performance grades (A+ to F)\n  * Subject-specific achievement (reading, math, science, writing)\n  * Growth metrics (overall and by subject)\n  * Student demographics (race/ethnicity, income proxies)\n  * Geographic information (district, coordinates)\n  * College readiness indicators (ACT benchmarks)\n\n**Evaluation Metrics:**\n* **Primary Evaluation:** Qualitative assessment of visualization quality by expert judges\n* **Evaluation Components:**\n  * Accuracy of data portrayal\n  * Effectiveness in distilling complex information\n  * Adherence to visualization best practices\n  * Narrative quality and ability to drive insights\n  * Contextual awareness of educational equity issues\n  * Creativity in identifying novel trends",
    "sections": {
      "Problem Description": "* **Problem Type:** Data Visualization & Exploratory Analysis\n* **Objective:** Create compelling visualizations to uncover trends and insights in Colorado public school performance data across three academic years (2009-2012). The competition focuses on translating complex educational metrics into accessible visual narratives.\n* **Key Points:**\n  * Analyze school grading trends over time (A-F scale)\n  * Investigate equity questions regarding access to high-performing schools\n  * Explore correlations between school performance and student demographics\n  * Identify schools excelling in academic growth metrics\n  * Compare performance across districts and school types (Elementary/Middle/High)\n  * Examine college/career readiness indicators",
      "Dataset Overview": "* **Data Type:** Tabular educational performance data with geographic components\n* **Context:** Three years of school grading data from Colorado Department of Education, including:\n  * Academic achievement metrics\n  * Student growth percentiles\n  * Demographic breakdowns\n  * College readiness indicators\n* **Data Files:**\n  * Annual files (2010, 2011, 2012) containing:\n    * Final grades (main dataset)\n    * Year-over-year changes\n    * Enrollment demographics\n    * Free/reduced lunch eligibility\n    * College readiness indicators\n    * School location data (addresses and GPS coordinates)\n* **Key Features:**\n  * School performance grades (A+ to F)\n  * Subject-specific achievement (reading, math, science, writing)\n  * Growth metrics (overall and by subject)\n  * Student demographics (race/ethnicity, income proxies)\n  * Geographic information (district, coordinates)\n  * College readiness indicators (ACT benchmarks)",
      "Evaluation Metrics": "* **Primary Evaluation:** Qualitative assessment of visualization quality by expert judges\n* **Evaluation Components:**\n  * Accuracy of data portrayal\n  * Effectiveness in distilling complex information\n  * Adherence to visualization best practices\n  * Narrative quality and ability to drive insights\n  * Contextual awareness of educational equity issues\n  * Creativity in identifying novel trends"
    },
    "file_path": "kaggle_datasets/60/problem_summary.md"
  },
  "399": {
    "problem_id": "399",
    "title": "Birdcall Identification in Soundscape Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Birdcall Identification in Soundscape Recordings\n\n## Problem Description\n* **Problem Type**: Audio Classification (Multi-label Classification)\n* **Objective**: Identify bird species vocalizing in long soundscape recordings, where:\n    * Recordings may contain multiple bird species calling simultaneously\n    * Background noise (anthropogenic sounds, other animals) is present\n    * Training data consists of short, clean recordings of individual birds (domain mismatch challenge)\n* **Key Points**:\n    * Weak labels in soundscape recordings (primary species labeled despite background noise)\n    * Three test sites with different labeling granularities:\n        * Sites 1-2: 5-second time windows\n        * Site 3: Whole-file labels\n    * Conservation application: Monitoring habitat quality/pollution via bird populations\n\n## Dataset Overview\n* **Data Type**: Audio recordings (MP3) with metadata (CSV)\n* **Context**: \n    * Training: Curated individual bird calls from xeno-canto.org\n    * Test: 10-minute field recordings from 3 North American sites\n* **Data Files**:\n    * `train_audio/`: Short individual bird call recordings\n    * `test_audio/`: Long soundscape recordings (hidden test set)\n    * `train.csv`: Metadata including ebird_code, location, date\n    * `test.csv`: Site info and time windows (partial download)\n* **Key Features**:\n    * Audio files vary in quality/background noise\n    * Metadata includes geographic/temporal recording details\n    * 264 bird species in training set\n\n## Evaluation Metrics\n* **Primary Metric**: Row-wise micro-averaged F1 score\n* **Submission Format**:\n    * For each time window, predict space-separated list of present bird ebird_codes\n    * Use \"nocall\" for windows with no bird vocalizations\n    * Example:\n        ```\n        row_id,birds\n        site_1_0a997dff022e3ad9744d4e7bbf923288_5,amecro\n        site_1_0a997dff022e3ad9744d4e7bbf923288_10,amecro amerob\n        site_1_0a997dff022e3ad9744d4e7bbf923288_15,nocall\n        ",
    "sections": {},
    "file_path": "kaggle_datasets/399/problem_summary.md"
  },
  "34": {
    "problem_id": "34",
    "title": "Predicting Ad Click-Through Rates with Search Session Data",
    "problem_type": "Binary Classification (with probability prediction)",
    "objective": "Predict the probability of a user clicking on a search ad (pCTR) given query and user context. The core task is to model user-ad interaction likelihood based on session logs from the Tencent search engine (soso.com).",
    "evaluation_metric": null,
    "full_content": "# Predicting Ad Click-Through Rates with Search Session Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with probability prediction)\n* **Objective:** Predict the probability of a user clicking on a search ad (pCTR) given query and user context. The core task is to model user-ad interaction likelihood based on session logs from the Tencent search engine (soso.com).\n    * **Key Points:**\n        * Requires handling aggregated session data where each instance represents an ad impression under specific conditions (depth, position)\n        * Must incorporate multiple data sources (user profiles, ad content, query context)\n        * Temporal aspect: Testing data comes from a later time period than training data\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data derived from search engine session logs, with additional text and profile data\n* **Key Data Files:**\n    * `training.txt`: Main dataset with impression/click counts and feature IDs (9.9GB)\n    * 5 auxiliary files mapping IDs to features:\n        * `queryid_tokensid.txt` (704MB)\n        * `purchasedkeywordid_tokensid.txt` (26MB)\n        * `titleid_tokensid.txt` (172MB)\n        * `descriptionid_tokensid.txt` (268MB)\n        * `userid_profile.txt` (284MB)\n* **Important Features:**\n    * Core features: UserID, AdID, Query, ImpressionDepth, AdPosition\n    * Content features: Hashed DisplayURL, Title tokens, Description tokens\n    * Contextual features: Query tokens, Purchased keywords\n    * User demographics: Gender, Age brackets (anonymized)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Interpretation:** Measures ranking quality - probability that a random clicked ad is ranked higher than a random non-clicked ad\n    * **Implementation:** Uses Algorithm 3 from Fawcett's ROC analysis paper\n    * **Scoring Protocol:**\n        * Validation set used for leaderboard during competition\n        * Final testing set used only for winner selection",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with probability prediction)\n* **Objective:** Predict the probability of a user clicking on a search ad (pCTR) given query and user context. The core task is to model user-ad interaction likelihood based on session logs from the Tencent search engine (soso.com).\n    * **Key Points:**\n        * Requires handling aggregated session data where each instance represents an ad impression under specific conditions (depth, position)\n        * Must incorporate multiple data sources (user profiles, ad content, query context)\n        * Temporal aspect: Testing data comes from a later time period than training data",
      "Dataset Overview": "* **Data Type & Context:** Tabular data derived from search engine session logs, with additional text and profile data\n* **Key Data Files:**\n    * `training.txt`: Main dataset with impression/click counts and feature IDs (9.9GB)\n    * 5 auxiliary files mapping IDs to features:\n        * `queryid_tokensid.txt` (704MB)\n        * `purchasedkeywordid_tokensid.txt` (26MB)\n        * `titleid_tokensid.txt` (172MB)\n        * `descriptionid_tokensid.txt` (268MB)\n        * `userid_profile.txt` (284MB)\n* **Important Features:**\n    * Core features: UserID, AdID, Query, ImpressionDepth, AdPosition\n    * Content features: Hashed DisplayURL, Title tokens, Description tokens\n    * Contextual features: Query tokens, Purchased keywords\n    * User demographics: Gender, Age brackets (anonymized)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Interpretation:** Measures ranking quality - probability that a random clicked ad is ranked higher than a random non-clicked ad\n    * **Implementation:** Uses Algorithm 3 from Fawcett's ROC analysis paper\n    * **Scoring Protocol:**\n        * Validation set used for leaderboard during competition\n        * Final testing set used only for winner selection"
    },
    "file_path": "kaggle_datasets/34/problem_summary.md"
  },
  "160": {
    "problem_id": "160",
    "title": "Diabetic Retinopathy Detection from Retinal Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Diabetic Retinopathy Detection from Retinal Images\n\n## Problem Description\n- **Problem Type**: Multi-class Classification (Ordinal)\n- **Objective**: Develop an automated system to detect and classify diabetic retinopathy (DR) in retinal images on a severity scale from 0 to 4. The goal is to replicate clinician assessments of DR presence and severity to enable scalable screening.\n- **Key Points**:\n  - Focus on clinical applicability: Models should handle real-world variability in image quality and artifacts\n  - Address class imbalance: DR progression stages have uneven distribution in populations\n  - Handle image variations: Images come from different camera models with potential inversion artifacts\n  - Five severity classes: 0 (No DR), 1 (Mild), 2 (Moderate), 3 (Severe), 4 (Proliferative DR)\n\n## Dataset Overview\n- **Data Type**: High-resolution retinal images (color fundus photographs)\n- **Context**: Medical imaging for diabetes complication screening\n- **Data Files**:\n  - train.zip (training images)\n  - test.zip (test images)\n  - trainLabels.csv (severity labels for training set)\n  - sampleSubmission.csv (submission format example)\n- **Features**:\n  - Images show retinal vasculature with potential DR lesions\n  - Each image labeled with patient ID and eye position (left/right)\n  - Images vary in quality (focus, exposure, artifacts)\n  - Some images are anatomically inverted\n\n## Evaluation Metrics\n- **Primary Metric**: Quadratic Weighted Kappa\n- **Metric Components**:\n  - Measures agreement between predicted and human rater scores (0-4 scale)\n  - Calculates:\n    1. Observed rating histogram matrix (O)\n    2. Expected rating matrix (E) assuming no correlation\n    3. Weight matrix (w) based on squared differences between ratings\n  - Formula: 1 - (sum(w*O)/sum(w*E))\n  - Ranges from -1 to 1, where 1 = perfect agreement\n  - Penalizes larger classification errors more severely than small ones",
    "sections": {},
    "file_path": "kaggle_datasets/160/problem_summary.md"
  },
  "352": {
    "problem_id": "352",
    "title": "Predicting Intersection Congestion with Commercial Vehicle Telematics Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Intersection Congestion with Commercial Vehicle Telematics Data\n\n## Problem Description\n* **Problem Type**: Regression (Multi-target)\n* **Objective**: Predict intersection congestion metrics in 4 major US cities (Atlanta, Boston, Chicago, Philadelphia) using aggregated commercial vehicle telematics data. The goal is to forecast six target outcomes representing different quantiles of wait times and stopping distances.\n    * **Key Points**:\n        * Predict three quantiles (20th, 50th, 80th) for two metrics:\n            * Total time stopped at intersection\n            * Distance between intersection and first stopping point\n        * Data is grouped by intersection, month, hour of day, direction, and weekend status\n        * Encourages use of external datasets for improved predictions\n        * Special awards for solutions using BigQuery ML (both SQL and TensorFlow implementations)\n\n## Dataset Overview\n* **Data Type**: Tabular data (aggregated vehicle telemetrics)\n* **Context**: Commercial fleet vehicle data collected by Geotab from semi-trucks and other commercial vehicles\n* **Data Files**:\n    * train.csv (training data with target metrics)\n    * test.csv (test data for prediction)\n    * sample_submission.csv (submission format example)\n    * submission_metric_map.json (metric ID mapping)\n* **Features**:\n    * Intersection identifiers\n    * Temporal features (month, hour, weekend flag)\n    * Direction of travel\n    * Aggregated vehicle movement metrics\n    * Optional training metric (TimeFromFirstStop) not required in test predictions\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Error (RMSE)\n    * **Calculation**: \n        * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n        * Where y_i = actual value, ŷ_i = predicted value\n    * **Implementation**:\n        * Each of the six target predictions evaluated separately\n        * All predictions concatenated into single RMSE calculation\n        * Lower values indicate better performance",
    "sections": {},
    "file_path": "kaggle_datasets/352/problem_summary.md"
  },
  "504": {
    "problem_id": "504",
    "title": "Predicting Effectiveness of Argumentative Writing Elements",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Effectiveness of Argumentative Writing Elements\n\n## Problem Description\n- **Problem Type**: Multi-class Classification (Text/NLP)\n- **Objective**: Classify argumentative elements in student writing (grades 6-12) into three quality ratings:\n  - \"Ineffective\"\n  - \"Adequate\"\n  - \"Effective\"\n- **Key Points**:\n  - Focus on minimizing bias in automated feedback for diverse student populations\n  - Two competition tracks:\n    - **Accuracy Track**: Traditional classification accuracy (leaderboard ranking)\n    - **Efficiency Track**: Balances accuracy with computational efficiency (lower carbon footprint, practical deployment)\n\n## Dataset Overview\n- **Data Type**: Text data (argumentative essays) with structured annotations\n- **Context**: Essays from U.S. students (grades 6-12) with expert-annotated discourse elements\n- **Data Files**:\n  - `train.csv`: Annotated discourse elements with quality ratings\n  - `train/`: Folder containing full essay texts (.txt files)\n  - `test.csv`: Test set annotations (without target labels)\n  - `test/`: Example test essays\n- **Key Features**:\n  - Discourse elements: Lead, Position, Claim, Counterclaim, Rebuttal, Evidence, Concluding Statement\n  - Each element has:\n    - Discourse text\n    - Type classification\n    - Effectiveness rating (target variable)\n\n## Evaluation Metrics\n- **Primary Metric**: Multi-class Logarithmic Loss (Accuracy Track)\n  - Formula: \n    ```\n    log loss = −1/N ∑(i=1 to N) ∑(j=1 to M) y_ij log(p_ij)\n    ```\n    Where:\n    - N = number of rows\n    - M = number of classes (3)\n    - y_ij = 1 if observation i is in class j, else 0\n    - p_ij = predicted probability of class j for observation i\n  - Probabilities are rescaled (row-wise) and clipped to avoid log(0)\n\n- **Efficiency Track Metric**:\n  - Combined score of accuracy and runtime:\n    ```\n    Efficiency = (1/(log3 - minLogLoss)) * LogLoss + (1/32400) * RuntimeSeconds\n    ```\n  - Lower scores indicate better efficiency (balances prediction quality with speed)",
    "sections": {},
    "file_path": "kaggle_datasets/504/problem_summary.md"
  },
  "194": {
    "problem_id": "194",
    "title": "Hotel Cluster Recommendation for Expedia Users",
    "problem_type": "Recommender Systems (Multi-class Ranking)",
    "objective": "Predict the likelihood that an Expedia user will book a specific hotel cluster from 100 possible options, based on their search behavior and contextual data. The goal is to provide personalized hotel recommendations by ranking the top 5 most probable hotel clusters for each user event.",
    "evaluation_metric": null,
    "full_content": "# Hotel Cluster Recommendation for Expedia Users\n\n**Problem Description:**\n* **Problem Type:** Recommender Systems (Multi-class Ranking)\n* **Objective:** Predict the likelihood that an Expedia user will book a specific hotel cluster from 100 possible options, based on their search behavior and contextual data. The goal is to provide personalized hotel recommendations by ranking the top 5 most probable hotel clusters for each user event.\n    * **Key Points:**\n        * Focuses on predicting hotel clusters (pre-defined groups of similar hotels) rather than individual hotels.\n        * Must handle missing data (e.g., new hotels without historical features).\n        * Training data includes both click and booking events, while test data only includes booking events.\n        * Time-based split: Training data from 2013-2014, test data from 2015.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing Expedia user search logs and booking behavior, with additional latent features from hotel reviews.\n    * **Data Files:**\n        * `train.csv`: Historical user events (searches, clicks, bookings).\n        * `test.csv`: User events to predict (booking events only).\n        * `destinations.csv`: Latent features (d1-d149) extracted from hotel reviews, linked by `srch_destination_id`.\n        * `sample_submission.csv`: Example submission format.\n    * **Key Features:**\n        * User/search context: Location, device type, booking channel, dates, occupancy details.\n        * Hotel attributes: Continent, country, market, distance from user.\n        * Behavioral flags: `is_booking`, `is_package`, `is_mobile`.\n        * Hotel cluster ID (`hotel_cluster`) as target variable.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision at 5 (MAP@5)\n    * **Components:**\n        * For each user event, precision is calculated at each cutoff point (k) in the top 5 predicted hotel clusters.\n        * Average precision is computed for each user event, then averaged across all events.\n        * Formula:  \n          𝑀𝐴𝑃@5 = (1/|U|) ∑𝑢=1..|U| ∑𝑘=1..min(5,𝑛) 𝑃(𝑘)  \n          Where |U| = number of user events, P(k) = precision at cutoff k, n = number of predicted hotel",
    "sections": {
      "Problem Description": "* **Problem Type:** Recommender Systems (Multi-class Ranking)\n* **Objective:** Predict the likelihood that an Expedia user will book a specific hotel cluster from 100 possible options, based on their search behavior and contextual data. The goal is to provide personalized hotel recommendations by ranking the top 5 most probable hotel clusters for each user event.\n    * **Key Points:**\n        * Focuses on predicting hotel clusters (pre-defined groups of similar hotels) rather than individual hotels.\n        * Must handle missing data (e.g., new hotels without historical features).\n        * Training data includes both click and booking events, while test data only includes booking events.\n        * Time-based split: Training data from 2013-2014, test data from 2015.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing Expedia user search logs and booking behavior, with additional latent features from hotel reviews.\n    * **Data Files:**\n        * `train.csv`: Historical user events (searches, clicks, bookings).\n        * `test.csv`: User events to predict (booking events only).\n        * `destinations.csv`: Latent features (d1-d149) extracted from hotel reviews, linked by `srch_destination_id`.\n        * `sample_submission.csv`: Example submission format.\n    * **Key Features:**\n        * User/search context: Location, device type, booking channel, dates, occupancy details.\n        * Hotel attributes: Continent, country, market, distance from user.\n        * Behavioral flags: `is_booking`, `is_package`, `is_mobile`.\n        * Hotel cluster ID (`hotel_cluster`) as target variable.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision at 5 (MAP@5)\n    * **Components:**\n        * For each user event, precision is calculated at each cutoff point (k) in the top 5 predicted hotel clusters.\n        * Average precision is computed for each user event, then averaged across all events.\n        * Formula:  \n          𝑀𝐴𝑃@5 = (1/|U|) ∑𝑢=1..|U| ∑𝑘=1..min(5,𝑛) 𝑃(𝑘)  \n          Where |U| = number of user events, P(k) = precision at cutoff k, n = number of predicted hotel"
    },
    "file_path": "kaggle_datasets/194/problem_summary.md"
  },
  "33": {
    "problem_id": "33",
    "title": "Automated Essay Scoring for Student-Written Essays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Automated Essay Scoring for Student-Written Essays\n\n## Problem Description\n* **Problem Type**: NLP - Text Regression (Automated Essay Scoring)\n* **Objective**: Develop an algorithm to automatically score student-written essays, matching the accuracy of human expert graders. The goal is to predict resolved human scores for essays across 8 different prompts, with varying characteristics (length, source-dependency, scoring rubrics).\n* **Key Points**:\n  * Essays range from 150-550 words, with different prompts testing different capabilities (holistic vs. trait-based scoring).\n  * Some essays are source-dependent while others are not.\n  * Must handle variability in scoring rubrics across essay sets.\n  * Phase 1 focuses on long-form essays; future phases will address short answers and mathematical reasoning.\n\n## Dataset Overview\n* **Data Type**: Text data (student essays) with associated numerical scores\n* **Context**: Essays written by students in grades 7-10, hand-graded by experts with resolved scores\n* **Data Files**:\n  * Training set (TSV/Excel): Contains essay text and human scores\n  * Validation/Test sets: Contain essay text without scores\n  * Sample submission files\n* **Key Features**:\n  * `essay`: ASCII text of student response (anonymized with NER)\n  * Multiple rater scores (`rater1_domain1`, etc.) and resolved scores (`domain1_score`)\n  * Essay sets vary in characteristics and scoring approaches\n  * For set 2 only: Additional domain scores (`domain2_score`)\n\n## Evaluation Metrics\n* **Primary Metric**: Quadratic Weighted Kappa (QWK)\n  * Measures agreement between automated scores and human resolved scores\n  * Ranges from 0 (random agreement) to 1 (complete agreement)\n* **Calculation Process**:\n  1. Construct NxN histogram matrix O of rating pairs\n  2. Calculate weight matrix w based on score differences\n  3. Compute expected rating matrix E assuming no correlation\n  4. Calculate kappa: κ = 1 - (sum(w*O)/sum(w*E))\n  5. Apply Fisher Transformation to kappa values\n  6. Take mean of transformed values (weighted 0.5 for set 2 domains)\n  7. Apply reverse transformation for final score",
    "sections": {},
    "file_path": "kaggle_datasets/33/problem_summary.md"
  },
  "158": {
    "problem_id": "158",
    "title": "Predicting Taxi Trip Destinations from Partial Trajectories",
    "problem_type": "Regression (Geospatial Prediction)",
    "objective": "Predict the final destination (latitude/longitude coordinates) of taxi trips in Porto, Portugal using partial trajectory data and contextual features. The goal is to improve taxi dispatch efficiency by anticipating where active trips will end.",
    "evaluation_metric": null,
    "full_content": "# Predicting Taxi Trip Destinations from Partial Trajectories\n\n**Problem Description:**\n* **Problem Type:** Regression (Geospatial Prediction)\n* **Objective:** Predict the final destination (latitude/longitude coordinates) of taxi trips in Porto, Portugal using partial trajectory data and contextual features. The goal is to improve taxi dispatch efficiency by anticipating where active trips will end.\n* **Key Points:**\n  * Uses initial portions of GPS trajectories (sampled every 15 seconds) to infer destinations\n  * Incorporates trip context: dispatch type (central/stand/street), taxi ID, timestamps, and day type\n  * 25% of trips have call-center metadata (anonymized caller ID) for additional pattern matching\n  * Companion competition predicts trip duration (separate evaluation)\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with spatiotemporal trajectories (GPS coordinates as strings)\n* **Data Files:**\n  * `train.csv`: Full trip records with complete trajectories (1 year of data)\n  * `test.csv`: Partial trajectories from 5 specific timestamps for evaluation\n  * `metaData_taxistandsID_name_GPSlocation.csv`: Reference data for taxi stands\n* **Features:**\n  * Trip metadata: CALL_TYPE, ORIGIN_CALL/STAND, TAXI_ID, TIMESTAMP, DAYTYPE\n  * Trajectory data: POLYLINE (WGS84 coordinate sequence as string)\n  * Data quality flag: MISSING_DATA (indicates GPS gaps)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Haversine Distance\n* **Calculation:**\n  * Computes great-circle distance between predicted and actual WGS84 coordinates\n  * Formula components:\n    * Latitude/longitude differences converted to radians\n    * Trigonometric terms account for Earth's curvature\n    * Uses Earth's radius (6371 km) for kilometer conversion\n  * Lower values indicate better performance (smaller prediction errors)",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Geospatial Prediction)\n* **Objective:** Predict the final destination (latitude/longitude coordinates) of taxi trips in Porto, Portugal using partial trajectory data and contextual features. The goal is to improve taxi dispatch efficiency by anticipating where active trips will end.\n* **Key Points:**\n  * Uses initial portions of GPS trajectories (sampled every 15 seconds) to infer destinations\n  * Incorporates trip context: dispatch type (central/stand/street), taxi ID, timestamps, and day type\n  * 25% of trips have call-center metadata (anonymized caller ID) for additional pattern matching\n  * Companion competition predicts trip duration (separate evaluation)",
      "Dataset Overview": "* **Data Type:** Tabular data with spatiotemporal trajectories (GPS coordinates as strings)\n* **Data Files:**\n  * `train.csv`: Full trip records with complete trajectories (1 year of data)\n  * `test.csv`: Partial trajectories from 5 specific timestamps for evaluation\n  * `metaData_taxistandsID_name_GPSlocation.csv`: Reference data for taxi stands\n* **Features:**\n  * Trip metadata: CALL_TYPE, ORIGIN_CALL/STAND, TAXI_ID, TIMESTAMP, DAYTYPE\n  * Trajectory data: POLYLINE (WGS84 coordinate sequence as string)\n  * Data quality flag: MISSING_DATA (indicates GPS gaps)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Haversine Distance\n* **Calculation:**\n  * Computes great-circle distance between predicted and actual WGS84 coordinates\n  * Formula components:\n    * Latitude/longitude differences converted to radians\n    * Trigonometric terms account for Earth's curvature\n    * Uses Earth's radius (6371 km) for kilometer conversion\n  * Lower values indicate better performance (smaller prediction errors)"
    },
    "file_path": "kaggle_datasets/158/problem_summary.md"
  },
  "193": {
    "problem_id": "193",
    "title": "San Francisco Crime Classification",
    "problem_type": "Multiclass Classification",
    "objective": "Predict the category of crimes that occurred in San Francisco based on time, location, and other features. The goal is to assign probabilities to each crime category for every incident in the test set.",
    "evaluation_metric": null,
    "full_content": "# San Francisco Crime Classification\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the category of crimes that occurred in San Francisco based on time, location, and other features. The goal is to assign probabilities to each crime category for every incident in the test set.\n    * **Key Points:**\n        * The dataset spans nearly 12 years of crime reports across all San Francisco neighborhoods.\n        * Participants are encouraged to explore the dataset visually for insights.\n        * The competition emphasizes both predictive accuracy and creative data exploration.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with spatiotemporal attributes\n* **Context:** Crime incident reports from the SFPD Crime Incident Reporting system (2003-2015)\n* **Data Files:**\n    * train.csv (contains target variable 'Category')\n    * test.csv\n    * sample_submission.csv\n* **Key Features:**\n    * Temporal: Dates, DayOfWeek\n    * Location: PdDistrict, Address, X (Longitude), Y (Latitude)\n    * Incident details: Descript (train only), Resolution (train only)\n    * Target: Category (39 crime classes)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * Formula: logloss = -1/N ∑(i=1 to N) ∑(j=1 to M) y_ij log(p_ij)\n        * N = number of test cases\n        * M = number of class labels (39 crime categories)\n        * y_ij = 1 if observation i is in class j, else 0\n        * p_ij = predicted probability observation i belongs to class j\n        * Probabilities are rescaled (row-wise normalization)\n        * Probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the category of crimes that occurred in San Francisco based on time, location, and other features. The goal is to assign probabilities to each crime category for every incident in the test set.\n    * **Key Points:**\n        * The dataset spans nearly 12 years of crime reports across all San Francisco neighborhoods.\n        * Participants are encouraged to explore the dataset visually for insights.\n        * The competition emphasizes both predictive accuracy and creative data exploration.",
      "Dataset Overview": "* **Data Type:** Tabular data with spatiotemporal attributes\n* **Context:** Crime incident reports from the SFPD Crime Incident Reporting system (2003-2015)\n* **Data Files:**\n    * train.csv (contains target variable 'Category')\n    * test.csv\n    * sample_submission.csv\n* **Key Features:**\n    * Temporal: Dates, DayOfWeek\n    * Location: PdDistrict, Address, X (Longitude), Y (Latitude)\n    * Incident details: Descript (train only), Resolution (train only)\n    * Target: Category (39 crime classes)",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * Formula: logloss = -1/N ∑(i=1 to N) ∑(j=1 to M) y_ij log(p_ij)\n        * N = number of test cases\n        * M = number of class labels (39 crime categories)\n        * y_ij = 1 if observation i is in class j, else 0\n        * p_ij = predicted probability observation i belongs to class j\n        * Probabilities are rescaled (row-wise normalization)\n        * Probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes"
    },
    "file_path": "kaggle_datasets/193/problem_summary.md"
  },
  "503": {
    "problem_id": "503",
    "title": "Unsupervised Clustering of Manufacturing Control Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Unsupervised Clustering of Manufacturing Control Data\n\n## Problem Description\n* **Problem Type:** Unsupervised Clustering\n* **Objective:**  \n    * Predict cluster assignments for each row in a manufacturing control dataset without any labeled training data or prior knowledge of the number of clusters.\n    * This is a pure unsupervised learning challenge where participants must discover inherent groupings in the data.\n* **Key Points:**\n    * No training labels provided\n    * Number of ground truth clusters is unknown\n    * Simulated manufacturing control data context\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data representing simulated manufacturing control states\n    * Contains both continuous and categorical features\n* **Data Files:**\n    * `data.csv` - Contains all data points to be clustered (no train/test split)\n    * `sample_submission.csv` - Example submission file format\n* **Features:**\n    * 32 columns (mix of continuous and categorical)\n    * Features represent manufacturing control parameters (specifics anonymized)\n\n## Evaluation Metrics\n* **Primary Metric:** Adjusted Rand Index (ARI)\n    * Measures similarity between predicted clusters and ground truth\n    * Accounts for chance grouping through adjustment\n* **Metric Characteristics:**\n    * Range: -1 to 1 (1 = perfect match, 0 = random labeling)\n    * Compares all pairs of samples:\n        * Counts pairs assigned to same/different clusters in both predictions and ground truth\n    * Adjusted version corrects for random chance",
    "sections": {},
    "file_path": "kaggle_datasets/503/problem_summary.md"
  },
  "355": {
    "problem_id": "355",
    "title": "Analyzing NFL Player Injuries and Performance Across Turf Types",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Analyzing NFL Player Injuries and Performance Across Turf Types\n\n## Problem Description\n* **Problem Type**: Exploratory Data Analysis (EDA) with Injury Risk Assessment\n* **Objective**: Investigate the relationship between playing surface (synthetic vs. natural turf) and:\n    * Player movement patterns\n    * Risk factors for lower extremity injuries\n    * Performance differences across surfaces\n* **Key Points**:\n    * Focus on non-contact lower limb injuries potentially influenced by turf interaction\n    * Requires characterization of movement metrics (speed, acceleration, directional changes)\n    * Must examine interactions between surface type and environmental/game factors\n    * Not a traditional predictive modeling competition - emphasis on analysis and insights\n\n## Dataset Overview\n* **Data Type**: Multi-source sports analytics data combining:\n    * Tabular injury records\n    * Play metadata\n    * High-frequency player tracking data (10Hz)\n* **Data Files**:\n    * `InjuryRecord.csv`: 105 lower-limb injuries with player/game identifiers\n    * `PlayList.csv`: 267,005 player-plays with game context metadata\n    * `PlayerTrackData.csv`: Position/movement data at 10 observations per second\n* **Key Features**:\n    * Injury data: Body part injured, surface type, play context\n    * Play data: Stadium type, weather conditions, play type, player position\n    * Tracking data: X/Y coordinates, speed, direction, orientation (with caveats)\n\n## Evaluation Metrics\n* **Evaluation Method**: Rubric-based judging by NFL experts (15-point scale)\n* **Scoring Components**:\n    * **Creativity and Presentation (5 pts)**:\n        * Novelty of movement characterization\n        * Effectiveness of visualizations\n        * Clarity of communication\n    * **Methodology (5 pts)**:\n        * Documentation quality\n        * Statistical appropriateness\n        * Model performance documentation\n    * **Application (5 pts)**:\n        * Usefulness for injury risk assessment\n        * Practical insights for injury reduction\n* **Key Analysis Focus Areas**:\n    * Development of novel movement metrics\n    * Identification of injury risk factors\n    * Surface comparison in non-injured population\n    * Interaction effects between variables",
    "sections": {},
    "file_path": "kaggle_datasets/355/problem_summary.md"
  },
  "167": {
    "problem_id": "167",
    "title": "Introducing Kaggle Scripts Platform",
    "problem_type": "Platform Demonstration / Script Showcase",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Introducing Kaggle Scripts Platform\n\n**Problem Description:**\n* **Problem Type:** Platform Demonstration / Script Showcase\n* **Objective:** \n    * Demonstrate Kaggle's new \"Scripts\" feature allowing users to run, version, and share code directly on Kaggle\n    * Encourage community participation through script creation and sharing across Python, R, and Julia\n    * Showcase educational content (dplyr tutorials) and creative coding challenges\n* **Key Points:**\n    * Focus on community collaboration and knowledge sharing\n    * Includes both educational and competitive aspects\n    * Features multiple prize categories for different script achievements\n\n**Dataset Overview:**\n* **Data Type:** Mixed (primarily demonstration data)\n    * Includes sample datasets from R datasets package\n    * Contains example image file (datascience.png)\n* **Data Files:**\n    * datascience.png (108.02 kB demonstration image)\n    * Access to R datasets package through provided scripts\n* **Features:**\n    * Not a traditional ML problem - focuses on platform capabilities\n    * Example scripts demonstrate data manipulation techniques\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Community Engagement Metrics\n    * Script upvotes (for popularity)\n    * Comments received (for discussion)\n    * Forks created (for reuse)\n* **Special Challenge Metrics:**\n    * Computational efficiency (prime number generation)\n    * Code obfuscation (for \"Hello World\")\n    * Mathematical proof (Riemann hypothesis)\n    * Data visualization popularity (Reddit votes)",
    "sections": {
      "Problem Description": "* **Problem Type:** Platform Demonstration / Script Showcase\n* **Objective:** \n    * Demonstrate Kaggle's new \"Scripts\" feature allowing users to run, version, and share code directly on Kaggle\n    * Encourage community participation through script creation and sharing across Python, R, and Julia\n    * Showcase educational content (dplyr tutorials) and creative coding challenges\n* **Key Points:**\n    * Focus on community collaboration and knowledge sharing\n    * Includes both educational and competitive aspects\n    * Features multiple prize categories for different script achievements",
      "Dataset Overview": "* **Data Type:** Mixed (primarily demonstration data)\n    * Includes sample datasets from R datasets package\n    * Contains example image file (datascience.png)\n* **Data Files:**\n    * datascience.png (108.02 kB demonstration image)\n    * Access to R datasets package through provided scripts\n* **Features:**\n    * Not a traditional ML problem - focuses on platform capabilities\n    * Example scripts demonstrate data manipulation techniques",
      "Evaluation Metrics": "* **Evaluation Metric:** Community Engagement Metrics\n    * Script upvotes (for popularity)\n    * Comments received (for discussion)\n    * Forks created (for reuse)\n* **Special Challenge Metrics:**\n    * Computational efficiency (prime number generation)\n    * Code obfuscation (for \"Hello World\")\n    * Mathematical proof (Riemann hypothesis)\n    * Data visualization popularity (Reddit votes)"
    },
    "file_path": "kaggle_datasets/167/problem_summary.md"
  },
  "397": {
    "problem_id": "397",
    "title": "Landmark Image Retrieval with Feature Embeddings",
    "problem_type": "Computer Vision - Image Retrieval",
    "objective": "Develop a model that retrieves relevant database images containing the same landmark as a given query image. The task involves:",
    "evaluation_metric": null,
    "full_content": "# Landmark Image Retrieval with Feature Embeddings\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Image Retrieval\n* **Objective:** Develop a model that retrieves relevant database images containing the same landmark as a given query image. The task involves:\n  * Generating feature embeddings for images\n  * Performing efficient similarity search in a large database\n  * Matching query images to index images of the same landmark\n* **Key Points:**\n  * This is a code competition where participants submit trained models rather than prediction files\n  * Models must output embeddings compatible with k-nearest-neighbors (k=100) lookup\n  * Competition is paired with the Landmark Recognition Challenge 2020\n  * Focus on instance-level recognition of landmarks\n\n**Dataset Overview:**\n* **Data Type & Context:** \n  * Large-scale collection of landmark images (Google Landmarks Dataset v2 - GLDv2)\n  * Images organized in hierarchical folder structure based on ID prefixes\n* **Data Files:**\n  * `train/` - Training images with corresponding labels in `train.csv`\n  * `test/` - Query images for retrieval\n  * `index/` - Database images to search against\n* **Features:**\n  * Image files in JPG format\n  * Each image has unique ID used for organization and matching\n  * Training data includes landmark labels for supervised learning\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision @ 100 (mAP@100)\n* **Metric Components:**\n  * Calculated over all query images (Q)\n  * For each query q:\n    * mq = number of relevant index images (must be >0)\n    * nq = number of predictions made\n    * Precision at rank k (Pq(k)) weighted by relevance (relq(k))\n  * Final score averages precision over top 100 predictions per query\n  * Relevance is binary (1 if correct match, 0 otherwise)",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Image Retrieval\n* **Objective:** Develop a model that retrieves relevant database images containing the same landmark as a given query image. The task involves:\n  * Generating feature embeddings for images\n  * Performing efficient similarity search in a large database\n  * Matching query images to index images of the same landmark\n* **Key Points:**\n  * This is a code competition where participants submit trained models rather than prediction files\n  * Models must output embeddings compatible with k-nearest-neighbors (k=100) lookup\n  * Competition is paired with the Landmark Recognition Challenge 2020\n  * Focus on instance-level recognition of landmarks",
      "Dataset Overview": "* **Data Type & Context:** \n  * Large-scale collection of landmark images (Google Landmarks Dataset v2 - GLDv2)\n  * Images organized in hierarchical folder structure based on ID prefixes\n* **Data Files:**\n  * `train/` - Training images with corresponding labels in `train.csv`\n  * `test/` - Query images for retrieval\n  * `index/` - Database images to search against\n* **Features:**\n  * Image files in JPG format\n  * Each image has unique ID used for organization and matching\n  * Training data includes landmark labels for supervised learning",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision @ 100 (mAP@100)\n* **Metric Components:**\n  * Calculated over all query images (Q)\n  * For each query q:\n    * mq = number of relevant index images (must be >0)\n    * nq = number of predictions made\n    * Precision at rank k (Pq(k)) weighted by relevance (relq(k))\n  * Final score averages precision over top 100 predictions per query\n  * Relevance is binary (1 if correct match, 0 otherwise)"
    },
    "file_path": "kaggle_datasets/397/problem_summary.md"
  },
  "363": {
    "problem_id": "363",
    "title": "Bengali Handwritten Grapheme Component Classification",
    "problem_type": "Multi-class Classification (Computer Vision - Optical Character Recognition)",
    "objective": "Classify three constituent elements of handwritten Bengali graphemes from images:",
    "evaluation_metric": null,
    "full_content": "# Bengali Handwritten Grapheme Component Classification\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Computer Vision - Optical Character Recognition)\n* **Objective:** Classify three constituent elements of handwritten Bengali graphemes from images:\n    * Grapheme root (primary character)\n    * Vowel diacritic (accent mark)\n    * Consonant diacritic (accent mark)\n* **Key Points:**\n    * Bengali has complex grapheme composition with ~13,000 possible combinations\n    * Test set contains graphemes not seen in training (but no new components)\n    * Special case: Two consonant diacritics can coexist in same grapheme (handled via corrected labels)\n\n**Dataset Overview:**\n* **Data Type:** Grayscale images (137×236 pixels) of handwritten Bengali characters\n* **Data Files:**\n    * `train.parquet`/`test.parquet` (image data)\n    * `train.csv`/`test.csv` (labels and metadata)\n    * `class_map.csv` (label-to-component mapping)\n* **Features:**\n    * Flattened image arrays (137×236 grayscale)\n    * Three target columns (grapheme_root, vowel_diacritic, consonant_diacritic)\n    * Approximately 1,000 grapheme roots represented in training\n\n**Evaluation Metrics:**\n* **Primary Metric:** Hierarchical macro-averaged recall\n    * Calculates separate recall scores for each component\n    * Final score is weighted average:\n        * Grapheme root: 2× weight\n        * Vowel diacritic: 1× weight\n        * Consonant diacritic: 1× weight\n* **Implementation:** Uses sklearn.metrics.recall_score with macro averaging per component",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Computer Vision - Optical Character Recognition)\n* **Objective:** Classify three constituent elements of handwritten Bengali graphemes from images:\n    * Grapheme root (primary character)\n    * Vowel diacritic (accent mark)\n    * Consonant diacritic (accent mark)\n* **Key Points:**\n    * Bengali has complex grapheme composition with ~13,000 possible combinations\n    * Test set contains graphemes not seen in training (but no new components)\n    * Special case: Two consonant diacritics can coexist in same grapheme (handled via corrected labels)",
      "Dataset Overview": "* **Data Type:** Grayscale images (137×236 pixels) of handwritten Bengali characters\n* **Data Files:**\n    * `train.parquet`/`test.parquet` (image data)\n    * `train.csv`/`test.csv` (labels and metadata)\n    * `class_map.csv` (label-to-component mapping)\n* **Features:**\n    * Flattened image arrays (137×236 grayscale)\n    * Three target columns (grapheme_root, vowel_diacritic, consonant_diacritic)\n    * Approximately 1,000 grapheme roots represented in training",
      "Evaluation Metrics": "* **Primary Metric:** Hierarchical macro-averaged recall\n    * Calculates separate recall scores for each component\n    * Final score is weighted average:\n        * Grapheme root: 2× weight\n        * Vowel diacritic: 1× weight\n        * Consonant diacritic: 1× weight\n* **Implementation:** Uses sklearn.metrics.recall_score with macro averaging per component"
    },
    "file_path": "kaggle_datasets/363/problem_summary.md"
  },
  "151": {
    "problem_id": "151",
    "title": "Bike Rental Demand Forecasting",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Bike Rental Demand Forecasting\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting (Regression)\n* **Objective:** Predict hourly bike rental demand in Washington D.C.'s Capital Bikeshare system by combining historical usage patterns with weather data.\n* **Key Points:**\n  * Focuses on forecasting total bike rentals (`count`) per hour\n  * Requires using only information available prior to the rental period\n  * Combines temporal patterns with weather and calendar features\n  * Test set consists of days 20-end of each month (train set is first 19 days)\n\n## Dataset Overview\n* **Data Type:** Tabular time series data with weather and calendar features\n* **Context:** Hourly bike rental records from Capital Bikeshare system in Washington D.C. spanning two years\n* **Data Files:**\n  * `train.csv` - First 19 days of each month with target values\n  * `test.csv` - Days 20-end of each month (prediction target)\n  * `sampleSubmission.csv` - Submission format example\n* **Key Features:**\n  * Temporal: `datetime` (hourly timestamps)\n  * Calendar: `season`, `holiday`, `workingday`\n  * Weather: `temp`, `atemp`, `humidity`, `windspeed`, `weather` (categorical)\n  * Target: `count` (total rentals = `casual` + `registered`)\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n* **Calculation:**\n  * Applies natural logarithm to both predicted and actual counts (with +1 adjustment)\n  * Computes mean squared error of these log-transformed values\n  * Takes square root of the result\n  * Formula: \n    ```\n    sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))^2)\n    ```\n    where:\n    * `n` = number of hours in test set\n    * `p_i` = predicted count\n    * `a_i` = actual count",
    "sections": {},
    "file_path": "kaggle_datasets/151/problem_summary.md"
  },
  "535": {
    "problem_id": "535",
    "title": "Breast Cancer Detection in Screening Mammograms",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Breast Cancer Detection in Screening Mammograms\n\n## Problem Description\n* **Problem Type**: Binary Classification (Medical Image Analysis)\n* **Objective**: Detect malignant breast cancer in screening mammograms to assist radiologists in early diagnosis while minimizing false positives.\n    * **Key Points**:\n        * Focus on improving accuracy and efficiency of breast cancer screening programs\n        * Address challenges of false positives that lead to unnecessary procedures\n        * Potential to help mitigate radiologist shortages in healthcare systems\n        * Clinical impact: Early detection can significantly reduce breast cancer mortality rates\n\n## Dataset Overview\n* **Data Type**: Medical imaging (DICOM format mammograms) with associated tabular metadata\n* **Context**: De-identified screening mammograms from multiple hospitals with clinical annotations\n* **Data Files**:\n    * `train_images/[patient_id]/[image_id].dcm` - Training mammograms (DICOM)\n    * `test_images/[patient_id]/[image_id].dcm` - Test mammograms (DICOM)\n    * `train.csv`/`test.csv` - Metadata files with clinical information\n    * `sample_submission.csv` - Submission format template\n* **Key Features**:\n    * Image metadata: laterality (L/R), view type, machine ID\n    * Patient data: age, breast density (train only), implant status\n    * Clinical outcomes: cancer label (0/1), biopsy status, invasiveness, BIRADS score (train only)\n    * Special flags: difficult_negative_case indicator\n\n## Evaluation Metrics\n* **Primary Metric**: Probabilistic F1 Score (pF1)\n    * **Components**:\n        * Extends traditional F1 score to accept probability outputs\n        * Calculated as harmonic mean of probabilistic precision and recall:\n            * pPrecision = pTP / (pTP + pFP)\n            * pRecall = pTP / (TP + FN)\n            * pF1 = 2*(pPrecision*pRecall)/(pPrecision+pRecall)\n    * Advantages:\n        * Better handles class imbalance common in medical screening\n        * Appropriately scores probabilistic predictions\n        * Penalizes both false positives and false negatives",
    "sections": {},
    "file_path": "kaggle_datasets/535/problem_summary.md"
  },
  "169": {
    "problem_id": "169",
    "title": "Denoising Dirty Documents",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Denoising Dirty Documents\n\n## Problem Description\n- **Problem Type**: Computer Vision - Image Denoising  \n- **Objective**: Develop an algorithm to remove synthetic noise from scanned text documents, restoring them to a clean, readable state. The goal is to enhance Optical Character Recognition (OCR) by preprocessing noisy documents.  \n- **Key Points**:  \n  - Simulates real-world document degradation (coffee stains, faded text, wrinkles).  \n  - Focuses on improving digitization of historical/printed materials.  \n\n## Dataset Overview  \n- **Data Type & Context**: Grayscale images of scanned text documents with synthetic noise.  \n- **Data Files**:  \n  - `train.zip`: Noisy training images.  \n  - `train_cleaned.zip`: Corresponding clean (ground truth) training images.  \n  - `test.zip`: Noisy test images to denoise.  \n  - `sampleSubmission.csv.zip`: Submission format example.  \n- **Features**:  \n  - Pixel intensity values (0=black to 1=white).  \n  - Images contain varied text styles and noise patterns.  \n\n## Evaluation Metrics  \n- **Primary Metric**: Root Mean Squared Error (RMSE) between predicted and ground truth pixel intensities.  \n- **Submission Format**:  \n  - Each pixel is labeled as `image_row_col` (e.g., `1_2_1` for image 1, row 2, column 1).  \n  - Predictions are continuous values in [0, 1] for each pixel.",
    "sections": {},
    "file_path": "kaggle_datasets/169/problem_summary.md"
  },
  "532": {
    "problem_id": "532",
    "title": "Multi-Objective Recommender System for E-Commerce",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Objective Recommender System for E-Commerce\n\n## Problem Description\n* **Problem Type:** Recommender System (Multi-Objective Prediction)\n* **Objective:**  \n  Build a single recommender system that simultaneously predicts three types of e-commerce user actions:\n  * Next product clicks\n  * Products added to cart\n  * Products ordered\n  * Based on previous events in a user session\n* **Key Points:**\n  * Must predict actions occurring *after* a session truncation point in test data\n  * Requires real-time prediction capability\n  * Focuses on improving relevance of recommendations from large product catalogs (>10M products)\n  * Unique challenge of optimizing for multiple objectives (clicks, carts, orders) simultaneously\n\n## Dataset Overview\n* **Data Type & Context:**  \n  JSONL files containing e-commerce session data from OTTO (Germany's largest online shop)\n* **Data Files:**\n  * `train.jsonl`: Complete session data with:\n    * `session`: Unique session ID\n    * `events`: Time-ordered sequence containing:\n      * `aid`: Article/product ID\n      * `ts`: Unix timestamp\n      * `type`: Event type (click/cart/order)\n  * `test.jsonl`: Truncated sessions requiring prediction of post-truncation events\n  * `sample_submission.csv`: Submission format example\n* **Key Features:**\n  * Session-based sequential data\n  * Three distinct event types to predict\n  * Large-scale product catalog (10M+ articles)\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted average of Recall@20 across three event types:\n  * Score = 0.10·R_clicks + 0.30·R_carts + 0.60·R_orders\n* **Metric Components:**\n  * For each event type (clicks/carts/orders):\n    * Recall@20 = (∑ correct predictions in top 20) / (∑ min(20, total ground truth items))\n  * Special cases:\n    * Clicks: Only one ground truth value (next clicked item)\n    * Carts/Orders: Multiple possible ground truth items\n  * Predictions truncated at 20 items per session-type combination\n  * Submission requires space-delimited predictions for each session-type pair",
    "sections": {},
    "file_path": "kaggle_datasets/532/problem_summary.md"
  },
  "156": {
    "problem_id": "156",
    "title": "Sentiment Analysis on IMDB Movie Reviews using Word2Vec",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Sentiment Analysis on IMDB Movie Reviews using Word2Vec\n\n## Problem Description\n- **Problem Type**: Binary Classification (Sentiment Analysis)\n- **Objective**: Predict the sentiment of IMDB movie reviews as positive (1) or negative (0) based on review text. The task involves using natural language processing (NLP) techniques, particularly focusing on Google's Word2Vec for deep learning-inspired text representation.\n- **Key Points**:\n  - Reviews with IMDB rating <5 are labeled negative (0), while ratings ≥7 are positive (1)\n  - No individual movie has more than 30 reviews in the dataset\n  - The competition serves as a tutorial for applying Word2Vec to NLP tasks\n  - Focuses on overcoming challenges like sarcasm, ambiguity, and nuanced language in sentiment analysis\n\n## Dataset Overview\n- **Data Type**: Text data (movie reviews)\n- **Context**: 100,000 IMDB movie reviews (50,000 labeled, 50,000 unlabeled)\n- **Data Files**:\n  - labeledTrainData.tsv: 25,000 labeled reviews for training\n  - testData.tsv: 25,000 unlabeled reviews for testing\n  - unlabeledTrainData.tsv: 50,000 additional unlabeled reviews\n  - sampleSubmission.csv: Example submission format\n- **Features**:\n  - Review text (multi-paragraph)\n  - Sentiment labels (for training set)\n  - Unique IDs for each review\n\n## Evaluation Metrics\n- **Evaluation Metric**: Area Under the ROC Curve (AUC)\n- **Submission Format**:\n  - CSV file with 25,000 predictions\n  - Two columns: \"id\" and \"sentiment\" (binary 0/1)\n  - Example:\n    ```\n    id,sentiment\n    123_45,0\n    678_90,1\n    ",
    "sections": {},
    "file_path": "kaggle_datasets/156/problem_summary.md"
  },
  "364": {
    "problem_id": "364",
    "title": "NCAA March Madness Tournament Outcome Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NCAA March Madness Tournament Outcome Prediction\n\n## Problem Description\n- **Problem Type**: Binary Classification (with probabilistic outputs)\n- **Objective**: Predict the probability of one team defeating another in NCAA Division I Men's Basketball Tournament (\"March Madness\") games. Participants must forecast outcomes for all possible matchups in the tournament.\n- **Key Points**:\n  - Two-stage competition: \n    - Stage 1: Model building using historical tournament data (2015-2019)\n    - Stage 2: Predicting actual 2020 tournament matchups\n  - Requires predicting all possible pairwise matchups (68×67/2=2,278 predictions per tournament)\n  - Game pairs are ordered by team ID (lower ID first)\n  - Predictions bounded away from 0% and 100% to avoid infinite log loss\n\n## Dataset Overview\n- **Data Type**: Tabular data with extensive basketball statistics and game results\n- **Context**: Comprehensive NCAA basketball data including:\n  - Team information and tournament seeds\n  - Game results (regular season and tournament)\n  - Team box scores\n  - Play-by-play event logs\n  - Public ranking systems\n  - Geographic game locations\n- **Key Files**:\n  - `MTeams.csv`: Team IDs and metadata\n  - `MSeasons.csv`: Season information\n  - `MNCAATourneySeeds.csv`: Tournament seeds\n  - `MRegularSeasonCompactResults.csv`: Regular season game outcomes\n  - `MNCAATourneyCompactResults.csv`: Tournament game outcomes\n  - `MRegularSeasonDetailedResults.csv`: Team-level box scores\n  - `MNCAATourneyDetailedResults.csv`: Tournament box scores\n  - `MMasseyOrdinals.csv`: Weekly team rankings\n  - `MEvents[2015-2019].csv`: Play-by-play event logs\n- **Important Features**:\n  - Team IDs and names\n  - Game scores and outcomes\n  - Team statistics (field goals, rebounds, assists, etc.)\n  - Tournament seeds and regions\n  - Play-by-play events with court locations (2018+)\n  - Multiple ranking systems\n\n## Evaluation Metrics\n- **Primary Metric**: Log Loss (binary cross-entropy)\n- **Calculation**:\n  ```\n  LogLoss = −1/n * Σ[y_i*log(ŷ_i) + (1−y_i)*log(1−ŷ_i",
    "sections": {},
    "file_path": "kaggle_datasets/364/problem_summary.md"
  },
  "390": {
    "problem_id": "390",
    "title": "Forecasting Walmart Retail Sales with Hierarchical Time Series Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Forecasting Walmart Retail Sales with Hierarchical Time Series Data\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict daily unit sales for Walmart retail products across multiple stores and hierarchical categories (item, department, product category) for a 28-day forecast horizon.\n    * Focuses on **point forecasts** (companion competition handles uncertainty estimation).\n    * Uses hierarchical sales data spanning 3 US states (California, Texas, Wisconsin).\n    * Incorporates explanatory variables like price, promotions, day-of-week, and special events.\n* **Key Points:**\n    * Two 28-day prediction periods: validation (public leaderboard) and evaluation (private leaderboard).\n    * Part of the M5 forecasting challenge series, emphasizing methodological advancement in business forecasting.\n    * Goal is to improve inventory/service level decisions through accurate predictions.\n\n## Dataset Overview\n* **Data Type & Context:** Hierarchical time series data of retail sales with:\n    * **Temporal granularity:** Daily sales records.\n    * **Hierarchy:** Item → product category → department → store → state.\n    * **Additional covariates:** Pricing, promotions, and calendar events.\n* **Data Files:**\n    * `sales_train_validation.csv`: Historical sales (d_1 to d_1913).\n    * `sales_train_evaluation.csv`: Extended sales (d_1 to d_1941) for public LB.\n    * `calendar.csv`: Date metadata (weekday, holidays, etc.).\n    * `sell_prices.csv`: Product pricing by store and date.\n    * `sample_submission.csv`: Submission template with F1-F28 forecast columns.\n* **Key Features:**\n    * Anonymized product/store identifiers.\n    * Time-based features (day-of-week, month, holidays).\n    * Promotional indicators and price data.\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Root Mean Squared Scaled Error (RMSSE)\n    * **Components:**\n        * **Scaling:** Error is scaled by in-sample MAE of a naive forecast for each series.\n        * **Hierarchical Weighting:** Applies higher weights to aggregate levels (e.g., department/store) than individual items.\n        * **Interpretation:** Lower values indicate better accuracy, with 1 representing naive forecast performance.\n    * **Customization:** Designed to handle hierarchical nature and varying scales of retail time series.",
    "sections": {},
    "file_path": "kaggle_datasets/390/problem_summary.md"
  },
  "216": {
    "problem_id": "216",
    "title": "Leaf Species Classification from Binary Images and Extracted Features",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Leaf Species Classification from Binary Images and Extracted Features\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (99 plant species)\n* **Objective:**  \n    * Accurately classify plant species using binary leaf images and pre-extracted features (shape, margin, texture).\n    * Participants are encouraged to:\n        * Build classifiers using provided features.\n        * Engineer new features from binary images.\n        * Analyze and improve model errors iteratively.\n* **Key Points:**\n    * Focus on differentiating nearly half a million potential plant species through leaf characteristics.\n    * Combines image-based features with traditional tabular data for classification.\n    * Educational focus: Introduces techniques for handling image-derived features.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * **Primary Data:** Tabular data containing extracted features from binary leaf images (black leaves on white backgrounds).\n    * **Supplemental Data:** Original binary leaf images (1,584 total, 16 samples per species).\n* **Data Files:**  \n    * `train.csv` (training set with features and labels)\n    * `test.csv` (test set with features only)\n    * `sample_submission.csv` (format guide)\n    * `images/` folder (binary leaf images named by ID)\n* **Key Features:**  \n    * Three 64-attribute vectors per leaf:\n        * `margin_1` to `margin_64` (margin histogram features)\n        * `shape_1` to `shape_64` (shape descriptor features)\n        * `texture_1` to `texture_64` (interior texture histogram features)\n    * `id` column as unique image identifier.\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n* **Metric Components:**  \n    * Formula:  \n        `logloss = -1/N * Σ(i=1 to N) Σ(j=1 to M) [y_ij * log(p_ij)]`  \n        Where:\n        * `N` = number of test images\n        * `M` = 99 species classes\n        * `y_ij` = 1 if observation `i` is class `j` (else 0)\n        * `p_ij` = predicted probability of observation `i` being class `j`\n    * Technical Notes:\n        * Probabilities are rescaled (row-wise) to sum",
    "sections": {},
    "file_path": "kaggle_datasets/216/problem_summary.md"
  },
  "440": {
    "problem_id": "440",
    "title": "Wildlife Species Counting in Camera Trap Image Sequences",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Wildlife Species Counting in Camera Trap Image Sequences\n\n## Problem Description\n* **Problem Type**: Computer Vision - Multi-label Classification & Counting\n* **Objective**: \n    * Categorize animal species and count the number of individuals across sequences of camera trap images\n    * Requires reasoning across temporal bursts to avoid over/undercounting when animals move through frame\n* **Key Points**:\n    * Training and test data come from different camera locations globally\n    * Species sets overlap but are not identical across cameras\n    * Allows multimodal solutions incorporating:\n        * Camera trap training data\n        * iNaturalist species datasets (2017-2021)\n        * Landsat 8 multispectral imagery of camera locations\n\n## Dataset Overview\n* **Data Type**: \n    * Primary: Image sequences from motion-triggered camera traps\n    * Supplemental: Multispectral satellite imagery (Landsat 8) of locations\n* **Key Files**:\n    * Training/test image sets (203,314 and 60,214 images respectively)\n    * MegaDetector outputs (bounding boxes/confidences)\n    * DeepMAC segmentation masks\n    * Metadata files mapping locations/species\n* **Notable Features**:\n    * Images organized by location bursts (temporal sequences)\n    * 9-band Landsat patches per camera location (200x200x9 pixels)\n    * Provided detection/segmentation outputs from pretrained models\n    * No explicit count labels in training data\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Columnwise Root Mean Squared Error (MCRMSE)\n    * Calculated per species (column) across all sequences (rows)\n    * Formula: \n        ```\n        MCRMSE = 1/m * Σ_j (RMSE for species j)\n        ```\n    * Where m = number of species\n* **Interpretation**:\n    * Captures both species misclassification and counting errors\n    * Can be converted to SCRSSE (Summed Columnwise Root Summed Squared Error) for ecological interpretation:\n        ```\n        SCRSSE = m * sqrt(n) * MCRMSE\n        ",
    "sections": {},
    "file_path": "kaggle_datasets/440/problem_summary.md"
  },
  "229": {
    "problem_id": "229",
    "title": "Predicting Russian Real Estate Prices with Macroeconomic Data",
    "problem_type": "Regression (Housing Price Prediction)",
    "objective": "Predict the sale price (`price_doc`) of individual properties in Russia, accounting for both housing characteristics and macroeconomic volatility. The goal is to provide accurate price forecasts despite Russia's unstable economy, helping Sberbank offer certainty to customers in real estate transactions.",
    "evaluation_metric": null,
    "full_content": "# Predicting Russian Real Estate Prices with Macroeconomic Data\n\n**Problem Description:**\n* **Problem Type:** Regression (Housing Price Prediction)\n* **Objective:** Predict the sale price (`price_doc`) of individual properties in Russia, accounting for both housing characteristics and macroeconomic volatility. The goal is to provide accurate price forecasts despite Russia's unstable economy, helping Sberbank offer certainty to customers in real estate transactions.\n    * **Key Points:**\n        * Must model complex interactions between property features (e.g., bedrooms, location) and macroeconomic trends.\n        * Time-sensitive data: Training data spans August 2011–June 2015; test data covers July 2015–May 2016.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data combining:\n    * Property-level transaction records (housing characteristics, local area details).\n    * Macroeconomic indicators (Russia’s economy/financial sector trends).\n* **Data Files:**\n    * `train.csv`, `test.csv`: Property transactions with `id` (per transaction) and `timestamp` for joining to macroeconomic data.\n    * `macro.csv`: Time-series macroeconomic data (joinable via `timestamp`).\n    * `sample_submission.csv`: Submission template.\n    * `data_dictionary.txt`: Field explanations.\n* **Key Features:**\n    * Target: `price_doc` (sale price).\n    * Property attributes (e.g., size, location) and macroeconomic indicators (e.g., GDP, interest rates).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE).\n    * **Calculation:** \n        * Log-transforms both predicted and actual prices before computing RMSE, reducing the impact of extreme outliers.\n        * Formula: `sqrt(mean((log(pred + 1) - log(actual + 1))^2)`.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Housing Price Prediction)\n* **Objective:** Predict the sale price (`price_doc`) of individual properties in Russia, accounting for both housing characteristics and macroeconomic volatility. The goal is to provide accurate price forecasts despite Russia's unstable economy, helping Sberbank offer certainty to customers in real estate transactions.\n    * **Key Points:**\n        * Must model complex interactions between property features (e.g., bedrooms, location) and macroeconomic trends.\n        * Time-sensitive data: Training data spans August 2011–June 2015; test data covers July 2015–May 2016.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data combining:\n    * Property-level transaction records (housing characteristics, local area details).\n    * Macroeconomic indicators (Russia’s economy/financial sector trends).\n* **Data Files:**\n    * `train.csv`, `test.csv`: Property transactions with `id` (per transaction) and `timestamp` for joining to macroeconomic data.\n    * `macro.csv`: Time-series macroeconomic data (joinable via `timestamp`).\n    * `sample_submission.csv`: Submission template.\n    * `data_dictionary.txt`: Field explanations.\n* **Key Features:**\n    * Target: `price_doc` (sale price).\n    * Property attributes (e.g., size, location) and macroeconomic indicators (e.g., GDP, interest rates).",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE).\n    * **Calculation:** \n        * Log-transforms both predicted and actual prices before computing RMSE, reducing the impact of extreme outliers.\n        * Formula: `sqrt(mean((log(pred + 1) - log(actual + 1))^2)`."
    },
    "file_path": "kaggle_datasets/229/problem_summary.md"
  },
  "447": {
    "problem_id": "447",
    "title": "Smartphone Decimeter-Level GNSS Positioning Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Smartphone Decimeter-Level GNSS Positioning Challenge\n\n## Problem Description\n* **Problem Type**: Geospatial Regression / Signal Processing\n* **Objective**: Improve smartphone GNSS positioning accuracy from meter-level (~3-5m) to decimeter/centimeter precision using raw sensor data. The goal is to reduce \"jumpy\" location estimates for applications like lane-level navigation, location-based gaming, and road safety mapping.\n    * **Key Points**:\n        * Focus on post-processing algorithms (future route data available)\n        * Must handle multi-phone data correlations\n        * Proprietary chipset locations excluded to encourage general solutions\n        * Uses raw GNSS signals, IMU data (accelerometer/gyroscope), and correction services\n\n## Dataset Overview\n* **Data Type**: Multimodal time-series data (GNSS signals, inertial measurements) with geospatial ground truth\n* **Primary Files**:\n    * `ground_truth.csv` (WGS84 coordinates with nanosecond timestamps)\n    * `*_derived.csv` (preprocessed GNSS measurements)\n    * `*_GnssLog.txt` (raw Android sensor logs)\n    * Supplemental files in GPS industry formats (.20o, .21o, .nmea)\n* **Key Features**:\n    * Satellite positions/velocities (ECEF coordinates)\n    * Pseudorange measurements with error corrections (ionospheric/tropospheric delays)\n    * Raw IMU readings (uncalibrated accelerometer/gyroscope/magnetometer)\n    * Signal quality metrics (CN0, DOP values)\n    * Precise timestamps (millisSinceGpsEpoch)\n\n## Evaluation Metrics\n* **Primary Metric**: Mean of 50th and 95th percentile horizontal distance errors (in meters)\n    * **Calculation Process**:\n        1. Compute Euclidean distance between predicted/true WGS84 coordinates per timestamp\n        2. For each phone:\n            * Calculate 50th and 95th percentile errors across all timestamps\n            * Take average of these two percentiles\n        3. Final score: Mean of phone-level averages across test set\n* **Key Aspect**: Emphasizes both typical performance (50th percentile) and outlier reduction (95th percentile)",
    "sections": {},
    "file_path": "kaggle_datasets/447/problem_summary.md"
  },
  "211": {
    "problem_id": "211",
    "title": "Santander Product Recommendation",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Santander Product Recommendation\n\n## Problem Description\n- **Problem Type**: Multi-class Classification (Recommendation System)\n- **Objective**: Predict which financial products existing Santander customers will add to their portfolio in the next month, based on their past behavior and similarities to other customers. The goal is to recommend personalized products to improve customer experience by addressing the current imbalance where some customers receive many recommendations while others receive few.\n- **Key Points**:\n  - Focus on predicting **additional** products customers will acquire, beyond what they already have.\n  - The task involves analyzing 1.5 years of customer behavior data to make recommendations for a specific future month (2016-06-28).\n  - The dataset is synthetic and does not represent real Santander Spain customers.\n\n## Dataset Overview\n- **Data Type**: Tabular data (time-series customer behavior and demographic data)\n- **Context**: Monthly records of customer interactions with financial products (e.g., credit cards, savings accounts) from 2015-01-28 to 2016-05-28.\n- **Data Files**:\n  - `train.csv`: Training set with historical customer data.\n  - `test.csv`: Test set for making predictions.\n  - `sample_submission.csv`: Example submission file in the required format.\n- **Features**:\n  - Customer demographics (age, sex, country of residence, etc.).\n  - Customer activity indicators (employment status, seniority, income, etc.).\n  - Product ownership flags (24 binary features indicating whether a customer has a specific product, e.g., `ind_ahor_fin_ult1` for Savings Account).\n  - Temporal features (e.g., `fecha_alta` - date when the customer became a primary holder).\n\n## Evaluation Metrics\n- **Evaluation Metric**: Mean Average Precision @ 7 (MAP@7)\n- **Components**:\n  - For each user, precision is calculated at each cutoff `k` (up to 7).\n  - The average precision is computed for each user and then averaged across all users.\n  - If no products are added (`m = 0`), the precision for that user is 0.\n  - The formula is:\n    ```\n    MAP@7 = (1/|U|) * Σ (1/min(m,7)) * Σ P(k)\n    ```\n    where:\n    - `|U|` = number of users.\n    - `P(k",
    "sections": {},
    "file_path": "kaggle_datasets/211/problem_summary.md"
  },
  "478": {
    "problem_id": "478",
    "title": "Argumentative Discourse Element Classification in Student Essays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Argumentative Discourse Element Classification in Student Essays\n\n## Problem Description\n* **Problem Type**: NLP - Text Segmentation and Classification\n* **Objective**: \n    * Automatically segment and classify argumentative/rhetorical elements in student essays (grades 6-12)\n    * Identify seven specific discourse elements in unstructured essay text\n* **Key Points**:\n    * Goal is to enable automated writing feedback tools\n    * Focus on improving educational accessibility (especially for under-served schools)\n    * Requires both text segmentation AND multi-class classification\n\n## Dataset Overview\n* **Data Type**: Text (student essays) with character-level annotations\n* **Context**: \n    * 15,601 argumentative essays from U.S. students (grades 6-12)\n    * Expert-annotated discourse elements\n* **Data Files**:\n    * `train.zip`: Folder of .txt files (full essay texts)\n    * `train.csv`: Annotations with discourse types, positions, and prediction strings\n    * `test.zip`: Unannotated test essays\n* **Key Features**:\n    * Raw essay text (unstructured)\n    * 7 discourse classes to predict:\n        * Lead, Position, Claim, Counterclaim, Rebuttal, Evidence, Concluding Statement\n    * Character-level start/end positions for each element\n\n## Evaluation Metrics\n* **Primary Metric**: Macro F1 Score\n* **Scoring Components**:\n    * Word-level overlap calculation between predictions and ground truth:\n        1. True Positive: Overlap ≥ 0.5 in both directions (pred→truth AND truth→pred)\n        2. Best match selected when multiple overlaps exist\n        3. Unmatched truths → False Negatives\n        4. Unmatched predictions → False Positives\n    * Final score: Macro-average F1 across all 7 classes\n    * Word indices determined by Python's `.split()` function",
    "sections": {},
    "file_path": "kaggle_datasets/478/problem_summary.md"
  },
  "227": {
    "problem_id": "227",
    "title": "Cervical Cancer Type Classification from Medical Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Cervical Cancer Type Classification from Medical Images\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Computer Vision - Image Classification)\n* **Objective:** Develop an algorithm to accurately classify cervical images into three distinct cervix types (Type_1, Type_2, Type_3) to determine appropriate cancer treatment eligibility. The classification helps prevent ineffective treatments by identifying cases requiring advanced referral.\n    * **Key Points:**\n        * Focus on distinguishing normal (non-cancerous) cervix types where transformation zone visibility varies\n        * Real-world impact: Incorrect classification leads to obscured future cancerous growth risks\n        * Images may contain graphic medical content\n\n## Dataset Overview\n* **Data Type & Context:** Medical images of cervixes captured via MobileODT's EVA System (colposcopy device)\n* **Data Files:**\n    * `train.7z` - Labeled training images organized by type (Type_1, Type_2, Type_3 folders)\n    * `test.7z` - Unlabeled test images for initial stage predictions\n    * `additional_Type_{x}.7z` - Supplemental labeled training images (some from duplicate patients)\n    * `test_stg2.7z` - Second-stage test images (released later in competition)\n* **Features:** \n    * High-resolution cervical images with varying lighting/angles\n    * Labels indicate physiological differences in cervix appearance affecting treatment\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class Logarithmic Loss (Log Loss)\n    * **Components:**\n        * Formula: \\(-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M}y_{ij}\\log(p_{ij})\\)\n        * N = number of test images, M = 3 classes\n        * \\(y_{ij}\\) = 1 if image i belongs to class j, else 0\n        * \\(p_{ij}\\) = predicted probability for image i and class j\n    * **Processing:**\n        * Probabilities rescaled per row (divided by row sum)\n        * Clipped to \\([10^{-15}, 1-10^{-15}]\\) to avoid log extremes",
    "sections": {},
    "file_path": "kaggle_datasets/227/problem_summary.md"
  },
  "471": {
    "problem_id": "471",
    "title": "Predicting Realized Volatility in Financial Markets",
    "problem_type": "Time Series Forecasting (Financial Markets)",
    "objective": "Predict short-term realized volatility for hundreds of stocks over 10-minute periods using high-frequency market data.",
    "evaluation_metric": null,
    "full_content": "# Predicting Realized Volatility in Financial Markets\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (Financial Markets)\n* **Objective:** Predict short-term realized volatility for hundreds of stocks over 10-minute periods using high-frequency market data.\n    * **Key Points:**\n        * Focuses on micro-structure of financial markets using order book and trade data\n        * Requires forecasting volatility during a future evaluation period with real market data\n        * Public and private leaderboards have zero overlap due to forecasting nature\n        * Targets financial market improvement through better volatility prediction\n\n**Dataset Overview:**\n* **Data Type & Context:** High-frequency financial market data (order book snapshots and executed trades) with 1-second resolution\n* **Data Files:**\n    * `book_[train/test].parquet`: Order book data (top two levels) with price/size for buy/sell orders\n    * `trade_[train/test].parquet`: Executed trade data including price, size and order count\n    * `train.csv`: Ground truth realized volatility values for training set\n    * `test.csv`: Mapping for submission (most data hidden in code competition)\n* **Key Features:**\n    * `stock_id`, `time_id`, `seconds_in_bucket` as temporal identifiers\n    * Normalized bid/ask prices and sizes for order book levels\n    * Trade execution details (price, size, order_count)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Percentage Error (RMSPE)\n    * **Calculation:** \n        * RMSPE = √(1/n Σ((y_i - ŷ_i)/y_i)²)\n        * Measures relative errors between predicted and actual volatility\n        * Penalizes larger percentage deviations equally across different volatility levels",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (Financial Markets)\n* **Objective:** Predict short-term realized volatility for hundreds of stocks over 10-minute periods using high-frequency market data.\n    * **Key Points:**\n        * Focuses on micro-structure of financial markets using order book and trade data\n        * Requires forecasting volatility during a future evaluation period with real market data\n        * Public and private leaderboards have zero overlap due to forecasting nature\n        * Targets financial market improvement through better volatility prediction",
      "Dataset Overview": "* **Data Type & Context:** High-frequency financial market data (order book snapshots and executed trades) with 1-second resolution\n* **Data Files:**\n    * `book_[train/test].parquet`: Order book data (top two levels) with price/size for buy/sell orders\n    * `trade_[train/test].parquet`: Executed trade data including price, size and order count\n    * `train.csv`: Ground truth realized volatility values for training set\n    * `test.csv`: Mapping for submission (most data hidden in code competition)\n* **Key Features:**\n    * `stock_id`, `time_id`, `seconds_in_bucket` as temporal identifiers\n    * Normalized bid/ask prices and sizes for order book levels\n    * Trade execution details (price, size, order_count)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Percentage Error (RMSPE)\n    * **Calculation:** \n        * RMSPE = √(1/n Σ((y_i - ŷ_i)/y_i)²)\n        * Measures relative errors between predicted and actual volatility\n        * Penalizes larger percentage deviations equally across different volatility levels"
    },
    "file_path": "kaggle_datasets/471/problem_summary.md"
  },
  "643": {
    "problem_id": "643",
    "title": "Efficient Chess AI Development with Resource Constraints",
    "problem_type": "Reinforcement Learning (Game AI) with Resource Constraints",
    "objective": "Develop a chess-playing AI agent that operates under strict computational resource limitations. The primary challenge is to create an agent that balances strategic chess-playing ability with efficiency in CPU and memory usage.",
    "evaluation_metric": null,
    "full_content": "# Efficient Chess AI Development with Resource Constraints\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning (Game AI) with Resource Constraints\n* **Objective:** Develop a chess-playing AI agent that operates under strict computational resource limitations. The primary challenge is to create an agent that balances strategic chess-playing ability with efficiency in CPU and memory usage.\n    * **Key Points:**\n        * Emphasis on elegant and efficient design rather than brute-force computation\n        * Must adhere to strict hardware constraints: 5 MiB RAM, single 2.20GHz CPU core\n        * Submission size limit of 64KiB (compressed)\n        * Agents must play under tournament time controls (10s with 0.1s simple delay)\n\n**Dataset Overview:**\n* **Data Type:** Simulation environment for chess gameplay\n* **Context:** Kaggle provides a chess environment for agent testing and evaluation\n* **Data Files:** \n    * Chess environment configuration (via kaggle-environments package)\n    * Note: No traditional dataset files provided - competitors develop agents to interact with the game environment\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Skill Rating System (Gaussian N(μ,σ²) model)\n    * **Components:**\n        * Initial rating μ₀ = 600 for new submissions\n        * Rating updates based on match outcomes (wins/losses/ties)\n        * Updates consider:\n            * Deviation from expected result based on prior μ values\n            * Each bot's uncertainty (σ)\n        * Rating changes are relative to opponent's strength\n        * Score margin doesn't affect rating updates\n    * **Process:**\n        * Agents continuously play against others with similar ratings\n        * Each submission plays until competition ends (newer bots play more frequently)\n        * Final ranking determined by performance in final evaluation period",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning (Game AI) with Resource Constraints\n* **Objective:** Develop a chess-playing AI agent that operates under strict computational resource limitations. The primary challenge is to create an agent that balances strategic chess-playing ability with efficiency in CPU and memory usage.\n    * **Key Points:**\n        * Emphasis on elegant and efficient design rather than brute-force computation\n        * Must adhere to strict hardware constraints: 5 MiB RAM, single 2.20GHz CPU core\n        * Submission size limit of 64KiB (compressed)\n        * Agents must play under tournament time controls (10s with 0.1s simple delay)",
      "Dataset Overview": "* **Data Type:** Simulation environment for chess gameplay\n* **Context:** Kaggle provides a chess environment for agent testing and evaluation\n* **Data Files:** \n    * Chess environment configuration (via kaggle-environments package)\n    * Note: No traditional dataset files provided - competitors develop agents to interact with the game environment",
      "Evaluation Metrics": "* **Evaluation Metric:** Skill Rating System (Gaussian N(μ,σ²) model)\n    * **Components:**\n        * Initial rating μ₀ = 600 for new submissions\n        * Rating updates based on match outcomes (wins/losses/ties)\n        * Updates consider:\n            * Deviation from expected result based on prior μ values\n            * Each bot's uncertainty (σ)\n        * Rating changes are relative to opponent's strength\n        * Score margin doesn't affect rating updates\n    * **Process:**\n        * Agents continuously play against others with similar ratings\n        * Each submission plays until competition ends (newer bots play more frequently)\n        * Final ranking determined by performance in final evaluation period"
    },
    "file_path": "kaggle_datasets/643/problem_summary.md"
  },
  "485": {
    "problem_id": "485",
    "title": "Clinical Concept Identification in Patient Notes",
    "problem_type": "NLP - Span Identification (Sequence Labeling)",
    "objective": "Develop an automated method to identify clinical concepts (features) from exam rubrics within patient notes written by medical students. The goal is to map diverse textual expressions (e.g., \"eating less\") to standardized clinical concepts (e.g., \"diminished appetite\").",
    "evaluation_metric": null,
    "full_content": "# Clinical Concept Identification in Patient Notes\n\n**Problem Description:**\n* **Problem Type:** NLP - Span Identification (Sequence Labeling)\n* **Objective:** Develop an automated method to identify clinical concepts (features) from exam rubrics within patient notes written by medical students. The goal is to map diverse textual expressions (e.g., \"eating less\") to standardized clinical concepts (e.g., \"diminished appetite\").\n* **Key Points:**\n  * Address variability in how clinical concepts are expressed (synonyms, paraphrasing).\n  * Handle complex cases like multi-span annotations (discontinuous text segments) and ambiguous negations.\n  * Improve transparency and interpretability of automated patient note scoring for medical licensing exams.\n\n**Dataset Overview:**\n* **Data Type & Context:** Textual patient notes from USMLE Step 2 Clinical Skills exams, with annotated spans corresponding to clinical features.\n* **Data Files:**\n  * `patient_notes.csv`: 40K+ patient notes (only subset annotated).\n  * `features.csv`: Rubric of clinical features per case.\n  * `train.csv`: 1K annotated patient notes (100 per case).\n  * `test.csv`: Example test instances (replaced during scoring).\n* **Key Features:**\n  * `pn_history`: Free-text patient notes.\n  * `feature_text`: Standardized clinical concept descriptions.\n  * `location`: Character spans indicating annotation positions (semicolon-delimited for multi-span annotations).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Micro-averaged F1 score calculated at character level.\n* **Scoring Components:**\n  * **TP (True Positive):** Character index present in both ground-truth and prediction spans.\n  * **FN (False Negative):** Character index in ground-truth but not prediction.\n  * **FP (False Positive):** Character index in prediction but not ground-truth.\n  * **Span Format:** Predictions are semicolon-delimited character ranges (e.g., `0 100;200 250`).",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Span Identification (Sequence Labeling)\n* **Objective:** Develop an automated method to identify clinical concepts (features) from exam rubrics within patient notes written by medical students. The goal is to map diverse textual expressions (e.g., \"eating less\") to standardized clinical concepts (e.g., \"diminished appetite\").\n* **Key Points:**\n  * Address variability in how clinical concepts are expressed (synonyms, paraphrasing).\n  * Handle complex cases like multi-span annotations (discontinuous text segments) and ambiguous negations.\n  * Improve transparency and interpretability of automated patient note scoring for medical licensing exams.",
      "Dataset Overview": "* **Data Type & Context:** Textual patient notes from USMLE Step 2 Clinical Skills exams, with annotated spans corresponding to clinical features.\n* **Data Files:**\n  * `patient_notes.csv`: 40K+ patient notes (only subset annotated).\n  * `features.csv`: Rubric of clinical features per case.\n  * `train.csv`: 1K annotated patient notes (100 per case).\n  * `test.csv`: Example test instances (replaced during scoring).\n* **Key Features:**\n  * `pn_history`: Free-text patient notes.\n  * `feature_text`: Standardized clinical concept descriptions.\n  * `location`: Character spans indicating annotation positions (semicolon-delimited for multi-span annotations).",
      "Evaluation Metrics": "* **Primary Metric:** Micro-averaged F1 score calculated at character level.\n* **Scoring Components:**\n  * **TP (True Positive):** Character index present in both ground-truth and prediction spans.\n  * **FN (False Negative):** Character index in ground-truth but not prediction.\n  * **FP (False Positive):** Character index in prediction but not ground-truth.\n  * **Span Format:** Predictions are semicolon-delimited character ranges (e.g., `0 100;200 250`)."
    },
    "file_path": "kaggle_datasets/485/problem_summary.md"
  },
  "218": {
    "problem_id": "218",
    "title": "Binary Classification of Dog and Cat Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Dog and Cat Images\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Image Classification)\n* **Objective:** Distinguish between images of dogs and cats by predicting the probability that a given image contains a dog.\n    * **Key Points:**\n        * Focuses on revisiting a classic image classification problem using modern deep learning techniques.\n        * Emphasizes practice with new methods (e.g., TensorFlow, Jupyter Notebooks) on a well-known dataset.\n        * Originally a challenging problem in 2013, now serves as a benchmark for testing contemporary approaches.\n\n## Dataset Overview\n* **Data Type & Context:** Image data (RGB photographs) of dogs and cats.\n* **Data Files:**\n    * `train.zip`: Contains 25,000 labeled images (filenames indicate class: e.g., \"dog.1234.jpg\" or \"cat.5678.jpg\").\n    * `test.zip`: Contains 12,500 unlabeled images (named by numeric ID only).\n    * `sample_submission.csv`: Example submission file with required format (ID, predicted probability).\n* **Features:**\n    * Raw pixel data from images (no predefined feature extraction).\n    * Filenames in training set explicitly indicate class (dog/cat).\n\n## Evaluation Metrics\n* **Primary Metric:** Log Loss (Binary Cross-Entropy)\n    * **Components:**\n        * Formula:  \n          `LogLoss = −1/n * Σ[y_i*log(ŷ_i) + (1−y_i)*log(1−ŷ_i)]`  \n          where:\n            * `n` = number of test images\n            * `ŷ_i` = predicted probability of image being a dog\n            * `y_i` = 1 if image is a dog, 0 if cat\n        * Natural logarithm (base e) is used.\n        * Lower values indicate better performance (perfect prediction = 0).",
    "sections": {},
    "file_path": "kaggle_datasets/218/problem_summary.md"
  },
  "482": {
    "problem_id": "482",
    "title": "Multi-Agent Strategy Game for Kore Mining Optimization",
    "problem_type": "Reinforcement Learning / Multi-Agent Strategy Game",
    "objective": "Develop an AI agent to control spaceship fleets in a turn-based simulation, competing against opponents to collect the maximum amount of \"kore\" mineral by:",
    "evaluation_metric": null,
    "full_content": "# Multi-Agent Strategy Game for Kore Mining Optimization\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Develop an AI agent to control spaceship fleets in a turn-based simulation, competing against opponents to collect the maximum amount of \"kore\" mineral by:\n    * Mining kore from a 21x21 grid\n    * Managing shipyards and fleet movements\n    * Engaging in strategic combat with opponents\n    * Surviving for 400 turns or eliminating all opponents\n* **Key Points:**\n    * 4-player competitive environment with simultaneous turns\n    * Complex game mechanics including:\n        * Fleet movement patterns via flight plans\n        * Shipyard creation/management\n        * Combat resolution with collision and damage mechanics\n        * Kore mining and regeneration dynamics\n    * Real-time decision making under constraints\n\n**Dataset Overview:**\n* **Data Type:** Game state observations (tabular) and action commands\n* **Context:** Complete information about all game entities (fleets, shipyards, kore distribution) provided each turn\n* **Data Files:**\n    * Training data format not explicitly specified (implied to be game state observations)\n    * Sample submission file demonstrating agent response format\n* **Features:**\n    * Board state (21x21 grid with kore values)\n    * Fleet positions, sizes, kore cargo, and flight plans\n    * Shipyard locations and ownership\n    * Player kore reserves\n    * Turn counter\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Skill Rating System (Gaussian N(μ,σ²))\n    * Initial μ=600 for new submissions\n    * Updated through competitive matches:\n        * μ increases with wins, decreases with losses\n        * μ values move toward mean for draws\n    * Updates weighted by:\n        * Deviation from expected outcome\n        * Current uncertainty (σ)\n        * σ decreases with more matches\n* **Match Outcomes:**\n    * Determined by:\n        * Total kore collected (deposited at shipyards)\n        * Elimination of all opponents\n    * Score differential doesn't affect rating updates",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Develop an AI agent to control spaceship fleets in a turn-based simulation, competing against opponents to collect the maximum amount of \"kore\" mineral by:\n    * Mining kore from a 21x21 grid\n    * Managing shipyards and fleet movements\n    * Engaging in strategic combat with opponents\n    * Surviving for 400 turns or eliminating all opponents\n* **Key Points:**\n    * 4-player competitive environment with simultaneous turns\n    * Complex game mechanics including:\n        * Fleet movement patterns via flight plans\n        * Shipyard creation/management\n        * Combat resolution with collision and damage mechanics\n        * Kore mining and regeneration dynamics\n    * Real-time decision making under constraints",
      "Dataset Overview": "* **Data Type:** Game state observations (tabular) and action commands\n* **Context:** Complete information about all game entities (fleets, shipyards, kore distribution) provided each turn\n* **Data Files:**\n    * Training data format not explicitly specified (implied to be game state observations)\n    * Sample submission file demonstrating agent response format\n* **Features:**\n    * Board state (21x21 grid with kore values)\n    * Fleet positions, sizes, kore cargo, and flight plans\n    * Shipyard locations and ownership\n    * Player kore reserves\n    * Turn counter",
      "Evaluation Metrics": "* **Evaluation Metric:** Skill Rating System (Gaussian N(μ,σ²))\n    * Initial μ=600 for new submissions\n    * Updated through competitive matches:\n        * μ increases with wins, decreases with losses\n        * μ values move toward mean for draws\n    * Updates weighted by:\n        * Deviation from expected outcome\n        * Current uncertainty (σ)\n        * σ decreases with more matches\n* **Match Outcomes:**\n    * Determined by:\n        * Total kore collected (deposited at shipyards)\n        * Elimination of all opponents\n    * Score differential doesn't affect rating updates"
    },
    "file_path": "kaggle_datasets/482/problem_summary.md"
  },
  "644": {
    "problem_id": "644",
    "title": "Predicting Multilingual Chatbot Response Preferences",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Multilingual Chatbot Response Preferences\n\n## Problem Description\n- **Problem Type**: Binary Classification (Preference Prediction)\n- **Objective**: Predict which of two chatbot responses (from different LLMs) a human user will prefer in a head-to-head comparison, based on multilingual conversation data from Chatbot Arena.\n- **Key Points**:\n  - Focuses on multilingual conversations (unlike previous English-only competitions)\n  - Aligns with Reinforcement Learning from Human Feedback (RLHF) concepts\n  - Must account for potential biases:\n    * Position bias (favoring first response)\n    * Verbosity bias (favoring longer responses)\n    * Self-enhancement bias (models favoring themselves)\n\n## Dataset Overview\n- **Data Type**: Text conversations (multilingual prompts and LLM responses)\n- **Context**: Real-world user interactions from Chatbot Arena where users compare two anonymous LLM responses\n- **Data Files**:\n  - `train.parquet`: Contains full training data with model identities and language tags\n  - `test.parquet`: Contains test prompts and responses (without labels)\n  - `sample_submission.csv`: Example submission format\n- **Key Features**:\n  - `prompt`: User's input to both models\n  - `response_a`/`response_b`: Competing model responses\n  - `winner`: Ground truth preference label\n  - `language`: Language identifier (in training data only)\n  - `model_a`/`model_b`: Model identifiers (in training data only)\n\n## Evaluation Metrics\n- **Primary Metric**: Accuracy (categorization accuracy)\n- **Implementation**:\n  - Simple percentage of correctly predicted preferences\n  - Binary comparison between predicted and actual `winner` values\n  - Phase-based evaluation:\n    1. Initial scoring on historical test data\n    2. Final scoring on newly collected data post-deadline",
    "sections": {},
    "file_path": "kaggle_datasets/644/problem_summary.md"
  },
  "476": {
    "problem_id": "476",
    "title": "Object Detection for Crown-of-Thorns Starfish in Underwater Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Object Detection for Crown-of-Thorns Starfish in Underwater Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - Object Detection\n* **Objective**: Build a real-time object detection model to identify crown-of-thorns starfish (COTS) in underwater video frames to help protect Australia's Great Barrier Reef.\n    * Key Points:\n        * Focus on detecting a single marine species (COTS) threatening coral reefs\n        * Requires bounding box predictions with confidence scores\n        * Must process images in sequential order (time-series API constraint)\n        * Emphasis on recall to minimize missed detections (tolerates some false positives)\n\n## Dataset Overview\n* **Data Type**: Image data (underwater video frames) with bounding box annotations\n* **Context**: Thousands of reef images collected by underwater cameras for ecological monitoring\n* **Data Files**:\n    * `train/` - Folder containing training images in structure `video_{video_id}/{video_frame_number}.jpg`\n    * `train.csv` - Metadata including:\n        * Video/frame identifiers\n        * Sequence information\n        * Bounding box annotations in Python-evaluable format\n    * `test.csv` - Similar metadata (without annotations)\n    * `example_sample_submission.csv` - Submission format example\n* **Key Features**:\n    * Sequential frames from multiple underwater videos\n    * Bounding box coordinates (x_min, y_min, width, height) for COTS\n    * Images may contain zero or multiple starfish\n\n## Evaluation Metrics\n* **Primary Metric**: Mean F2 Score across IoU thresholds (0.3 to 0.8, step 0.05)\n    * Components:\n        * F2-score weights recall higher than precision (β=2)\n        * Evaluates at multiple IoU thresholds\n        * Uses micro-averaging across all predictions\n        * Bounding boxes evaluated in confidence score order\n        * Final score is mean of F2 scores at each threshold\n* **Submission Format**:\n    * Space-delimited bounding boxes per image\n    * Each box requires: confidence, x, y, width, height\n    * Must use provided time-series API for evaluation",
    "sections": {},
    "file_path": "kaggle_datasets/476/problem_summary.md"
  },
  "220": {
    "problem_id": "220",
    "title": "Transfer Learning for Multi-label Text Tagging on Stack Exchange Questions",
    "problem_type": "Multi-label Text Classification (with Transfer Learning focus)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Transfer Learning for Multi-label Text Tagging on Stack Exchange Questions\n\n**Problem Description:**\n* **Problem Type:** Multi-label Text Classification (with Transfer Learning focus)\n* **Objective:**  \n    Predict relevant tags for physics-related Stack Exchange questions using models trained *only* on text data from unrelated domains (biology, cooking, cryptography, DIY, robotics, and travel). The challenge emphasizes transfer learning, as participants cannot use physics-specific training data.\n* **Key Points:**\n    * Domain adaptation: Training data comes from six non-physics Stack Exchange sites, while test data is exclusively from physics.stackexchange.com.\n    * Multi-label prediction: Each question can have multiple space-delimited tags.\n    * Raw HTML content: Question text includes HTML formatting, requiring preprocessing.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    Tabular text data containing Stack Exchange question titles, HTML-formatted content, and user-assigned tags from six domains (biology, cooking, etc.). Test set contains physics questions without tags.\n* **Data Files:**\n    * `biology.csv.zip`, `cooking.csv.zip`, `crypto.csv.zip`, `diy.csv.zip`, `robotics.csv.zip`, `travel.csv.zip` (training)\n    * `test.csv.zip` (physics questions for prediction)\n    * `sample_submission.csv.zip` (submission format example)\n* **Key Features:**\n    * Title: Short question summary\n    * Content: Detailed question text in HTML\n    * Tags: Space-separated labels (absent in test set)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean F1-Score (macro-averaged)\n    * **Components:**\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * F1 = 2 * (Precision * Recall) / (Precision + Recall)\n    * Metric favors balance between precision and recall\n    * Calculated per tag, then averaged across all tags",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Text Classification (with Transfer Learning focus)\n* **Objective:**  \n    Predict relevant tags for physics-related Stack Exchange questions using models trained *only* on text data from unrelated domains (biology, cooking, cryptography, DIY, robotics, and travel). The challenge emphasizes transfer learning, as participants cannot use physics-specific training data.\n* **Key Points:**\n    * Domain adaptation: Training data comes from six non-physics Stack Exchange sites, while test data is exclusively from physics.stackexchange.com.\n    * Multi-label prediction: Each question can have multiple space-delimited tags.\n    * Raw HTML content: Question text includes HTML formatting, requiring preprocessing.",
      "Dataset Overview": "* **Data Type & Context:**  \n    Tabular text data containing Stack Exchange question titles, HTML-formatted content, and user-assigned tags from six domains (biology, cooking, etc.). Test set contains physics questions without tags.\n* **Data Files:**\n    * `biology.csv.zip`, `cooking.csv.zip`, `crypto.csv.zip`, `diy.csv.zip`, `robotics.csv.zip`, `travel.csv.zip` (training)\n    * `test.csv.zip` (physics questions for prediction)\n    * `sample_submission.csv.zip` (submission format example)\n* **Key Features:**\n    * Title: Short question summary\n    * Content: Detailed question text in HTML\n    * Tags: Space-separated labels (absent in test set)",
      "Evaluation Metrics": "* **Primary Metric:** Mean F1-Score (macro-averaged)\n    * **Components:**\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * F1 = 2 * (Precision * Recall) / (Precision + Recall)\n    * Metric favors balance between precision and recall\n    * Calculated per tag, then averaged across all tags"
    },
    "file_path": "kaggle_datasets/220/problem_summary.md"
  },
  "449": {
    "problem_id": "449",
    "title": "Multi-Agent Reinforcement Learning for Hungry Geese Game",
    "problem_type": "Reinforcement Learning (Multi-Agent Simulation)",
    "objective": "Develop an AI agent to control a goose in a competitive grid-based environment where the goal is to survive longer than opponent geese by eating food while avoiding collisions.",
    "evaluation_metric": null,
    "full_content": "# Multi-Agent Reinforcement Learning for Hungry Geese Game\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning (Multi-Agent Simulation)\n* **Objective:** Develop an AI agent to control a goose in a competitive grid-based environment where the goal is to survive longer than opponent geese by eating food while avoiding collisions.\n    * **Key Points:**\n        * Agents compete on an 11x7 grid, moving in cardinal directions (NORTH, SOUTH, EAST, WEST) with no immediate reversals.\n        * Survival requires balancing food consumption (donuts, pizza, pie, peppers) with collision avoidance (own body, other geese).\n        * Episode length: 200 turns, with geese losing a segment every 40 steps.\n        * Reward = current turn + goose length (survivors get maximum reward: 2×episode_steps + length).\n\n**Dataset Overview:**\n* **Data Type:** Simulation Environment (State Observations)\n    * **Context:** Real-time game state observations provided to agents each turn, including:\n        * Positions of all geese (as segmented lists)\n        * Food locations\n        * Current turn and configuration parameters\n* **Key Features:**\n    * Grid dimensions (11x7)\n    * Goose positions (head + body segments)\n    * Food coordinates\n    * Player index and current reward status\n\n**Evaluation Metrics:**\n* **Primary Metric:** Gaussian Skill Rating (μ, σ²)\n    * **Components:**\n        * Initial μ=600 for new submissions\n        * Updated via head-to-head matches:\n            * Winner's μ increases, loser's decreases (magnitude depends on skill difference)\n            * Draws move both agents' μ toward their mean\n        * σ (uncertainty) decreases with more matches\n    * **Matchmaking:**\n        * Submissions face opponents with similar ratings\n        * ~8 matches/day per agent (prioritizing newer submissions)",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning (Multi-Agent Simulation)\n* **Objective:** Develop an AI agent to control a goose in a competitive grid-based environment where the goal is to survive longer than opponent geese by eating food while avoiding collisions.\n    * **Key Points:**\n        * Agents compete on an 11x7 grid, moving in cardinal directions (NORTH, SOUTH, EAST, WEST) with no immediate reversals.\n        * Survival requires balancing food consumption (donuts, pizza, pie, peppers) with collision avoidance (own body, other geese).\n        * Episode length: 200 turns, with geese losing a segment every 40 steps.\n        * Reward = current turn + goose length (survivors get maximum reward: 2×episode_steps + length).",
      "Dataset Overview": "* **Data Type:** Simulation Environment (State Observations)\n    * **Context:** Real-time game state observations provided to agents each turn, including:\n        * Positions of all geese (as segmented lists)\n        * Food locations\n        * Current turn and configuration parameters\n* **Key Features:**\n    * Grid dimensions (11x7)\n    * Goose positions (head + body segments)\n    * Food coordinates\n    * Player index and current reward status",
      "Evaluation Metrics": "* **Primary Metric:** Gaussian Skill Rating (μ, σ²)\n    * **Components:**\n        * Initial μ=600 for new submissions\n        * Updated via head-to-head matches:\n            * Winner's μ increases, loser's decreases (magnitude depends on skill difference)\n            * Draws move both agents' μ toward their mean\n        * σ (uncertainty) decreases with more matches\n    * **Matchmaking:**\n        * Submissions face opponents with similar ratings\n        * ~8 matches/day per agent (prioritizing newer submissions)"
    },
    "file_path": "kaggle_datasets/449/problem_summary.md"
  },
  "280": {
    "problem_id": "280",
    "title": "Predicting Loan Repayment Risk with Alternative Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Loan Repayment Risk with Alternative Data\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict the probability that a loan applicant will default on their repayment. The goal is to identify clients capable of repayment to ensure loans are granted responsibly while minimizing risk for the lender.\n- **Key Points:**\n  - Focus on applicants with insufficient or non-existent credit histories (unbanked population).\n  - Uses alternative data sources (e.g., telco, transactional information) beyond traditional credit history.\n  - Aims to reduce rejection of creditworthy applicants while avoiding high-risk loans.\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing loan application details and historical financial behavior.\n- **Data Files:**\n  - `application_{train|test}.csv`: Main table with static application data (train includes `TARGET` column).\n  - `bureau.csv`: Previous credits from other financial institutions.\n  - `bureau_balance.csv`: Monthly balances of previous Credit Bureau credits.\n  - `POS_CASH_balance.csv`: Monthly snapshots of point-of-sale/cash loans with Home Credit.\n  - `credit_card_balance.csv`: Monthly snapshots of previous Home Credit credit cards.\n  - `previous_application.csv`: All prior Home Credit loan applications.\n  - `installments_payments.csv`: Repayment history for previous Home Credit loans.\n  - `HomeCredit_columns_description.csv`: Column descriptions for all files.\n- **Features:** Includes anonymized numerical/categorical features related to applicant demographics, loan details, and historical financial behavior across multiple linked tables.\n\n## Evaluation Metrics\n- **Evaluation Metric:** Area Under the ROC Curve (AUC)\n  - Measures the model's ability to distinguish between defaulters and non-defaulters.\n  - Higher values indicate better performance (1.0 = perfect discrimination, 0.5 = random guessing).\n- **Submission Format:** Requires predicted probabilities (`TARGET`) for each `SK_ID_CURR` in the test set.",
    "sections": {},
    "file_path": "kaggle_datasets/280/problem_summary.md"
  },
  "422": {
    "problem_id": "422",
    "title": "Multi-Armed Bandit Competition for Candy Cane Collection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Armed Bandit Competition for Candy Cane Collection\n\n## Problem Description\n* **Problem Type:** Reinforcement Learning (Multi-Armed Bandit Problem)\n* **Objective:** \n    * Design an agent to maximize candy cane collection from 100 vending machines in a competitive two-player environment.\n    * Outperform an opponent by strategically selecting vending machines over 2000 rounds (4000 total pulls).\n* **Key Points:**\n    * Exploration-exploitation tradeoff challenge\n    * Competitive aspect: Opponent's moves are visible but rewards are hidden\n    * Dynamic probability: Reward likelihood decreases by 3% per pull on a machine\n    * Real-time constraints: 0.25s decision time with 60s total overage allowance\n\n## Dataset Overview\n* **Data Type:** Simulation environment (no traditional dataset files)\n* **Context:** \n    * Virtual vending machines with decaying reward probabilities\n    * Agent receives observation state containing:\n        * Current reward total\n        * Both players' last actions\n        * Current step count\n        * Remaining overage time\n* **Key Features:**\n    * 100 bandits (vending machines) with hidden, decaying reward probabilities\n    * Action space: Machine indices (0-99)\n    * Observation space: (reward, lastActions, step, remainingOverageTime)\n\n## Evaluation Metrics\n* **Primary Metric:** Gaussian Skill Rating (μ, σ²)\n    * Initialized at μ₀=600 for new submissions\n    * Updated through competitive matches:\n        * Winners: μ increases\n        * Losers: μ decreases\n        * Draws: μ values move toward their mean\n* **Rating Components:**\n    * Updates are relative to:\n        * Deviation from expected result\n        * Current uncertainty (σ)\n    * Uncertainty decreases with more matches\n    * Leaderboard shows highest μ submission\n* **Matchmaking:**\n    * Agents face opponents with similar ratings\n    * Target: ~8 episodes/day per submission\n    * Validation against self before joining pool",
    "sections": {},
    "file_path": "kaggle_datasets/422/problem_summary.md"
  },
  "610": {
    "problem_id": "610",
    "title": "Predicting Plant Traits from Citizen Science Images for Ecosystem Health",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Plant Traits from Citizen Science Images for Ecosystem Health\n\n## Problem Description\n* **Problem Type:** Multi-Task Regression (Computer Vision - Image Analysis)\n* **Objective:** Predict 6 vital plant traits (e.g., leaf area, plant height) from crowd-sourced plant photographs and ancillary geodata to understand ecosystem health and climate change impacts.\n    * **Key Points:**\n        * Traits describe plant functionality and environmental interactions (e.g., canopy height for light competition, leaf mass for drought resistance).\n        * Focus on weakly supervised learning due to trait-image pairs not being from the same plant individuals/time.\n        * Multi-modal approach encouraged: Combine image data with climate, soil, and satellite data.\n* **Challenge Nuances:**\n    * Moderate accuracy expected due to heterogeneous citizen science data.\n    * Multi-task learning (predicting all traits jointly) is prioritized over single-trait models.\n\n## Dataset Overview\n* **Data Type:** Multi-modal (Image + Tabular)\n    * **Primary Data:** 30,000+ plant photographs (.jpeg) from citizen science platforms (iNaturalist).\n    * **Ancillary Data:** Climate (WORLDCLIM), soil properties (SOIL), and satellite metrics (MODIS/VOD) linked via geocoordinates.\n* **Key Files:**\n    * `train_images/`, `test_images/`: Image folders.\n    * `train.csv`, `test.csv`: Labels (6 traits) and ancillary data for each image.\n    * `target_name_meta.csv`: Trait descriptions from the TRY database.\n* **Notable Features:**\n    * **Image Data:** Crowd-sourced plant photos with species labels.\n    * **Tabular Data:** \n        * Climate variables (temperature, precipitation).\n        * Soil properties (pH, sand content).\n        * Satellite-derived optical/radar metrics (biomass, water content).\n    * **Targets:** 6 trait columns (`X1080_mean`, `X50_mean`, etc.) with species-level standard deviations (`X[*]_sd`).\n\n## Evaluation Metrics\n* **Primary Metric:** Mean R² (Coefficient of Determination) across all 6 traits.\n    * **Calculation:** \n        * R² = 1 - (SS_residual / SS_total).\n        * Only R² values > 0 are considered (negative values clipped to 0).\n    * **Submission Format:** CSV with `id",
    "sections": {},
    "file_path": "kaggle_datasets/610/problem_summary.md"
  },
  "274": {
    "problem_id": "274",
    "title": "Predicting Online Classified Ad Demand",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Online Classified Ad Demand\n\n## Problem Description\n* **Problem Type:** Regression\n* **Objective:** Predict the likelihood (deal_probability) that an online classified ad will result in a successful sale, based on ad content, context, and historical demand data.\n    * **Key Points:**\n        * Focuses on nuanced details in product descriptions that influence buyer interest.\n        * Considers contextual factors like geographic location and similar existing ads.\n        * Aims to help sellers optimize listings and set realistic demand expectations.\n        * Target variable represents probability of sale (continuous value between 0 and 1).\n\n## Dataset Overview\n* **Data Type:** Multimodal (Tabular + Text + Image)\n* **Context:** Russian classified ads platform (Avito) listings with historical performance data.\n* **Data Files:**\n    * Primary files:\n        * train.csv (with deal_probability target)\n        * test.csv\n        * sample_submission.csv\n    * Supplemental files:\n        * train_active.csv/test_active.csv (additional ads without target)\n        * periods_train.csv/periods_test.csv (ad display date ranges)\n        * train_jpg.zip/test_jpg.zip (ad images)\n* **Key Features:**\n    * Structured data: region, price, category hierarchy, parameters, user type\n    * Unstructured data: title text, description text\n    * Image data: product photos with classification codes\n    * Temporal data: activation dates and display periods\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:** \n        * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n        * Where y_i = actual deal probability, ŷ_i = predicted probability\n    * **Key Characteristics:**\n        * Penalizes larger errors more severely\n        * Maintains original units of the target variable (probability)\n        * Requires predictions constrained to [0,1] range",
    "sections": {},
    "file_path": "kaggle_datasets/274/problem_summary.md"
  },
  "628": {
    "problem_id": "628",
    "title": "ARC Prize 2024: Novel Abstract Reasoning Task Solving",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# ARC Prize 2024: Novel Abstract Reasoning Task Solving\n\n## Problem Description\n* **Problem Type**: Abstract Reasoning / Artificial General Intelligence (AGI) Benchmark\n* **Objective**: Develop an AI system capable of solving novel reasoning tasks it has never encountered before, demonstrating human-like generalization without relying on extensive pre-training data.\n    * Tasks involve predicting output grids from input grids based on demonstrated patterns.\n    * Systems must generate exact matches for output grids (all cells correct).\n    * Participants get 2 attempts per test input to predict the correct output.\n* **Key Points**:\n    * Focuses on generalization beyond memorization (tasks are unseen during training).\n    * Targets improvement toward human-level performance (humans score ~85% vs. current AI ~34%).\n    * Encourages approaches beyond traditional LLMs that struggle with novel problems.\n\n## Dataset Overview\n* **Data Type**: JSON files containing grid-based reasoning tasks (2D matrices of integers 0-9 representing colored cells).\n* **Context**: Tasks involve abstract pattern recognition and transformation between input/output grids.\n* **Data Files**:\n    * Training: `arc-agi_training-challenges.json` (demonstration pairs) + `arc-agi_training-solutions.json` (ground truth)\n    * Validation: `arc-agi_evaluation-challenges.json` + `arc-agi_evaluation-solutions.json`\n    * Test: `arc-agi_test-challenges.json` (leaderboard evaluation)\n    * Sample submission: `sample_submission.json`\n* **Features**:\n    * Each task contains:\n        * `train`: 3+ input/output demonstration pairs (grids)\n        * `test`: input grid(s) to predict output for\n    * Grids are rectangular matrices (1x1 to 30x30) with integer values 0-9.\n\n## Evaluation Metrics\n* **Primary Metric**: Percentage of correct predictions (exact grid matches).\n* **Scoring Details**:\n    * For each test output, 2 predictions allowed (`attempt_1`, `attempt_2`).\n    * Score `1` if either attempt exactly matches ground truth, else `0`.\n    * Final score = (Sum of highest scores per task output) / (Total task test outputs).\n* **Submission Format**: JSON file with exactly 2 predictions per test output, maintaining original task order.",
    "sections": {},
    "file_path": "kaggle_datasets/628/problem_summary.md"
  },
  "273": {
    "problem_id": "273",
    "title": "Instance-Level Video Segmentation for Autonomous Driving",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Instance-Level Video Segmentation for Autonomous Driving\n\n## Problem Description\n- **Problem Type**: Computer Vision - Instance Segmentation  \n- **Objective**: Segment movable objects (e.g., cars, pedestrians) at the instance level within image frames captured by vehicle cameras. The goal is to enable autonomous vehicles to distinguish critical objects in their environment accurately.  \n- **Key Points**:  \n  - Focus on **7 evaluated classes**: car, motorcycle, bicycle, pedestrian, truck, bus, and tricycle.  \n  - Groups (e.g., \"car group\") are annotated but excluded from evaluation.  \n  - High-value problem for autonomous driving perception systems.  \n\n## Dataset Overview  \n- **Data Type**: Image data (vehicle camera frames) with instance segmentation labels.  \n- **Context**: Large-scale dataset for autonomous driving, with 10x more fine-labeled images than comparable public datasets.  \n- **Data Files**:  \n  - `train_color.zip`: Original training images.  \n  - `train_label.zip`: Training labels (encoded pixel values for class + instance ID).  \n  - `test.zip`: Test set images.  \n  - `sample_submission.csv`: Example submission in Run-Length Encoding (RLE) format.  \n- **Key Features**:  \n  - **Label Encoding**: Pixel values combine class (`int(PixelValue / 1000)`) and instance ID (`PixelValue % 1000`).  \n  - **Submission Format**: Requires RLE-encoded masks per instance with `ImageId`, `LabelId`, `Confidence`, `PixelCount`, and `EncodedPixels`.  \n\n## Evaluation Metrics  \n- **Primary Metric**: **Mean Average Precision (mAP)** at IoU thresholds (0.5–1.0, step 0.05).  \n- **Components**:  \n  - **IoU Calculation**: `IoU(A, B) = (A ∩ B) / (A ∪ B)`.  \n  - **Matching Logic**:  \n    - Predicted instances are \"matched\" if IoU > threshold.  \n    - For multiple matches, the highest-confidence prediction is a true positive; others are false positives.  \n    - Unmatched predictions or overlaps with ignored labels are penalized.  \n  - **Precision-Recall Curve**: Generated across 10 IoU thresholds.  \n- **Constraints**:  \n  - High computational cost limits submissions to **1 per user per day**.  \n  -",
    "sections": {},
    "file_path": "kaggle_datasets/273/problem_summary.md"
  },
  "617": {
    "problem_id": "617",
    "title": "Emulating Subgrid Atmospheric Processes in Climate Models",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Emulating Subgrid Atmospheric Processes in Climate Models\n\n## Problem Description\n- **Problem Type**: Multivariate Regression (Climate Physics Emulation)\n- **Objective**: Develop ML models to accurately emulate subgrid-scale atmospheric physics processes (e.g., storms, clouds, turbulence, rainfall, radiation) within the E3SM-MMF climate model. The goal is to replace computationally expensive high-resolution climate simulations with cheaper ML emulators while maintaining physical credibility.\n- **Key Points**:\n  - Focus on reducing uncertainty in climate projections (warming trends, precipitation patterns, extreme events)\n  - Must handle multi-scale atmospheric processes with both vertical profiles (60 levels) and surface-level scalar variables\n  - Part of a broader effort to make high-resolution climate projections more accessible for policy decisions\n\n## Dataset Overview\n- **Data Type & Context**: Tabular data generated by the E3SM-MMF climate model, representing inputs/outputs of cloud-resolving models at various locations and timesteps.\n- **Data Files**:\n  - `train.csv` (556 input columns + 368 target columns)\n  - `test.csv` (input columns for prediction)\n  - `sample_submission.csv` (serves as both submission template and weighting file)\n- **Key Features**:\n  - **Inputs**: 25 variables including temperature, humidity, wind speeds, surface fluxes, albedo, and trace gas concentrations (many with 60 vertical levels)\n  - **Targets**: 14 variables including heating/moistening tendencies, wind accelerations, precipitation rates, and radiation fluxes (some with vertical profiles)\n\n## Evaluation Metrics\n- **Primary Metric**: Weighted R-squared (R²) score\n- **Implementation Details**:\n  - Predictions must be element-wise multiplied by weights in `sample_submission.csv`\n  - Top 12 levels of certain variables are zero-weighted\n  - Metric calculation: \n    ```math\n    R² = 1 - (SS_res / SS_tot)\n    ```\n    where SS_res is sum of squared residuals and SS_tot is total sum of squares\n  - Values can range from -∞ to 1 (higher is better)\n  - Negative scores indicate worse performance than predicting the mean",
    "sections": {},
    "file_path": "kaggle_datasets/617/problem_summary.md"
  },
  "425": {
    "problem_id": "425",
    "title": "VinBigData Chest X-ray Abnormalities Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# VinBigData Chest X-ray Abnormalities Detection\n\n## Problem Description\n* **Problem Type:**  \n  *Computer Vision - Object Detection & Multi-class Classification*\n* **Objective:**  \n  Automatically localize and classify 14 types of thoracic abnormalities (e.g., lung opacity, pleural effusion, pneumothorax) from chest radiographs (X-rays). The goal is to provide radiologists with precise bounding boxes and labels for abnormalities to improve diagnostic accuracy.\n  * **Key Points:**\n    * Must handle multi-label detection (images may contain multiple abnormalities).\n    * Requires predicting \"No finding\" (class 14) with a dummy bounding box if no abnormalities are detected.\n    * Ground truth includes annotations from multiple radiologists, introducing variability in labels.\n\n## Dataset Overview\n* **Data Type & Context:**  \n  *DICOM-format chest X-ray images* (postero-anterior views) annotated for 14 thoracic abnormalities. Data is de-identified and sourced from Vietnamese hospitals.\n* **Data Files:**  \n  * `train.csv`: Metadata with class IDs, bounding boxes, and radiologist IDs for each object.  \n  * `sample_submission.csv`: Example submission format.  \n  * Image folders (`train/`, `test/`) containing 18,000 DICOM files (15k for training, 3k for testing).  \n* **Features:**  \n  * Images contain pixel data and DICOM metadata (e.g., patient age, scan parameters).  \n  * Annotations include `class_id`, `class_name`, and bounding box coordinates (`x_min`, `y_min`, `x_max`, `y_max`).  \n\n## Evaluation Metrics\n* **Primary Metric:**  \n  *Mean Average Precision (mAP)* at Intersection-over-Union (IoU) threshold > 0.4 (PASCAL VOC 2010 standard).  \n  * **Components:**  \n    * For each class, compute Average Precision (AP) by ranking predictions by confidence and calculating precision-recall curves.  \n    * mAP is the mean of AP scores across all 14 classes.  \n    * Predictions must include class ID, confidence score, and bounding box coordinates.  \n    * \"No finding\" predictions require a dummy bounding box (`0 0 1 1`) and confidence score.",
    "sections": {},
    "file_path": "kaggle_datasets/425/problem_summary.md"
  },
  "287": {
    "problem_id": "287",
    "title": "Store Item Demand Forecasting Challenge",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict 3 months of sales for 50 different items across 10 different stores using 5 years of historical sales data.",
    "evaluation_metric": null,
    "full_content": "# Store Item Demand Forecasting Challenge\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict 3 months of sales for 50 different items across 10 different stores using 5 years of historical sales data.\n    * **Key Points:**\n        * Explore time series techniques on a clean dataset.\n        * Investigate handling of seasonality.\n        * Compare modeling approaches (e.g., separate store models vs. pooled models).\n        * Evaluate performance of different methods (e.g., ARIMA, XGBoost, deep learning).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data of store-item sales without holiday effects or store closures.\n* **Data Files:**\n    * `train.csv` - Training data with historical sales.\n    * `test.csv` - Test data (time-based public/private split).\n    * `sample_submission.csv` - Submission template.\n* **Features:**\n    * `date` - Date of sale.\n    * `store` - Store ID.\n    * `item` - Item ID.\n    * `sales` - Number of items sold (target variable).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE).\n    * **Components:**\n        * SMAPE = 0 when both actual and predicted values are 0.\n        * Measures accuracy as a percentage of error, symmetric in over- and under-prediction.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict 3 months of sales for 50 different items across 10 different stores using 5 years of historical sales data.\n    * **Key Points:**\n        * Explore time series techniques on a clean dataset.\n        * Investigate handling of seasonality.\n        * Compare modeling approaches (e.g., separate store models vs. pooled models).\n        * Evaluate performance of different methods (e.g., ARIMA, XGBoost, deep learning).",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data of store-item sales without holiday effects or store closures.\n* **Data Files:**\n    * `train.csv` - Training data with historical sales.\n    * `test.csv` - Test data (time-based public/private split).\n    * `sample_submission.csv` - Submission template.\n* **Features:**\n    * `date` - Date of sale.\n    * `store` - Store ID.\n    * `item` - Item ID.\n    * `sales` - Number of items sold (target variable).",
      "Evaluation Metrics": "* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE).\n    * **Components:**\n        * SMAPE = 0 when both actual and predicted values are 0.\n        * Measures accuracy as a percentage of error, symmetric in over- and under-prediction."
    },
    "file_path": "kaggle_datasets/287/problem_summary.md"
  },
  "621": {
    "problem_id": "621",
    "title": "LLM 20 Questions: Cooperative Word Deduction Game",
    "problem_type": "NLP - Cooperative Question Answering & Deduction",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# LLM 20 Questions: Cooperative Word Deduction Game\n\n**Problem Description:**\n* **Problem Type:** NLP - Cooperative Question Answering & Deduction\n* **Objective:**  \n    * Develop a language model (LLM) capable of playing the game \"20 Questions\" in a cooperative 2v2 format. \n    * Teams consist of paired LLMs: one acting as a *guesser* (asking yes/no questions and making guesses) and one as an *answerer* (responding to questions with \"yes\" or \"no\"). \n    * The goal is to correctly identify a secret word in as few rounds as possible (max 20 rounds) through strategic questioning and logical inference.\n* **Key Points:**\n    * **Cooperative Dynamics:** Teams win/lose as a pair, incentivizing collaboration between guesser and answerer LLMs.\n    * **Constrained Interaction:** \n        * Questions limited to 750 characters, guesses to 100 characters.\n        * Strict \"yes/no\" responses required from answerers.\n    * **Evaluation Focus:** Tests LLMs' abilities in deductive reasoning, information gathering via targeted questioning, and collaborative strategy.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Simulation-based competition; no traditional dataset provided. \n    * Participants submit LLM agents that interact dynamically within the game environment.\n* **Key Features:**  \n    * **Secret Words:** Unspecified in training data (evaluation uses unpublished words during final rounds).\n    * **Interaction Logs:** Game rounds record questions, answers, and guesses between paired agents.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Skill Rating (Gaussian model `N(μ, σ²)`)  \n    * **Components:**  \n        * **μ (Skill Estimate):** Updated based on match outcomes (wins increase μ, losses decrease μ, ties move μ toward the mean of paired agents).\n        * **σ (Uncertainty):** Decreases over time as more games are played.\n    * **Ranking Logic:**  \n        * Teams face off in episodes; skill ratings adjust relative to opponents' ratings and result deviation from expectations.\n        * Final leaderboard determined by performance against secret words in a post-deadline evaluation phase.",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Cooperative Question Answering & Deduction\n* **Objective:**  \n    * Develop a language model (LLM) capable of playing the game \"20 Questions\" in a cooperative 2v2 format. \n    * Teams consist of paired LLMs: one acting as a *guesser* (asking yes/no questions and making guesses) and one as an *answerer* (responding to questions with \"yes\" or \"no\"). \n    * The goal is to correctly identify a secret word in as few rounds as possible (max 20 rounds) through strategic questioning and logical inference.\n* **Key Points:**\n    * **Cooperative Dynamics:** Teams win/lose as a pair, incentivizing collaboration between guesser and answerer LLMs.\n    * **Constrained Interaction:** \n        * Questions limited to 750 characters, guesses to 100 characters.\n        * Strict \"yes/no\" responses required from answerers.\n    * **Evaluation Focus:** Tests LLMs' abilities in deductive reasoning, information gathering via targeted questioning, and collaborative strategy.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Simulation-based competition; no traditional dataset provided. \n    * Participants submit LLM agents that interact dynamically within the game environment.\n* **Key Features:**  \n    * **Secret Words:** Unspecified in training data (evaluation uses unpublished words during final rounds).\n    * **Interaction Logs:** Game rounds record questions, answers, and guesses between paired agents.",
      "Evaluation Metrics": "* **Primary Metric:** Skill Rating (Gaussian model `N(μ, σ²)`)  \n    * **Components:**  \n        * **μ (Skill Estimate):** Updated based on match outcomes (wins increase μ, losses decrease μ, ties move μ toward the mean of paired agents).\n        * **σ (Uncertainty):** Decreases over time as more games are played.\n    * **Ranking Logic:**  \n        * Teams face off in episodes; skill ratings adjust relative to opponents' ratings and result deviation from expectations.\n        * Final leaderboard determined by performance against secret words in a post-deadline evaluation phase."
    },
    "file_path": "kaggle_datasets/621/problem_summary.md"
  },
  "413": {
    "problem_id": "413",
    "title": "Analyzing the 2020 Kaggle Machine Learning & Data Science Survey",
    "problem_type": "Survey Analysis / Data Storytelling",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Analyzing the 2020 Kaggle Machine Learning & Data Science Survey\n\n**Problem Description:**\n* **Problem Type:** Survey Analysis / Data Storytelling\n* **Objective:**  \n    * Participants are tasked with creating a compelling data-driven narrative about a specific subset of the data science community represented in the 2020 Kaggle survey. \n    * The goal is to deeply explore the impact, priorities, or concerns of a defined group (e.g., Python users, female students in ML, etc.) through a combination of narrative text and data exploration.\n* **Key Points:**\n    * Focus on **storytelling** through data, not traditional ML modeling.\n    * Submissions must define a **specific community** within the broader survey respondents.\n    * Creativity in defining the subgroup and uncovering insights is encouraged.\n    * Submissions must be self-contained in a single, well-documented notebook.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Tabular survey data containing responses from 20,036 participants in the 2020 Kaggle ML & DS Survey.\n    * Covers diverse topics including demographics, tools, algorithms, industry trends, and education in data science.\n* **Data Files:**  \n    * `kaggle_survey_2020_responses.csv`: Contains 39+ questions and 20,036 responses (355 columns).\n    * Supplementary PDFs with answer choices and survey methodology.\n* **Features:**  \n    * Mix of multiple-choice (single-select) and multiple-selection questions (split into multiple columns).\n    * Anonymized responses covering topics like programming languages, job roles, compensation, and ML practices.\n\n**Evaluation Metrics:**\n* **Evaluation Criteria:**  \n    Submissions are judged holistically on:\n    * **Composition:**  \n        * Clear narrative thread supported by data.\n        * Well-defined subject with robust data/visualization support.\n    * **Originality:**  \n        * Novel insights or thought-provoking perspectives.\n        * Informative and fresh analysis.\n    * **Documentation:**  \n        * Code and notebook clarity and reproducibility.\n        * Proper source citation and rationale explanation.\n* **Note:** No traditional quantitative metric (e.g., AUC, RMSE) is used; judging is qualitative based on storytelling quality.",
    "sections": {
      "Problem Description": "* **Problem Type:** Survey Analysis / Data Storytelling\n* **Objective:**  \n    * Participants are tasked with creating a compelling data-driven narrative about a specific subset of the data science community represented in the 2020 Kaggle survey. \n    * The goal is to deeply explore the impact, priorities, or concerns of a defined group (e.g., Python users, female students in ML, etc.) through a combination of narrative text and data exploration.\n* **Key Points:**\n    * Focus on **storytelling** through data, not traditional ML modeling.\n    * Submissions must define a **specific community** within the broader survey respondents.\n    * Creativity in defining the subgroup and uncovering insights is encouraged.\n    * Submissions must be self-contained in a single, well-documented notebook.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Tabular survey data containing responses from 20,036 participants in the 2020 Kaggle ML & DS Survey.\n    * Covers diverse topics including demographics, tools, algorithms, industry trends, and education in data science.\n* **Data Files:**  \n    * `kaggle_survey_2020_responses.csv`: Contains 39+ questions and 20,036 responses (355 columns).\n    * Supplementary PDFs with answer choices and survey methodology.\n* **Features:**  \n    * Mix of multiple-choice (single-select) and multiple-selection questions (split into multiple columns).\n    * Anonymized responses covering topics like programming languages, job roles, compensation, and ML practices.",
      "Evaluation Metrics": "* **Evaluation Criteria:**  \n    Submissions are judged holistically on:\n    * **Composition:**  \n        * Clear narrative thread supported by data.\n        * Well-defined subject with robust data/visualization support.\n    * **Originality:**  \n        * Novel insights or thought-provoking perspectives.\n        * Informative and fresh analysis.\n    * **Documentation:**  \n        * Code and notebook clarity and reproducibility.\n        * Proper source citation and rationale explanation.\n* **Note:** No traditional quantitative metric (e.g., AUC, RMSE) is used; judging is qualitative based on storytelling quality."
    },
    "file_path": "kaggle_datasets/413/problem_summary.md"
  },
  "245": {
    "problem_id": "245",
    "title": "Binary Classification for Auto Insurance Claim Prediction",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that an auto insurance policyholder will file a claim in the next year. The goal is to improve the accuracy of claim predictions to enable fairer pricing for drivers.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification for Auto Insurance Claim Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that an auto insurance policyholder will file a claim in the next year. The goal is to improve the accuracy of claim predictions to enable fairer pricing for drivers.\n* **Key Points:**\n  * The competition focuses on addressing inaccuracies in current insurance claim prediction models.\n  * The model's predictions will be used to tailor insurance prices more effectively.\n  * Features are anonymized and grouped by similarity (e.g., `ind`, `reg`, `car`, `calc`).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing policyholder information, with anonymized features related to insurance policies and drivers.\n* **Data Files:**\n  * `train.csv`: Training data with policyholder features and a binary `target` column indicating whether a claim was filed.\n  * `test.csv`: Test data for making predictions.\n  * `sample_submission.csv`: Example submission file in the correct format.\n* **Features:**\n  * Features include binary (`bin`), categorical (`cat`), continuous, and ordinal types.\n  * Missing values are indicated by `-1`.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Normalized Gini Coefficient\n* **Components:**\n  * Observations are ranked by predicted probabilities (magnitude is irrelevant for scoring).\n  * The metric compares the cumulative proportion of positive class observations to a theoretical uniform proportion.\n  * The Gini Coefficient ranges from ~0 (random guessing) to ~0.5 (perfect score), normalized to a maximum of 1.\n  * The theoretical maximum for the discrete calculation is `(1 - frac_pos) / 2`.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that an auto insurance policyholder will file a claim in the next year. The goal is to improve the accuracy of claim predictions to enable fairer pricing for drivers.\n* **Key Points:**\n  * The competition focuses on addressing inaccuracies in current insurance claim prediction models.\n  * The model's predictions will be used to tailor insurance prices more effectively.\n  * Features are anonymized and grouped by similarity (e.g., `ind`, `reg`, `car`, `calc`).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing policyholder information, with anonymized features related to insurance policies and drivers.\n* **Data Files:**\n  * `train.csv`: Training data with policyholder features and a binary `target` column indicating whether a claim was filed.\n  * `test.csv`: Test data for making predictions.\n  * `sample_submission.csv`: Example submission file in the correct format.\n* **Features:**\n  * Features include binary (`bin`), categorical (`cat`), continuous, and ordinal types.\n  * Missing values are indicated by `-1`.",
      "Evaluation Metrics": "* **Evaluation Metric:** Normalized Gini Coefficient\n* **Components:**\n  * Observations are ranked by predicted probabilities (magnitude is irrelevant for scoring).\n  * The metric compares the cumulative proportion of positive class observations to a theoretical uniform proportion.\n  * The Gini Coefficient ranges from ~0 (random guessing) to ~0.5 (perfect score), normalized to a maximum of 1.\n  * The theoretical maximum for the discrete calculation is `(1 - frac_pos) / 2`."
    },
    "file_path": "kaggle_datasets/245/problem_summary.md"
  },
  "289": {
    "problem_id": "289",
    "title": "New York City Taxi Fare Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# New York City Taxi Fare Prediction\n\n## Problem Description\n* **Problem Type**: Regression\n* **Objective**: Predict the fare amount (inclusive of tolls) for a taxi ride in New York City given pickup and dropoff locations, datetime, and passenger count. The goal is to achieve a lower RMSE than a basic distance-based estimate (which yields $5-$8 RMSE).\n* **Key Points**:\n  * The challenge involves improving upon simple distance-based fare estimates using machine learning.\n  * The problem is framed as a playground competition with a focus on handling large datasets (55M training rows).\n\n## Dataset Overview\n* **Data Type & Context**: Tabular data containing historical NYC taxi ride records with spatiotemporal and passenger count features.\n* **Data Files**:\n  * `train.csv`: Contains features and target (`fare_amount`) for ~55M rows.\n  * `test.csv`: Contains features for ~10K rows (target to be predicted).\n  * `sample_submission.csv`: Example submission with mean fare prediction.\n* **Features**:\n  * **Spatial**: `pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, `dropoff_latitude`\n  * **Temporal**: `pickup_datetime` (timestamp)\n  * **Passenger**: `passenger_count` (integer)\n  * **ID**: `key` (unique string combining pickup datetime and integer)\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean-Squared Error (RMSE)\n  * Measures average deviation between predicted and actual fare amounts in dollars.\n  * Formula:  \n    ```RMSE = sqrt(1/n * Σ(ŷ_i - y_i)^2)```  \n    where `y_i` = actual fare, `ŷ_i` = predicted fare, `n` = number of observations.\n  * Key properties:\n    * Error is in the same units as the target (dollars).\n    * Smaller values indicate better performance.",
    "sections": {},
    "file_path": "kaggle_datasets/289/problem_summary.md"
  },
  "619": {
    "problem_id": "619",
    "title": "Binary Classification of Insurance Cross Selling",
    "problem_type": "Binary Classification",
    "objective": "Predict which customers will respond positively to an automobile insurance cross-selling offer. The goal is to classify customers into two categories (responders vs. non-responders) based on their likelihood to accept the insurance offer.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Insurance Cross Selling\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict which customers will respond positively to an automobile insurance cross-selling offer. The goal is to classify customers into two categories (responders vs. non-responders) based on their likelihood to accept the insurance offer.\n* **Key Points:**\n  * The dataset is synthetically generated but based on real-world health insurance cross-sell data.\n  * Participants are encouraged to explore the original dataset for potential performance improvements.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing customer-related features for insurance cross-selling prediction.\n* **Data Files:**\n  * `train.csv`: Training dataset with the binary target column `Response`.\n  * `test.csv`: Test dataset for which predictions need to be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:** The dataset includes various customer attributes (exact features not specified, but likely similar to the original dataset which contained demographic and insurance-related features).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n  * **Components:**\n    * Measures the ability of the model to distinguish between positive and negative classes\n    * Calculated by plotting the True Positive Rate against the False Positive Rate at various threshold settings\n    * Higher values indicate better model performance (1.0 = perfect classifier, 0.5 = random guessing)\n  * Submissions require predicted probabilities for the positive class (`Response = 1`)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict which customers will respond positively to an automobile insurance cross-selling offer. The goal is to classify customers into two categories (responders vs. non-responders) based on their likelihood to accept the insurance offer.\n* **Key Points:**\n  * The dataset is synthetically generated but based on real-world health insurance cross-sell data.\n  * Participants are encouraged to explore the original dataset for potential performance improvements.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing customer-related features for insurance cross-selling prediction.\n* **Data Files:**\n  * `train.csv`: Training dataset with the binary target column `Response`.\n  * `test.csv`: Test dataset for which predictions need to be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:** The dataset includes various customer attributes (exact features not specified, but likely similar to the original dataset which contained demographic and insurance-related features).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n  * **Components:**\n    * Measures the ability of the model to distinguish between positive and negative classes\n    * Calculated by plotting the True Positive Rate against the False Positive Rate at various threshold settings\n    * Higher values indicate better model performance (1.0 = perfect classifier, 0.5 = random guessing)\n  * Submissions require predicted probabilities for the positive class (`Response = 1`)"
    },
    "file_path": "kaggle_datasets/619/problem_summary.md"
  },
  "242": {
    "problem_id": "242",
    "title": "Web Traffic Time Series Forecasting",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Web Traffic Time Series Forecasting\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting (Multi-step, Multi-series)\n* **Objective:**  \n  Forecast future daily web traffic (page views) for approximately 145,000 Wikipedia articles. The competition involves predicting traffic for two distinct phases:  \n  - Historical validation period (Jan 1, 2017 – Mar 1, 2017)  \n  - Real future period (Sep 13, 2017 – Nov 13, 2017)  \n* **Key Points:**  \n  - **Multi-series challenge:** Simultaneously model thousands of time series with potential interdependencies.  \n  - **Metadata utilization:** Articles have associated metadata (project, access type, agent type) that can be leveraged.  \n  - **Data ambiguity:** Zero values and missing data are indistinguishable in the dataset.  \n  - **Flexible modeling approaches:** Participants can use univariate/multivariate models, hierarchical modeling, external data (e.g., Google Trends), and custom preprocessing.  \n\n## Dataset Overview\n* **Data Type:** Tabular time series data (daily frequency) with metadata  \n* **Context:** Wikipedia page view statistics spanning 2015-2017  \n* **Data Files:**  \n  - `train_[1|2].csv`: Rows represent articles, columns represent dates (Jul 1, 2015 – Dec 31, 2016 for stage 1)  \n  - `key_[1|2].csv`: Mapping between article IDs and metadata strings  \n  - `sample_submission_[1|2].csv`: Submission format templates  \n* **Key Features:**  \n  - **Time series values:** Daily view counts (with missing/zero ambiguity)  \n  - **Metadata fields:** Embedded in article names as `name_project_access_agent` (e.g., \"AKB48_zh.wikipedia.org_all-access_spider\")  \n\n## Evaluation Metrics\n* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE)  \n* **SMAPE Details:**  \n  - Defined as 0 when both actual and predicted values are 0  \n  - Formula: `SMAPE = (200% * Σ|F_t - A_t|) / Σ(F_t + A_t)` where:  \n    - `F_t` = Forecast value  \n    - `A_t` = Actual value  \n  - Penalizes both over- and under-predictions symmetrically",
    "sections": {},
    "file_path": "kaggle_datasets/242/problem_summary.md"
  },
  "414": {
    "problem_id": "414",
    "title": "Volcanic Eruption Time Prediction with Seismic Sensor Data",
    "problem_type": "Regression (Time-to-Event Prediction)",
    "objective": "Predict the time remaining until the next volcanic eruption using seismic sensor data. The goal is to identify precursor patterns in geophysical signals that can enable longer-term eruption forecasts beyond current minute-scale predictions.",
    "evaluation_metric": null,
    "full_content": "# Volcanic Eruption Time Prediction with Seismic Sensor Data\n\n**Problem Description:**\n* **Problem Type:** Regression (Time-to-Event Prediction)\n* **Objective:** Predict the time remaining until the next volcanic eruption using seismic sensor data. The goal is to identify precursor patterns in geophysical signals that can enable longer-term eruption forecasts beyond current minute-scale predictions.\n    * **Key Points:**\n        * Focus on detecting early seismic signatures preceding eruptions\n        * Improve upon current methods that only work for short-term predictions\n        * Potential real-world impact for timely evacuations and risk mitigation\n\n**Dataset Overview:**\n* **Data Type & Context:** Time-series sensor data from 10 seismic instruments deployed around active volcanoes, with normalized readings spanning 10-minute segments.\n* **Data Files:**\n    * `train.csv`: Metadata with segment IDs and target values (time_to_eruption)\n    * `train/*.csv`: Individual sensor reading files for training (10 sensors per segment)\n    * `test/*.csv`: Analogous test set files without target values\n    * `sample_submission.csv`: Submission format template\n* **Features:**\n    * Normalized seismic waveforms (int16 values with some nulls)\n    * Sensor arrays capturing volcanic tremor patterns\n    * Time-aligned multi-sensor measurements\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)\n    * Measures average absolute difference between predicted and actual time-to-eruption\n    * Lower values indicate better performance (perfect prediction would yield MAE=0)",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Time-to-Event Prediction)\n* **Objective:** Predict the time remaining until the next volcanic eruption using seismic sensor data. The goal is to identify precursor patterns in geophysical signals that can enable longer-term eruption forecasts beyond current minute-scale predictions.\n    * **Key Points:**\n        * Focus on detecting early seismic signatures preceding eruptions\n        * Improve upon current methods that only work for short-term predictions\n        * Potential real-world impact for timely evacuations and risk mitigation",
      "Dataset Overview": "* **Data Type & Context:** Time-series sensor data from 10 seismic instruments deployed around active volcanoes, with normalized readings spanning 10-minute segments.\n* **Data Files:**\n    * `train.csv`: Metadata with segment IDs and target values (time_to_eruption)\n    * `train/*.csv`: Individual sensor reading files for training (10 sensors per segment)\n    * `test/*.csv`: Analogous test set files without target values\n    * `sample_submission.csv`: Submission format template\n* **Features:**\n    * Normalized seismic waveforms (int16 values with some nulls)\n    * Sensor arrays capturing volcanic tremor patterns\n    * Time-aligned multi-sensor measurements",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)\n    * Measures average absolute difference between predicted and actual time-to-eruption\n    * Lower values indicate better performance (perfect prediction would yield MAE=0)"
    },
    "file_path": "kaggle_datasets/414/problem_summary.md"
  },
  "626": {
    "problem_id": "626",
    "title": "Extracting Exoplanet Atmospheric Spectra from Noisy Telescope Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Extracting Exoplanet Atmospheric Spectra from Noisy Telescope Data\n\n## Problem Description\n* **Problem Type**: Multimodal Regression (with uncertainty estimation)\n* **Objective**: Extract faint exoplanetary atmospheric spectra from noisy telescope observations by detrending instrument noise (primarily \"jitter noise\" from spacecraft vibrations). Participants must predict both the spectra (mean signal) and associated uncertainty for each wavelength.\n* **Key Points**:\n  * Signals are extremely faint (50-200 parts per million) and corrupted by noise comparable in magnitude to the signal.\n  * Data comes from simulated observations of ESA's Ariel Mission (launching 2029).\n  * Multimodal nature: Solutions may leverage image, time, or spectral domains (or combinations).\n\n## Dataset Overview\n* **Data Type**: Time-series of 2D spectral images from two telescope instruments (FGS1 and AIRS-CH0)\n* **Context**: Simulated exoplanet transit observations with calibration data\n* **Data Files**:\n  * `[train/test]/[planet_id]/AIRS-CH0_signal.parquet` (11,250 frames per planet)\n  * `[train/test]/[planet_id]/FGS1_signal.parquet` (135,000 frames per planet)\n  * Calibration files (dark, flat, dead pixel maps, etc.)\n  * `train_labels.csv` (ground truth spectra)\n  * `wavelengths.csv` (spectral grid)\n* **Key Features**:\n  * Images are flattened time-series (32x356 for AIRS-CH0, 32x32 for FGS1)\n  * Requires ADC conversion using gain/offset from metadata files\n  * Calibration files provided for noise correction\n\n## Evaluation Metrics\n* **Primary Metric**: Custom Gaussian Log-Likelihood (GLL) score\n* **Components**:\n  * For each wavelength: `GLL = -0.5 * [log(2π) + log(σ²) + (y-μ)²/σ²]`\n  * Final score transformation: `score = (L - L_ref)/(L_ideal - L_ref)`\n    * `L`: Sum of GLL across all wavelengths/test samples\n    * `L_ideal`: Perfect prediction with 10ppm uncertainty\n    * `L_ref`: Baseline using training set mean/variance\n  * Scores range [0,1",
    "sections": {},
    "file_path": "kaggle_datasets/626/problem_summary.md"
  },
  "477": {
    "problem_id": "477",
    "title": "Multiclass Classification of Bacterial Species from Genomic Histograms",
    "problem_type": "Multiclass Classification",
    "objective": "Predict the correct bacterial species (10 possible classes) based on lossy genomic data derived from 10-mer DNA snippets. The data represents histograms of base counts (A, T, G, C) after compression (e.g., ATATGGCCTT → A2T4G2C2).",
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of Bacterial Species from Genomic Histograms\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the correct bacterial species (10 possible classes) based on lossy genomic data derived from 10-mer DNA snippets. The data represents histograms of base counts (A, T, G, C) after compression (e.g., ATATGGCCTT → A2T4G2C2).\n* **Key Points:**\n  * Data contains simulated measurement errors of varying rates, adding noise to the classification task.\n  * The genomic analysis technique involves data compression and loss, requiring models to handle imperfect representations.\n  * Competition designed as an approachable playground for intermediate practitioners between beginner and featured competitions.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing histograms of DNA base counts from Raman spectroscopy analysis of 10-mer snippets.\n* **Data Files:**\n  * `train.csv`: Contains spectrum of 10-mer histograms (286 features) for training samples with species labels.\n  * `test.csv`: Contains test samples requiring species prediction.\n  * `sample_submission.csv`: Example submission file format.\n* **Features:**\n  * 286 histogram possibilities representing base count combinations (A0T0G0C10 to A10T0G0C0 etc.).\n  * Features are derived from bias-subtracted spectra of repeated measurements.\n  * Target variable: 10 bacterial species classes (e.g., Streptococcus_pneumoniae, Enterococcus_hirae).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy (simple classification accuracy).\n  * Straightforward percentage of correctly predicted bacterial species.\n  * Matches standard multiclass classification evaluation approach.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the correct bacterial species (10 possible classes) based on lossy genomic data derived from 10-mer DNA snippets. The data represents histograms of base counts (A, T, G, C) after compression (e.g., ATATGGCCTT → A2T4G2C2).\n* **Key Points:**\n  * Data contains simulated measurement errors of varying rates, adding noise to the classification task.\n  * The genomic analysis technique involves data compression and loss, requiring models to handle imperfect representations.\n  * Competition designed as an approachable playground for intermediate practitioners between beginner and featured competitions.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing histograms of DNA base counts from Raman spectroscopy analysis of 10-mer snippets.\n* **Data Files:**\n  * `train.csv`: Contains spectrum of 10-mer histograms (286 features) for training samples with species labels.\n  * `test.csv`: Contains test samples requiring species prediction.\n  * `sample_submission.csv`: Example submission file format.\n* **Features:**\n  * 286 histogram possibilities representing base count combinations (A0T0G0C10 to A10T0G0C0 etc.).\n  * Features are derived from bias-subtracted spectra of repeated measurements.\n  * Target variable: 10 bacterial species classes (e.g., Streptococcus_pneumoniae, Enterococcus_hirae).",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy (simple classification accuracy).\n  * Straightforward percentage of correctly predicted bacterial species.\n  * Matches standard multiclass classification evaluation approach."
    },
    "file_path": "kaggle_datasets/477/problem_summary.md"
  },
  "645": {
    "problem_id": "645",
    "title": "Multi-Agent Reinforcement Learning for Space Exploration Strategy",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Agent Reinforcement Learning for Space Exploration Strategy\n\n## Problem Description\n* **Problem Type**: Reinforcement Learning (Multi-Agent Strategy Game)\n* **Objective**: \n    * Design AI agents that can play a 1v1 strategy game involving resource gathering, allocation, and tactical decision-making in a procedurally generated space environment\n    * Agents must optimize exploration vs. exploitation while adapting to randomized game mechanics and opponent strategies\n* **Key Points**:\n    * Game consists of best-of-5 match sequences with 100 time steps each\n    * Core mechanics include fog-of-war, randomized map generation, and dynamic unit interactions\n    * Agents must handle partial observability and adapt to procedurally generated parameters\n    * Strategic elements include energy management, unit positioning, and relic point capture\n\n## Dataset Overview\n* **Data Type**: Simulation environment with programmatic API\n* **Context**: \n    * 2D grid-based space exploration game with procedurally generated maps (24x24 grid)\n    * Environment features include asteroids, nebula clouds, energy nodes, and relic nodes\n* **Key Components**:\n    * Game state observations provided through Python API\n    * Randomized parameters affecting movement costs, vision range, energy mechanics\n    * Starter kits available in multiple programming languages\n* **Important Features**:\n    * Unit positions and energy levels\n    * Map terrain features (nebula, asteroids)\n    * Resource node locations\n    * Fog-of-war visibility constraints\n\n## Evaluation Metrics\n* **Primary Metric**: Skill Rating System (Gaussian N(μ,σ²))\n    * μ represents estimated skill\n    * σ represents uncertainty in rating\n* **Rating Updates**:\n    * Initial μ₀ = 600 for new submissions\n    * Updated based on match outcomes against similarly-rated opponents\n    * Wins increase μ, losses decrease μ\n    * Draws move ratings toward their mean\n* **Final Ranking**:\n    * Based on stable skill ratings after extensive match play\n    * Only best submission per team counts for final leaderboard\n    * Rating updates consider deviation from expected results and current uncertainty",
    "sections": {},
    "file_path": "kaggle_datasets/645/problem_summary.md"
  },
  "221": {
    "problem_id": "221",
    "title": "NCAA Basketball Tournament Outcome Prediction",
    "problem_type": "Binary Classification (with probabilistic outputs)",
    "objective": "Predict the probability of one team beating another in NCAA basketball tournament matchups. Participants must forecast outcomes for:",
    "evaluation_metric": null,
    "full_content": "# NCAA Basketball Tournament Outcome Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. Participants must forecast outcomes for:\n    * **Stage 1:** Historical tournaments (2013-2016) for model validation\n    * **Stage 2:** All possible matchups in the 2017 tournament before it begins\n* **Key Points:**\n    * Requires predicting every possible team pairing (n*(n-1)/2 predictions)\n    * Play-in games are excluded from evaluation\n    * Team1 vs Team2 is treated identically to Team2 vs Team1 (ordered by team ID)\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with game results, team statistics, and tournament structure\n* **Primary Data Files:**\n    * `Teams.csv` - Team IDs and metadata\n    * `Seasons.csv` - Yearly tournament configurations\n    * `RegularSeasonCompactResults.csv` - Basic game results (1985-2015)\n    * `RegularSeasonDetailedResults.csv` - Advanced game stats (2003-2016)\n    * `Tourney[Compact/Detailed]Results.csv` - Tournament-specific results\n    * `TourneySeeds.csv` - Team seeding information\n    * `TourneySlots.csv` - Tournament bracket structure\n* **Key Features:**\n    * Game outcomes (win/loss, scores)\n    * Team performance metrics (FG%, rebounds, assists, etc.)\n    * Tournament seeds and bracket positions\n    * Temporal features (daynum relative to season start)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n    * Formula: \n        ```\n        LogLoss = −1/n ∑[y_i·log(ŷ_i) + (1−y_i)·log(1−ŷ_i)]\n        ```\n    * Components:\n        * n = number of scored games (excluding play-ins)\n        * ŷ_i = predicted probability of Team1 beating Team2\n        * y_i = 1 if Team1 wins, 0 otherwise\n    * Key Properties:\n        * Heavily penalizes confident incorrect predictions\n        * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n        * All games weighted equally (no extra weight for later rounds)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. Participants must forecast outcomes for:\n    * **Stage 1:** Historical tournaments (2013-2016) for model validation\n    * **Stage 2:** All possible matchups in the 2017 tournament before it begins\n* **Key Points:**\n    * Requires predicting every possible team pairing (n*(n-1)/2 predictions)\n    * Play-in games are excluded from evaluation\n    * Team1 vs Team2 is treated identically to Team2 vs Team1 (ordered by team ID)",
      "Dataset Overview": "* **Data Type:** Tabular data with game results, team statistics, and tournament structure\n* **Primary Data Files:**\n    * `Teams.csv` - Team IDs and metadata\n    * `Seasons.csv` - Yearly tournament configurations\n    * `RegularSeasonCompactResults.csv` - Basic game results (1985-2015)\n    * `RegularSeasonDetailedResults.csv` - Advanced game stats (2003-2016)\n    * `Tourney[Compact/Detailed]Results.csv` - Tournament-specific results\n    * `TourneySeeds.csv` - Team seeding information\n    * `TourneySlots.csv` - Tournament bracket structure\n* **Key Features:**\n    * Game outcomes (win/loss, scores)\n    * Team performance metrics (FG%, rebounds, assists, etc.)\n    * Tournament seeds and bracket positions\n    * Temporal features (daynum relative to season start)",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (LogLoss)\n    * Formula: \n        ```\n        LogLoss = −1/n ∑[y_i·log(ŷ_i) + (1−y_i)·log(1−ŷ_i)]\n        ```\n    * Components:\n        * n = number of scored games (excluding play-ins)\n        * ŷ_i = predicted probability of Team1 beating Team2\n        * y_i = 1 if Team1 wins, 0 otherwise\n    * Key Properties:\n        * Heavily penalizes confident incorrect predictions\n        * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n        * All games weighted equally (no extra weight for later rounds)"
    },
    "file_path": "kaggle_datasets/221/problem_summary.md"
  },
  "483": {
    "problem_id": "483",
    "title": "Whale and Dolphin Identification via Image Matching",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Whale and Dolphin Identification via Image Matching\n\n## Problem Description\n- **Problem Type:** Multi-class Image Classification (with open-set recognition for unseen individuals)\n- **Objective:** Develop a model to identify individual whales and dolphins by their unique natural markings (e.g., dorsal fins, body patterns) in photographs. The goal is to automate photo-ID for marine mammal research, reducing manual matching efforts by 99%.\n- **Key Points:**\n  - Must handle **30 different species** with varying distinctiveness of features\n  - Some individuals have **highly distinct markings** while others are nearly indistinguishable\n  - Features may **change over time** due to natural causes or injuries\n  - Test set contains **previously unseen individuals** that should be classified as \"new_individual\"\n  - Focus on **dorsal fins and lateral body views** as key identification features\n\n## Dataset Overview\n- **Data Type & Context:** Image dataset of marine mammals collected by 28 research institutions\n- **Data Files:**\n  - `train_images/`: Folder containing training images (JPEG)\n  - `train.csv`: Contains `image` filename, `species` (30 classes), and `individual_id` (15,000+ unique individuals)\n  - `test_images/`: Folder containing test images (JPEG)\n  - `sample_submission.csv`: Submission template with format requirements\n- **Features:**\n  - Images show **natural markings** on dorsal fins, backs, heads, and flanks\n  - Some images may have **quality issues** due to multi-source collection\n  - No species labels provided for test images\n\n## Evaluation Metrics\n- **Primary Metric:** Mean Average Precision @ 5 (MAP@5)\n- **Metric Components:**\n  - For each test image, predict up to 5 possible `individual_id` labels (ordered by confidence)\n  - Correct predictions score higher when ranked earlier\n  - Formula: \n    ```\n    MAP@5 = (1/U) * Σ(u=1 to U) Σ(k=1 to min(n,5)) P(k) × rel(k)\n    ```\n    Where:\n    - U = number of images\n    - P(k) = precision at cutoff k\n    - n = number of predictions per image (max 5)\n    - rel(k) = 1 if item at rank k is correct, else 0\n  - Once a correct label is found",
    "sections": {},
    "file_path": "kaggle_datasets/483/problem_summary.md"
  },
  "448": {
    "problem_id": "448",
    "title": "COVID-19 Detection and Localization in Chest Radiographs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# COVID-19 Detection and Localization in Chest Radiographs\n\n## Problem Description\n* **Problem Type**:  \n  * Computer Vision - Object Detection (Bounding Box Localization)  \n  * Multi-label Classification (Study-Level Labels)  \n\n* **Objective**:  \n  * Detect and localize COVID-19-related abnormalities (opacities) in chest radiographs via bounding boxes.  \n  * Classify each study (multi-image set) into one or more of the following categories:  \n    * Negative for Pneumonia  \n    * Typical Appearance for COVID-19  \n    * Indeterminate Appearance  \n    * Atypical Appearance  \n\n* **Key Points**:  \n  * Dual prediction task:  \n    1. Image-level bounding box detection (\"opacity\" or \"none\").  \n    2. Study-level multi-label classification.  \n  * Medical diagnostic focus: Aims to reduce radiologist variability and speed up COVID-19 diagnosis.  \n  * Annotations provided by expert radiologists.  \n\n## Dataset Overview\n* **Data Type**:  \n  * Medical imaging (DICOM format chest radiographs) with structured metadata.  \n\n* **Data Files**:  \n  * `train_study_level.csv`: Study-level labels (one row per study).  \n  * `train_image_level.csv`: Image-level labels and bounding box coordinates (dictionary format).  \n  * `sample_submission.csv`: Submission format template.  \n\n* **Features**:  \n  * DICOM images containing pixel data and metadata (e.g., imaging parameters).  \n  * Study-level labels: Binary flags for each COVID-19 appearance type.  \n  * Image-level annotations: Bounding boxes (`xmin, ymin, xmax, ymax`) for opacities.  \n\n## Evaluation Metrics\n* **Primary Metric**:  \n  * **Mean Average Precision (mAP)** at IoU > 0.5 (PASCAL VOC 2010 standard).  \n\n* **Components**:  \n  * **Study-Level Predictions**:  \n    * Format: `[label] [confidence] 0 0 1 1` (one-pixel bounding box required).  \n    * Example: `typical 0.9 0 0 1 1 indeterminate 0.3 0 0 1 1`.  \n  * **Image-Level Predictions**:  \n    * For opacities: `opacity [confidence] [xmin ymin",
    "sections": {},
    "file_path": "kaggle_datasets/448/problem_summary.md"
  },
  "484": {
    "problem_id": "484",
    "title": "Time Series Classification of Biological Sensor Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Time Series Classification of Biological Sensor Data\n\n## Problem Description\n* **Problem Type:** Time Series Classification (Binary Classification)\n* **Objective:** Predict the activity state of participants based on 60-second sequences of biological sensor data. The goal is to classify whether a subject was in one of two possible states during each sequence.\n* **Key Points:**\n  * Sequences are recorded from several hundred participants.\n  * Each sequence represents 60 seconds of sensor data (one-second intervals).\n  * The task involves modeling time-dependent patterns in sensor data to classify the state.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular time series data from biological sensors, recorded during experimental participant activities.\n* **Data Files:**\n  * `train.csv`: Contains ~26,000 sequences with 13 sensor readings per time step.\n  * `train_labels.csv`: Provides the target labels (`state`) for each sequence in the training set.\n  * `test.csv`: ~12,000 sequences for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:**\n  * `sequence`: Unique identifier for each sequence.\n  * `subject`: Unique identifier for the participant.\n  * `step`: Time step in one-second intervals (0-59).\n  * `sensor_00` to `sensor_12`: Values from 13 biological sensors at each time step.\n  * `state` (in train_labels.csv): Binary target variable indicating the activity state.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Area Under the ROC Curve (AUC-ROC)\n  * Measures the model's ability to distinguish between the two activity states.\n  * Higher values indicate better classification performance (1.0 = perfect separation).",
    "sections": {},
    "file_path": "kaggle_datasets/484/problem_summary.md"
  },
  "226": {
    "problem_id": "226",
    "title": "Quora Question Pairs Duplicate Detection",
    "problem_type": "Binary Classification (NLP - Semantic Similarity)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Quora Question Pairs Duplicate Detection\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (NLP - Semantic Similarity)\n* **Objective:**  \n  Predict whether pairs of questions from Quora have the same intent (i.e., are duplicates). The goal is to improve Quora's content organization by identifying redundant questions, enhancing user experience for seekers and writers.\n  * **Key Points:**\n    * Human-labeled ground truth is subjective and may contain noise due to inherent ambiguity in language interpretation.\n    * Test set includes computer-generated question pairs as anti-cheating measures (excluded from scoring).\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  Text data (question pairs) from Quora’s platform, with labels indicating semantic equivalence.\n* **Data Files:**\n  * `train.csv`: Contains question pairs with human-annotated labels (`is_duplicate`).\n  * `test.csv`: Question pairs for prediction (no labels provided).\n  * `sample_submission.csv`: Example submission file format.\n* **Features:**\n  * `question1`, `question2`: Full text of the questions to compare.\n  * `is_duplicate`: Binary target (1 = duplicate, 0 = non-duplicate).\n  * Metadata: `id`, `qid1`, `qid2` (unique identifiers for questions in training set only).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (Log Loss).\n  * **Components:**  \n    Penalizes confident incorrect predictions (e.g., predicting 0.9 for a false duplicate). Predictions must be probabilities between 0 and 1.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (NLP - Semantic Similarity)\n* **Objective:**  \n  Predict whether pairs of questions from Quora have the same intent (i.e., are duplicates). The goal is to improve Quora's content organization by identifying redundant questions, enhancing user experience for seekers and writers.\n  * **Key Points:**\n    * Human-labeled ground truth is subjective and may contain noise due to inherent ambiguity in language interpretation.\n    * Test set includes computer-generated question pairs as anti-cheating measures (excluded from scoring).",
      "Dataset Overview": "* **Data Type & Context:**  \n  Text data (question pairs) from Quora’s platform, with labels indicating semantic equivalence.\n* **Data Files:**\n  * `train.csv`: Contains question pairs with human-annotated labels (`is_duplicate`).\n  * `test.csv`: Question pairs for prediction (no labels provided).\n  * `sample_submission.csv`: Example submission file format.\n* **Features:**\n  * `question1`, `question2`: Full text of the questions to compare.\n  * `is_duplicate`: Binary target (1 = duplicate, 0 = non-duplicate).\n  * Metadata: `id`, `qid1`, `qid2` (unique identifiers for questions in training set only).",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (Log Loss).\n  * **Components:**  \n    Penalizes confident incorrect predictions (e.g., predicting 0.9 for a false duplicate). Predictions must be probabilities between 0 and 1."
    },
    "file_path": "kaggle_datasets/226/problem_summary.md"
  },
  "642": {
    "problem_id": "642",
    "title": "Equity in Post-HCT Survival Predictions",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Equity in Post-HCT Survival Predictions\n\n## Problem Description\n* **Problem Type**: Survival Analysis with Fairness Constraints\n* **Objective**: Develop predictive models for event-free survival in allogeneic Hematopoietic Cell Transplantation (HCT) patients that are both accurate and equitable across racial groups. The goal is to address disparities in current models related to socioeconomic status, race, and geography.\n* **Key Points**:\n  * Focus on improving fairness in survival predictions across diverse patient demographics\n  * Use synthetic data that mirrors real-world scenarios while protecting patient privacy\n  * Models must perform well across racial groups (White, Asian, African-American, Native American, Pacific Islander, and More than One Race)\n\n## Dataset Overview\n* **Data Type**: Tabular data with time-to-event outcomes\n* **Context**: Synthetic healthcare data generated from real CIBMTR records on hematopoietic stem cell transplantation\n* **Data Files**:\n  * `train.csv` - Training set with target variable `efs` (event-free survival) and `efs_time` (time to event)\n  * `test.csv` - Test set for making predictions\n  * `sample_submission.csv` - Example submission format\n  * `data_dictionary.csv` - Feature descriptions\n* **Features**:\n  * 59 variables including demographic and medical characteristics of recipients and donors\n  * Key variables: age, sex, ethnicity, disease status, treatment details\n  * Equal representation across racial categories\n\n## Evaluation Metrics\n* **Primary Metric**: Stratified Concordance Index (C-index)\n* **Metric Components**:\n  * Standard C-index measures model's ability to correctly rank survival times (range 0-1, where 0.5 = random, 1 = perfect)\n  * Stratified version calculates mean minus standard deviation of C-indices across racial groups\n    * Rewards high average performance (large mean)\n    * Penalizes performance disparities (small standard deviation)\n  * Implementation considers censored outcomes in survival data",
    "sections": {},
    "file_path": "kaggle_datasets/642/problem_summary.md"
  },
  "470": {
    "problem_id": "470",
    "title": "Analyzing NFL Special Teams Performance with Tracking Data",
    "problem_type": "Sports Analytics / Performance Evaluation",
    "objective": "Develop novel insights and metrics for evaluating NFL special teams plays (punts, kickoffs, field goals, extra points) using player tracking data. The competition seeks actionable analyses that could be adopted by NFL teams to better understand special teams performance.",
    "evaluation_metric": null,
    "full_content": "# Analyzing NFL Special Teams Performance with Tracking Data\n\n**Problem Description:**\n* **Problem Type:** Sports Analytics / Performance Evaluation\n* **Objective:** Develop novel insights and metrics for evaluating NFL special teams plays (punts, kickoffs, field goals, extra points) using player tracking data. The competition seeks actionable analyses that could be adopted by NFL teams to better understand special teams performance.\n* **Key Points:**\n  * Focus areas include creating new metrics, quantifying team/player strategies, or ranking players\n  * Submissions should examine one idea thoroughly rather than multiple ideas superficially\n  * Analysis must use the provided player tracking data\n  * Special considerations for football-specific complexities in the data\n\n**Dataset Overview:**\n* **Data Type:** Multi-source sports tracking data with temporal and spatial components\n* **Context:** NFL special teams plays from 2018-2020 seasons with player movement data and scouting metrics\n* **Data Files:**\n  * `games.csv` - Game metadata\n  * `plays.csv` - Play-level information\n  * `players.csv` - Player attributes\n  * `tracking[year].csv` - Player movement data (2018-2020)\n  * `PFFScoutingData.csv` - Football-specific scouting metrics\n* **Key Features:**\n  * Player tracking: Position (x,y), speed, acceleration, orientation\n  * Play context: Special teams type, results, yardage\n  * Scouting data: Snap details, hang time, kick types, return formations\n  * Key identifiers: gameId, playId, nflId for joining datasets\n\n**Evaluation Metrics:**\n* **Evaluation Method:** Multi-criteria judging by NFL analytics professionals\n* **Scoring Components (each weighted equally 0-10):**\n  * *Innovation:* Novelty and actionability of findings\n  * *Accuracy:* Correctness and appropriate statistical modeling\n  * *Relevance:* Practical utility for NFL teams\n  * *Clarity:* Presentation quality\n  * *Data Visualization:* Effectiveness of visual communication\n* **Submission Format:** Notebook (≤2000 words, ≤10 figures/tables) with public code",
    "sections": {
      "Problem Description": "* **Problem Type:** Sports Analytics / Performance Evaluation\n* **Objective:** Develop novel insights and metrics for evaluating NFL special teams plays (punts, kickoffs, field goals, extra points) using player tracking data. The competition seeks actionable analyses that could be adopted by NFL teams to better understand special teams performance.\n* **Key Points:**\n  * Focus areas include creating new metrics, quantifying team/player strategies, or ranking players\n  * Submissions should examine one idea thoroughly rather than multiple ideas superficially\n  * Analysis must use the provided player tracking data\n  * Special considerations for football-specific complexities in the data",
      "Dataset Overview": "* **Data Type:** Multi-source sports tracking data with temporal and spatial components\n* **Context:** NFL special teams plays from 2018-2020 seasons with player movement data and scouting metrics\n* **Data Files:**\n  * `games.csv` - Game metadata\n  * `plays.csv` - Play-level information\n  * `players.csv` - Player attributes\n  * `tracking[year].csv` - Player movement data (2018-2020)\n  * `PFFScoutingData.csv` - Football-specific scouting metrics\n* **Key Features:**\n  * Player tracking: Position (x,y), speed, acceleration, orientation\n  * Play context: Special teams type, results, yardage\n  * Scouting data: Snap details, hang time, kick types, return formations\n  * Key identifiers: gameId, playId, nflId for joining datasets",
      "Evaluation Metrics": "* **Evaluation Method:** Multi-criteria judging by NFL analytics professionals\n* **Scoring Components (each weighted equally 0-10):**\n  * *Innovation:* Novelty and actionability of findings\n  * *Accuracy:* Correctness and appropriate statistical modeling\n  * *Relevance:* Practical utility for NFL teams\n  * *Clarity:* Presentation quality\n  * *Data Visualization:* Effectiveness of visual communication\n* **Submission Format:** Notebook (≤2000 words, ≤10 figures/tables) with public code"
    },
    "file_path": "kaggle_datasets/470/problem_summary.md"
  },
  "219": {
    "problem_id": "219",
    "title": "Satellite Imagery Feature Detection with MultiPolygon Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Satellite Imagery Feature Detection with MultiPolygon Classification\n\n## Problem Description\n- **Problem Type**: Computer Vision - Object Detection & Multi-class Classification (with Polygon Prediction)\n- **Objective**: Accurately detect and classify 10 types of objects/features in satellite imagery by predicting their precise polygonal boundaries. The goal is to automate feature labeling that is traditionally done manually by image analysts.\n- **Key Points**:\n  - Requires predicting both *classification* (1 of 10 classes) and *precise spatial boundaries* (as MultiPolygons in WKT format)\n  - Features vary greatly in scale (from small vehicles to large buildings)\n  - Uses obscured geo-coordinates (x=[0,1], y=[-1,0]) to hide real-world locations\n  - Must handle both 3-band (RGB) and 16-band (multispectral + SWIR) imagery\n\n## Dataset Overview\n- **Data Type**: Satellite imagery (GeoTIFF format) with polygon annotations\n- **Context**: 1km x 1km overhead images from WorldView-3 sensor, with features labeled by UK defence analysts\n- **Data Files**:\n  - `three_band.zip`: RGB images (3391x3348 pixels)\n  - `sixteen_band.zip`: Multispectral/SWIR images (8 bands each)\n  - `train_wkt.csv`: Polygon labels in WKT format\n  - `grid_sizes.csv`: Coordinate scaling parameters per image\n  - `train_geojson.zip`: Alternative label format (GeoJSON)\n- **Key Features**:\n  - 10 object classes (buildings, roads, water, vehicles etc.)\n  - Multi-resolution imagery (panchromatic: 0.31m, multispectral: 1.24m, SWIR: 7.5m)\n  - 11-14 bit dynamic range per pixel\n\n## Evaluation Metrics\n- **Primary Metric**: Average Jaccard Index (Intersection-over-Union) across all classes\n- **Calculation**:\n  1. For each class in each image:\n     - Compute True Positive (TP) area: Intersection of predicted/actual polygons\n     - Compute False Positive (FP) area: Predicted area not in ground truth\n     - Compute False Negative (FN) area: Ground truth area not predicted\n  2. Aggregate TP/FP/FN areas across all images",
    "sections": {},
    "file_path": "kaggle_datasets/219/problem_summary.md"
  },
  "446": {
    "problem_id": "446",
    "title": "Predicting Text Readability for Educational Passages",
    "problem_type": "Regression (Text-based)",
    "objective": "Develop algorithms to predict the reading ease/complexity of literary passages for grade 3-12 classroom use. The goal is to improve upon traditional readability formulas by incorporating text cohesion and semantics.",
    "evaluation_metric": null,
    "full_content": "# Predicting Text Readability for Educational Passages\n\n**Problem Description:**\n* **Problem Type:** Regression (Text-based)\n* **Objective:** Develop algorithms to predict the reading ease/complexity of literary passages for grade 3-12 classroom use. The goal is to improve upon traditional readability formulas by incorporating text cohesion and semantics.\n* **Key Points:**\n  * Focus on generalizing to modern texts (test set has higher proportion than training set)\n  * Address limitations of existing methods (Flesch-Kincaid, Lexile) which rely on weak proxies\n  * Applications include aiding curriculum developers, teachers, and students in evaluating text appropriateness\n\n**Dataset Overview:**\n* **Data Type & Context:** Text data (literary excerpts) with associated readability scores\n* **Data Files:**\n  * `train.csv` - Contains excerpts with target readability scores\n  * `test.csv` - Excerpts for prediction (with blank license/URL info)\n  * `sample_submission.csv` - Submission format example\n* **Key Features:**\n  * `excerpt` - Text passage to evaluate\n  * `target` - Reading ease score (continuous target variable)\n  * `standard_error` - Variability in human ratings (training set only)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Calculation:**\n  * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n  * Where y_i = true value, ŷ_i = predicted value, n = number of samples\n  * Lower values indicate better performance (closer predictions to true scores)",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Text-based)\n* **Objective:** Develop algorithms to predict the reading ease/complexity of literary passages for grade 3-12 classroom use. The goal is to improve upon traditional readability formulas by incorporating text cohesion and semantics.\n* **Key Points:**\n  * Focus on generalizing to modern texts (test set has higher proportion than training set)\n  * Address limitations of existing methods (Flesch-Kincaid, Lexile) which rely on weak proxies\n  * Applications include aiding curriculum developers, teachers, and students in evaluating text appropriateness",
      "Dataset Overview": "* **Data Type & Context:** Text data (literary excerpts) with associated readability scores\n* **Data Files:**\n  * `train.csv` - Contains excerpts with target readability scores\n  * `test.csv` - Excerpts for prediction (with blank license/URL info)\n  * `sample_submission.csv` - Submission format example\n* **Key Features:**\n  * `excerpt` - Text passage to evaluate\n  * `target` - Reading ease score (continuous target variable)\n  * `standard_error` - Variability in human ratings (training set only)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Calculation:**\n  * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n  * Where y_i = true value, ŷ_i = predicted value, n = number of samples\n  * Lower values indicate better performance (closer predictions to true scores)"
    },
    "file_path": "kaggle_datasets/446/problem_summary.md"
  },
  "210": {
    "problem_id": "210",
    "title": "Predicting Insurance Claim Severity",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Insurance Claim Severity\n\n## Problem Description\n* **Problem Type:** Regression\n* **Objective:** Predict the severity (cost) of insurance claims based on given features. The goal is to develop an algorithm that accurately estimates the financial loss associated with each claim to help Allstate improve their claims service.\n* **Key Points:**\n  * Focus on automating claims severity prediction\n  * Dataset represents individual insurance claims\n  * Challenge was part of Allstate's recruitment process\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing insurance claim records with mixed categorical and continuous features\n* **Data Files:**\n  * train.csv - training set with target values\n  * test.csv - test set for which predictions must be made\n  * sample_submission.csv - example of submission format\n* **Features:**\n  * Categorical variables (prefixed with 'cat')\n  * Continuous variables (prefixed with 'cont')\n  * Target variable: 'loss' (claim severity amount)\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE)\n  * Measures the average magnitude of errors in predictions\n  * Calculated as the average of absolute differences between predicted and actual claim amounts\n  * Lower values indicate better performance",
    "sections": {},
    "file_path": "kaggle_datasets/210/problem_summary.md"
  },
  "479": {
    "problem_id": "479",
    "title": "Traffic Congestion Forecasting in a U.S. Metropolis",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Traffic Congestion Forecasting in a U.S. Metropolis\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting (Spatio-Temporal)\n* **Objective:** Forecast twelve-hours of traffic flow (congestion levels) across a network of 65 roadways in a major U.S. metropolitan area. The challenge involves modeling interactions between time, location coordinates, and direction of travel.\n* **Key Points:**\n  * Focus on *spatio-temporal forecasting*: combining spatial (roadway coordinates) and temporal (20-minute intervals) features.\n  * Predictions must account for dynamic traffic patterns across a network of roadways with varying directions of travel.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular time series data representing traffic congestion measurements from April-September 1991 in a U.S. city (derived from Chicago traffic data).\n* **Data Files:**\n  * `train.csv`: Historical congestion measurements (time-stamped, with location/direction features)\n  * `test.csv`: Roadway segments to forecast for 1991-09-30\n  * `sample_submission.csv`: Submission template\n* **Key Features:**\n  * `time`: 20-minute measurement periods\n  * `x`, `y`: East-west and north-south roadway coordinates\n  * `direction`: Travel direction (e.g., EB for eastbound, SW for southwest)\n  * `congestion`: Target variable (normalized 0-100)\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE)\n  * Calculated as the average absolute difference between predicted and actual congestion values\n  * Predictions can be floating-point numbers (despite integer targets)",
    "sections": {},
    "file_path": "kaggle_datasets/479/problem_summary.md"
  },
  "217": {
    "problem_id": "217",
    "title": "Financial Market Value Prediction with Anonymized Time Series Data",
    "problem_type": "Time Series Regression",
    "objective": "Predict the future value ('y') of financial instruments using anonymized time-varying features. The goal is to uncover predictive signals in highly uncertain financial market data.",
    "evaluation_metric": null,
    "full_content": "# Financial Market Value Prediction with Anonymized Time Series Data\n\n**Problem Description:**\n* **Problem Type:** Time Series Regression\n* **Objective:** Predict the future value ('y') of financial instruments using anonymized time-varying features. The goal is to uncover predictive signals in highly uncertain financial market data.\n    * **Key Points:**\n        * Data is anonymized with no disclosure of feature meanings or instrument types\n        * Must use only provided competition data (external data prohibited)\n        * API enforces time-sequential prediction to prevent lookahead bias\n        * Includes reinforcement learning aspect via API-provided \"reward\" (average R value from previous predictions)\n\n**Dataset Overview:**\n* **Data Type & Context:** Time series data of anonymized financial instruments with:\n    * Timestamped observations\n    * Multiple features per instrument (all anonymized)\n    * Target variable 'y' representing instrument value\n* **Data Files:**\n    * Stored in HDF5 format (.h5) for faster read speeds\n    * Training set available for download (first half used for training, second half as holdout in API)\n    * Test set only accessible via competition API\n* **Key Features:**\n    * 'timestamp' - temporal indicator\n    * 'id' - instrument identifier\n    * 'y' - target value to predict\n    * Multiple anonymized features (nature/quantity not specified)\n\n**Evaluation Metrics:**\n* **Primary Metric:** R value (derived from R² coefficient of determination)\n    * Calculation:\n        * 𝑅² = 1 - (∑(𝑦ᵢ - ŷᵢ)²) / (∑(𝑦ᵢ - μ)²)\n        * 𝑅 = sign(𝑅²) × √|𝑅²|\n    * **Special Handling:**\n        * Negative R values clipped at -1 (score = max(-1, R))\n        * Submissions erroring receive \"Error\" status",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Regression\n* **Objective:** Predict the future value ('y') of financial instruments using anonymized time-varying features. The goal is to uncover predictive signals in highly uncertain financial market data.\n    * **Key Points:**\n        * Data is anonymized with no disclosure of feature meanings or instrument types\n        * Must use only provided competition data (external data prohibited)\n        * API enforces time-sequential prediction to prevent lookahead bias\n        * Includes reinforcement learning aspect via API-provided \"reward\" (average R value from previous predictions)",
      "Dataset Overview": "* **Data Type & Context:** Time series data of anonymized financial instruments with:\n    * Timestamped observations\n    * Multiple features per instrument (all anonymized)\n    * Target variable 'y' representing instrument value\n* **Data Files:**\n    * Stored in HDF5 format (.h5) for faster read speeds\n    * Training set available for download (first half used for training, second half as holdout in API)\n    * Test set only accessible via competition API\n* **Key Features:**\n    * 'timestamp' - temporal indicator\n    * 'id' - instrument identifier\n    * 'y' - target value to predict\n    * Multiple anonymized features (nature/quantity not specified)",
      "Evaluation Metrics": "* **Primary Metric:** R value (derived from R² coefficient of determination)\n    * Calculation:\n        * 𝑅² = 1 - (∑(𝑦ᵢ - ŷᵢ)²) / (∑(𝑦ᵢ - μ)²)\n        * 𝑅 = sign(𝑅²) × √|𝑅²|\n    * **Special Handling:**\n        * Negative R values clipped at -1 (score = max(-1, R))\n        * Submissions erroring receive \"Error\" status"
    },
    "file_path": "kaggle_datasets/217/problem_summary.md"
  },
  "441": {
    "problem_id": "441",
    "title": "Multiclass Classification of eCommerce Product Categories",
    "problem_type": "Multiclass Classification",
    "objective": "Predict the probability that a product belongs to each of several predefined categories based on anonymized product attributes.",
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of eCommerce Product Categories\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the probability that a product belongs to each of several predefined categories based on anonymized product attributes.\n    * **Key Points:**\n        * The dataset is synthetic but based on real eCommerce product data.\n        * Features are anonymized but maintain real-world properties.\n        * Competition designed as beginner-friendly practice between Titanic and Featured competitions.\n        * Participants must submit probabilities for all classes per product.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing eCommerce product listings with anonymized features.\n* **Data Files:**\n    * `train.csv` - Contains product IDs, anonymized features (`feature_*` columns), and class labels (`target`).\n    * `test.csv` - Contains product IDs and anonymized features for prediction.\n    * `sample_submission.csv` - Example submission format.\n* **Features:** Anonymized numerical features (`feature_0` to `feature_99`) representing various product attributes.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * Calculated as: -1/N Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n        * Where N = number of rows, M = number of classes\n        * y_ij = 1 if observation i is in class j, 0 otherwise\n        * p_ij = predicted probability observation i belongs to class j\n    * **Special Considerations:**\n        * Submitted probabilities are rescaled (each row divided by row sum)\n        * Probabilities clipped to [10^-15, 1-10^-15] to avoid log(0) extremes",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the probability that a product belongs to each of several predefined categories based on anonymized product attributes.\n    * **Key Points:**\n        * The dataset is synthetic but based on real eCommerce product data.\n        * Features are anonymized but maintain real-world properties.\n        * Competition designed as beginner-friendly practice between Titanic and Featured competitions.\n        * Participants must submit probabilities for all classes per product.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing eCommerce product listings with anonymized features.\n* **Data Files:**\n    * `train.csv` - Contains product IDs, anonymized features (`feature_*` columns), and class labels (`target`).\n    * `test.csv` - Contains product IDs and anonymized features for prediction.\n    * `sample_submission.csv` - Example submission format.\n* **Features:** Anonymized numerical features (`feature_0` to `feature_99`) representing various product attributes.",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * Calculated as: -1/N Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n        * Where N = number of rows, M = number of classes\n        * y_ij = 1 if observation i is in class j, 0 otherwise\n        * p_ij = predicted probability observation i belongs to class j\n    * **Special Considerations:**\n        * Submitted probabilities are rescaled (each row divided by row sum)\n        * Probabilities clipped to [10^-15, 1-10^-15] to avoid log(0) extremes"
    },
    "file_path": "kaggle_datasets/441/problem_summary.md"
  },
  "228": {
    "problem_id": "228",
    "title": "Computer Vision-Based Steller Sea Lion Population Counting",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Computer Vision-Based Steller Sea Lion Population Counting\n\n## Problem Description\n- **Problem Type**: Computer Vision - Object Counting (Multi-class Regression)\n- **Objective**: Develop algorithms to accurately count Steller sea lions in aerial photographs, categorized by five demographic classes:\n  * Adult males (bulls)\n  * Subadult males\n  * Adult females\n  * Juveniles\n  * Pups\n- **Key Points**:\n  * Automates a manual process that traditionally takes biologists 4 months\n  * Supports conservation efforts for an endangered species (94% population decline in 30 years)\n  * Blacked-out regions in images should be ignored (prevent double-counting)\n  * Anti-cheating measures include non-scored test images\n\n## Dataset Overview\n- **Data Type**: Aerial imagery with object-level annotations\n- **Context**: Conservation biology, wildlife population monitoring\n- **Data Files**:\n  * `Train/*.jpg`: Training images\n  * `Train/train.csv`: Ground-truth counts per image\n  * `TrainDotted/*.jpg`: Training images with colored dots marking sea lions\n  * `Test/*.jpg`: Test images for evaluation\n  * `sample_submission.csv`: Submission format example\n- **Features**:\n  * High-resolution aerial photographs\n  * Color-coded dots in TrainDotted images (red=adult males, magenta=subadult males, brown=adult females, blue=juveniles, green=pups)\n  * Blacked-out regions indicating areas to ignore\n\n## Evaluation Metrics\n- **Evaluation Metric**: Mean Columnwise Root Mean Squared Error (RMSE)\n- **Components**:\n  1. Calculate RMSE separately for each sea lion class\n  2. Average the RMSE scores across all five classes\n  3. Formula: \n     ```\n     Final Score = (RMSE_adult_males + RMSE_subadult_males + RMSE_adult_females + RMSE_juveniles + RMSE_pups) / 5\n     ",
    "sections": {},
    "file_path": "kaggle_datasets/228/problem_summary.md"
  },
  "618": {
    "problem_id": "618",
    "title": "Boolean Query Generation for Patent Search Explainability",
    "problem_type": "NLP - Query Generation / Information Retrieval",
    "objective": "Generate Boolean search queries that accurately retrieve a given set of related patent documents when executed against a patent search database. The goal is to create explainable AI outputs that patent professionals can understand and validate.",
    "evaluation_metric": null,
    "full_content": "# Boolean Query Generation for Patent Search Explainability\n\n**Problem Description:**\n* **Problem Type:** NLP - Query Generation / Information Retrieval\n* **Objective:** Generate Boolean search queries that accurately retrieve a given set of related patent documents when executed against a patent search database. The goal is to create explainable AI outputs that patent professionals can understand and validate.\n* **Key Points:**\n  * Queries must return exactly 50 patents matching the provided target set\n  * Solution must translate AI-generated patent similarities into human-interpretable Boolean logic\n  * Supports adoption of AI in intellectual property workflows by providing familiar search syntax\n\n**Dataset Overview:**\n* **Data Type:** Structured patent metadata and unstructured patent text\n* **Context:** U.S. patent documents with technical descriptions, claims, and classification codes\n* **Data Files:**\n  * `patent_metadata.parquet`: Publication numbers, dates, family IDs, and CPC codes\n  * `nearest_neighbors.csv`: Pre-computed similar patents for each target\n  * `patent_data/[year_month].parquet`: Title, abstract, claims, and description text\n  * `test.csv`: Target patent sets for evaluation\n* **Key Features:**\n  * Patent text fields (title, abstract, claims, description)\n  * Cooperative Patent Classification (CPC) codes\n  * Pre-computed nearest neighbor relationships\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision at 50 (mAP@50)\n  * Measures how well the retrieved patents match the target set\n  * Precision is calculated for each query at different recall levels\n  * Average precision is computed and then averaged across all queries\n* **Implementation Constraints:**\n  * Queries limited to 50 tokens\n  * Results truncated to 50 patents per query\n  * Evaluation must complete within 60 minutes of query execution time\n  * Uses Whoosh search index with specific field mappings (title, abstract, claims, etc.)",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Query Generation / Information Retrieval\n* **Objective:** Generate Boolean search queries that accurately retrieve a given set of related patent documents when executed against a patent search database. The goal is to create explainable AI outputs that patent professionals can understand and validate.\n* **Key Points:**\n  * Queries must return exactly 50 patents matching the provided target set\n  * Solution must translate AI-generated patent similarities into human-interpretable Boolean logic\n  * Supports adoption of AI in intellectual property workflows by providing familiar search syntax",
      "Dataset Overview": "* **Data Type:** Structured patent metadata and unstructured patent text\n* **Context:** U.S. patent documents with technical descriptions, claims, and classification codes\n* **Data Files:**\n  * `patent_metadata.parquet`: Publication numbers, dates, family IDs, and CPC codes\n  * `nearest_neighbors.csv`: Pre-computed similar patents for each target\n  * `patent_data/[year_month].parquet`: Title, abstract, claims, and description text\n  * `test.csv`: Target patent sets for evaluation\n* **Key Features:**\n  * Patent text fields (title, abstract, claims, description)\n  * Cooperative Patent Classification (CPC) codes\n  * Pre-computed nearest neighbor relationships",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision at 50 (mAP@50)\n  * Measures how well the retrieved patents match the target set\n  * Precision is calculated for each query at different recall levels\n  * Average precision is computed and then averaged across all queries\n* **Implementation Constraints:**\n  * Queries limited to 50 tokens\n  * Results truncated to 50 patents per query\n  * Evaluation must complete within 60 minutes of query execution time\n  * Uses Whoosh search index with specific field mappings (title, abstract, claims, etc.)"
    },
    "file_path": "kaggle_datasets/618/problem_summary.md"
  },
  "288": {
    "problem_id": "288",
    "title": "Forest Cover Type Classification from Cartographic Variables",
    "problem_type": "Multi-class Classification",
    "objective": "Predict the predominant forest cover type (1 of 7 classes) for 30x30 meter patches in wilderness areas using cartographic variables. The goal is to classify each patch into one of the following forest types:",
    "evaluation_metric": null,
    "full_content": "# Forest Cover Type Classification from Cartographic Variables\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the predominant forest cover type (1 of 7 classes) for 30x30 meter patches in wilderness areas using cartographic variables. The goal is to classify each patch into one of the following forest types:\n  * 1 - Spruce/Fir\n  * 2 - Lodgepole Pine\n  * 3 - Ponderosa Pine\n  * 4 - Cottonwood/Willow\n  * 5 - Aspen\n  * 6 - Douglas-fir\n  * 7 - Krummholz\n* **Key Points:**\n  * Data represents minimally disturbed forests in Roosevelt National Forest (Colorado)\n  * Features include both continuous variables (elevation, slope) and binary indicators (wilderness areas, soil types)\n  * Original ecological data (not managed forests)\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with cartographic and environmental features\n* **Context:** 30x30 meter patches from four wilderness areas in Colorado\n* **Data Files:**\n  * train.csv (15,120 observations with features + target)\n  * test.csv (565,892 observations with features only)\n  * sample_submission.csv\n* **Key Features:**\n  * Continuous variables: Elevation, Aspect, Slope, Distance metrics\n  * Binary variables: 4 Wilderness Area indicators, 40 Soil Type indicators\n  * Derived indices: Hillshade indices at different times\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class Classification Accuracy\n  * Simple percentage of correctly classified instances\n  * No class weighting or stratification mentioned\n  * Direct comparison of predicted vs. true cover type integers (1-7)",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the predominant forest cover type (1 of 7 classes) for 30x30 meter patches in wilderness areas using cartographic variables. The goal is to classify each patch into one of the following forest types:\n  * 1 - Spruce/Fir\n  * 2 - Lodgepole Pine\n  * 3 - Ponderosa Pine\n  * 4 - Cottonwood/Willow\n  * 5 - Aspen\n  * 6 - Douglas-fir\n  * 7 - Krummholz\n* **Key Points:**\n  * Data represents minimally disturbed forests in Roosevelt National Forest (Colorado)\n  * Features include both continuous variables (elevation, slope) and binary indicators (wilderness areas, soil types)\n  * Original ecological data (not managed forests)",
      "Dataset Overview": "* **Data Type:** Tabular data with cartographic and environmental features\n* **Context:** 30x30 meter patches from four wilderness areas in Colorado\n* **Data Files:**\n  * train.csv (15,120 observations with features + target)\n  * test.csv (565,892 observations with features only)\n  * sample_submission.csv\n* **Key Features:**\n  * Continuous variables: Elevation, Aspect, Slope, Distance metrics\n  * Binary variables: 4 Wilderness Area indicators, 40 Soil Type indicators\n  * Derived indices: Hillshade indices at different times",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class Classification Accuracy\n  * Simple percentage of correctly classified instances\n  * No class weighting or stratification mentioned\n  * Direct comparison of predicted vs. true cover type integers (1-7)"
    },
    "file_path": "kaggle_datasets/288/problem_summary.md"
  },
  "243": {
    "problem_id": "243",
    "title": "Text Normalization for Russian Language",
    "problem_type": "NLP - Text Normalization (Sequence-to-Sequence Transformation)",
    "objective": "Automate the conversion of Russian text from written expressions into appropriate spoken forms for applications like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). The task involves handling diverse linguistic elements including:",
    "evaluation_metric": null,
    "full_content": "# Text Normalization for Russian Language\n\n**Problem Description:**\n* **Problem Type:** NLP - Text Normalization (Sequence-to-Sequence Transformation)\n* **Objective:** Automate the conversion of Russian text from written expressions into appropriate spoken forms for applications like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). The task involves handling diverse linguistic elements including:\n    * Numbers, dates, and times (e.g., \"12 февраля 2013\" → \"двенадцатого февраля две тысячи тринадцатого года\")\n    * Currency expressions\n    * Abbreviations and semiotic classes\n    * Transliterated foreign words (marked with '_trans' postfix)\n* **Key Points:**\n    * Focus on Russian language (parallel English challenge existed separately)\n    * Requires handling of transliterated words in Cyrillic (e.g., \"Julius\" → \"д_trans ж_trans у_trans л_trans и_trans у_trans с_trans\")\n    * Duplicate sentences in train/test sets are ignored during scoring\n\n**Dataset Overview:**\n* **Data Type:** Text data (Russian language tokens with normalization pairs)\n* **Context:** Linguistic normalization for speech synthesis/recognition systems\n* **Data Files:**\n    * ru_train.csv (contains raw text in \"before\" column and normalized text in \"after\" column)\n    * ru_test.csv (contains only raw text for prediction)\n    * ru_sample_submission.csv (example submission format)\n* **Important Features:**\n    * sentence_id and token_id for sequence tracking\n    * \"before\" column (raw input text)\n    * \"after\" column (target normalized text)\n    * \"class\" column in training data only (token type information)\n    * Special handling for transliterated words with '_trans' markers\n\n**Evaluation Metrics:**\n* **Primary Metric:** Prediction Accuracy (exact match sequence accuracy)\n    * Calculated as the percentage of correctly predicted tokens\n    * Requires exact string matching between prediction and ground truth\n    * Any error in output sequence counts as wrong (e.g., \"one forty five\" vs correct \"one hundred forty five\" counts as error)\n* **Scoring Notes:**\n    * Evaluated at token level within sequences\n    * Duplicate sentences in test set are ignored during scoring\n    * Case-sensitive comparison",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Text Normalization (Sequence-to-Sequence Transformation)\n* **Objective:** Automate the conversion of Russian text from written expressions into appropriate spoken forms for applications like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). The task involves handling diverse linguistic elements including:\n    * Numbers, dates, and times (e.g., \"12 февраля 2013\" → \"двенадцатого февраля две тысячи тринадцатого года\")\n    * Currency expressions\n    * Abbreviations and semiotic classes\n    * Transliterated foreign words (marked with '_trans' postfix)\n* **Key Points:**\n    * Focus on Russian language (parallel English challenge existed separately)\n    * Requires handling of transliterated words in Cyrillic (e.g., \"Julius\" → \"д_trans ж_trans у_trans л_trans и_trans у_trans с_trans\")\n    * Duplicate sentences in train/test sets are ignored during scoring",
      "Dataset Overview": "* **Data Type:** Text data (Russian language tokens with normalization pairs)\n* **Context:** Linguistic normalization for speech synthesis/recognition systems\n* **Data Files:**\n    * ru_train.csv (contains raw text in \"before\" column and normalized text in \"after\" column)\n    * ru_test.csv (contains only raw text for prediction)\n    * ru_sample_submission.csv (example submission format)\n* **Important Features:**\n    * sentence_id and token_id for sequence tracking\n    * \"before\" column (raw input text)\n    * \"after\" column (target normalized text)\n    * \"class\" column in training data only (token type information)\n    * Special handling for transliterated words with '_trans' markers",
      "Evaluation Metrics": "* **Primary Metric:** Prediction Accuracy (exact match sequence accuracy)\n    * Calculated as the percentage of correctly predicted tokens\n    * Requires exact string matching between prediction and ground truth\n    * Any error in output sequence counts as wrong (e.g., \"one forty five\" vs correct \"one hundred forty five\" counts as error)\n* **Scoring Notes:**\n    * Evaluated at token level within sequences\n    * Duplicate sentences in test set are ignored during scoring\n    * Case-sensitive comparison"
    },
    "file_path": "kaggle_datasets/243/problem_summary.md"
  },
  "627": {
    "problem_id": "627",
    "title": "Loan Approval Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Loan Approval Prediction\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether an applicant is approved for a loan (`loan_status`). Participants must output probabilities of loan approval for each test set applicant.\n* **Key Points:**\n  * Synthetic dataset generated from a deep learning model trained on real-world loan approval data.\n  * Focus on probabilistic prediction (not just binary labels).\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing loan applicant information (synthetically generated to mimic real-world loan approval data).\n* **Data Files:**\n  * `train.csv`: Contains features + binary target (`loan_status`).\n  * `test.csv`: Features only; requires prediction of `loan_status` probabilities.\n  * `sample_submission.csv`: Example submission format.\n* **Features:** \n  * 27 columns (specifics not detailed, but likely include applicant financial/demographic attributes).\n  * Target: `loan_status` (binary approval outcome).\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).\n  * Measures model performance across all classification thresholds.\n  * Submissions evaluated on predicted probabilities (not hard class labels).",
    "sections": {},
    "file_path": "kaggle_datasets/627/problem_summary.md"
  },
  "415": {
    "problem_id": "415",
    "title": "Evaluating Defensive Performance in NFL Passing Plays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Evaluating Defensive Performance in NFL Passing Plays\n\n## **Problem Description:**\n* **Problem Type:** Sports Analytics / Performance Evaluation\n* **Objective:**  \n    * Analyze NFL player tracking data to develop novel metrics for evaluating defensive performance on passing plays.  \n    * Identify strategic advantages, player skills, and team dynamics that contribute to successful pass defense.  \n* **Key Points:**  \n    * Focus on defensive strategies (e.g., man vs. zone coverage) and player actions (e.g., tracking receivers, closing speed).  \n    * Potential sub-tasks include:  \n        * Classifying coverage schemes.  \n        * Quantifying individual player effectiveness (e.g., coverage tightness, reaction time).  \n        * Predicting penalties like defensive pass interference.  \n        * Correlating player attributes (height, speed) with defensive performance.  \n\n## **Dataset Overview:**\n* **Data Type & Context:**  \n    * Tabular tracking data from NFL's Next Gen Stats, capturing player positions, speeds, and events during 2018 regular-season passing plays.  \n* **Data Files:**  \n    * `games.csv`: Game metadata (teams, date, week).  \n    * `players.csv`: Player attributes (height, weight, position).  \n    * `plays.csv`: Play details (down, distance, outcome, penalties).  \n    * `week[1-17].csv`: Player tracking data (position, speed, acceleration per frame).  \n* **Key Features:**  \n    * Tracking data: `x`, `y` coordinates, speed (`s`), acceleration (`a`), orientation (`o`), direction (`dir`), and event tags (e.g., ball snap, pass release).  \n    * Play outcomes: Pass result (`complete`, `incomplete`, `sack`), penalties, and expected points added (`epa`).  \n\n## **Evaluation Metrics:**\n* **Evaluation Method:**  \n    * Submissions are judged holistically by NFL analysts on five equally weighted criteria:  \n        1. **Innovation:** Novelty and actionability of insights.  \n        2. **Accuracy:** Statistical correctness and model appropriateness.  \n        3. **Relevance:** Practical utility for NFL teams.  \n        4. **Clarity:** Explanation of findings.  \n        5. **Visualization:** Quality and accessibility of charts/tables.  \n* **Submission Format:**  \n    * Notebooks (≤2,000 words, ≤7",
    "sections": {},
    "file_path": "kaggle_datasets/415/problem_summary.md"
  },
  "412": {
    "problem_id": "412",
    "title": "Helmet Impact Detection in NFL Videos",
    "problem_type": "Computer Vision - Object Detection (with temporal context)",
    "objective": "Develop a computer vision model to automatically detect helmet impacts in NFL game videos, specifically:",
    "evaluation_metric": null,
    "full_content": "# Helmet Impact Detection in NFL Videos\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Detection (with temporal context)\n* **Objective:** Develop a computer vision model to automatically detect helmet impacts in NFL game videos, specifically:\n    * Identify bounding boxes around helmets involved in impacts\n    * Localize impacts temporally within video sequences\n* **Key Points:**\n    * Focus on definitive impacts (meeting confidence and visibility thresholds)\n    * Must handle multiple camera views (sideline and endzone) per play\n    * Temporal tolerance: Predictions within ±4 frames of ground truth are acceptable\n    * Part of NFL's \"Digital Athlete\" initiative for player safety research\n\n**Dataset Overview:**\n* **Data Type & Context:** Video data of NFL plays with annotated helmet impacts and player tracking data\n* **Data Files:**\n    * `train/` and `test/` folders containing MP4 videos (sideline + endzone views per play)\n    * `train_labels.csv`: Bounding box annotations with impact metadata\n    * `images/` + `image_labels.csv`: Supplemental helmet images for detection training\n    * `*_player_tracking.csv`: Precise player position/speed/orientation data\n* **Key Features:**\n    * Frame-synced multi-view videos (59.94 FPS)\n    * Impact annotations include:\n        * Bounding box coordinates\n        * Impact type (helmet, shoulder, body, etc.)\n        * Confidence and visibility ratings\n    * Player tracking data (position, speed, acceleration, orientation)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Micro F1 Score at IoU threshold of 0.35\n* **Scoring Components:**\n    * **True Positives:** Predicted boxes with IoU ≥ 0.35 within ±4 frames of ground truth\n    * **Assignment Logic:**\n        * Optimal matching between predictions and ground truth to maximize TP\n        * One-to-one matching enforced (no duplicate assignments)\n    * **F1 Calculation:**\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * F1 = 2 * (precision * recall) / (precision + recall)",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Detection (with temporal context)\n* **Objective:** Develop a computer vision model to automatically detect helmet impacts in NFL game videos, specifically:\n    * Identify bounding boxes around helmets involved in impacts\n    * Localize impacts temporally within video sequences\n* **Key Points:**\n    * Focus on definitive impacts (meeting confidence and visibility thresholds)\n    * Must handle multiple camera views (sideline and endzone) per play\n    * Temporal tolerance: Predictions within ±4 frames of ground truth are acceptable\n    * Part of NFL's \"Digital Athlete\" initiative for player safety research",
      "Dataset Overview": "* **Data Type & Context:** Video data of NFL plays with annotated helmet impacts and player tracking data\n* **Data Files:**\n    * `train/` and `test/` folders containing MP4 videos (sideline + endzone views per play)\n    * `train_labels.csv`: Bounding box annotations with impact metadata\n    * `images/` + `image_labels.csv`: Supplemental helmet images for detection training\n    * `*_player_tracking.csv`: Precise player position/speed/orientation data\n* **Key Features:**\n    * Frame-synced multi-view videos (59.94 FPS)\n    * Impact annotations include:\n        * Bounding box coordinates\n        * Impact type (helmet, shoulder, body, etc.)\n        * Confidence and visibility ratings\n    * Player tracking data (position, speed, acceleration, orientation)",
      "Evaluation Metrics": "* **Primary Metric:** Micro F1 Score at IoU threshold of 0.35\n* **Scoring Components:**\n    * **True Positives:** Predicted boxes with IoU ≥ 0.35 within ±4 frames of ground truth\n    * **Assignment Logic:**\n        * Optimal matching between predictions and ground truth to maximize TP\n        * One-to-one matching enforced (no duplicate assignments)\n    * **F1 Calculation:**\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * F1 = 2 * (precision * recall) / (precision + recall)"
    },
    "file_path": "kaggle_datasets/412/problem_summary.md"
  },
  "620": {
    "problem_id": "620",
    "title": "Predicting Human Preferences in Chatbot Arena Conversations",
    "problem_type": "Multi-class Classification (Preference Prediction)",
    "objective": "Predict which of two chatbot responses a human user will prefer in a head-to-head comparison, given a prompt and the two responses. The goal is to model human preferences in chatbot interactions to improve alignment with user expectations.",
    "evaluation_metric": null,
    "full_content": "# Predicting Human Preferences in Chatbot Arena Conversations\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Preference Prediction)\n* **Objective:** Predict which of two chatbot responses a human user will prefer in a head-to-head comparison, given a prompt and the two responses. The goal is to model human preferences in chatbot interactions to improve alignment with user expectations.\n    * **Key Points:**\n        * Focuses on real-world data from Chatbot Arena, where users anonymously compare responses from different LLMs.\n        * Aligns with Reinforcement Learning from Human Feedback (RLHF) concepts, specifically reward/preference modeling.\n        * Must account for known biases in human preference judgments (position bias, verbosity bias, self-enhancement bias).\n\n**Dataset Overview:**\n* **Data Type & Context:** Text data consisting of:\n    * User prompts\n    * Paired responses from two anonymous LLMs\n    * Human preference judgments (winner selection or tie)\n* **Data Files:**\n    * `train.csv` (55K rows): Contains prompt, model identities, responses, and human preference labels.\n    * `test.csv` (~25K rows): Contains prompt and responses (no model identities or labels).\n    * `sample_submission.csv`: Example submission format.\n* **Key Features:**\n    * `prompt`: User-provided input text\n    * `response_a`/`response_b`: Competing LLM responses\n    * `winner_model_[a/b/tie]`: Target labels indicating human preference\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss (multi-class version)\n    * Measures the quality of predicted probabilities for all three possible outcomes (model A wins, model B wins, or tie)\n    * Uses `eps=auto` parameter for automatic handling of edge cases in probability estimation\n    * Submission requires predicted probabilities for all three classes per test instance",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Preference Prediction)\n* **Objective:** Predict which of two chatbot responses a human user will prefer in a head-to-head comparison, given a prompt and the two responses. The goal is to model human preferences in chatbot interactions to improve alignment with user expectations.\n    * **Key Points:**\n        * Focuses on real-world data from Chatbot Arena, where users anonymously compare responses from different LLMs.\n        * Aligns with Reinforcement Learning from Human Feedback (RLHF) concepts, specifically reward/preference modeling.\n        * Must account for known biases in human preference judgments (position bias, verbosity bias, self-enhancement bias).",
      "Dataset Overview": "* **Data Type & Context:** Text data consisting of:\n    * User prompts\n    * Paired responses from two anonymous LLMs\n    * Human preference judgments (winner selection or tie)\n* **Data Files:**\n    * `train.csv` (55K rows): Contains prompt, model identities, responses, and human preference labels.\n    * `test.csv` (~25K rows): Contains prompt and responses (no model identities or labels).\n    * `sample_submission.csv`: Example submission format.\n* **Key Features:**\n    * `prompt`: User-provided input text\n    * `response_a`/`response_b`: Competing LLM responses\n    * `winner_model_[a/b/tie]`: Target labels indicating human preference",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss (multi-class version)\n    * Measures the quality of predicted probabilities for all three possible outcomes (model A wins, model B wins, or tie)\n    * Uses `eps=auto` parameter for automatic handling of edge cases in probability estimation\n    * Submission requires predicted probabilities for all three classes per test instance"
    },
    "file_path": "kaggle_datasets/620/problem_summary.md"
  },
  "244": {
    "problem_id": "244",
    "title": "Text Normalization for English Language",
    "problem_type": "NLP - Text Normalization (Sequence-to-Sequence Transformation)",
    "objective": "Automate the conversion of English text from written expressions into appropriate spoken forms for applications like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). The task involves transforming elements like:",
    "evaluation_metric": null,
    "full_content": "# Text Normalization for English Language\n\n**Problem Description:**\n* **Problem Type:** NLP - Text Normalization (Sequence-to-Sequence Transformation)\n* **Objective:** Automate the conversion of English text from written expressions into appropriate spoken forms for applications like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). The task involves transforming elements like:\n  * Numbers (e.g., \"12:47\" → \"twelve forty-seven\")\n  * Currency (e.g., \"$3.16\" → \"three dollars, sixteen cents\")\n  * Abbreviations and other non-standard words\n* **Key Points:**\n  * Focuses specifically on English language normalization (a separate competition existed for Russian)\n  * Requires exact sequence accuracy in transformations\n  * Training data includes token class labels (omitted in test set), adding complexity\n\n**Dataset Overview:**\n* **Data Type:** Text data (tokenized English sentences with raw/normalized pairs)\n* **Context:** Linguistic normalization for speech applications\n* **Data Files:**\n  * `en_train.csv`: Training set with columns: `id`, `sentence_id`, `token_id`, `before` (raw text), `after` (normalized text), `class` (token type)\n  * `en_test.csv`: Test set (same structure but without `after` and `class` columns)\n  * `en_sample_submission.csv`: Submission format example\n* **Key Features:**\n  * Sentence-level and token-level identifiers\n  * Raw/normalized text pairs in training data\n  * Token class labels (e.g., numbers, dates, abbreviations) provided only in training\n\n**Evaluation Metrics:**\n* **Primary Metric:** Prediction Accuracy (Exact Match)\n  * Calculated as percentage of tokens where predicted normalization exactly matches ground truth\n  * Entire predicted sequence must be correct (e.g., \"one forty five\" vs \"one hundred forty five\" counts as error)\n  * Strict evaluation reflecting real-world TTS/ASR requirements",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Text Normalization (Sequence-to-Sequence Transformation)\n* **Objective:** Automate the conversion of English text from written expressions into appropriate spoken forms for applications like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). The task involves transforming elements like:\n  * Numbers (e.g., \"12:47\" → \"twelve forty-seven\")\n  * Currency (e.g., \"$3.16\" → \"three dollars, sixteen cents\")\n  * Abbreviations and other non-standard words\n* **Key Points:**\n  * Focuses specifically on English language normalization (a separate competition existed for Russian)\n  * Requires exact sequence accuracy in transformations\n  * Training data includes token class labels (omitted in test set), adding complexity",
      "Dataset Overview": "* **Data Type:** Text data (tokenized English sentences with raw/normalized pairs)\n* **Context:** Linguistic normalization for speech applications\n* **Data Files:**\n  * `en_train.csv`: Training set with columns: `id`, `sentence_id`, `token_id`, `before` (raw text), `after` (normalized text), `class` (token type)\n  * `en_test.csv`: Test set (same structure but without `after` and `class` columns)\n  * `en_sample_submission.csv`: Submission format example\n* **Key Features:**\n  * Sentence-level and token-level identifiers\n  * Raw/normalized text pairs in training data\n  * Token class labels (e.g., numbers, dates, abbreviations) provided only in training",
      "Evaluation Metrics": "* **Primary Metric:** Prediction Accuracy (Exact Match)\n  * Calculated as percentage of tokens where predicted normalization exactly matches ground truth\n  * Entire predicted sequence must be correct (e.g., \"one forty five\" vs \"one hundred forty five\" counts as error)\n  * Strict evaluation reflecting real-world TTS/ASR requirements"
    },
    "file_path": "kaggle_datasets/244/problem_summary.md"
  },
  "629": {
    "problem_id": "629",
    "title": "Binary Classification of Depression Risk from Mental Health Survey Data",
    "problem_type": "Binary Classification",
    "objective": "Predict whether individuals are at risk of depression (`Depression` target variable) based on mental health survey data. The goal is to identify key factors contributing to depression risk.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Depression Risk from Mental Health Survey Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether individuals are at risk of depression (`Depression` target variable) based on mental health survey data. The goal is to identify key factors contributing to depression risk.\n    * **Key Points:**\n        * Dataset is synthetically generated from real-world depression survey data, maintaining similar feature distributions but with intentional artifacts.\n        * Encourages exploration of visualization techniques alongside predictive modeling.\n        * Original real-world dataset can be incorporated for comparison/improvement.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing anonymized mental health survey responses (41 features total).\n* **Data Files:**\n    * `train.csv` - Training data with binary target column (`Depression`)\n    * `test.csv` - Test data for predictions\n    * `sample_submission.csv` - Submission template\n* **Features:** Survey-based attributes (specifics not detailed, but typical mental health survey dimensions likely included - e.g., behavioral indicators, demographic factors, lifestyle variables).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Accuracy Score\n    * Simple proportion of correct predictions (both positive and negative cases weighted equally)\n    * Calculated as: (True Positives + True Negatives) / Total Predictions",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether individuals are at risk of depression (`Depression` target variable) based on mental health survey data. The goal is to identify key factors contributing to depression risk.\n    * **Key Points:**\n        * Dataset is synthetically generated from real-world depression survey data, maintaining similar feature distributions but with intentional artifacts.\n        * Encourages exploration of visualization techniques alongside predictive modeling.\n        * Original real-world dataset can be incorporated for comparison/improvement.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing anonymized mental health survey responses (41 features total).\n* **Data Files:**\n    * `train.csv` - Training data with binary target column (`Depression`)\n    * `test.csv` - Test data for predictions\n    * `sample_submission.csv` - Submission template\n* **Features:** Survey-based attributes (specifics not detailed, but typical mental health survey dimensions likely included - e.g., behavioral indicators, demographic factors, lifestyle variables).",
      "Evaluation Metrics": "* **Primary Metric:** Accuracy Score\n    * Simple proportion of correct predictions (both positive and negative cases weighted equally)\n    * Calculated as: (True Positives + True Negatives) / Total Predictions"
    },
    "file_path": "kaggle_datasets/629/problem_summary.md"
  },
  "286": {
    "problem_id": "286",
    "title": "Cuisine Classification from Recipe Ingredients",
    "problem_type": "Multi-class Classification (Text Data)",
    "objective": "Predict the cuisine category of a dish based solely on its list of ingredients.",
    "evaluation_metric": null,
    "full_content": "# Cuisine Classification from Recipe Ingredients\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Text Data)\n* **Objective:** Predict the cuisine category of a dish based solely on its list of ingredients.\n    * **Key Points:**\n        * The task leverages cultural/geographic associations between ingredients and cuisines.\n        * Ingredients are provided as variable-length lists of text strings.\n        * The challenge involves feature engineering from unstructured text data (ingredient lists).\n\n**Dataset Overview:**\n* **Data Type & Context:** JSON files containing recipe data (id, cuisine label, and ingredient lists).\n    * **Data Files:**\n        * `train.json`: Contains recipe IDs, cuisine labels, and ingredient lists for training.\n        * `test.json`: Contains recipe IDs and ingredient lists (cuisine labels removed for prediction).\n        * `sample_submission.csv`: Example submission file with ID-cuisine pairs.\n    * **Key Features:**\n        * `ingredients`: Raw text entries representing recipe components (e.g., \"turmeric\", \"naan\", \"sweet potatoes\").\n        * `cuisine`: Target variable (multi-class label like \"indian\", \"italian\", etc.).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy (percentage of correctly classified dishes).\n    * **Calculation:** \n        * Simple ratio: (Correct Predictions) / (Total Predictions) * 100.\n        * Submission requires pairing recipe IDs with predicted cuisine labels in a CSV.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Text Data)\n* **Objective:** Predict the cuisine category of a dish based solely on its list of ingredients.\n    * **Key Points:**\n        * The task leverages cultural/geographic associations between ingredients and cuisines.\n        * Ingredients are provided as variable-length lists of text strings.\n        * The challenge involves feature engineering from unstructured text data (ingredient lists).",
      "Dataset Overview": "* **Data Type & Context:** JSON files containing recipe data (id, cuisine label, and ingredient lists).\n    * **Data Files:**\n        * `train.json`: Contains recipe IDs, cuisine labels, and ingredient lists for training.\n        * `test.json`: Contains recipe IDs and ingredient lists (cuisine labels removed for prediction).\n        * `sample_submission.csv`: Example submission file with ID-cuisine pairs.\n    * **Key Features:**\n        * `ingredients`: Raw text entries representing recipe components (e.g., \"turmeric\", \"naan\", \"sweet potatoes\").\n        * `cuisine`: Target variable (multi-class label like \"indian\", \"italian\", etc.).",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy (percentage of correctly classified dishes).\n    * **Calculation:** \n        * Simple ratio: (Correct Predictions) / (Total Predictions) * 100.\n        * Submission requires pairing recipe IDs with predicted cuisine labels in a CSV."
    },
    "file_path": "kaggle_datasets/286/problem_summary.md"
  },
  "272": {
    "problem_id": "272",
    "title": "Fine-Grained Image Classification of 8,000 Species with Long-Tailed Data",
    "problem_type": "Multi-class Classification (Fine-Grained Visual Categorization)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Image Classification of 8,000 Species with Long-Tailed Data\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Fine-Grained Visual Categorization)\n* **Objective:**  \n    * Develop an automatic image classification system capable of distinguishing between 8,000+ visually similar species of plants and animals in real-world conditions.\n    * Address challenges of class imbalance (long-tailed distribution) and fine-grained visual differences between species.\n* **Key Points:**\n    * Dataset contains many visually similar species with high intra-class variation (different poses, lighting, backgrounds).\n    * Images are collected globally under uncontrolled conditions, mimicking real-world biodiversity observation scenarios.\n    * Additional metadata (latitude, longitude, date) may help disambiguate species but isn't required for predictions.\n\n**Dataset Overview:**\n* **Data Type:** Image data (JPEG) with JSON annotations\n* **Context:** Crowdsourced biodiversity observations from iNaturalist platform\n* **Data Files:**\n    * `train_val2018.tar.gz` - Training/validation images (450,000 total)\n    * `train2018.json.tar.gz`, `val2018.json.tar.gz` - Annotations\n    * `test2018.tar.gz` - Test images\n    * Sample submission file in CSV format\n* **Features:**\n    * Images resized to max 800px dimension\n    * Hierarchical species labels (kingdom → genus)\n    * Optional metadata in annotations (geolocation, license info)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Top-3 Error Rate (custom implementation)\n    * For each test image, model must predict 3 possible species IDs (ordered by confidence)\n    * Error = 0 if ground truth appears in top-3 predictions, else 1\n    * Final score = average error across all test images\n* **Implementation Notes:**\n    * Metric allows for taxonomic ambiguity and multi-species images\n    * Predictions should be confidence-ordered (leftmost = most confident)\n    * Inspired by ILSVRC evaluation but adapted for fine-grained classification",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Fine-Grained Visual Categorization)\n* **Objective:**  \n    * Develop an automatic image classification system capable of distinguishing between 8,000+ visually similar species of plants and animals in real-world conditions.\n    * Address challenges of class imbalance (long-tailed distribution) and fine-grained visual differences between species.\n* **Key Points:**\n    * Dataset contains many visually similar species with high intra-class variation (different poses, lighting, backgrounds).\n    * Images are collected globally under uncontrolled conditions, mimicking real-world biodiversity observation scenarios.\n    * Additional metadata (latitude, longitude, date) may help disambiguate species but isn't required for predictions.",
      "Dataset Overview": "* **Data Type:** Image data (JPEG) with JSON annotations\n* **Context:** Crowdsourced biodiversity observations from iNaturalist platform\n* **Data Files:**\n    * `train_val2018.tar.gz` - Training/validation images (450,000 total)\n    * `train2018.json.tar.gz`, `val2018.json.tar.gz` - Annotations\n    * `test2018.tar.gz` - Test images\n    * Sample submission file in CSV format\n* **Features:**\n    * Images resized to max 800px dimension\n    * Hierarchical species labels (kingdom → genus)\n    * Optional metadata in annotations (geolocation, license info)",
      "Evaluation Metrics": "* **Primary Metric:** Top-3 Error Rate (custom implementation)\n    * For each test image, model must predict 3 possible species IDs (ordered by confidence)\n    * Error = 0 if ground truth appears in top-3 predictions, else 1\n    * Final score = average error across all test images\n* **Implementation Notes:**\n    * Metric allows for taxonomic ambiguity and multi-species images\n    * Predictions should be confidence-ordered (leftmost = most confident)\n    * Inspired by ILSVRC evaluation but adapted for fine-grained classification"
    },
    "file_path": "kaggle_datasets/272/problem_summary.md"
  },
  "424": {
    "problem_id": "424",
    "title": "Multi-label Classification of Catheter and Line Positions in Chest X-rays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Catheter and Line Positions in Chest X-rays\n\n## Problem Description\n* **Problem Type:** Multi-label Classification (Computer Vision - Medical Imaging)\n* **Objective:** Detect the presence and correct placement of catheters and lines on chest x-rays to identify malpositioned tubes that could lead to life-threatening complications. Participants must classify x-rays into:\n    * **Normal:** Appropriately positioned tubes\n    * **Borderline:** Tubes needing ideal repositioning but still functional\n    * **Abnormal:** Tubes requiring immediate repositioning\n* **Key Points:**\n    * Focuses on 4 tube/line types: Endotracheal Tube (ETT), Nasogastric Tube (NGT), Central Venous Catheter (CVC), Swan Ganz Catheter\n    * Includes an \"Incompletely Imaged\" category for NGT when positioning is inconclusive\n    * Real-world medical application with potential to reduce human error in high-stress hospital environments\n\n## Dataset Overview\n* **Data Type:** Medical images (Chest X-rays) with tabular labels\n* **Context:** 40,000 annotated chest x-rays from hospital patients, labeled by radiologists\n* **Data Files:**\n    * `train/` - Training images\n    * `test/` - Test images (public)\n    * Hidden test set (~14k images)\n    * `train.csv` - Image IDs, binary labels, and patient IDs\n    * `train_annotations.csv` - Segmentation annotations for some training samples\n* **Key Features:**\n    * Images labeled for multiple tube types simultaneously (multi-label)\n    * Patient IDs enable tracking of multiple x-rays per patient\n    * Labels include 11 target classes across 4 tube/line types\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Column-wise Area Under the ROC Curve (AUC)\n    * AUC calculated separately for each of the 11 labels\n    * Final score is the average of all individual AUCs\n* **Submission Format:**\n    * Requires probability predictions for all 11 target variables per test image\n    * Format: `StudyInstanceUID, ETT - Abnormal, ETT - Borderline, ..., Swan Ganz Catheter Present`",
    "sections": {},
    "file_path": "kaggle_datasets/424/problem_summary.md"
  },
  "616": {
    "problem_id": "616",
    "title": "Predicting Small Molecule-Protein Binding Affinity with BELKA",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Small Molecule-Protein Binding Affinity with BELKA\n\n## Problem Description\n* **Problem Type**: Binary Classification (with strong class imbalance)\n* **Objective**: Predict whether a given small molecule will bind to one of three specific protein targets (EPHX2/sEH, BRD4, or ALB/HSA), which is a critical step in computational drug discovery.\n    * **Key Points**:\n        * Focuses on predicting binding affinity from molecular structure data\n        * Aims to accelerate drug discovery by replacing laborious physical experiments with ML predictions\n        * Test set contains novel building blocks not seen in training (evaluates model generalizability)\n        * Encourages exploration of different molecular representation methods (SMILES, graphs, 3D structures, etc.)\n\n## Dataset Overview\n* **Data Type**: Chemical/biological data with tabular structure\n* **Context**: DNA-encoded chemical library (DEL) screening results from pharmaceutical research\n* **Data Files**:\n    * train.csv/train.parquet (98M examples per protein)\n    * test.csv/test.parquet (360K molecules per protein)\n    * sample_submission.csv\n* **Key Features**:\n    * SMILES strings for 3 building blocks and complete molecule\n    * Protein target name (3 possible targets)\n    * Binary binding label (highly imbalanced - ~0.5% positives)\n    * Unique ID for each molecule-protein pair\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Average Precision (mAP)\n    * **Implementation Details**:\n        * Average precision calculated for each (protein, split group) combination\n        * Final score is average of these values\n        * Custom implementation provided in competition code\n* **Submission Format**:\n    * Probability predictions for binary `binds` target\n    * CSV with `id,binds` header and rows like `295246830,0.5`",
    "sections": {},
    "file_path": "kaggle_datasets/616/problem_summary.md"
  },
  "611": {
    "problem_id": "611",
    "title": "3D Scene Reconstruction from Diverse 2D Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 3D Scene Reconstruction from Diverse 2D Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - 3D Reconstruction (Structure from Motion)\n* **Objective**: Develop a model to reconstruct precise 3D scenes from collections of 2D images across six challenging domains:\n  * Phototourism/historical preservation (varying viewpoints, sensors, times)\n  * Night vs day/temporal changes (lighting/weather variations)\n  * Aerial/mixed aerial-ground (drone + ground perspectives)\n  * Repeated structures (symmetrical objects)\n  * Natural environments (trees/foliage)\n  * Transparencies/reflections (glass/textureless objects)\n* **Key Points**:\n  * Must handle extreme domain variation in single competition (\"Hexathlon\")\n  * Focus on Structure from Motion (SfM) from unstructured image collections\n  * Ground truth includes camera poses (rotation + translation)\n\n## Dataset Overview\n* **Data Type**: Image sets with associated 3D reconstructions\n* **Context**: Unstructured collections of photographs from diverse real-world scenarios\n* **Data Files**:\n  * `train/*/images/`: Training images (some with `images_full` containing extras)\n  * `train/*/sfm/`: Ground truth 3D reconstructions (COLMAP format)\n  * `train_labels.csv`: Ground truth camera poses for training images\n  * `sample_submission.csv`: Submission format template\n* **Key Features**:\n  * Images vary in capture conditions (viewpoint, lighting, sensor)\n  * Test set has limited image-to-image overlap vs. training\n  * Camera pose targets: 3x3 rotation matrix + 3D translation vector per image\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Average Accuracy (mAA) of registered camera centers\n* **Calculation Process**:\n  1. For each scene, find best similarity transformation (scale/rotation/translation) that registers maximal camera centers to ground truth\n  2. A camera is registered if `||Cg - T(C)|| < threshold` (thresholds range 1cm-1m)\n  3. Compute percentage of successfully registered cameras per threshold\n  4. Average registration percentages across thresholds → mAA per scene\n  5. Final score = average mAA across all scenes\n* **Key Aspects**:\n  * Uses RANSAC-like exhaustive",
    "sections": {},
    "file_path": "kaggle_datasets/611/problem_summary.md"
  },
  "423": {
    "problem_id": "423",
    "title": "Insurance Claim Amount Prediction with Synthetic Tabular Data",
    "problem_type": "Regression",
    "objective": "Predict a continuous target variable representing the amount of an insurance claim based on anonymized feature columns. The competition is designed as an approachable playground for practicing ML skills, positioned between beginner and advanced levels.",
    "evaluation_metric": null,
    "full_content": "# Insurance Claim Amount Prediction with Synthetic Tabular Data\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict a continuous target variable representing the amount of an insurance claim based on anonymized feature columns. The competition is designed as an approachable playground for practicing ML skills, positioned between beginner and advanced levels.\n    * **Key Points:**\n        * Dataset is synthetic but based on real-world insurance claim data (generated using CTGAN)\n        * Features are anonymized but maintain properties of real-world features\n        * Intended as a learning exercise rather than highly competitive challenge\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with anonymized insurance-related features\n* **Data Files:**\n    * train.csv (contains target column)\n    * test.csv\n    * sample_submission.csv\n* **Features:**\n    * 10 categorical features (cat0-cat9)\n    * 14 continuous features (cont0-cont13)\n    * One continuous target variable to predict\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * Square root of the average of squared differences between predicted and actual values\n        * Formula: RMSE = √[Σ(y_i - ŷ_i)²/n]\n        * Where y_i = actual value, ŷ_i = predicted value, n = number of observations",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict a continuous target variable representing the amount of an insurance claim based on anonymized feature columns. The competition is designed as an approachable playground for practicing ML skills, positioned between beginner and advanced levels.\n    * **Key Points:**\n        * Dataset is synthetic but based on real-world insurance claim data (generated using CTGAN)\n        * Features are anonymized but maintain properties of real-world features\n        * Intended as a learning exercise rather than highly competitive challenge",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with anonymized insurance-related features\n* **Data Files:**\n    * train.csv (contains target column)\n    * test.csv\n    * sample_submission.csv\n* **Features:**\n    * 10 categorical features (cat0-cat9)\n    * 14 continuous features (cont0-cont13)\n    * One continuous target variable to predict",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * Square root of the average of squared differences between predicted and actual values\n        * Formula: RMSE = √[Σ(y_i - ŷ_i)²/n]\n        * Where y_i = actual value, ŷ_i = predicted value, n = number of observations"
    },
    "file_path": "kaggle_datasets/423/problem_summary.md"
  },
  "275": {
    "problem_id": "275",
    "title": "Humpback Whale Identification from Fluke Images",
    "problem_type": "Multi-class Image Classification (with open-set recognition for unknown whales).",
    "objective": "Build an algorithm to identify individual humpback whales from images of their flukes (tails). The goal is to:",
    "evaluation_metric": null,
    "full_content": "# Humpback Whale Identification from Fluke Images\n\n**Problem Description:**\n* **Problem Type:** Multi-class Image Classification (with open-set recognition for unknown whales).\n* **Objective:** Build an algorithm to identify individual humpback whales from images of their flukes (tails). The goal is to:\n    * Classify known whales from a set of 3,000+ unique IDs in the training data.\n    * Detect unknown whales (not in the training set) by labeling them as `new_whale`.\n* **Key Points:**\n    * **Fine-grained classification:** Many whale IDs have very few training examples (few-shot learning challenge).\n    * **Open-set recognition:** Must handle test images of whales not seen during training.\n    * **Conservation focus:** Aims to automate whale monitoring for ecological research.\n\n**Dataset Overview:**\n* **Data Type:** Image data (whale fluke photographs) with ID labels.\n* **Context:** Images collected by research institutions and public contributors via Happy Whale platform.\n* **Data Files:**\n    * `train.zip`: Folder containing >25,000 training images.\n    * `train.csv`: Maps image filenames to whale IDs (`new_whale` indicates unknown individuals).\n    * `test.zip`: Test images for prediction.\n    * `sample_submission.csv`: Submission format example.\n* **Features:**\n    * Images show flukes with unique natural markings (shape, pigmentation patterns).\n    * Labels represent individual whale IDs (3,000+ classes) or `new_whale`.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision @ 5 (MAP@5)\n    * **Components:**\n        * For each test image, predict up to 5 possible whale IDs (ordered by confidence).\n        * Unknown whales should be predicted as `new_whale`.\n        * Precision is calculated at each cutoff rank (k=1 to 5).\n        * Averages precision scores across all test images.\n    * **Key Property:** Rewards models that place correct IDs (or `new_whale`) higher in the prediction list.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Image Classification (with open-set recognition for unknown whales).\n* **Objective:** Build an algorithm to identify individual humpback whales from images of their flukes (tails). The goal is to:\n    * Classify known whales from a set of 3,000+ unique IDs in the training data.\n    * Detect unknown whales (not in the training set) by labeling them as `new_whale`.\n* **Key Points:**\n    * **Fine-grained classification:** Many whale IDs have very few training examples (few-shot learning challenge).\n    * **Open-set recognition:** Must handle test images of whales not seen during training.\n    * **Conservation focus:** Aims to automate whale monitoring for ecological research.",
      "Dataset Overview": "* **Data Type:** Image data (whale fluke photographs) with ID labels.\n* **Context:** Images collected by research institutions and public contributors via Happy Whale platform.\n* **Data Files:**\n    * `train.zip`: Folder containing >25,000 training images.\n    * `train.csv`: Maps image filenames to whale IDs (`new_whale` indicates unknown individuals).\n    * `test.zip`: Test images for prediction.\n    * `sample_submission.csv`: Submission format example.\n* **Features:**\n    * Images show flukes with unique natural markings (shape, pigmentation patterns).\n    * Labels represent individual whale IDs (3,000+ classes) or `new_whale`.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision @ 5 (MAP@5)\n    * **Components:**\n        * For each test image, predict up to 5 possible whale IDs (ordered by confidence).\n        * Unknown whales should be predicted as `new_whale`.\n        * Precision is calculated at each cutoff rank (k=1 to 5).\n        * Averages precision scores across all test images.\n    * **Key Property:** Rewards models that place correct IDs (or `new_whale`) higher in the prediction list."
    },
    "file_path": "kaggle_datasets/275/problem_summary.md"
  },
  "281": {
    "problem_id": "281",
    "title": "Large-Scale Object Detection in Complex Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Large-Scale Object Detection in Complex Images\n\n## Problem Description\n- **Problem Type:** Computer Vision - Object Detection\n- **Objective:** Build an algorithm to detect objects in complex images with high precision, leveraging a massive training dataset with diverse object classes and bounding-box annotations. The goal is to push the boundaries of computer vision beyond current state-of-the-art performance.\n    * **Key Points:**\n        * Focus on detecting objects in varied and complex scenes (average of 7 objects per image).\n        * Handle 500 object classes, including novel categories like \"fedora\" and \"snowman\".\n        * Address challenges like class hierarchy relationships and diverse real-world scenes.\n\n## Dataset Overview\n- **Data Type & Context:** RGB images of complex real-world scenes with bounding-box annotations for object detection.\n- **Data Files:**\n    * `test.zip` - 99,999 test images (independent from Open Images V4 release).\n    * `sample_submission.csv` - Submission template with required format.\n- **Features:**\n    * Training set contains 12M bounding-box annotations across 1.7M images.\n    * Images contain multiple objects per scene with high variability.\n    * Object classes organized in a hierarchical structure.\n\n## Evaluation Metrics\n- **Evaluation Metric:** Mean Average Precision (mAP) across 500 classes.\n    * **Components:**\n        * AP calculated per class using precision-recall curves.\n        * Final score is average of all per-class AP values.\n        * Metric accounts for Open Images' annotation process specifics.\n        * Implementation available in TensorFlow Object Detection API.",
    "sections": {},
    "file_path": "kaggle_datasets/281/problem_summary.md"
  },
  "257": {
    "problem_id": "257",
    "title": "Camera Model Identification from Image Traces",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Camera Model Identification from Image Traces\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Computer Vision)\n* **Objective:** Build an algorithm to identify the specific camera model that captured a given image by analyzing intrinsic traces left in the image data. This is critical for authenticating digital evidence in legal and journalistic contexts.\n    * **Key Points:**\n        * Focuses on distinguishing between 10 different camera models (e.g., iPhone 6, Samsung Galaxy S4, Sony NEX-7).\n        * Must generalize to unseen devices of the same model (train and test images come from different physical cameras of the same model).\n        * Test set includes both unaltered images and images altered via common manipulations (JPEG compression, resizing, gamma correction).\n\n## Dataset Overview\n* **Data Type & Context:** Image data (RGB) with metadata labels indicating camera models and manipulation status.\n    * **Data Files:**\n        * `train.zip`: Full images organized by camera model (275 images per model).\n        * `test.zip`: 512x512 pixel center-cropped blocks from images (half manipulated, half unaltered).\n        * `sample_submission.csv`: Submission template with `fname` and `camera` columns.\n    * **Features:**\n        * Raw image pixels (no metadata provided).\n        * Filenames indicate manipulation status (`_manip` or `_unalt` suffixes).\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Categorization Accuracy  \n    * **Components:**\n        * Standard accuracy weighted by image type:  \n            * 0.7 weight for correct predictions on unaltered images (`_unalt`).  \n            * 0.3 weight for correct predictions on manipulated images (`_manip`).  \n        * Final score = (Sum of weights for correct predictions) / (Total sum of weights).  \n    * **Nuance:** Emphasizes robustness to both original and altered images, with higher importance placed on unaltered samples.",
    "sections": {},
    "file_path": "kaggle_datasets/257/problem_summary.md"
  },
  "401": {
    "problem_id": "401",
    "title": "Landmark Recognition in Images with Large Class Space",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Landmark Recognition in Images with Large Class Space\n\n## Problem Description\n* **Problem Type:** Multi-class Image Classification (with potential null class for non-landmark images)\n* **Objective:**  \n    Build models to recognize specific landmarks depicted in images from a large set of possible classes (over 81K landmarks). The task involves:\n    * Predicting the correct landmark label directly from image pixels\n    * Handling cases where images may contain no landmarks at all\n    * Managing a highly imbalanced class distribution with many classes having few training examples\n* **Key Points:**\n    * Distinct from general image classification (e.g., ImageNet) due to:\n        * Much larger number of classes (81K+ vs. 1K in ImageNet)\n        * Potentially limited training examples per class\n    * Competition format: Synchronous rerun code competition (notebook submission)\n    * Related to instance-level recognition challenges in computer vision\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Image dataset of landmarks worldwide\n    * Includes both famous and obscure landmarks\n    * Cleaned version of Google Landmarks Dataset v2 (GLDv2)\n* **Data Files:**\n    * `train/` - Folder containing training images (organized in subfolders by first 3 characters of image ID)\n    * `train.csv` - Contains image IDs and corresponding landmark labels\n    * `test/` - Folder containing test images (same subfolder structure)\n    * `sample_submission.csv` - Submission format example\n* **Key Features:**\n    * Images vary in content, quality, and viewpoint\n    * Each image has unique ID used for organization\n    * Private test set contains different distribution than public data\n\n## Evaluation Metrics\n* **Primary Metric:** Global Average Precision (GAP) at k=1 (also called micro Average Precision - μAP)\n* **Metric Components:**\n    * For each test image, predict:\n        * One landmark label (or none)\n        * Confidence score for prediction\n    * Evaluation process:\n        1. All predictions across all test images are combined into single list\n        2. List is sorted by descending confidence scores\n        3. GAP is computed as:  \n           𝐺𝐴𝑃 = (1/𝑀) * Σ[𝑃(𝑖) * 𝑟𝑒𝑙(𝑖)]  \n           Where:\n           * 𝑁 = total predictions across all queries",
    "sections": {},
    "file_path": "kaggle_datasets/401/problem_summary.md"
  },
  "633": {
    "problem_id": "633",
    "title": "Predicting Problematic Internet Use from Physical Activity Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Problematic Internet Use from Physical Activity Data\n\n## Problem Description\n* **Problem Type:** Multiclass Classification\n* **Objective:**  \n    * Predict the Severity Impairment Index (`sii`) of children/adolescents, indicating their level of problematic internet use (None, Mild, Moderate, Severe).  \n    * Use physical activity and fitness data as proxies for identifying early signs of problematic internet use, bypassing traditional clinical assessments.  \n* **Key Points:**  \n    * Focus on accessible physical/fitness measures (e.g., accelerometer data, sleep patterns) to overcome barriers of clinical assessments.  \n    * Target variable (`sii`) is derived from the Parent-Child Internet Addiction Test (PCIAT).  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * **Tabular data:** Demographic, fitness, sleep, and internet usage metrics from clinical screenings.  \n    * **Time-series data:** Accelerometer recordings (actigraphy) from wrist-worn devices.  \n* **Data Files:**  \n    * `train.csv`/`test.csv`: Tabular features (e.g., demographics, fitness assessments, sleep scores).  \n    * `series_{train|test}.parquet`: Actigraphy time-series (acceleration, angle, wear flags, timestamps).  \n    * `data_dictionary.csv`: Descriptions of instruments/features.  \n* **Key Features:**  \n    * **Actigraphy:** `X/Y/Z` acceleration, `enmo`, `anglez`, wear flags, timestamps.  \n    * **Tabular:** Age, sex, fitness scores, sleep disturbances, PCIAT totals.  \n    * **Note:** High missingness; unsupervised techniques may be needed.  \n\n## Evaluation Metrics\n* **Primary Metric:** Quadratic Weighted Kappa (QWK)  \n    * **Components:**  \n        * Measures agreement between predicted and actual ordinal labels (`sii` values 0–3).  \n        * Weights errors by squared differences:  \n            ```math\n            W_{i,j} = \\frac{(i-j)^2}{(N-1)^2}\n            ```  \n        * Compares observed (`O`) vs. expected (`E`) agreement matrices:  \n            ```math\n            \\kappa = 1 - \\frac{\\sum_{i,j} W_{i,j} O_{i,j}}{\\sum_{i,j} W_{i,j} E_{i,j}}",
    "sections": {},
    "file_path": "kaggle_datasets/633/problem_summary.md"
  },
  "268": {
    "problem_id": "268",
    "title": "Landmark Image Retrieval Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Landmark Image Retrieval Challenge\n\n## Problem Description\n* **Problem Type:** Computer Vision - Image Retrieval\n* **Objective:** Given a query image, retrieve all images from a large database that depict the same landmark. The goal is to rank the retrieved images by relevance, with the most similar images appearing first.\n    * **Key Points:**\n        * The dataset contains over 1 million images of 15,000 unique landmarks.\n        * Query images may have no associated matches in the database, which are ignored in scoring.\n        * The challenge is paired with a Landmark Recognition Challenge, sharing the same test set but with no overlapping training/index landmarks.\n\n## Dataset Overview\n* **Data Type:** Image data (landmark photos)\n* **Context:** Largest worldwide dataset for image retrieval research at the time, with images sourced from URLs (not directly provided).\n* **Data Files:**\n    * `index.csv` - Database of images to retrieve from.\n    * `test.csv` - Query images for which predictions are made.\n    * `sample_submission.csv` - Example submission file.\n* **Features:**\n    * Each image has a unique `id` and is referenced by URL.\n    * No geolocation metadata is included, but other Exif data may be present.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Mean Average Precision @ 100 (mAP@100)\n    * **Components:**\n        * For each query image depicting a landmark in the index set:\n            * Precision at rank `k` is calculated for the top 100 predictions.\n            * Relevance of each prediction (`relq(k)`) is 1 if correct, 0 otherwise.\n            * Average precision is computed and averaged across all valid queries.\n        * Queries with no matches in the index set are ignored.",
    "sections": {},
    "file_path": "kaggle_datasets/268/problem_summary.md"
  },
  "634": {
    "problem_id": "634",
    "title": "Predicting Insurance Premiums with Tabular Data",
    "problem_type": "Regression",
    "objective": "Predict continuous insurance premium amounts based on various factors. The goal is to build a model that accurately estimates the premium cost for individuals or policies using the provided features.",
    "evaluation_metric": null,
    "full_content": "# Predicting Insurance Premiums with Tabular Data\n\n**Problem Description:**\n*   **Problem Type:** Regression\n*   **Objective:** Predict continuous insurance premium amounts based on various factors. The goal is to build a model that accurately estimates the premium cost for individuals or policies using the provided features.\n*   **Key Points:**\n    *   The dataset is synthetically generated but based on real-world insurance premium prediction data.\n    *   Participants are encouraged to explore differences between the synthetic and original datasets and potentially incorporate the original data for improved performance.\n\n**Dataset Overview:**\n*   **Data Type & Context:** Tabular data containing anonymized insurance-related features (e.g., demographics, policy details, risk factors) with a continuous target variable (premium amount).\n*   **Data Files:**\n    *   `train.csv` - Training data with target variable (`Premium Amount`)\n    *   `test.csv` - Test data for which predictions must be made\n    *   `sample_submission.csv` - Example submission file format\n*   **Features:** While specific features aren't listed, they likely include insurance-relevant variables similar to the original dataset (e.g., age, coverage type, claims history, vehicle details for auto insurance).\n\n**Evaluation Metrics:**\n*   **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    *   **Components:**\n        *   Calculates the root mean squared error between the log-transformed predicted values and log-transformed actual values\n        *   Formula: `RMSLE = sqrt(mean((log(pred + 1) - log(actual + 1))^2)`\n        *   Less sensitive to large outliers than RMSE since it uses log scaling\n        *   Penalizes underestimates more than overestimates",
    "sections": {
      "Problem Description": "*   **Problem Type:** Regression\n*   **Objective:** Predict continuous insurance premium amounts based on various factors. The goal is to build a model that accurately estimates the premium cost for individuals or policies using the provided features.\n*   **Key Points:**\n    *   The dataset is synthetically generated but based on real-world insurance premium prediction data.\n    *   Participants are encouraged to explore differences between the synthetic and original datasets and potentially incorporate the original data for improved performance.",
      "Dataset Overview": "*   **Data Type & Context:** Tabular data containing anonymized insurance-related features (e.g., demographics, policy details, risk factors) with a continuous target variable (premium amount).\n*   **Data Files:**\n    *   `train.csv` - Training data with target variable (`Premium Amount`)\n    *   `test.csv` - Test data for which predictions must be made\n    *   `sample_submission.csv` - Example submission file format\n*   **Features:** While specific features aren't listed, they likely include insurance-relevant variables similar to the original dataset (e.g., age, coverage type, claims history, vehicle details for auto insurance).",
      "Evaluation Metrics": "*   **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    *   **Components:**\n        *   Calculates the root mean squared error between the log-transformed predicted values and log-transformed actual values\n        *   Formula: `RMSLE = sqrt(mean((log(pred + 1) - log(actual + 1))^2)`\n        *   Less sensitive to large outliers than RMSE since it uses log scaling\n        *   Penalizes underestimates more than overestimates"
    },
    "file_path": "kaggle_datasets/634/problem_summary.md"
  },
  "406": {
    "problem_id": "406",
    "title": "Reverse Prediction in Conway's Game of Life",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Reverse Prediction in Conway's Game of Life\n\n## Problem Description\n* **Problem Type:** Binary Classification (with a combinatorial optimization aspect)\n* **Objective:** Predict the starting configuration of a 25x25 grid in Conway's Game of Life that, when evolved forward by a given number of steps (`delta`), matches the provided ending configuration.\n    * The task involves reversing the deterministic but chaotic evolution of cellular automata.\n    * **Key Points:**\n        * Grid wraps around (toroidal topology)\n        * Multiple valid starting configurations may exist for a given ending state (many-to-one mapping)\n        * Competition accepts *any* valid starting configuration as correct\n        * Focuses on short-term reversibility (delta ≤ 5 steps)\n\n## Dataset Overview\n* **Data Type:** Tabular data representing 25x25 binary grids (wrapped as 625 columns)\n* **Context:** Simulated Game of Life evolutions with warm-up periods\n* **Data Files:**\n    * `train.csv`: 50,000 games with known starting/ending configurations\n    * `test.csv`: 50,000 games where only ending configurations are provided\n    * `sample_submission.csv`: Submission format template\n* **Features:**\n    * `id`: Unique game identifier\n    * `delta`: Number of evolution steps between start and stop (1-5)\n    * `start_0` to `start_624`: Binary values (0=dead, 1=alive) of initial 25x25 grid (row-major order)\n    * `stop_0` to `stop_624`: Binary values of final grid after `delta` steps\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE) between predicted starting configuration (evolved forward) and true ending configuration\n    * Equivalent to 1 - classification accuracy across all cells\n    * **Implementation Details:**\n        * Submissions must predict binary values (0 or 1) for all 625 cells\n        * Predictions are evaluated by:\n            1. Applying the Game of Life rules forward `delta` steps\n            2. Comparing the result to the true ending configuration\n            3. Calculating MAE across all cells/games",
    "sections": {},
    "file_path": "kaggle_datasets/406/problem_summary.md"
  },
  "250": {
    "problem_id": "250",
    "title": "Music Recommendation with User-Song Interaction Prediction",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that a user will listen to a song repetitively within one month after their first observable listening event. The goal is to build an improved music recommendation system that can handle cold-start problems (new users/songs) and personalize recommendations effectively.",
    "evaluation_metric": null,
    "full_content": "# Music Recommendation with User-Song Interaction Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a user will listen to a song repetitively within one month after their first observable listening event. The goal is to build an improved music recommendation system that can handle cold-start problems (new users/songs) and personalize recommendations effectively.\n    * **Key Points:**\n        * Focuses on predicting recurring listening behavior (binary target: 1 = repeated listen within month, 0 = no repeat).\n        * Uses implicit feedback (listening events) rather than explicit ratings.\n        * Dataset is time-split to simulate real-world recommendation scenarios.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing user-song interactions, song metadata, and user demographics from KKBOX's music streaming platform.\n* **Data Files:**\n    * `train.csv`: User-song interaction records with target labels.\n    * `test.csv`: User-song interaction records for prediction (no target).\n    * `songs.csv`: Song metadata (length, genre, artist, language).\n    * `members.csv`: User demographics (age, gender, registration details).\n    * `song_extra_info.csv`: Additional song details (name, ISRC code).\n    * `sample_submission.csv`: Submission format example.\n* **Key Features:**\n    * Interaction features: `source_system_tab`, `source_screen_name`, `source_type` (context of first listen).\n    * User features: `city`, `bd` (age), `gender`, `registered_via`.\n    * Song features: `song_length`, `genre_ids`, `artist_name`, `language`.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    * **Interpretation:** Measures the model's ability to distinguish between users who will/won't replay songs. Higher AUC indicates better ranking of positive instances (repeat listens) above negative ones.\n    * **Submission Format:** Requires predicted probabilities (range 0-1) for each test-set user-song pair.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a user will listen to a song repetitively within one month after their first observable listening event. The goal is to build an improved music recommendation system that can handle cold-start problems (new users/songs) and personalize recommendations effectively.\n    * **Key Points:**\n        * Focuses on predicting recurring listening behavior (binary target: 1 = repeated listen within month, 0 = no repeat).\n        * Uses implicit feedback (listening events) rather than explicit ratings.\n        * Dataset is time-split to simulate real-world recommendation scenarios.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing user-song interactions, song metadata, and user demographics from KKBOX's music streaming platform.\n* **Data Files:**\n    * `train.csv`: User-song interaction records with target labels.\n    * `test.csv`: User-song interaction records for prediction (no target).\n    * `songs.csv`: Song metadata (length, genre, artist, language).\n    * `members.csv`: User demographics (age, gender, registration details).\n    * `song_extra_info.csv`: Additional song details (name, ISRC code).\n    * `sample_submission.csv`: Submission format example.\n* **Key Features:**\n    * Interaction features: `source_system_tab`, `source_screen_name`, `source_type` (context of first listen).\n    * User features: `city`, `bd` (age), `gender`, `registered_via`.\n    * Song features: `song_length`, `genre_ids`, `artist_name`, `language`.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    * **Interpretation:** Measures the model's ability to distinguish between users who will/won't replay songs. Higher AUC indicates better ranking of positive instances (repeat listens) above negative ones.\n    * **Submission Format:** Requires predicted probabilities (range 0-1) for each test-set user-song pair."
    },
    "file_path": "kaggle_datasets/250/problem_summary.md"
  },
  "439": {
    "problem_id": "439",
    "title": "Fine-Grained Plant Species Classification from Herbarium Specimens",
    "problem_type": "Multi-class Image Classification (Fine-Grained Visual Categorization)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Plant Species Classification from Herbarium Specimens\n\n**Problem Description:**\n* **Problem Type:** Multi-class Image Classification (Fine-Grained Visual Categorization)\n* **Objective:**  \n  * Identify vascular plant species from herbarium specimen images across ~65,000 classes  \n  * Develop models to accelerate species discovery by classifying unnamed specimens in herbaria\n* **Key Points:**  \n  * Long-tail distribution: Species have 3-100+ training examples  \n  * Focus on vascular plants (lycophytes, ferns, gymnosperms, flowering plants)  \n  * Real-world application: Top models will be used on unidentified NYBG specimens  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  * 2.5M JPEG images of pressed plant specimens with blurred text labels  \n  * Metadata includes species, family, order, and collection region  \n  * Maximum image dimension: 1000px (downsampled from originals)  \n* **Data Files:**  \n  * `train/images/` (organized by category_id subfolders)  \n  * `test/images/` (organized by image_id groupings)  \n  * `train/metadata.json` (COCO-format with species annotations)  \n  * `test/metadata.json` (image info only)  \n* **Key Features:**  \n  * Visual morphology of dried specimens  \n  * Taxonomic hierarchy (species → family → order)  \n  * Geographic collection regions  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Macro F1-Score  \n  * Calculated per species then averaged  \n  * Formula:  \n    * Precision = TP / (TP + FP)  \n    * Recall = TP / (TP + FN)  \n    * F1 = 2 × (precision × recall) / (precision + recall)  \n* **Implementation:**  \n  * sklearn's `f1_score` with `average='macro'`  \n  * Test set has max 10 examples per species (capped)",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Image Classification (Fine-Grained Visual Categorization)\n* **Objective:**  \n  * Identify vascular plant species from herbarium specimen images across ~65,000 classes  \n  * Develop models to accelerate species discovery by classifying unnamed specimens in herbaria\n* **Key Points:**  \n  * Long-tail distribution: Species have 3-100+ training examples  \n  * Focus on vascular plants (lycophytes, ferns, gymnosperms, flowering plants)  \n  * Real-world application: Top models will be used on unidentified NYBG specimens",
      "Dataset Overview": "* **Data Type & Context:**  \n  * 2.5M JPEG images of pressed plant specimens with blurred text labels  \n  * Metadata includes species, family, order, and collection region  \n  * Maximum image dimension: 1000px (downsampled from originals)  \n* **Data Files:**  \n  * `train/images/` (organized by category_id subfolders)  \n  * `test/images/` (organized by image_id groupings)  \n  * `train/metadata.json` (COCO-format with species annotations)  \n  * `test/metadata.json` (image info only)  \n* **Key Features:**  \n  * Visual morphology of dried specimens  \n  * Taxonomic hierarchy (species → family → order)  \n  * Geographic collection regions",
      "Evaluation Metrics": "* **Primary Metric:** Macro F1-Score  \n  * Calculated per species then averaged  \n  * Formula:  \n    * Precision = TP / (TP + FP)  \n    * Recall = TP / (TP + FN)  \n    * F1 = 2 × (precision × recall) / (precision + recall)  \n* **Implementation:**  \n  * sklearn's `f1_score` with `average='macro'`  \n  * Test set has max 10 examples per species (capped)"
    },
    "file_path": "kaggle_datasets/439/problem_summary.md"
  },
  "292": {
    "problem_id": "292",
    "title": "Inclusive Image Classification with Geographic Distribution Robustness",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Inclusive Image Classification with Geographic Distribution Robustness\n\n## Problem Description\n- **Problem Type:** Multi-label Image Classification with Geographic Distribution Robustness\n- **Objective:** Develop image classification models that perform well when tested on images from geographic distributions different from the training data. The goal is to create models robust to \"blind spots\" in training data coverage.\n- **Key Points:**\n  - Focus on handling distributional skew between training and test sets\n  - Test sets contain images from underrepresented geographic regions (primarily Asia, Africa, South America)\n  - Models must generalize beyond North America/Western Europe-centric training data\n  - Two-stage competition design with different geographic distributions in each stage\n  - Faces in images are blurred for privacy reasons\n  - Specifically gendered tags (e.g., \"man\", \"woman\") are excluded from labels\n\n## Dataset Overview\n- **Data Type & Context:** \n  - Training: 1,743,042 images from Open Images dataset (subset with bounding boxes)\n  - Test: Images collected via Google's Crowdsource app from underrepresented regions\n  - Optional: Wikipedia text data for potential multi-modal approaches\n- **Data Files:**\n  - Training: train_human_labels.csv, train_machine_labels.csv, train_bounding_boxes.csv\n  - Test: stage_1_test_images.zip, stage_2_test_images.zip\n  - Metadata: class-descriptions.csv, classes-trainable.csv\n  - Tuning: tuning_labels.csv (1000 labeled test images)\n- **Key Features:**\n  - Images rescaled to max 1024px dimension\n  - All faces blurred\n  - Labels aligned with Open Images but with gendered tags removed\n  - Multiple possible labels per image\n\n## Evaluation Metrics\n- **Primary Metric:** Mean F2 Score (example-based F-score with β=2)\n- **Metric Characteristics:**\n  - Weights recall more heavily than precision (β=2)\n  - Designed to balance performance across precision and recall\n  - Accounts for multiple possible correct labels per image\n  - Submission format requires space-delimited predicted labels for each image",
    "sections": {},
    "file_path": "kaggle_datasets/292/problem_summary.md"
  },
  "266": {
    "problem_id": "266",
    "title": "Predicting DonorsChoose.org Project Proposal Approval",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting DonorsChoose.org Project Proposal Approval\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict whether a teacher's project proposal submitted to DonorsChoose.org will be approved, using project description text and metadata about the project, teacher, and school.  \n* **Key Points:**  \n  * Goal is to help DonorsChoose.org scale manual screening processes for ~500k annual proposals.  \n  * Solution should:  \n    * Improve vetting consistency across volunteers  \n    * Prioritize applications needing most human review  \n    * Enable auto-approval of low-risk proposals  \n\n## Dataset Overview\n* **Data Type & Context:** Tabular and text data from US public school teachers' funding applications, including:  \n  * **Primary Files:**  \n    * `train.csv`/`test.csv`: Project metadata + teacher/school attributes  \n    * `resources.csv`: Itemized resource requests per project  \n    * `sample_submission.csv`: Submission template  \n  * **Key Features:**  \n    * **Text Data:** 4 project essays (reduced to 2 post-2016 with prompt changes)  \n    * **Metadata:** Teacher prefix, school state, grade level, subject categories  \n    * **Temporal:** Submission timestamp, teacher's prior project count  \n    * **Resources:** Item descriptions, quantities, and prices  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)  \n  * Measures model's ability to rank approved projects higher than rejected ones  \n  * Threshold-independent evaluation of probabilistic predictions",
    "sections": {},
    "file_path": "kaggle_datasets/266/problem_summary.md"
  },
  "602": {
    "problem_id": "602",
    "title": "AI Assistants for Data Science Tasks with Gemma",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# AI Assistants for Data Science Tasks with Gemma\n\n## Problem Description\n* **Problem Type:** Natural Language Processing (NLP) - Text Generation / LLM Adaptation\n* **Objective:**  \n    Participants are challenged to create a notebook demonstrating how to use Google's Gemma LLM to accomplish one or more of five specified data science-oriented tasks:\n    * Explaining/teaching basic data science concepts\n    * Answering Python programming questions\n    * Summarizing Kaggle solution write-ups\n    * Teaching concepts from Kaggle competition solutions\n    * Answering Kaggle platform questions\n* **Key Points:**\n    * Focuses on adapting foundation models for specialized use cases\n    * Requires building practical tools for data science assistance\n    * Contributes to collective knowledge of LLM optimization techniques\n\n## Dataset Overview\n* **Data Type:** No specific dataset required\n* **Context:** Participants work with Google's Gemma LLM model\n* **Data Files Provided:**\n    * submission_categories.txt (reference file)\n    * submission_instructions.txt (reference file)\n* **Features:**\n    * Competition focuses on model usage rather than data processing\n    * Participants may need to source their own examples/texts for demonstration purposes\n\n## Evaluation Metrics\n* **Evaluation Method:** Rubric-based scoring of notebook submissions\n* **Evaluation Criteria:**\n    * **Eligibility (Binary):**\n        * Compliance with guidelines\n        * Topical relevance\n        * Openness (public notebook/data)\n    * **Graded Components (0-10pts each):**\n        * Technical: Use of advanced strategies (few-shot prompting, RAG, fine-tuning)\n        * Descriptive: Quality of documentation and explanations\n        * Useful: Practical helpfulness of outputs\n        * Robust: Performance with additional inputs",
    "sections": {},
    "file_path": "kaggle_datasets/602/problem_summary.md"
  },
  "430": {
    "problem_id": "430",
    "title": "Predicting NCAA Basketball Tournament Outcomes",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Basketball Tournament Outcomes\n\n## Problem Description\n- **Problem Type**: Binary Classification (with probability prediction)\n- **Objective**: Predict the outcome (win/loss probability) of every possible matchup in the NCAA Men's Basketball Tournament (March Madness®). The competition has two stages:\n  * **Stage 1**: Build and test models using historical tournament data (2015-2019 seasons)\n  * **Stage 2**: Predict outcomes for the 2021 tournament before it begins\n- **Key Points**:\n  * Participants are encouraged to use external data sources beyond the provided dataset\n  * Predictions must be made for all possible matchups (n*(n-1)/2 pairs for n teams)\n  * The competition focuses on probabilistic predictions (confidence matters)\n\n## Dataset Overview\n- **Data Type**: Tabular data with extensive historical records of NCAA basketball games, teams, and rankings\n- **Context**: College basketball games (regular season and tournaments) from 1985-2021 season\n- **Key Data Files**:\n  * `Teams.csv` - Team IDs and metadata (357 Division-I teams)\n  * `MSeasons.csv` - Season timelines and tournament structure\n  * `MRegularSeasonCompactResults.csv` - Game results for regular seasons (1985-2021)\n  * `MNCAATourneyCompactResults.csv` - NCAA tournament game results\n  * `MNCAATourneySeeds.csv` - Tournament seeds for all teams\n  * `MMasseyOrdinals.csv` - Weekly team rankings from various systems (2003-2021)\n  * Detailed box score files (2003-2021) and supplementary data files\n- **Important Features**:\n  * Game outcomes with team IDs, scores, locations, and dates\n  * Team performance statistics (FG%, rebounds, assists, etc. in detailed files)\n  * Tournament seeds and bracket structure\n  * Multiple ranking systems for teams\n\n## Evaluation Metrics\n- **Primary Metric**: Logarithmic Loss (LogLoss)\n  * Formula: `LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n  * Where:\n    - `n` = number of games predicted\n    - `ŷ_i` = predicted probability of team 1 winning\n    - `y_i` = 1 if team 1 wins,",
    "sections": {},
    "file_path": "kaggle_datasets/430/problem_summary.md"
  },
  "259": {
    "problem_id": "259",
    "title": "Mercari Price Suggestion Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Mercari Price Suggestion Challenge\n\n## Problem Description\n* **Problem Type**: Regression (Price Prediction)\n* **Objective**: Predict the sale price of products listed on Mercari's marketplace based on user-provided information. The goal is to automate price suggestions for sellers, accounting for factors like product category, brand, condition, and textual descriptions.\n    * **Key Points**:\n        * Pricing is influenced by nuanced details (e.g., brand names, seasonal trends, product specs).\n        * Data includes user-generated text (titles and descriptions), with price-related text removed to prevent leakage (marked as `[rm]`).\n        * Competition is \"Kernels Only,\" requiring submissions via Kaggle Kernels with a two-stage evaluation (public/private test data).\n\n## Dataset Overview\n* **Data Type**: Tabular data with text features (product listings).\n* **Context**: Data from Mercari's marketplace, including product metadata and seller-provided descriptions.\n* **Data Files**:\n    * `train.tsv`, `test.tsv`: Tab-delimited files with product listings.\n    * `sample_submission.csv`: Submission template with `test_id` and predicted `price`.\n* **Features**:\n    * `name`: Product title (cleaned of price-like text).\n    * `item_condition_id`: Seller-reported condition.\n    * `category_name`, `brand_name`: Product categorization.\n    * `shipping`: Binary flag indicating who pays shipping.\n    * `item_description`: Full product description (cleaned of price-like text).\n    * `price`: Target variable (USD; only in training data).\n\n## Evaluation Metrics\n* **Evaluation Metric**: Root Mean Squared Logarithmic Error (RMSLE).\n    * **Components**:\n        * Calculated as:  \n          $$\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\log(p_i + 1) - \\log(a_i + 1))^2}$$  \n          where:\n            * \\(p_i\\) = predicted price.\n            * \\(a_i\\) = actual price.\n            * \\(\\log\\) = natural logarithm.\n        * Penalizes relative errors more than absolute errors, suitable for price prediction across varying scales.",
    "sections": {},
    "file_path": "kaggle_datasets/259/problem_summary.md"
  },
  "437": {
    "problem_id": "437",
    "title": "Hotel Recognition for Human Trafficking Investigations via Fine-Grained Visual Categorization",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Hotel Recognition for Human Trafficking Investigations via Fine-Grained Visual Categorization\n\n## Problem Description\n* **Problem Type:** Computer Vision - Fine-Grained Image Classification\n* **Objective:**  \n    * Identify specific hotels from images of hotel room interiors to aid human trafficking investigations.  \n    * The task involves matching test images (potentially low-quality or from uncommon angles) to a large gallery of training images with known hotel IDs.  \n* **Key Points:**  \n    * Focus on combating human trafficking by providing tools for investigators.  \n    * High intraclass and low interclass variation due to similar decor in hotel chains.  \n    * Large number of classes (~7700 hotels) with imbalanced distribution across chains.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Image dataset of hotel room interiors (no people present) collected via the TraffickCam mobile app.  \n    * Includes metadata about hotel chains and timestamps for training images.  \n* **Data Files:**  \n    * `train.csv`: Metadata (image ID, chain ID, hotel ID, timestamp).  \n    * `train_images/`: ~97,000 training images (organized by chain subfolders).  \n    * `test_images/`: Hidden test set (13,000 images; only 3 samples provided).  \n    * `sample_submission.csv`: Submission format example.  \n* **Features:**  \n    * Images vary in quality and perspective (e.g., investigative vs. app-submitted).  \n    * Chains range from independent hotels (chain ID = 0) to large chains with shared decor.  \n\n## Evaluation Metrics\n* **Primary Metric:** Mean Average Precision @ 5 (MAP@5).  \n* **Components:**  \n    * For each test image, predict a ranked list of up to 5 hotel IDs.  \n    * Precision is calculated at each cutoff *k* (1 to 5), with relevance (`rel(k)`) = 1 if the prediction at rank *k* is correct.  \n    * Once a correct label is found, subsequent predictions of the same label are ignored.  \n    * Final score averages precision across all test images.  \n* **Example:**  \n    * Predictions `[A, B, C]` for ground truth `A` yield MAP@5 = 1.0 (since the first prediction is correct).",
    "sections": {},
    "file_path": "kaggle_datasets/437/problem_summary.md"
  },
  "605": {
    "problem_id": "605",
    "title": "Predicting Prosumer Energy Behavior to Minimize Imbalance Costs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Prosumer Energy Behavior to Minimize Imbalance Costs\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting (Regression)\n* **Objective**: Predict the electricity production and consumption of prosumers (consumers who also generate energy) in Estonia to reduce energy imbalance costs. The goal is to accurately forecast hourly energy behavior to minimize discrepancies between expected and actual energy usage/production.\n    * **Key Points**:\n        * Focus on prosumers with solar panels, whose behavior is highly variable and impacts grid stability.\n        * Address financial and logistical challenges caused by energy imbalance for energy companies.\n        * Improve renewable energy integration by enabling better management of prosumer energy flows.\n\n## Dataset Overview\n* **Data Type & Context**: Tabular time-series data with energy, weather, and pricing information.\n    * **Data Files**:\n        * `train.csv`: Hourly energy consumption/production records with metadata (county, business flag, product type).\n        * `gas_prices.csv` & `electricity_prices.csv`: Day-ahead market prices for gas and electricity.\n        * `client.csv`: Prosumer metadata (installed solar capacity, location, etc.).\n        * `forecast_weather.csv` & `historical_weather.csv`: Weather forecasts and historical measurements.\n    * **Key Features**:\n        * Temporal features (`datetime`, `data_block_id`).\n        * Energy targets (`target`, `is_consumption`).\n        * Weather variables (temperature, solar radiation, wind components).\n        * Pricing data (`euros_per_mwh`, `lowest/highest_price_per_mwh`).\n        * Spatial identifiers (`county`, `latitude/longitude`).\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE).\n    * **Calculation**:  \n      \\( \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - x_i| \\)  \n      Where:\n        * \\( n \\): Total number of predictions.\n        * \\( y_i \\): Predicted value.\n        * \\( x_i \\): Observed (true) value.\n* **Nuance**: Submissions are evaluated against future ground truth data post-deadline, simulating real-world forecasting.",
    "sections": {},
    "file_path": "kaggle_datasets/605/problem_summary.md"
  },
  "261": {
    "problem_id": "261",
    "title": "Plant Seedlings Classification from Images",
    "problem_type": "Multi-class Image Classification",
    "objective": "Develop a classifier to determine the species of plant seedlings from images, distinguishing between 12 different plant species (including crops and weeds).",
    "evaluation_metric": null,
    "full_content": "# Plant Seedlings Classification from Images\n\n**Problem Description:**\n* **Problem Type:** Multi-class Image Classification\n* **Objective:** Develop a classifier to determine the species of plant seedlings from images, distinguishing between 12 different plant species (including crops and weeds).\n* **Key Points:**\n  * Focus on agricultural application: differentiating weeds from crop seedlings can improve crop yields and environmental stewardship.\n  * Dataset contains plants at various growth stages, adding complexity to the classification task.\n  * Competition aims to explore different image recognition techniques for botanical applications.\n\n**Dataset Overview:**\n* **Data Type & Context:** RGB images of plant seedlings (PNG format) organized by species, with approximately 960 unique plants across 12 species.\n* **Data Files:**\n  * `train.csv` - Training set with images organized by species folders\n  * `test.csv` - Test set images for prediction\n  * `sample_submission.csv` - Example submission file format\n* **Features:**\n  * Images contain seedlings at various growth stages\n  * 12 plant species including crops (Maize, Common wheat, Sugar beet) and weeds (Black-grass, Charlock, etc.)\n  * Each image has a unique filename as identifier\n\n**Evaluation Metrics:**\n* **Primary Metric:** Micro-averaged F1-score (MeanFScore)\n* **Metric Components:**\n  * Calculated by first computing micro-averaged precision and recall across all classes:\n    * Precisionₘᵢ𝒸𝓇ₒ = ∑TPₖ / (∑TPₖ + ∑FPₖ)\n    * Recallₘᵢ𝒸𝓇ₒ = ∑TPₖ / (∑TPₖ + ∑FNₖ)\n  * Then computing harmonic mean:\n    * MeanFScore = 2 × (Precisionₘᵢ𝒸𝓇ₒ × Recallₘᵢ𝒸𝓇ₒ) / (Precisionₘᵢ𝒸𝓇ₒ + Recallₘᵢ𝒸𝓇ₒ)",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Image Classification\n* **Objective:** Develop a classifier to determine the species of plant seedlings from images, distinguishing between 12 different plant species (including crops and weeds).\n* **Key Points:**\n  * Focus on agricultural application: differentiating weeds from crop seedlings can improve crop yields and environmental stewardship.\n  * Dataset contains plants at various growth stages, adding complexity to the classification task.\n  * Competition aims to explore different image recognition techniques for botanical applications.",
      "Dataset Overview": "* **Data Type & Context:** RGB images of plant seedlings (PNG format) organized by species, with approximately 960 unique plants across 12 species.\n* **Data Files:**\n  * `train.csv` - Training set with images organized by species folders\n  * `test.csv` - Test set images for prediction\n  * `sample_submission.csv` - Example submission file format\n* **Features:**\n  * Images contain seedlings at various growth stages\n  * 12 plant species including crops (Maize, Common wheat, Sugar beet) and weeds (Black-grass, Charlock, etc.)\n  * Each image has a unique filename as identifier",
      "Evaluation Metrics": "* **Primary Metric:** Micro-averaged F1-score (MeanFScore)\n* **Metric Components:**\n  * Calculated by first computing micro-averaged precision and recall across all classes:\n    * Precisionₘᵢ𝒸𝓇ₒ = ∑TPₖ / (∑TPₖ + ∑FPₖ)\n    * Recallₘᵢ𝒸𝓇ₒ = ∑TPₖ / (∑TPₖ + ∑FNₖ)\n  * Then computing harmonic mean:\n    * MeanFScore = 2 × (Precisionₘᵢ𝒸𝓇ₒ × Recallₘᵢ𝒸𝓇ₒ) / (Precisionₘᵢ𝒸𝓇ₒ + Recallₘᵢ𝒸𝓇ₒ)"
    },
    "file_path": "kaggle_datasets/261/problem_summary.md"
  },
  "295": {
    "problem_id": "295",
    "title": "Multi-class Doodle Recognition from Noisy Drawing Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Doodle Recognition from Noisy Drawing Data\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Image Recognition)\n* **Objective:** Build a classifier that can accurately identify doodles from the Quick, Draw! dataset, despite noisy/incomplete training data. The model must:\n  * Recognize drawings across 340 categories\n  * Handle mismatches between drawings and labels in training data\n  * Generalize to a manually-labeled test set with different distribution\n* **Key Points:**\n  * Training data contains user-generated drawings with potential label noise\n  * Test set is manually curated (cleaner distribution)\n  * Goal is to advance pattern recognition for applications in OCR, ASR, and NLP\n\n## Dataset Overview\n* **Data Type:** Vector-based drawing data (timestamped stroke coordinates)\n* **Context:** Crowdsourced doodles from the \"Quick, Draw!\" game (50M drawings)\n* **Data Files:**\n  * `train_raw.zip` / `train_simplified.zip` (per-category CSV files)\n  * `test_raw.csv` / `test_simplified.csv`\n  * `sample_submission.csv`\n* **Features:**\n  * Vector representations of drawings (either raw or simplified)\n  * Key metadata: drawing strokes, timestamps, country code\n  * Multi-word labels represented with underscores in submission format\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Average Precision @ 3 (MAP@3)\n* **Metric Components:**\n  * For each test drawing (U total):\n    * Calculate precision at cutoff positions 1-3\n    * Average these precision values\n  * Final score is the mean across all test drawings\n  * Predictions must replace spaces with underscores in multi-word labels",
    "sections": {},
    "file_path": "kaggle_datasets/295/problem_summary.md"
  },
  "408": {
    "problem_id": "408",
    "title": "Environmental and Social KPIs for Climate Solutions",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Environmental and Social KPIs for Climate Solutions\n\n## Problem Description\n- **Problem Type**:  \n  - **Analytics & Insight Generation** (with elements of **Survey Analysis** and **Environmental Data Mining**)\n- **Objective**:  \n  - Develop **key performance indicators (KPIs)** to measure the intersection of environmental and social issues using CDP survey data.  \n  - Mine insights to evaluate whether city/corporate climate actions account for social equity and environmental risks.  \n- **Key Points**:  \n  - Focus on **actionable solutions** for climate adaptation, recession recovery, and social equity.  \n  - Leverage **external datasets** (must be public and hosted on Kaggle) to enrich analysis.  \n  - Automated insight generation to assess alignment between city/corporate ambitions and environmental-social goals.  \n\n## Dataset Overview\n- **Data Type & Context**:  \n  - **Structured survey responses** from corporations and cities (2018–2020) covering:  \n    - Climate change disclosures  \n    - Water security disclosures  \n    - City-level environmental actions  \n- **Key Data Files**:  \n  - Annual full datasets for Climate Change (`2018-2020_Full_Climate_Change_Dataset.csv`), Water Security (`2018-2020_Full_Water_Security_Dataset.csv`), and Cities (`2018-2020_Full_Cities_Dataset.csv`).  \n  - Supplementary datasets (e.g., city boundaries, corporate sustainability metrics).  \n- **Features**:  \n  - Survey responses include **anonymized metrics** on emissions, water usage, deforestation risks, and social equity initiatives.  \n  - Mixed data types (numerical, categorical, text-based responses).  \n\n## Evaluation Metrics\n- **Evaluation Criteria**:  \n  Submissions are judged holistically based on:  \n  - **Accuracy/Completeness**:  \n    - Relevance and robustness of proposed KPIs.  \n    - Demonstrated alignment between KPIs and environmental-social intersections.  \n  - **Communication**:  \n    - Clarity of narrative and visualizations.  \n    - Depth of discussion on environmental-social linkages.  \n  - **Documentation**:  \n    - Reproducibility of code and proper citation of external data.  \n- **Custom Aspects**:  \n  - No single quantitative metric; emphasis on **narrative quality** and **actionability of insights**.  \n  - Judges assess whether KPIs address the competition’s",
    "sections": {},
    "file_path": "kaggle_datasets/408/problem_summary.md"
  },
  "463": {
    "problem_id": "463",
    "title": "Hindi and Tamil Question Answering",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Hindi and Tamil Question Answering\n\n## Problem Description\n* **Problem Type:** NLP - Question Answering (Multilingual)\n* **Objective:** Predict answers to questions in Hindi and Tamil by extracting relevant text spans from given Wikipedia passages. The task involves:\n    * Identifying precise answers within provided contexts\n    * Handling two underrepresented Indian languages (Hindi and Tamil)\n    * Working with native-speaker annotated data collected without translation\n* **Key Points:**\n    * Focus on improving NLU performance for Indian languages\n    * Dataset designed to reflect real-world information-seeking tasks\n    * Answers must be exact quoted substrings from the context\n    * Training data contains some annotation noise to reflect real-world conditions\n\n## Dataset Overview\n* **Data Type:** Text data (question-context-answer triples)\n* **Context:** Wikipedia articles in Hindi and Tamil\n* **Data Files:**\n    * `train.csv` (context, questions, answers with character spans)\n    * `test.csv` (context and questions for prediction)\n    * `sample_submission.csv` (example submission format)\n* **Key Features:**\n    * `context`: Wikipedia passage text (Hindi/Tamil)\n    * `question`: Native-language question\n    * `answer_text`: Manually annotated answer substring\n    * `answer_start`: Character position of answer in context\n    * `language`: Hindi or Tamil identifier\n\n## Evaluation Metrics\n* **Primary Metric:** Word-level Jaccard Score\n* **Calculation:**\n    * Compares predicted answer string against ground truth\n    * Formula: `score = (1/n) * Σ Jaccard(ground_truth_i, prediction_i)`\n    * Where Jaccard is calculated as:\n        ```python\n        def jaccard(str1, str2):\n            a = set(str1.lower().split()) \n            b = set(str2.lower().split())\n            c = a.intersection(b)\n            return float(len(c)) / (len(a) + len(b) - len(c))\n        ```\n* **Key Requirements:**\n    * Predictions must be exact quoted substrings from the context\n    * Includes punctuation and complete phrases\n    * Case-insensitive comparison via lowercasing",
    "sections": {},
    "file_path": "kaggle_datasets/463/problem_summary.md"
  },
  "235": {
    "problem_id": "235",
    "title": "Binary Classification of Invasive Hydrangea in Forest Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Invasive Hydrangea in Forest Images\n\n## Problem Description\n- **Problem Type:** Binary Classification (Computer Vision - Image Classification)\n- **Objective:**  \n  - Develop a model to accurately classify whether images of forests and foliage contain invasive hydrangea species.  \n  - The goal is to improve upon traditional, costly manual monitoring methods by automating invasive species detection using computer vision techniques.\n- **Key Points:**  \n  - Focus on ecological impact: Invasive species monitoring is critical for environmental preservation but is traditionally resource-intensive.  \n  - Scalability: The solution aims to cover large areas more efficiently than human experts.  \n  - Real-world application: Techniques like aerial imaging and CV can make monitoring cheaper and faster.\n\n## Dataset Overview\n- **Data Type & Context:**  \n  - Image data (RGB photographs) taken in a Brazilian national forest, containing both invasive hydrangea and non-invasive foliage.  \n- **Data Files:**  \n  - `train.7z`: 2,295 training images.  \n  - `train_labels.csv`: Binary labels (0 or 1) for training images.  \n  - `test.7z`: 1,531 test images (no labels).  \n  - `sample_submission.csv`: Submission template with `name` (image ID) and `invasive` (predicted probability).  \n- **Features:**  \n  - Images vary in composition (forest scenes, foliage close-ups).  \n  - Labels: `invasive = 1` indicates hydrangea presence, `0` otherwise.  \n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC).  \n- **Components:**  \n  - Predictions must be probabilities (continuous values between 0 and 1).  \n  - AUC measures the model’s ability to distinguish between invasive and non-invasive images across all classification thresholds.  \n  - Higher AUC indicates better performance (1.0 = perfect separation, 0.5 = random guessing).",
    "sections": {},
    "file_path": "kaggle_datasets/235/problem_summary.md"
  },
  "497": {
    "problem_id": "497",
    "title": "Tabular Data Imputation for Manufacturing Control Data",
    "problem_type": "Data Imputation (Regression task for missing continuous values)",
    "objective": "Predict missing values in a manufacturing control dataset where electronic errors caused data gaps. Participants must impute all missing continuous values in the dataset.",
    "evaluation_metric": null,
    "full_content": "# Tabular Data Imputation for Manufacturing Control Data\n\n**Problem Description:**\n* **Problem Type:** Data Imputation (Regression task for missing continuous values)\n* **Objective:** Predict missing values in a manufacturing control dataset where electronic errors caused data gaps. Participants must impute all missing continuous values in the dataset.\n    * **Key Points:**\n        * Only continuous features contain missing values (categorical features are complete)\n        * No explicit target variable exists - the task is purely imputation\n        * Similar structure to May 2022 Tabular Playground series (but focused on imputation rather than prediction)\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing simulated manufacturing control metrics with:\n    * Normalized continuous features\n    * Categorical features\n* **Data Files:**\n    * `data.csv` - Primary dataset with missing values\n    * `sample_submission.csv` - Submission template with row-column indices for missing values\n* **Features:**\n    * Mixed feature types (continuous + categorical)\n    * 83 total columns\n    * Continuous features are normalized\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n        * Where y_i = true value, ŷ_i = predicted value\n        * Evaluated across all imputed missing values\n* **Submission Format:**\n    * Requires predictions for each missing value identified by row-column pairs\n    * Only continuous feature imputations are evaluated",
    "sections": {
      "Problem Description": "* **Problem Type:** Data Imputation (Regression task for missing continuous values)\n* **Objective:** Predict missing values in a manufacturing control dataset where electronic errors caused data gaps. Participants must impute all missing continuous values in the dataset.\n    * **Key Points:**\n        * Only continuous features contain missing values (categorical features are complete)\n        * No explicit target variable exists - the task is purely imputation\n        * Similar structure to May 2022 Tabular Playground series (but focused on imputation rather than prediction)",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing simulated manufacturing control metrics with:\n    * Normalized continuous features\n    * Categorical features\n* **Data Files:**\n    * `data.csv` - Primary dataset with missing values\n    * `sample_submission.csv` - Submission template with row-column indices for missing values\n* **Features:**\n    * Mixed feature types (continuous + categorical)\n    * 83 total columns\n    * Continuous features are normalized",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n        * Where y_i = true value, ŷ_i = predicted value\n        * Evaluated across all imputed missing values\n* **Submission Format:**\n    * Requires predictions for each missing value identified by row-column pairs\n    * Only continuous feature imputations are evaluated"
    },
    "file_path": "kaggle_datasets/497/problem_summary.md"
  },
  "490": {
    "problem_id": "490",
    "title": "Hotel Recognition via Image Classification to Combat Human Trafficking",
    "problem_type": "Computer Vision - Fine-Grained Image Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Hotel Recognition via Image Classification to Combat Human Trafficking\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Fine-Grained Image Classification\n* **Objective:**  \n    * Identify the hotel from images of hotel rooms (often with occlusions/persons blocking parts of the view) to aid human trafficking investigations.  \n    * The task involves classifying test images into one of many possible hotel IDs using a training set of hotel room images.\n* **Key Points:**  \n    * Focus on fine-grained visual recognition with high intra-class and low inter-class variation.  \n    * Test images are partially masked to simulate real-world investigative conditions (e.g., victims obstructing parts of the image).  \n    * The solution could be integrated into an image search system used by the National Center for Missing and Exploited Children.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Image data (hotel room interiors) collected via the TraffickCam mobile application.  \n    * Includes both clean training images and masked versions to emulate real-world challenges.\n* **Data Files:**  \n    * `train_images/`: Images of hotel rooms (organized by hotel ID).  \n    * `train_masks/`: Occlusion masks applied to training images (simulating test conditions).  \n    * `test_images/`: Partially masked test images (hidden test set with only one sample provided initially).  \n    * `sample_submission.csv`: Example submission file with `image_id` and predicted `hotel_id` (space-delimited ranked list).\n* **Features:**  \n    * Images vary in quality, angle, and lighting.  \n    * High number of classes (hotel IDs), with potential for subtle differences between hotels.  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision @ 5 (MAP@5)  \n    * **Calculation Breakdown:**  \n        * For each image, precision is computed at each rank (up to 5 predictions).  \n        * A prediction is \"relevant\" if it matches the correct hotel ID.  \n        * Once a correct label is found, subsequent predictions of the same label are ignored.  \n        * Final score averages precision across all test images.  \n    * **Example:** If the correct label is `A`, predictions like `A B C D E` or `A A A A A` both score 1.0.  \n* **Submission Format:**  \n    * Space-del",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Fine-Grained Image Classification\n* **Objective:**  \n    * Identify the hotel from images of hotel rooms (often with occlusions/persons blocking parts of the view) to aid human trafficking investigations.  \n    * The task involves classifying test images into one of many possible hotel IDs using a training set of hotel room images.\n* **Key Points:**  \n    * Focus on fine-grained visual recognition with high intra-class and low inter-class variation.  \n    * Test images are partially masked to simulate real-world investigative conditions (e.g., victims obstructing parts of the image).  \n    * The solution could be integrated into an image search system used by the National Center for Missing and Exploited Children.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Image data (hotel room interiors) collected via the TraffickCam mobile application.  \n    * Includes both clean training images and masked versions to emulate real-world challenges.\n* **Data Files:**  \n    * `train_images/`: Images of hotel rooms (organized by hotel ID).  \n    * `train_masks/`: Occlusion masks applied to training images (simulating test conditions).  \n    * `test_images/`: Partially masked test images (hidden test set with only one sample provided initially).  \n    * `sample_submission.csv`: Example submission file with `image_id` and predicted `hotel_id` (space-delimited ranked list).\n* **Features:**  \n    * Images vary in quality, angle, and lighting.  \n    * High number of classes (hotel IDs), with potential for subtle differences between hotels.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision @ 5 (MAP@5)  \n    * **Calculation Breakdown:**  \n        * For each image, precision is computed at each rank (up to 5 predictions).  \n        * A prediction is \"relevant\" if it matches the correct hotel ID.  \n        * Once a correct label is found, subsequent predictions of the same label are ignored.  \n        * Final score averages precision across all test images.  \n    * **Example:** If the correct label is `A`, predictions like `A B C D E` or `A A A A A` both score 1.0.  \n* **Submission Format:**  \n    * Space-del"
    },
    "file_path": "kaggle_datasets/490/problem_summary.md"
  },
  "232": {
    "problem_id": "232",
    "title": "Predicting Mercedes-Benz Test Bench Time with Anonymized Features",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Mercedes-Benz Test Bench Time with Anonymized Features\n\n## Problem Description\n* **Problem Type:** Regression\n* **Objective:** Predict the time (in seconds) a Mercedes-Benz car configuration takes to pass testing on the test bench, based on anonymized features representing custom car configurations.\n    * **Key Points:**\n        * Focus on reducing testing time to improve manufacturing efficiency and lower carbon dioxide emissions.\n        * Address the \"curse of dimensionality\" due to many possible feature combinations in premium car configurations.\n        * Maintain safety/reliability standards while optimizing testing speed.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing anonymized Mercedes-Benz car features and their associated testing times.\n    * **Data Files:**\n        * `train.csv` - Training set with features and ground truth testing time ('y').\n        * `test.csv` - Test set (requires predicting 'y' for given IDs).\n        * `sample_submission.csv` - Example submission format.\n    * **Features:**\n        * Mix of categorical variables (represented by letters) and binary variables (0/1).\n        * Features anonymized but represent real custom car options (e.g., 4WD, air suspension).\n\n## Evaluation Metrics\n* **Primary Metric:** R² (Coefficient of Determination)\n    * **Components:**\n        * Measures proportion of variance in the dependent variable predictable from independent variables.\n        * Ranges from 0 to 1, where 1 indicates perfect prediction of target variance.\n        * Formula: R² = 1 - (SS_res / SS_tot), where SS_res is residual sum of squares and SS_tot is total sum of squares.",
    "sections": {},
    "file_path": "kaggle_datasets/232/problem_summary.md"
  },
  "464": {
    "problem_id": "464",
    "title": "Analyzing the 2021 Kaggle Machine Learning & Data Science Survey",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Analyzing the 2021 Kaggle Machine Learning & Data Science Survey\n\n## Problem Description\n* **Problem Type**: Survey Analysis & Data Storytelling\n* **Objective**: \n    * Participants are tasked with creating a compelling data-driven narrative about a specific subset of the data science community represented in the survey.\n    * The goal is to deeply explore the impact, priorities, or concerns of a defined group of practitioners through a combination of narrative text and data exploration.\n* **Key Points**:\n    * Stories can focus on macro groups (e.g., Python users) or micro groups (e.g., female master's students in ML).\n    * Submissions must combine data analysis with storytelling elements.\n    * Participants can supplement the survey data with additional publicly available Kaggle datasets.\n\n## Dataset Overview\n* **Data Type**: Tabular survey response data with mixed question types (single-select and multi-select).\n* **Context**: Annual industry survey capturing the state of ML/data science with 25,973 responses.\n* **Data Files**:\n    * `kaggle_survey_2021_responses.csv`: 42+ questions across 25,973 responses (369 columns)\n    * `kaggle_survey_2021_answer_choices.pdf`: Complete list of answer choices\n    * `kaggle_survey_2021_methodology.pdf`: Survey methodology documentation\n* **Features**:\n    * Demographic information (age, gender, country, education level)\n    * Technical questions (primary tools, programming languages, ML frameworks)\n    * Career-related questions (job title, industry, compensation)\n    * Multi-select questions are split into multiple binary columns\n\n## Evaluation Metrics\n* **Evaluation Criteria**:\n    * **Composition**: Quality of narrative thread supported by data and visualizations\n    * **Originality**: Novelty of insights and thought-provoking nature\n    * **Documentation**: Clarity, reproducibility, and proper citation of sources\n* **Judging Approach**:\n    * Qualitative assessment by judges based on the three criteria above\n    * No quantitative scoring metric - relies on expert evaluation of storytelling quality",
    "sections": {},
    "file_path": "kaggle_datasets/464/problem_summary.md"
  },
  "499": {
    "problem_id": "499",
    "title": "Medical Image Segmentation for GI Tract Organs in MRI Scans",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Medical Image Segmentation for GI Tract Organs in MRI Scans\n\n## Problem Description\n* **Problem Type**: Computer Vision - Semantic Segmentation (Multi-class)\n* **Objective**: \n    * Develop a deep learning model to automatically segment the stomach and intestines (large bowel, small bowel) in MRI scans of cancer patients undergoing radiation therapy.\n    * The goal is to assist radiation oncologists in precisely targeting tumors while avoiding healthy organs, reducing treatment time from ~1 hour to 15 minutes per session.\n* **Key Points**:\n    * Focus on segmenting three classes: large bowel, small bowel, and stomach\n    * Must handle variability in organ positions across different scan days\n    * Intended to improve cancer treatment precision and patient comfort\n    * Uses real clinical MRI data from UW-Madison Carbone Cancer Center patients\n\n## Dataset Overview\n* **Data Type**: \n    * 16-bit grayscale MRI images (PNG format) \n    * Annotations provided as Run-Length Encoded (RLE) masks\n* **Context**:\n    * Serial abdominal MRI scans from cancer patients (1-5 scans per patient across treatment days)\n    * Images show gastrointestinal tract anatomy for radiation therapy planning\n* **Data Files**:\n    * `train.csv` - Contains image IDs, class labels, and RLE-encoded masks\n    * `sample_submission.csv` - Submission format placeholder\n    * `train/` - Folder structure organizing scans by case/day/slice\n* **Key Features**:\n    * Filenames encode slice dimensions (width/height in pixels) and physical pixel spacing (in mm)\n    * Physical slice thickness: 3mm (superior-inferior direction)\n    * Test set completely hidden during competition (50 unseen cases)\n\n## Evaluation Metrics\n* **Primary Metrics**:\n    * Combined score weighting:\n        * 40% Dice coefficient (Sørensen-Dice similarity)\n        * 60% 3D Hausdorff distance (normalized by image size)\n* **Dice Coefficient**:\n    * Measures pixel-wise overlap between prediction and ground truth\n    * Formula: `2*|X∩Y|/(|X|+|Y|)` where X=prediction, Y=ground truth\n    * Defined as 0 when both sets are empty\n* **3D Hausdorff Distance**:\n    * Calculates maximum distance between nearest points of two 3",
    "sections": {},
    "file_path": "kaggle_datasets/499/problem_summary.md"
  },
  "452": {
    "problem_id": "452",
    "title": "Tabular Regression for Loan Default Loss Prediction",
    "problem_type": "Regression (predicting continuous target)",
    "objective": "Predict the `loss` value associated with loan defaults based on anonymized tabular features. The goal is to minimize prediction error against ground truth loss values.",
    "evaluation_metric": null,
    "full_content": "# Tabular Regression for Loan Default Loss Prediction\n\n**Problem Description:**\n* **Problem Type:** Regression (predicting continuous target)\n* **Objective:** Predict the `loss` value associated with loan defaults based on anonymized tabular features. The goal is to minimize prediction error against ground truth loss values.\n    * **Key Points:**\n        * Target (`loss`) is integer-valued in ground truth but accepts continuous predictions\n        * Dataset is synthetic but based on real-world loan default data\n        * Designed as beginner-friendly competition between Titanic tutorial and advanced featured competitions\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing anonymized loan default characteristics\n* **Data Files:**\n    * `train.csv` (contains target `loss` column)\n    * `test.csv` (requires `loss` predictions)\n    * `sample_submission.csv` (format template)\n* **Features:**\n    * 204 anonymized feature columns (properties relate to real-world loan characteristics)\n    * Generated using CTGAN (synthetic data generator)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * Square root of average squared differences between predicted and actual values\n        * Formula: RMSE = √[Σ(y� - ŷᵢ)²/n]\n        * Lower values indicate better performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (predicting continuous target)\n* **Objective:** Predict the `loss` value associated with loan defaults based on anonymized tabular features. The goal is to minimize prediction error against ground truth loss values.\n    * **Key Points:**\n        * Target (`loss`) is integer-valued in ground truth but accepts continuous predictions\n        * Dataset is synthetic but based on real-world loan default data\n        * Designed as beginner-friendly competition between Titanic tutorial and advanced featured competitions",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing anonymized loan default characteristics\n* **Data Files:**\n    * `train.csv` (contains target `loss` column)\n    * `test.csv` (requires `loss` predictions)\n    * `sample_submission.csv` (format template)\n* **Features:**\n    * 204 anonymized feature columns (properties relate to real-world loan characteristics)\n    * Generated using CTGAN (synthetic data generator)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * Square root of average squared differences between predicted and actual values\n        * Formula: RMSE = √[Σ(y� - ŷᵢ)²/n]\n        * Lower values indicate better performance"
    },
    "file_path": "kaggle_datasets/452/problem_summary.md"
  },
  "204": {
    "problem_id": "204",
    "title": "Predicting Customer Potential for Red Hat Business Value",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Customer Potential for Red Hat Business Value\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Develop a classification algorithm to predict which customers have the highest potential business value for Red Hat based on their characteristics and activities.\n    * The goal is to prioritize resources efficiently to generate more business and improve customer service.\n* **Key Points:**\n    * Predict whether a customer will complete a specific outcome (yes/no) within a fixed time window after performing an activity.\n    * The outcome is tied to unique activities performed by customers.\n    * Activities are categorized into types (1-7), with type 1 activities having more known characteristics than others.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data containing customer characteristics and activity logs.\n    * Data is split into two main files: people data (demographics/characteristics) and activity data (customer actions).\n* **Data Files:**\n    * `people.csv`: Contains unique people and their characteristics (e.g., `people_id`, categorical features, and one continuous numerical feature `char_38`).\n    * `act_train.csv` & `act_test.csv`: Contain unique activities performed by people, including activity characteristics and the target outcome (for training data).\n    * `sample_submission.csv`: Submission template with `activity_id` and predicted probability for the outcome.\n* **Key Features:**\n    * All features are categorical except `char_38` (numerical).\n    * Activities are labeled by type (1-7), with type 1 having more associated characteristics.\n    * The target variable is a binary outcome (yes/no) indicating business value potential.\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    * Measures the model's ability to distinguish between positive (high potential) and negative (low potential) classes.\n    * Submissions require predicted probabilities (values between 0 and 1) for the `outcome` variable.",
    "sections": {},
    "file_path": "kaggle_datasets/204/problem_summary.md"
  },
  "203": {
    "problem_id": "203",
    "title": "Predicting Mobile User Demographics from App Usage Data",
    "problem_type": "Multiclass Classification",
    "objective": "Predict users' demographic characteristics (gender and age groups) based on their mobile app usage, geolocation, and device properties. The goal is to help brands personalize marketing efforts by understanding user demographics through behavioral data.",
    "evaluation_metric": null,
    "full_content": "# Predicting Mobile User Demographics from App Usage Data\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict users' demographic characteristics (gender and age groups) based on their mobile app usage, geolocation, and device properties. The goal is to help brands personalize marketing efforts by understanding user demographics through behavioral data.\n    * **Key Points:**\n        * 12 target classes combining gender and age brackets (e.g., 'F23-', 'M29-31')\n        * Focus on behavioral data from mobile devices (app usage patterns)\n        * Privacy-preserving approach with anonymized data\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing mobile app usage events, device information, and app metadata from Chinese users (500M+ devices)\n* **Data Files:**\n    * `gender_age_train.csv`, `gender_age_test.csv` (target variable: `group`)\n    * `events.csv`, `app_events.csv` (timestamped app usage with geolocation)\n    * `app_labels.csv`, `label_categories.csv` (app categorization metadata)\n    * `phone_brand_device_model.csv` (device information with Chinese brand names)\n    * `sample_submission.csv`\n* **Key Features:**\n    * App usage events with timestamps and locations\n    * Device brands/models (anonymized)\n    * App categories/labels\n    * Ground truth demographics for training set\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (logloss)\n    * **Components:**\n        * For each device, predicted probabilities must be provided for all 12 classes\n        * Probabilities are rescaled to sum to 1 before scoring\n        * Clipped to [10⁻¹⁵, 1-10⁻¹⁵] to avoid log(0) issues\n        * Formula: \n            ```\n            logloss = -1/N Σ_i Σ_j y_ij log(p_ij)\n            ```\n            where N = devices, y_ij = 1 if device i is in class j (else 0), p_ij = predicted probability",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict users' demographic characteristics (gender and age groups) based on their mobile app usage, geolocation, and device properties. The goal is to help brands personalize marketing efforts by understanding user demographics through behavioral data.\n    * **Key Points:**\n        * 12 target classes combining gender and age brackets (e.g., 'F23-', 'M29-31')\n        * Focus on behavioral data from mobile devices (app usage patterns)\n        * Privacy-preserving approach with anonymized data",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing mobile app usage events, device information, and app metadata from Chinese users (500M+ devices)\n* **Data Files:**\n    * `gender_age_train.csv`, `gender_age_test.csv` (target variable: `group`)\n    * `events.csv`, `app_events.csv` (timestamped app usage with geolocation)\n    * `app_labels.csv`, `label_categories.csv` (app categorization metadata)\n    * `phone_brand_device_model.csv` (device information with Chinese brand names)\n    * `sample_submission.csv`\n* **Key Features:**\n    * App usage events with timestamps and locations\n    * Device brands/models (anonymized)\n    * App categories/labels\n    * Ground truth demographics for training set",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (logloss)\n    * **Components:**\n        * For each device, predicted probabilities must be provided for all 12 classes\n        * Probabilities are rescaled to sum to 1 before scoring\n        * Clipped to [10⁻¹⁵, 1-10⁻¹⁵] to avoid log(0) issues\n        * Formula: \n            ```\n            logloss = -1/N Σ_i Σ_j y_ij log(p_ij)\n            ```\n            where N = devices, y_ij = 1 if device i is in class j (else 0), p_ij = predicted probability"
    },
    "file_path": "kaggle_datasets/203/problem_summary.md"
  },
  "455": {
    "problem_id": "455",
    "title": "Binary Classification with Synthetic Insurance Claims Data",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of a customer making an insurance claim based on anonymized policy data. The target variable `claim` is binary (0 or 1), but submissions require probability predictions between 0.0 and 1.0.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Synthetic Insurance Claims Data\n\n**Problem Description:**\n*   **Problem Type:** Binary Classification\n*   **Objective:** Predict the probability of a customer making an insurance claim based on anonymized policy data. The target variable `claim` is binary (0 or 1), but submissions require probability predictions between 0.0 and 1.0.\n    *   **Key Points:**\n        *   Dataset is synthetic but based on real-world insurance claim data, generated using CTGAN.\n        *   Features are anonymized and may contain missing values.\n        *   Competition is designed as a beginner-friendly playground, focusing on learning rather than extreme optimization.\n\n**Dataset Overview:**\n*   **Data Type & Context:** Tabular data representing anonymized insurance policy features and claim outcomes.\n*   **Data Files:**\n    *   `train.csv` - Contains features and the target `claim` column.\n    *   `test.csv` - Contains features for which predictions need to be made.\n    *   `sample_submission.csv` - Demonstrates the required submission format.\n*   **Features:** 241 anonymized columns (exact features not specified, but described as having properties relating to real-world insurance policy features).\n\n**Evaluation Metrics:**\n*   **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    *   **Components:**\n        *   Measures the ability of the model to distinguish between classes (claim vs no claim).\n        *   Plots True Positive Rate against False Positive Rate at various threshold settings.\n        *   Higher AUC values indicate better model performance (1.0 = perfect, 0.5 = random).",
    "sections": {
      "Problem Description": "*   **Problem Type:** Binary Classification\n*   **Objective:** Predict the probability of a customer making an insurance claim based on anonymized policy data. The target variable `claim` is binary (0 or 1), but submissions require probability predictions between 0.0 and 1.0.\n    *   **Key Points:**\n        *   Dataset is synthetic but based on real-world insurance claim data, generated using CTGAN.\n        *   Features are anonymized and may contain missing values.\n        *   Competition is designed as a beginner-friendly playground, focusing on learning rather than extreme optimization.",
      "Dataset Overview": "*   **Data Type & Context:** Tabular data representing anonymized insurance policy features and claim outcomes.\n*   **Data Files:**\n    *   `train.csv` - Contains features and the target `claim` column.\n    *   `test.csv` - Contains features for which predictions need to be made.\n    *   `sample_submission.csv` - Demonstrates the required submission format.\n*   **Features:** 241 anonymized columns (exact features not specified, but described as having properties relating to real-world insurance policy features).",
      "Evaluation Metrics": "*   **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    *   **Components:**\n        *   Measures the ability of the model to distinguish between classes (claim vs no claim).\n        *   Plots True Positive Rate against False Positive Rate at various threshold settings.\n        *   Higher AUC values indicate better model performance (1.0 = perfect, 0.5 = random)."
    },
    "file_path": "kaggle_datasets/455/problem_summary.md"
  },
  "294": {
    "problem_id": "294",
    "title": "Binary Classification of Turkey Sounds in Audio Clips",
    "problem_type": "Binary Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Turkey Sounds in Audio Clips\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Participants must predict whether a given audio clip contains a turkey sound based on pre-extracted audio features. The target variable is binary (1 for turkey sounds, 0 otherwise).\n    * The competition emphasizes a playful, holiday-themed challenge with a focus on creativity and avoiding over-optimization (e.g., using pre-trained models that might leak answers).\n* **Key Points:**  \n    * The labels are \"soft\" (probabilistic) and may include turkey-related sounds beyond just direct turkey calls.\n    * The competition discourages using models trained on external datasets (e.g., AudioSet) to preserve fairness and fun.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Tabular data derived from audio features extracted from YouTube clips using AudioSet's VGGish tool (128-dimensional embeddings per frame).\n* **Data Files:**  \n    * `train.json`: Training set with labeled audio embeddings.\n    * `test.json`: Test set for predictions.\n    * `sample_submission.csv`: Example submission file in the required format.\n* **Key Features:**  \n    * `vid_id`: YouTube video ID.\n    * `start_time_seconds_youtube_clip`/`end_time_seconds_youtube_clip`: Temporal bounds of the audio clip.\n    * `audio_embedding`: 128-dimensional VGGish audio features.\n    * `is_turkey`: Binary target (soft label).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).\n* **Components:**  \n    * Submissions require predicted probabilities (`is_turkey`) for each test sample.\n    * The ROC curve evaluates the trade-off between true positive rate and false positive rate across probability thresholds.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Participants must predict whether a given audio clip contains a turkey sound based on pre-extracted audio features. The target variable is binary (1 for turkey sounds, 0 otherwise).\n    * The competition emphasizes a playful, holiday-themed challenge with a focus on creativity and avoiding over-optimization (e.g., using pre-trained models that might leak answers).\n* **Key Points:**  \n    * The labels are \"soft\" (probabilistic) and may include turkey-related sounds beyond just direct turkey calls.\n    * The competition discourages using models trained on external datasets (e.g., AudioSet) to preserve fairness and fun.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Tabular data derived from audio features extracted from YouTube clips using AudioSet's VGGish tool (128-dimensional embeddings per frame).\n* **Data Files:**  \n    * `train.json`: Training set with labeled audio embeddings.\n    * `test.json`: Test set for predictions.\n    * `sample_submission.csv`: Example submission file in the required format.\n* **Key Features:**  \n    * `vid_id`: YouTube video ID.\n    * `start_time_seconds_youtube_clip`/`end_time_seconds_youtube_clip`: Temporal bounds of the audio clip.\n    * `audio_embedding`: 128-dimensional VGGish audio features.\n    * `is_turkey`: Binary target (soft label).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).\n* **Components:**  \n    * Submissions require predicted probabilities (`is_turkey`) for each test sample.\n    * The ROC curve evaluates the trade-off between true positive rate and false positive rate across probability thresholds."
    },
    "file_path": "kaggle_datasets/294/problem_summary.md"
  },
  "604": {
    "problem_id": "604",
    "title": "PII Detection in Educational Text Data",
    "problem_type": "Text Classification (Named Entity Recognition for PII)",
    "objective": "Develop a model to automatically detect and classify Personally Identifiable Information (PII) in student-written essays, specifically:",
    "evaluation_metric": null,
    "full_content": "# PII Detection in Educational Text Data\n\n**Problem Description:**\n* **Problem Type:** Text Classification (Named Entity Recognition for PII)\n* **Objective:** Develop a model to automatically detect and classify Personally Identifiable Information (PII) in student-written essays, specifically:\n    * Identify 7 types of PII in educational text data\n    * Output token-level predictions in BIO format\n* **Key Points:**\n    * Focus on distinguishing sensitive PII (e.g., student names) from non-sensitive names (e.g., cited authors)\n    * Challenge includes handling varied PII formats beyond standardized patterns\n    * Secondary efficiency track evaluates models on computational performance\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * JSON files containing tokenized student essays from an online course\n    * Approximately 22,000 essays (70% reserved for test)\n    * Original PII replaced with surrogate identifiers\n* **Data Files:**\n    * train.json (annotated training data)\n    * test.json (illustrative test data, replaced during evaluation)\n    * sample_submission.csv\n* **Key Features:**\n    * Document IDs and full essay texts\n    * SpaCy-tokenized text with whitespace markers\n    * BIO-formatted labels for 7 PII types in training data\n\n**Evaluation Metrics:**\n* **Primary Metric:** Micro Fβ score (β=5)\n    * Emphasizes recall 5x more than precision\n    * Calculated at token level for PII detection\n* **Submission Format:**\n    * Requires row_id, document, token, and predicted label\n    * Only positive PII predictions (no 'O' labels) included\n* **Efficiency Track:**\n    * Additional evaluation combining F5 score and runtime\n    * Efficiency Score = (F5Benchmark - maxF5) + (RuntimeSeconds/32400)\n    * CPU-only submissions eligible",
    "sections": {
      "Problem Description": "* **Problem Type:** Text Classification (Named Entity Recognition for PII)\n* **Objective:** Develop a model to automatically detect and classify Personally Identifiable Information (PII) in student-written essays, specifically:\n    * Identify 7 types of PII in educational text data\n    * Output token-level predictions in BIO format\n* **Key Points:**\n    * Focus on distinguishing sensitive PII (e.g., student names) from non-sensitive names (e.g., cited authors)\n    * Challenge includes handling varied PII formats beyond standardized patterns\n    * Secondary efficiency track evaluates models on computational performance",
      "Dataset Overview": "* **Data Type & Context:** \n    * JSON files containing tokenized student essays from an online course\n    * Approximately 22,000 essays (70% reserved for test)\n    * Original PII replaced with surrogate identifiers\n* **Data Files:**\n    * train.json (annotated training data)\n    * test.json (illustrative test data, replaced during evaluation)\n    * sample_submission.csv\n* **Key Features:**\n    * Document IDs and full essay texts\n    * SpaCy-tokenized text with whitespace markers\n    * BIO-formatted labels for 7 PII types in training data",
      "Evaluation Metrics": "* **Primary Metric:** Micro Fβ score (β=5)\n    * Emphasizes recall 5x more than precision\n    * Calculated at token level for PII detection\n* **Submission Format:**\n    * Requires row_id, document, token, and predicted label\n    * Only positive PII predictions (no 'O' labels) included\n* **Efficiency Track:**\n    * Additional evaluation combining F5 score and runtime\n    * Efficiency Score = (F5Benchmark - maxF5) + (RuntimeSeconds/32400)\n    * CPU-only submissions eligible"
    },
    "file_path": "kaggle_datasets/604/problem_summary.md"
  },
  "436": {
    "problem_id": "436",
    "title": "Traffic Signal Optimization for City Traffic Flow",
    "problem_type": "Optimization (Traffic Signal Scheduling)",
    "objective": "Optimize traffic light schedules in a city to minimize total time spent in traffic and maximize the number of cars reaching their destinations before a given deadline.",
    "evaluation_metric": null,
    "full_content": "# Traffic Signal Optimization for City Traffic Flow\n\n**Problem Description:**\n* **Problem Type:** Optimization (Traffic Signal Scheduling)\n* **Objective:** Optimize traffic light schedules in a city to minimize total time spent in traffic and maximize the number of cars reaching their destinations before a given deadline.\n    * **Key Points:**\n        * Participants must design automatic traffic light timing systems for intersections.\n        * The solution must account for:\n            * Multiple intersections connected by streets with varying travel times\n            * Predefined car routes through the street network\n            * Traffic light state transitions (red/green) at each intersection\n        * Goal is to minimize overall traffic congestion while ensuring cars meet deadlines.\n\n**Dataset Overview:**\n* **Data Type:** Structured text data describing city traffic network and car paths\n* **Data Files:**\n    * `hashcode.in` (primary input file)\n    * `full_problem_description.pdf` (detailed problem specification)\n* **Features:**\n    * Simulation parameters (duration, bonus points)\n    * Intersection details (IDs and connections)\n    * Street information (start/end intersections, names, travel times)\n    * Car paths (ordered sequences of streets to traverse)\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Custom scoring system based on car completion times\n    * **Scoring Components:**\n        * For each car that finishes before deadline D:\n            * Base bonus points (F)\n            * Additional points: (D - T) where T is completion time\n        * Cars finishing after deadline score 0\n        * Final score = sum of all individual car scores\n    * **Example Calculation:**\n        * Car finishing at T=4 with D=6 and F=1000 → 1000 + (6-4) = 1002 points\n        * Car finishing at T=7 with D=6 → 0 points",
    "sections": {
      "Problem Description": "* **Problem Type:** Optimization (Traffic Signal Scheduling)\n* **Objective:** Optimize traffic light schedules in a city to minimize total time spent in traffic and maximize the number of cars reaching their destinations before a given deadline.\n    * **Key Points:**\n        * Participants must design automatic traffic light timing systems for intersections.\n        * The solution must account for:\n            * Multiple intersections connected by streets with varying travel times\n            * Predefined car routes through the street network\n            * Traffic light state transitions (red/green) at each intersection\n        * Goal is to minimize overall traffic congestion while ensuring cars meet deadlines.",
      "Dataset Overview": "* **Data Type:** Structured text data describing city traffic network and car paths\n* **Data Files:**\n    * `hashcode.in` (primary input file)\n    * `full_problem_description.pdf` (detailed problem specification)\n* **Features:**\n    * Simulation parameters (duration, bonus points)\n    * Intersection details (IDs and connections)\n    * Street information (start/end intersections, names, travel times)\n    * Car paths (ordered sequences of streets to traverse)",
      "Evaluation Metrics": "* **Evaluation Metric:** Custom scoring system based on car completion times\n    * **Scoring Components:**\n        * For each car that finishes before deadline D:\n            * Base bonus points (F)\n            * Additional points: (D - T) where T is completion time\n        * Cars finishing after deadline score 0\n        * Final score = sum of all individual car scores\n    * **Example Calculation:**\n        * Car finishing at T=4 with D=6 and F=1000 → 1000 + (6-4) = 1002 points\n        * Car finishing at T=7 with D=6 → 0 points"
    },
    "file_path": "kaggle_datasets/436/problem_summary.md"
  },
  "260": {
    "problem_id": "260",
    "title": "Dog Breed Identification from Images",
    "problem_type": "Multi-class Classification (Computer Vision - Image Classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Dog Breed Identification from Images\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Computer Vision - Image Classification)\n* **Objective:**  \n    * Develop a model to classify images of dogs into one of 120 distinct breeds.  \n    * The task involves fine-grained image categorization, where breeds may have subtle visual differences (e.g., Norfolk Terriers vs. Norwich Terriers).  \n* **Key Points:**  \n    * Limited training images per class, making the problem challenging (\"ruff\").  \n    * Subset of ImageNet focused exclusively on dogs.  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Image data (JPEG) of dogs, sourced from a canine subset of ImageNet and the Stanford Dogs Dataset.  \n* **Data Files:**  \n    * `train.zip`: Training images with corresponding breed labels in `labels.csv`.  \n    * `test.zip`: Test images (no labels provided).  \n    * `sample_submission.csv`: Example submission file in the required format.  \n* **Features:**  \n    * Images are identified by unique `id` filenames.  \n    * Labels are breed names (120 classes).  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-Class Log Loss (Cross-Entropy Loss).  \n* **Components:**  \n    * Predictions must be probabilities for each breed (summing to 1 per image).  \n    * Submission format: CSV with columns for `id` and each breed, e.g.:  \n      ```csv\n      id,affenpinscher,afghan_hound,...,yorkshire_terrier\n      000621fb3cbb32d8935728e48679680e,0.0083,0.0,...,0.0083\n      ",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Computer Vision - Image Classification)\n* **Objective:**  \n    * Develop a model to classify images of dogs into one of 120 distinct breeds.  \n    * The task involves fine-grained image categorization, where breeds may have subtle visual differences (e.g., Norfolk Terriers vs. Norwich Terriers).  \n* **Key Points:**  \n    * Limited training images per class, making the problem challenging (\"ruff\").  \n    * Subset of ImageNet focused exclusively on dogs.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Image data (JPEG) of dogs, sourced from a canine subset of ImageNet and the Stanford Dogs Dataset.  \n* **Data Files:**  \n    * `train.zip`: Training images with corresponding breed labels in `labels.csv`.  \n    * `test.zip`: Test images (no labels provided).  \n    * `sample_submission.csv`: Example submission file in the required format.  \n* **Features:**  \n    * Images are identified by unique `id` filenames.  \n    * Labels are breed names (120 classes).",
      "Evaluation Metrics": "* **Primary Metric:** Multi-Class Log Loss (Cross-Entropy Loss).  \n* **Components:**  \n    * Predictions must be probabilities for each breed (summing to 1 per image).  \n    * Submission format: CSV with columns for `id` and each breed, e.g.:  \n      ```csv\n      id,affenpinscher,afghan_hound,...,yorkshire_terrier\n      000621fb3cbb32d8935728e48679680e,0.0083,0.0,...,0.0083"
    },
    "file_path": "kaggle_datasets/260/problem_summary.md"
  },
  "409": {
    "problem_id": "409",
    "title": "Reinforcement Learning for Football Agent Control",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Reinforcement Learning for Football Agent Control\n\n## Problem Description\n* **Problem Type**: Reinforcement Learning (Multi-agent Control in a Simulated Environment)\n* **Objective**: Develop AI agents that can effectively control football (soccer) players in an 11 vs 11 simulated match environment. The core task is to create agents that:\n  * Control a single active player at any given time (the player with ball possession or closest to the ball when defending)\n  * Choose optimal actions from 19 possible moves to outscore the opponent\n  * Demonstrate strategic understanding of football concepts like passing, shooting, and positioning\n* **Key Points**:\n  * Environment is physics-based 3D football simulation with realistic rules (offsides, cards, etc.)\n  * Observations are mirrored to always represent the agent as controlling the left team\n  * Simplified control scheme compared to real football (only 19 discrete actions)\n  * Competition focuses on developing strategic AI rather than low-level player control\n\n## Dataset Overview\n* **Data Type**: Simulation environment observations and actions (not traditional static datasets)\n* **Context**: The Google Research Football Environment provides:\n  * Real-time game state observations (player positions, ball ownership, score, etc.)\n  * Physics-based 3D rendering of matches\n* **Key Features**:\n  * Observations include:\n    * Positions and velocities of all players\n    * Ball position and ownership status\n    * Current score and game mode (e.g., normal play, kickoff)\n    * Player stamina levels\n  * Actions include 19 discrete moves (e.g., sprint, shoot, pass, slide tackle)\n\n## Evaluation Metrics\n* **Evaluation Metric**: Skill Rating System (Gaussian N(μ,σ²) model)\n* **Components**:\n  * Each submission plays multiple matches against similarly-rated opponents\n  * Initial rating μ₀ = 600 for new submissions\n  * Rating updates after each match based on:\n    * Win/loss/draw outcome (score margin doesn't matter)\n    * Difference between expected and actual result\n    * Current uncertainty (σ) of both agents' ratings\n  * Uncertainty σ decreases with more matches played\n  * Leaderboard shows highest-rated submission per team",
    "sections": {},
    "file_path": "kaggle_datasets/409/problem_summary.md"
  },
  "267": {
    "problem_id": "267",
    "title": "Mobile App Ad Click Fraud Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Mobile App Ad Click Fraud Detection\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict whether a user will download a mobile app after clicking on its advertisement, thereby identifying potential fraudulent click traffic.\n    - **Key Points:**\n        - Focus on detecting IPs/devices that generate clicks but rarely result in app installations.\n        - Goal is to improve upon existing IP/device blacklist methods.\n        - High-class imbalance expected (90% of clicks may be fraudulent).\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing mobile ad click records with user/device attributes and timestamps.\n- **Data Files:**\n    - `train.csv` (main training set)\n    - `train_sample.csv` (100K-row subset)\n    - `test.csv` (evaluation set)\n    - `test_supplement.csv` (optional extended test data)\n    - `sampleSubmission.csv`\n- **Key Features:**\n    - Encoded categorical features: `ip`, `app`, `device`, `os`, `channel`\n    - Timestamps: `click_time`, `attributed_time` (download timestamp if applicable)\n    - Target: `is_attributed` (binary indicator of app download)\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n    - **Components:**\n        - Measures model's ability to distinguish between fraudulent (no download) and legitimate (download) clicks.\n        - Evaluates predicted probabilities (`is_attributed`) against ground truth.\n        - Robust to class imbalance prevalent in fraud detection tasks.",
    "sections": {},
    "file_path": "kaggle_datasets/267/problem_summary.md"
  },
  "431": {
    "problem_id": "431",
    "title": "Binary Classification on Synthetic Titanic Passenger Survival Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification on Synthetic Titanic Passenger Survival Data\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether passengers survived (1) or did not survive (0) the sinking of the Synthanic (a synthetic version of the Titanic). The task requires binary predictions for each passenger in the test set.\n    * **Key Points:**\n        * Dataset is synthetic but statistically similar to the original Titanic dataset\n        * Designed as an approachable competition between beginner and featured competitions\n        * Explicitly prevents \"cheating\" by using original Titanic labels since test labels are private\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing passenger information from a synthetic ship disaster scenario\n* **Data Files:**\n    * train.csv (with ground truth survival labels)\n    * test.csv (without survival labels)\n    * sample_submission.csv (example submission format)\n* **Key Features:**\n    * Demographic features: sex, age\n    * Socio-economic indicators: pclass (ticket class)\n    * Travel companions: sibsp (# siblings/spouses), parch (# parents/children)\n    * Journey details: fare, cabin, embarked (port of embarkation)\n    * Unique identifiers: PassengerId, ticket number\n\n## Evaluation Metrics\n* **Primary Metric:** Accuracy (percentage of correctly predicted survival outcomes)\n    * **Calculation:** \n        * (True Positives + True Negatives) / Total Predictions\n        * Binary predictions (0/1) compared against private ground truth\n    * **Submission Format Requirements:**\n        * CSV with exactly 100,000 rows + header\n        * Two columns: PassengerId and Survived (binary predictions)",
    "sections": {},
    "file_path": "kaggle_datasets/431/problem_summary.md"
  },
  "603": {
    "problem_id": "603",
    "title": "LLM Prompt Recovery from Rewritten Text",
    "problem_type": "NLP - Prompt Reconstruction / Reverse Engineering",
    "objective": "Recover the original LLM prompt used to transform a given text into its rewritten version. Participants must analyze pairs of original and rewritten texts to deduce the exact instructions given to the Gemma 7b-it model.",
    "evaluation_metric": null,
    "full_content": "# LLM Prompt Recovery from Rewritten Text\n\n**Problem Description:**\n* **Problem Type:** NLP - Prompt Reconstruction / Reverse Engineering\n* **Objective:** Recover the original LLM prompt used to transform a given text into its rewritten version. Participants must analyze pairs of original and rewritten texts to deduce the exact instructions given to the Gemma 7b-it model.\n* **Key Points:**\n  * Focuses on understanding LLM behavior and prompt engineering\n  * Involves reverse-engineering stylistic or content-based transformations\n  * Requires handling diverse rewriting styles (e.g., different author voices, structural changes)\n\n**Dataset Overview:**\n* **Data Type:** Text data (original and rewritten text pairs)\n* **Context:** Contains outputs from Google's Gemma 7b-it model with undisclosed prompts\n* **Data Files:**\n  * `train.csv`/`test.csv`: Contains id, original_text, rewrite_prompt (target), and rewritten_text\n  * `sample_submission.csv`: Template for submission format\n* **Features:**\n  * Original text (input to LLM)\n  * Rewritten text (LLM output)\n  * Target: The hidden rewrite_prompt used to generate the transformation\n\n**Evaluation Metrics:**\n* **Primary Metric:** Sharpened Cosine Similarity (SCS) with exponent=3\n* **Components:**\n  * Uses sentence-t5-base embeddings for text representation\n  * Compares embedding vectors of predicted vs. ground truth prompts\n  * SCS attenuates scores for incorrect answers more aggressively than standard cosine similarity\n  * Null answers are not allowed (will throw errors)",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Prompt Reconstruction / Reverse Engineering\n* **Objective:** Recover the original LLM prompt used to transform a given text into its rewritten version. Participants must analyze pairs of original and rewritten texts to deduce the exact instructions given to the Gemma 7b-it model.\n* **Key Points:**\n  * Focuses on understanding LLM behavior and prompt engineering\n  * Involves reverse-engineering stylistic or content-based transformations\n  * Requires handling diverse rewriting styles (e.g., different author voices, structural changes)",
      "Dataset Overview": "* **Data Type:** Text data (original and rewritten text pairs)\n* **Context:** Contains outputs from Google's Gemma 7b-it model with undisclosed prompts\n* **Data Files:**\n  * `train.csv`/`test.csv`: Contains id, original_text, rewrite_prompt (target), and rewritten_text\n  * `sample_submission.csv`: Template for submission format\n* **Features:**\n  * Original text (input to LLM)\n  * Rewritten text (LLM output)\n  * Target: The hidden rewrite_prompt used to generate the transformation",
      "Evaluation Metrics": "* **Primary Metric:** Sharpened Cosine Similarity (SCS) with exponent=3\n* **Components:**\n  * Uses sentence-t5-base embeddings for text representation\n  * Compares embedding vectors of predicted vs. ground truth prompts\n  * SCS attenuates scores for incorrect answers more aggressively than standard cosine similarity\n  * Null answers are not allowed (will throw errors)"
    },
    "file_path": "kaggle_datasets/603/problem_summary.md"
  },
  "293": {
    "problem_id": "293",
    "title": "Ship Detection in Satellite Imagery",
    "problem_type": "Computer Vision - Object Segmentation",
    "objective": "Detect and segment ships in satellite images with high accuracy and speed. The primary goal is to locate all ships in each image, even under challenging conditions like clouds or haze.",
    "evaluation_metric": null,
    "full_content": "# Ship Detection in Satellite Imagery\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Segmentation\n* **Objective:** Detect and segment ships in satellite images with high accuracy and speed. The primary goal is to locate all ships in each image, even under challenging conditions like clouds or haze.\n    * **Key Points:**\n        * Ships may vary significantly in size and location (open sea, docks, etc.).\n        * Many images contain no ships, while others may have multiple.\n        * **Algorithm Speed Prize:** Post-competition, participants can submit models for speed evaluation on 40,000+ image chips, emphasizing fast inference without significant accuracy degradation.\n\n**Dataset Overview:**\n* **Data Type & Context:** Satellite imagery (JPEG) with ships annotated via run-length encoded masks.\n    * **Data Files:**\n        * `train_ship_segmentations_v2.csv`: Contains ground truth masks (run-length encoded) for training images.\n        * `train_v2/`: Folder with training images.\n        * `test_v2/`: Folder with test images for submission.\n        * `sample_submission_v2.csv`: Example submission file with `ImageId` and `EncodedPixels` columns.\n    * **Features:**\n        * Images may include clouds, haze, or varying backgrounds (sea, docks).\n        * Masks are non-overlapping aligned bounding boxes (minor overlaps removed in ground truth).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean F2 Score averaged over Intersection over Union (IoU) thresholds (0.5 to 0.95, step 0.05).\n    * **Components:**\n        * For each threshold `t`, calculate F2 Score (weighted harmonic mean of precision/recall, favoring recall with β=2):\n            ```\n            Fβ(t) = (1+β²) * TP(t) / [(1+β²)*TP(t) + β²*FN(t) + FP(t)]\n            ```\n        * **True Positive (TP):** Predicted mask IoU > threshold with a ground truth mask.\n        * **False Positive (FP):** Predicted mask with no matching ground truth.\n        * **False Negative (FN):** Ground truth mask with no matching prediction.\n        * Final score: Mean of per-image average F2 Scores across all thresholds.\n* **Submission Format:** Run-length encoded pixel segments (e.g",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Segmentation\n* **Objective:** Detect and segment ships in satellite images with high accuracy and speed. The primary goal is to locate all ships in each image, even under challenging conditions like clouds or haze.\n    * **Key Points:**\n        * Ships may vary significantly in size and location (open sea, docks, etc.).\n        * Many images contain no ships, while others may have multiple.\n        * **Algorithm Speed Prize:** Post-competition, participants can submit models for speed evaluation on 40,000+ image chips, emphasizing fast inference without significant accuracy degradation.",
      "Dataset Overview": "* **Data Type & Context:** Satellite imagery (JPEG) with ships annotated via run-length encoded masks.\n    * **Data Files:**\n        * `train_ship_segmentations_v2.csv`: Contains ground truth masks (run-length encoded) for training images.\n        * `train_v2/`: Folder with training images.\n        * `test_v2/`: Folder with test images for submission.\n        * `sample_submission_v2.csv`: Example submission file with `ImageId` and `EncodedPixels` columns.\n    * **Features:**\n        * Images may include clouds, haze, or varying backgrounds (sea, docks).\n        * Masks are non-overlapping aligned bounding boxes (minor overlaps removed in ground truth).",
      "Evaluation Metrics": "* **Primary Metric:** Mean F2 Score averaged over Intersection over Union (IoU) thresholds (0.5 to 0.95, step 0.05).\n    * **Components:**\n        * For each threshold `t`, calculate F2 Score (weighted harmonic mean of precision/recall, favoring recall with β=2):\n            ```\n            Fβ(t) = (1+β²) * TP(t) / [(1+β²)*TP(t) + β²*FN(t) + FP(t)]\n            ```\n        * **True Positive (TP):** Predicted mask IoU > threshold with a ground truth mask.\n        * **False Positive (FP):** Predicted mask with no matching ground truth.\n        * **False Negative (FN):** Ground truth mask with no matching prediction.\n        * Final score: Mean of per-image average F2 Scores across all thresholds.\n* **Submission Format:** Run-length encoded pixel segments (e.g"
    },
    "file_path": "kaggle_datasets/293/problem_summary.md"
  },
  "258": {
    "problem_id": "258",
    "title": "Predicting Transparent Conductor Properties",
    "problem_type": "Regression (Multi-target)",
    "objective": "Predict two key properties of novel transparent semiconductor materials:",
    "evaluation_metric": null,
    "full_content": "# Predicting Transparent Conductor Properties\n\n**Problem Description:**\n* **Problem Type:** Regression (Multi-target)\n* **Objective:** Predict two key properties of novel transparent semiconductor materials:\n    * Formation energy (indicator of material stability)\n    * Bandgap energy (indicator of optical transparency potential)\n* **Key Points:**\n    * Focus on (AlxGayInz)2NO3N compounds with varying compositions (x+y+z=1)\n    * Aims to accelerate materials discovery by replacing expensive DFT calculations\n    * Targets improved transparent conductors for energy/optoelectronic applications\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with associated spatial geometry files\n* **Context:** Computational materials science data for 3,000 potential transparent conductors\n* **Data Files:**\n    * train.csv (with target values)\n    * test.csv (for prediction)\n    * /{train|test}/{id}/geometry.xyz (atomic position files)\n* **Features:**\n    * Spacegroup (crystal symmetry)\n    * Compositional features (Al/Ga/In/O atom counts, x/y/z ratios)\n    * Lattice parameters (vectors and angles)\n    * Atomic positions (in separate XYZ files)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE)\n    * Calculated separately for each target (formation energy and bandgap energy)\n    * Final score = average of both RMSLE values\n* **RMSLE Calculation:**\n    * For each column: sqrt(mean((log(p_i + 1) - log(a_i + 1))^2))\n    * Where:\n        * p_i = predicted value\n        * a_i = actual value\n        * n = number of observations",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Multi-target)\n* **Objective:** Predict two key properties of novel transparent semiconductor materials:\n    * Formation energy (indicator of material stability)\n    * Bandgap energy (indicator of optical transparency potential)\n* **Key Points:**\n    * Focus on (AlxGayInz)2NO3N compounds with varying compositions (x+y+z=1)\n    * Aims to accelerate materials discovery by replacing expensive DFT calculations\n    * Targets improved transparent conductors for energy/optoelectronic applications",
      "Dataset Overview": "* **Data Type:** Tabular data with associated spatial geometry files\n* **Context:** Computational materials science data for 3,000 potential transparent conductors\n* **Data Files:**\n    * train.csv (with target values)\n    * test.csv (for prediction)\n    * /{train|test}/{id}/geometry.xyz (atomic position files)\n* **Features:**\n    * Spacegroup (crystal symmetry)\n    * Compositional features (Al/Ga/In/O atom counts, x/y/z ratios)\n    * Lattice parameters (vectors and angles)\n    * Atomic positions (in separate XYZ files)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE)\n    * Calculated separately for each target (formation energy and bandgap energy)\n    * Final score = average of both RMSLE values\n* **RMSLE Calculation:**\n    * For each column: sqrt(mean((log(p_i + 1) - log(a_i + 1))^2))\n    * Where:\n        * p_i = predicted value\n        * a_i = actual value\n        * n = number of observations"
    },
    "file_path": "kaggle_datasets/258/problem_summary.md"
  },
  "407": {
    "problem_id": "407",
    "title": "Multi-label Classification of Drug Mechanisms of Action (MoA)",
    "problem_type": "Multi-label Classification (with binary targets for each MoA)",
    "objective": "Predict the probability of multiple Mechanism of Action (MoA) responses for drugs based on cellular signature data (gene expression and cell viability). Each drug can have multiple MoA annotations.",
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Drug Mechanisms of Action (MoA)\n\n**Problem Description:**\n* **Problem Type:** Multi-label Classification (with binary targets for each MoA)\n* **Objective:** Predict the probability of multiple Mechanism of Action (MoA) responses for drugs based on cellular signature data (gene expression and cell viability). Each drug can have multiple MoA annotations.\n    * **Key Points:**\n        * MoA represents the biological activity of a drug (e.g., which proteins it targets).\n        * The task involves analyzing combined gene expression (`g-` features) and cell viability (`c-` features) data from human cells treated with drugs.\n        * Control perturbations (`cp_vehicle`) have no MoAs and must be handled separately.\n        * Includes optional non-scored MoA labels in training data (not used for evaluation).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data combining:\n    * Gene expression measurements (`g-` prefix features)\n    * Cell viability measurements (`c-` prefix features)\n    * Metadata: treatment duration (`cp_time`), dose (`cp_dose`), and compound type (`cp_type`).\n* **Data Files:**\n    * `train_features.csv` - Input features for training.\n    * `train_targets_scored.csv` - Scored binary MoA labels (primary targets).\n    * `train_targets_nonscored.csv` - Optional non-scored MoA labels.\n    * `test_features.csv` - Test set features for prediction.\n    * `sample_submission.csv` - Submission format template.\n* **Key Features:**\n    * Features are anonymized (e.g., `g-0`, `c-10`).\n    * Metadata includes `cp_time` (24/48/72 hours) and `cp_dose` (high/low).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Columnwise Logarithmic Loss (log loss).\n    * **Calculation Breakdown:**\n        * For each `sig_id` (sample) and MoA target pair, predict the probability of a positive response.\n        * Log loss is computed as:  \n          `score = -1/M * Σ_m [1/N * Σ_i (y_i,m * log(ŷ_i,m) + (1-y_i,m) * log(1-ŷ_i,m))]`  \n          Where:\n            * `N` = number of test samples",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Classification (with binary targets for each MoA)\n* **Objective:** Predict the probability of multiple Mechanism of Action (MoA) responses for drugs based on cellular signature data (gene expression and cell viability). Each drug can have multiple MoA annotations.\n    * **Key Points:**\n        * MoA represents the biological activity of a drug (e.g., which proteins it targets).\n        * The task involves analyzing combined gene expression (`g-` features) and cell viability (`c-` features) data from human cells treated with drugs.\n        * Control perturbations (`cp_vehicle`) have no MoAs and must be handled separately.\n        * Includes optional non-scored MoA labels in training data (not used for evaluation).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data combining:\n    * Gene expression measurements (`g-` prefix features)\n    * Cell viability measurements (`c-` prefix features)\n    * Metadata: treatment duration (`cp_time`), dose (`cp_dose`), and compound type (`cp_type`).\n* **Data Files:**\n    * `train_features.csv` - Input features for training.\n    * `train_targets_scored.csv` - Scored binary MoA labels (primary targets).\n    * `train_targets_nonscored.csv` - Optional non-scored MoA labels.\n    * `test_features.csv` - Test set features for prediction.\n    * `sample_submission.csv` - Submission format template.\n* **Key Features:**\n    * Features are anonymized (e.g., `g-0`, `c-10`).\n    * Metadata includes `cp_time` (24/48/72 hours) and `cp_dose` (high/low).",
      "Evaluation Metrics": "* **Primary Metric:** Mean Columnwise Logarithmic Loss (log loss).\n    * **Calculation Breakdown:**\n        * For each `sig_id` (sample) and MoA target pair, predict the probability of a positive response.\n        * Log loss is computed as:  \n          `score = -1/M * Σ_m [1/N * Σ_i (y_i,m * log(ŷ_i,m) + (1-y_i,m) * log(1-ŷ_i,m))]`  \n          Where:\n            * `N` = number of test samples"
    },
    "file_path": "kaggle_datasets/407/problem_summary.md"
  },
  "635": {
    "problem_id": "635",
    "title": "Predicting NFL Team and Player Tendencies from Pre-Snap Behavior",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NFL Team and Player Tendencies from Pre-Snap Behavior\n\n## Problem Description\n* **Problem Type**: Multi-class Classification, Time Series Analysis, and Sports Analytics\n* **Objective**: \n    * Analyze pre-snap player tracking data to predict post-snap outcomes in NFL games.\n    * Generate actionable insights about team/player tendencies that could be used by NFL teams.\n* **Key Points**:\n    * Focus on patterns in pre-snap behavior (formations, motions, alignments) that predict post-snap actions.\n    * Three submission tracks with different focuses:\n        * Undergraduate track (for students)\n        * Metric track (creating novel performance metrics)\n        * Coaching presentation track (scouting-style reports)\n    * Encourages creativity in defining specific prediction tasks (e.g., play type, schemes, player actions).\n\n## Dataset Overview\n* **Data Type**: Time-series tracking data (player movements) with tabular play metadata\n* **Context**: NFL player tracking data from Next Gen Stats system covering pre-snap moments\n* **Data Files**:\n    * `games.csv` - Game metadata\n    * `plays.csv` - Play-level information\n    * `players.csv` - Player attributes\n    * `player_play.csv` - Player performance metrics per play\n    * `tracking_week_[1-9].csv` - Player position/movement data (weekly files)\n* **Key Features**:\n    * Player coordinates (x,y), speed, acceleration, orientation\n    * Play characteristics (formation, down/distance, play type)\n    * Pre-snap events (motion, shifts, alignment changes)\n    * Post-snap outcomes (play result, yardage, defensive schemes)\n\n## Evaluation Metrics\n* **Evaluation Method**: Multi-component scoring by NFL analysts:\n    * **Football Score (30%)**:\n        * Practical utility for NFL teams\n        * Handling football-specific complexities\n        * Uniqueness of insights\n    * **Data Science Score (30%)**:\n        * Methodological correctness\n        * Evidence-based claims\n        * Model appropriateness\n        * Analytical innovation\n    * **Report Score (20%)**:\n        * Clarity of writing\n        * Logical flow\n        * Problem motivation\n    * **Data Visualization Score (20%)**:\n        * Accessibility of charts\n        * Accuracy of visualizations\n        * Innovation in presentation",
    "sections": {},
    "file_path": "kaggle_datasets/635/problem_summary.md"
  },
  "251": {
    "problem_id": "251",
    "title": "Zillow Home Value Prediction (Zestimate)",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Zillow Home Value Prediction (Zestimate)\n\n## Problem Description\n* **Problem Type**: Regression (Time Series Forecasting)\n* **Objective**: Predict the log-error between Zillow's automated home valuation (Zestimate) and actual sale prices for residential properties across multiple future time periods. The goal is to improve the accuracy of Zillow's existing valuation algorithm.\n* **Key Points**:\n  * Predictions required for 6 specific months (Oct-Dec 2016 and Oct-Dec 2017)\n  * Competition structured in two rounds: qualifying round (improving Zestimate residuals) and final round (building valuation from scratch)\n  * Real-world impact on 110M US homes\n  * Uses actual sales data tracked post-competition for final evaluation\n\n## Dataset Overview\n* **Data Type**: Tabular real estate data with temporal components\n* **Context**: Property features and transaction records from Los Angeles, Orange, and Ventura counties (California)\n* **Data Files**:\n  * `properties_2016.csv` / `properties_2017.csv`: Home features by year\n  * `train_2016.csv` / `train_2017.csv`: Transaction records with sale prices\n  * `sample_submission.csv`: Submission format template\n* **Features**:\n  * Property characteristics (e.g., square footage, room counts, location)\n  * Transaction dates and prices\n  * Zestimate valuations\n  * Temporal indicators for prediction months\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE) of predicted log-error\n* **Log-Error Calculation**:\n  * `logerror = log(Zestimate) - log(SalePrice)`\n  * Only evaluated on properties that sold during each time period\n  * Multiple sales within 31 days resolved to most reasonable transaction\n* **Submission Format**:\n  * Requires predictions for all properties across 6 specified months\n  * Format: `ParcelId,201610,201611,201612,201710,201711,201712`",
    "sections": {},
    "file_path": "kaggle_datasets/251/problem_summary.md"
  },
  "438": {
    "problem_id": "438",
    "title": "Multi-label Classification of Apple Leaf Diseases from Images",
    "problem_type": "Multi-label Image Classification (Computer Vision)",
    "objective": "Develop a machine learning model to:",
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Apple Leaf Diseases from Images\n\n**Problem Description:**\n* **Problem Type:** Multi-label Image Classification (Computer Vision)\n* **Objective:** Develop a machine learning model to:\n    * Accurately classify apple leaf images into disease categories\n    * Handle multiple simultaneous disease labels per image\n    * Address real-world challenges like:\n        * Variations across apple cultivars\n        * Different leaf maturity stages\n        * Non-uniform backgrounds\n        * Varying lighting/imaging conditions\n* **Key Points:**\n    * Must classify leaves with \"complex\" cases (too many diseases to label individually)\n    * Builds on previous year's competition with expanded dataset and disease categories\n    * Focus on practical agricultural applications for disease detection\n\n**Dataset Overview:**\n* **Data Type:** RGB images of apple leaves with multi-label disease annotations\n* **Context:** Real-world field images with diverse conditions (backgrounds, lighting, focal settings)\n* **Data Files:**\n    * `train.csv`: Image IDs and space-delimited disease labels\n    * `train_images/`: ~23,000 training images\n    * `test_images/`: Hidden test set (only 3 sample images provided locally)\n    * `sample_submission.csv`: Submission format example\n* **Features:**\n    * Images show foliar diseases at various stages\n    * Labels include both single and multiple disease cases\n    * \"complex\" label for leaves with unidentifiable disease combinations\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean F1-Score\n    * Calculated as the mean of per-class F1 scores\n    * F1 score balances precision and recall for each class\n    * Particularly suitable for:\n        * Multi-label classification\n        * Potentially imbalanced class distributions\n        * Cases where both false positives and false negatives matter",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Image Classification (Computer Vision)\n* **Objective:** Develop a machine learning model to:\n    * Accurately classify apple leaf images into disease categories\n    * Handle multiple simultaneous disease labels per image\n    * Address real-world challenges like:\n        * Variations across apple cultivars\n        * Different leaf maturity stages\n        * Non-uniform backgrounds\n        * Varying lighting/imaging conditions\n* **Key Points:**\n    * Must classify leaves with \"complex\" cases (too many diseases to label individually)\n    * Builds on previous year's competition with expanded dataset and disease categories\n    * Focus on practical agricultural applications for disease detection",
      "Dataset Overview": "* **Data Type:** RGB images of apple leaves with multi-label disease annotations\n* **Context:** Real-world field images with diverse conditions (backgrounds, lighting, focal settings)\n* **Data Files:**\n    * `train.csv`: Image IDs and space-delimited disease labels\n    * `train_images/`: ~23,000 training images\n    * `test_images/`: Hidden test set (only 3 sample images provided locally)\n    * `sample_submission.csv`: Submission format example\n* **Features:**\n    * Images show foliar diseases at various stages\n    * Labels include both single and multiple disease cases\n    * \"complex\" label for leaves with unidentifiable disease combinations",
      "Evaluation Metrics": "* **Primary Metric:** Mean F1-Score\n    * Calculated as the mean of per-class F1 scores\n    * F1 score balances precision and recall for each class\n    * Particularly suitable for:\n        * Multi-label classification\n        * Potentially imbalanced class distributions\n        * Cases where both false positives and false negatives matter"
    },
    "file_path": "kaggle_datasets/438/problem_summary.md"
  },
  "256": {
    "problem_id": "256",
    "title": "Restaurant Visitor Forecasting with Time-Series Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Restaurant Visitor Forecasting with Time-Series Data\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting (Regression)\n* **Objective:** Predict the total number of visitors to specific restaurants on future dates using historical reservation, visitation, and restaurant metadata.\n    * Key challenge: Forecasting during irregular periods like Japan's \"Golden Week\" holiday.\n    * Must handle restaurants with limited historical data (newer establishments).\n    * Requires merging relational data from two different restaurant systems (AirREGI and Hot Pepper Gourmet).\n\n## Dataset Overview\n* **Data Type:** Relational time-series data (tabular) with geospatial and categorical features.\n* **Context:** Restaurant reservations, visit logs, and metadata from two Japanese platforms (POS system and review service).\n* **Key Files:**\n    * `air_visit_data.csv`: Core training data (historical daily visitors per restaurant).\n    * `air_reserve.csv`/`hpg_reserve.csv`: Reservation records with timestamps.\n    * `air_store_info.csv`/`hpg_store_info.csv`: Restaurant metadata (genre, area, approximate coordinates).\n    * `store_id_relation.csv`: Cross-reference for restaurants present in both systems.\n    * `date_info.csv`: Calendar context (weekdays/holidays).\n* **Notable Features:**\n    * Temporal features: Reservation lead times, visit dates, holidays.\n    * Restaurant characteristics: Genre, geographic area.\n    * Multi-source data requiring joins across systems.\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * Formula: \n        ```\n        sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))^2)\n        ```\n        Where:\n        * `n` = total observations\n        * `p_i` = predicted visitors\n        * `a_i` = actual visitors\n    * Properties:\n        * Penalizes underestimates more than overestimates.\n        * Logarithmic transformation reduces impact of large value errors.\n    * Implementation Note: Days with closed restaurants (zero visitors) are excluded from scoring.",
    "sections": {},
    "file_path": "kaggle_datasets/256/problem_summary.md"
  },
  "632": {
    "problem_id": "632",
    "title": "Predicting Misconception-Distractor Affinity in Mathematics Questions",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Misconception-Distractor Affinity in Mathematics Questions\n\n## Problem Description\n- **Problem Type**: NLP - Multi-label Ranking (with affinity prediction between misconceptions and distractors)\n- **Objective**: Develop an NLP model to predict the affinity between misconceptions and incorrect answers (distractors) in multiple-choice mathematics questions. The goal is to suggest candidate misconceptions for each distractor to assist human teachers in tagging.\n- **Key Points**:\n  - Focus on mathematical content which adds complexity to standard NLP approaches\n  - Must handle both existing and newly discovered misconceptions\n  - Aims to improve consistency and efficiency in educational tagging processes\n  - Each distractor is carefully crafted to capture specific student misconceptions\n\n## Dataset Overview\n- **Data Type**: Text data (mathematical questions and answers) with structured metadata\n- **Context**: Diagnostic multiple-choice questions from mathematics education\n- **Data Files**:\n  - `train.csv`/`test.csv`: Contains questions, answers, constructs, and misconception labels\n  - `misconception_mapping.csv`: Maps misconception IDs to their descriptions\n  - `sample_submission.csv`: Example submission format\n- **Key Features**:\n  - `QuestionText`: OCR-extracted text of mathematical questions\n  - `Answer[A/B/C/D]Text`: Text of each answer option\n  - `ConstructName`: Granular skill being tested\n  - `SubjectName`: Broader subject category\n  - `Misconception[A/B/C/D]Id`: Target labels to predict (only in train)\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Average Precision @ 25 (MAP@25)\n- **Metric Components**:\n  - Calculates precision at each rank position (up to 25)\n  - Only one correct label per observation (distractor-misconception pair)\n  - Subsequent predictions of the same correct label are ignored\n  - Formula: \n    ```\n    MAP@25 = (1/U) * Σ(u=1 to U) Σ(k=1 to min(n,25)) P(k) × rel(k)\n    ```\n    Where:\n    - U = number of observations\n    - P(k) = precision at cutoff k\n    - n = number of predictions per observation (max 25)\n    - rel(k) = 1 if item at rank k is correct, else 0",
    "sections": {},
    "file_path": "kaggle_datasets/632/problem_summary.md"
  },
  "400": {
    "problem_id": "400",
    "title": "Halite IV: Multi-Agent Resource Management Game",
    "problem_type": "Reinforcement Learning / Multi-Agent Strategy Game",
    "objective": "Develop an AI agent to control a fleet of ships that compete against 3 opponents to collect the most halite (luminous energy resource) on a 21x21 grid over 400 turns. The agent must manage resource collection, ship movement, shipyard creation, and strategic interactions with opponents.",
    "evaluation_metric": null,
    "full_content": "# Halite IV: Multi-Agent Resource Management Game\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Develop an AI agent to control a fleet of ships that compete against 3 opponents to collect the most halite (luminous energy resource) on a 21x21 grid over 400 turns. The agent must manage resource collection, ship movement, shipyard creation, and strategic interactions with opponents.\n* **Key Points:**\n  * Real-time strategy game with simultaneous moves\n  * Four-player competitive environment with elimination mechanics\n  * Key strategic elements:\n    * Ship movement and collision mechanics (smallest ship survives)\n    * Halite mining and regeneration (2% per turn)\n    * Ship-to-shipyard conversion (costs 500 halite)\n    * Ship spawning from shipyards (costs 500 halite)\n    * Territory control and opponent elimination\n\n**Dataset Overview:**\n* **Data Type:** Game state observations (JSON format) containing complete information about:\n  * Board configuration (21x21 grid)\n  * Halite distribution\n  * Ship positions and cargo\n  * Shipyard locations\n  * Player resources\n* **Data Files:** \n  * Game engine provided in kaggle-environments.zip\n  * Includes Python SDK for agent development\n* **Key Features:**\n  * Complete game state observation each turn\n  * Positional data (wrapping grid coordinates)\n  * Resource amounts (halite in cells and ship cargo)\n  * Player inventories (total collected halite)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Skill Rating (Gaussian N(μ,σ²) estimation)\n  * Updated through head-to-head matches against similarly-rated bots\n  * Rating changes based on win/loss/draw outcomes\n* **Game Resolution:**\n  * Players ranked by total collected halite at game end (400 turns or elimination)\n  * Four-player matches treated as 6 two-agent comparisons for rating updates\n  * Rating updates consider:\n    * Deviation from expected outcome\n    * Current uncertainty (σ) in rating\n    * Does not consider margin of victory",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Develop an AI agent to control a fleet of ships that compete against 3 opponents to collect the most halite (luminous energy resource) on a 21x21 grid over 400 turns. The agent must manage resource collection, ship movement, shipyard creation, and strategic interactions with opponents.\n* **Key Points:**\n  * Real-time strategy game with simultaneous moves\n  * Four-player competitive environment with elimination mechanics\n  * Key strategic elements:\n    * Ship movement and collision mechanics (smallest ship survives)\n    * Halite mining and regeneration (2% per turn)\n    * Ship-to-shipyard conversion (costs 500 halite)\n    * Ship spawning from shipyards (costs 500 halite)\n    * Territory control and opponent elimination",
      "Dataset Overview": "* **Data Type:** Game state observations (JSON format) containing complete information about:\n  * Board configuration (21x21 grid)\n  * Halite distribution\n  * Ship positions and cargo\n  * Shipyard locations\n  * Player resources\n* **Data Files:** \n  * Game engine provided in kaggle-environments.zip\n  * Includes Python SDK for agent development\n* **Key Features:**\n  * Complete game state observation each turn\n  * Positional data (wrapping grid coordinates)\n  * Resource amounts (halite in cells and ship cargo)\n  * Player inventories (total collected halite)",
      "Evaluation Metrics": "* **Primary Metric:** Skill Rating (Gaussian N(μ,σ²) estimation)\n  * Updated through head-to-head matches against similarly-rated bots\n  * Rating changes based on win/loss/draw outcomes\n* **Game Resolution:**\n  * Players ranked by total collected halite at game end (400 turns or elimination)\n  * Four-player matches treated as 6 two-agent comparisons for rating updates\n  * Rating updates consider:\n    * Deviation from expected outcome\n    * Current uncertainty (σ) in rating\n    * Does not consider margin of victory"
    },
    "file_path": "kaggle_datasets/400/problem_summary.md"
  },
  "269": {
    "problem_id": "269",
    "title": "Google Landmark Recognition Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Google Landmark Recognition Challenge\n\n## Problem Description\n* **Problem Type:** Computer Vision - Multi-class Image Classification\n* **Objective:**  \n    * Build models to recognize landmarks depicted in images from a large-scale dataset.  \n    * Predict the correct landmark label (if any) for each test image, where some images may contain no landmarks or multiple landmarks.  \n* **Key Points:**  \n    * Contains 15K unique landmark classes, making it a large-scale classification task.  \n    * Training images each depict exactly one landmark, while test images may contain zero, one, or multiple landmarks.  \n    * Dataset was constructed to address the lack of large annotated datasets for landmark recognition research.  \n    * Challenge is related to the Landmark Retrieval Challenge, sharing the same test set but no overlapping training landmarks.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Image data of landmarks worldwide, sourced from URLs (actual image files not directly provided).  \n    * Each image is associated with a unique ID and may belong to one of 15K landmark classes.  \n* **Data Files:**  \n    * `train.csv` - Training images with landmark labels.  \n    * `test.csv` - Test images for prediction (may contain no landmarks).  \n    * `sample_submission.csv` - Example submission file format.  \n* **Features:**  \n    * Image URLs (for downloading).  \n    * Unique image IDs (hashes) and landmark IDs (integers).  \n    * Potential metadata leakage (e.g., Exif data) is permitted but not guaranteed to be predictive.  \n\n## Evaluation Metrics\n* **Evaluation Metric:** Global Average Precision (GAP) at \\(k=1\\) (also called micro Average Precision).  \n* **Components of GAP:**  \n    * Predictions consist of landmark labels and confidence scores, sorted in descending order.  \n    * Formula:  \n        \\[\n        GAP = \\frac{1}{M} \\sum_{i=1}^{N} P(i) \\cdot rel(i)\n        \\]  \n        * \\(N\\) = Total predictions across all queries.  \n        * \\(M\\) = Total queries with at least one visible landmark.  \n        * \\(P(i)\\) = Precision at rank \\(i\\).  \n        * \\(rel(i)\\) = 1 if prediction \\(i\\) is correct, else 0.  \n    * Submissions must include at most one prediction",
    "sections": {},
    "file_path": "kaggle_datasets/269/problem_summary.md"
  },
  "202": {
    "problem_id": "202",
    "title": "Predicting Bakery Inventory Demand for Grupo Bimbo",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Bakery Inventory Demand for Grupo Bimbo\n\n## Problem Description\n* **Problem Type**: Regression (Time Series Forecasting)\n* **Objective**: \n    * Accurately forecast inventory demand for Grupo Bimbo's bakery products across 1+ million stores in Mexico.\n    * The goal is to balance supply and demand - minimizing both empty shelves (lost sales) and product returns due to overstocking.\n* **Key Points**:\n    * Demand is defined as current week's sales minus next week's returns.\n    * Must handle real-world inventory challenges:\n        * New products appearing in test set that weren't in training data\n        * Noisy client names in the dataset\n        * Products with 1-week shelf life requiring precise forecasting\n\n## Dataset Overview\n* **Data Type & Context**: \n    * Tabular time series data of 9 weeks of bakery product sales/returns across Mexican stores\n    * Includes store, product, route, and client information\n* **Data Files**:\n    * train.csv - Historical sales/returns data\n    * test.csv - Stores/products to forecast\n    * sample_submission.csv - Submission format\n    * cliente_tabla.csv - Client information\n    * producto_tabla.csv - Product information  \n    * town_state.csv - Store location data\n* **Key Features**:\n    * Temporal: Week number (Semana)\n    * Location: Sales depot ID, Route ID\n    * Product: Product ID, Product Name\n    * Sales: Current week sales (units/pesos)\n    * Returns: Next week returns (units/pesos)\n    * Target: Adjusted Demand (Demanda_uni_equil)\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Logarithmic Error (RMSLE)\n* **Metric Calculation**:\n    * 𝜖 = √[1/n Σ(log(pᵢ+1) - log(aᵢ+1))²]\n    * Where:\n        * n = number of observations\n        * pᵢ = predicted demand\n        * aᵢ = actual demand\n    * Uses log transformation to penalize underestimates more than overestimates\n    * +1 terms prevent issues with zero values",
    "sections": {},
    "file_path": "kaggle_datasets/202/problem_summary.md"
  },
  "454": {
    "problem_id": "454",
    "title": "G2Net Gravitational Wave Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# G2Net Gravitational Wave Detection\n\n## Problem Description\n* **Problem Type:** Binary Classification (Signal Detection)\n* **Objective:** Detect gravitational wave (GW) signals from binary black hole mergers in simulated time-series data. Participants must build a model to distinguish between:\n    * Noise-only time series (`target=0`)\n    * Noise + simulated GW signal time series (`target=1`)\n* **Key Points:**\n    * Focuses on analyzing data from a network of 3 gravitational wave detectors (LIGO Hanford, LIGO Livingston, Virgo)\n    * Signals are buried in detector noise and typically not visible by eye\n    * Simulated signals incorporate 15 randomized astrophysical parameters (masses, spins, orientation, etc.)\n    * Aims to advance interdisciplinary collaboration between physics and data science\n\n## Dataset Overview\n* **Data Type:** Time series data (3-channel detector measurements)\n* **Context:** Simulated gravitational wave measurements from Earth-based interferometers\n* **Data Files:**\n    * `train/` - Directory containing `.npy` files (one per observation)\n    * `training_labels.csv` - Target labels for training data\n    * `test/` - Test set files (same format as training)\n    * `sample_submission.csv` - Submission template\n* **Features:**\n    * Each sample contains 3 time series (one per detector)\n    * Time series span 2 seconds sampled at 2,048 Hz (4,096 data points per detector)\n    * Raw measurements without astrophysical parameters\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model's ability to rank positive examples (signals) higher than negative examples (noise)\n    * Robust to class imbalance common in detection problems\n* **Implementation:**\n    * Submissions require probability predictions (`target` between 0 and 1)\n    * Evaluated on ROC curve between predicted probabilities and true labels",
    "sections": {},
    "file_path": "kaggle_datasets/454/problem_summary.md"
  },
  "498": {
    "problem_id": "498",
    "title": "POI Matching with Noisy Location Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# POI Matching with Noisy Location Data\n\n## Problem Description\n* **Problem Type:** Record Linkage / Entity Matching (Multi-instance Matching)\n* **Objective:**  \n    * Develop an algorithm to identify which entries in a dataset represent the same physical Point-of-Interest (POI) despite containing noise, duplications, and incorrect information. \n    * Key challenge involves matching POIs across heavily altered entries with unstructured attributes like names, addresses, and coordinates.\n* **Key Points:**\n    * Must handle real-world data imperfections: extraneous info, missing attributes, and artificial noise.\n    * Matches must be bidirectional (if A matches B, submission must include B matching A).\n    * Focus on commercial POIs (restaurants, stores, etc.) with global coverage.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data representing commercial Points-of-Interest (POIs) with geographical and textual attributes.\n    * Simulates real-world POI datasets with intentional noise and duplicates.\n* **Data Files:**\n    * `train.csv`: Contains >1M place entries with 11 attributes + ground-truth `point_of_interest` IDs.\n    * `pairs.csv`: Pre-generated positive/negative pairs for training (with `match` labels).\n    * `test.csv`: ~600K entries with distinct POIs from training set (evaluation phase).\n* **Key Features:**\n    * Core attributes: `id`, name, street address, coordinates (latitude/longitude).\n    * Other likely features (implied): categorical business types, country/city data.\n    * All features contain potential noise or corruption.\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Sample-wise Intersection over Union (IoU/Jaccard Index)\n    * Compares predicted matches against ground truth for each test entry.\n    * Calculation:  \n        * For each entry `id`, compute:  \n            ```IoU = |Ground Truth Matches ∩ Predicted Matches| / |Ground Truth Matches ∪ Predicted Matches|```  \n        * Final score = average IoU across all test entries.\n* **Submission Format:**\n    * Space-delimited lists of matching `id`s per row in `submission.csv`.\n    * Mandatory self-matching (each entry must include its own `id` in matches).",
    "sections": {},
    "file_path": "kaggle_datasets/498/problem_summary.md"
  },
  "453": {
    "problem_id": "453",
    "title": "MLB Player Digital Engagement Forecasting",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# MLB Player Digital Engagement Forecasting\n\n## Problem Description\n- **Problem Type**: Time Series Forecasting\n- **Objective**: Predict daily digital engagement metrics (`target1`-`target4`) for MLB players on a 0-100 scale, based on historical player performance, social media data, and team factors. The goal is to forecast engagement for future dates, leveraging time-series data without peeking ahead.\n  - **Key Points**:\n    - Focus on active MLB players in the 2021 season.\n    - Engagement metrics are influenced by player performance, team dynamics, and social media activity.\n    - Models must generalize to unseen future dates during the evaluation phase.\n\n## Dataset Overview\n- **Data Type**: Tabular and nested JSON time-series data.\n- **Context**: Daily MLB player and team statistics, social media metrics, and awards data from 2018 onwards.\n- **Data Files**:\n  - `train.csv`: Daily time-series data with nested JSON fields (e.g., `games`, `rosters`, `playerBoxScores`, `playerTwitterFollowers`).\n  - Static files: `players.csv`, `teams.csv`, `seasons.csv`, `awards.csv`.\n  - Example files: `example_test.csv`, `example_sample_submission.csv`.\n- **Features**:\n  - Player performance (e.g., `homeRuns`, `strikeOuts`, `inningsPitched`).\n  - Team metrics (e.g., `standings`, `teamBoxScores`).\n  - Social media (e.g., `playerTwitterFollowers`, `teamTwitterFollowers`).\n  - Temporal features (e.g., `date`, `gameDate`).\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Column-wise Mean Absolute Error (MCMAE).\n  - **Components**:\n    1. Calculate Mean Absolute Error (MAE) for each of the four target variables (`target1`-`target4`).\n    2. Average the four MAE values to derive the final score.\n- **Submission Format**: Predictions must be generated using the provided `mlb` Python module to enforce time-series integrity.",
    "sections": {},
    "file_path": "kaggle_datasets/453/problem_summary.md"
  },
  "205": {
    "problem_id": "205",
    "title": "Integer Sequence Learning",
    "problem_type": "Sequence Prediction (Integer Sequence Completion)",
    "objective": "Predict the next integer in a given sequence of numbers, leveraging patterns from the On-Line Encyclopedia of Integer Sequences (OEIS). The task involves analyzing partial sequences and generating the most likely next value.",
    "evaluation_metric": null,
    "full_content": "# Integer Sequence Learning\n\n**Problem Description:**\n* **Problem Type:** Sequence Prediction (Integer Sequence Completion)\n* **Objective:** Predict the next integer in a given sequence of numbers, leveraging patterns from the On-Line Encyclopedia of Integer Sequences (OEIS). The task involves analyzing partial sequences and generating the most likely next value.\n* **Key Points:**\n  * Sequences vary in complexity, from simple arithmetic progressions to mathematically intricate patterns (e.g., powers of primes, Fibonacci-like sequences).\n  * Many sequences may share identical or similar prefixes, requiring disambiguation.\n  * The challenge emphasizes pattern recognition and mathematical reasoning rather than traditional tabular or structured data analysis.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data (sequences of integers) with metadata from OEIS.\n* **Context:** Sequences are derived from the OEIS, a comprehensive repository of integer patterns curated by mathematicians.\n* **Data Files:**\n  * `train.csv`: Contains full sequences for training.\n  * `test.csv`: Contains sequences with the last integer removed (to be predicted).\n  * `sample_submission.csv`: Example submission file with the required format (`Id, Last`).\n* **Features:**\n  * Sequences are provided as ordered lists of integers.\n  * No explicit feature engineering is described; the focus is on the raw sequence values.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Accuracy (percentage of sequences where the predicted next integer is correct).\n* **Components:**\n  * Submissions are scored based on exact matches between predicted and actual next integers.\n  * No partial credit or proximity-based scoring is applied.",
    "sections": {
      "Problem Description": "* **Problem Type:** Sequence Prediction (Integer Sequence Completion)\n* **Objective:** Predict the next integer in a given sequence of numbers, leveraging patterns from the On-Line Encyclopedia of Integer Sequences (OEIS). The task involves analyzing partial sequences and generating the most likely next value.\n* **Key Points:**\n  * Sequences vary in complexity, from simple arithmetic progressions to mathematically intricate patterns (e.g., powers of primes, Fibonacci-like sequences).\n  * Many sequences may share identical or similar prefixes, requiring disambiguation.\n  * The challenge emphasizes pattern recognition and mathematical reasoning rather than traditional tabular or structured data analysis.",
      "Dataset Overview": "* **Data Type:** Tabular data (sequences of integers) with metadata from OEIS.\n* **Context:** Sequences are derived from the OEIS, a comprehensive repository of integer patterns curated by mathematicians.\n* **Data Files:**\n  * `train.csv`: Contains full sequences for training.\n  * `test.csv`: Contains sequences with the last integer removed (to be predicted).\n  * `sample_submission.csv`: Example submission file with the required format (`Id, Last`).\n* **Features:**\n  * Sequences are provided as ordered lists of integers.\n  * No explicit feature engineering is described; the focus is on the raw sequence values.",
      "Evaluation Metrics": "* **Evaluation Metric:** Accuracy (percentage of sequences where the predicted next integer is correct).\n* **Components:**\n  * Submissions are scored based on exact matches between predicted and actual next integers.\n  * No partial credit or proximity-based scoring is applied."
    },
    "file_path": "kaggle_datasets/205/problem_summary.md"
  },
  "233": {
    "problem_id": "233",
    "title": "Multi-label Classification of Amazon Satellite Imagery",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Amazon Satellite Imagery\n\n## Problem Description\n* **Problem Type:** Multi-label Image Classification (Computer Vision)\n* **Objective:**  \n  Participants are tasked with labeling satellite image chips from the Amazon rainforest with multiple tags representing:\n  * Atmospheric conditions (e.g., cloudy, haze)\n  * Land cover/land use (e.g., agriculture, water, habitation)\n  * Rare phenomena (e.g., artisanal mining, blow_down)\n* **Key Points:**\n  * Must handle noisy labels due to crowd-sourced annotation\n  * Some chips may have multiple valid tags (multi-label)\n  * Atmospheric tags are mutually exclusive (e.g., a chip can't be both \"clear\" and \"cloudy\")\n  * Focus on detecting human impact vs. natural phenomena\n\n## Dataset Overview\n* **Data Type:** Satellite imagery (4-band GeoTIFFs + JPEGs)\n  * 3-5 meter resolution images covering the Amazon basin\n  * Collected by Planet's Flock 2 satellites (2016-2017)\n* **Data Files:**\n  * `train.csv` (image filenames + space-delimited tags)\n  * `train-tif-v2.tar.7z` (4-band TIFFs: red, green, blue, near infrared)\n  * `train-jpg.tar.7z` (reference JPEGs)\n  * Corresponding test set files\n* **Features:**\n  * 17 possible tags including:\n    * Atmospheric: `clear`, `partly_cloudy`, `cloudy`, `haze`\n    * Common: `primary`, `water`, `habitation`, `agriculture`, `road`\n    * Rare: `artisanal_mine`, `blow_down`, `selective_logging`\n\n## Evaluation Metrics\n* **Primary Metric:** Mean F₂ Score (micro-averaged)\n  * Weighted harmonic mean of precision and recall\n  * Formula: (1+β²)pr/(β²p+r) where β=2 (weights recall higher)\n  * Calculated per-image then averaged across test set\n* **Submission Format:**\n  * CSV with `image_name` and space-delimited `tags` columns\n  * Must predict all applicable tags for each test image",
    "sections": {},
    "file_path": "kaggle_datasets/233/problem_summary.md"
  },
  "465": {
    "problem_id": "465",
    "title": "Binary Classification with Synthetic Email Spam Features",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of a binary `target` variable (likely indicating spam vs. non-spam) based on anonymized email features. The competition is designed as an approachable challenge for beginners to practice ML skills on tabular data.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Synthetic Email Spam Features\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary `target` variable (likely indicating spam vs. non-spam) based on anonymized email features. The competition is designed as an approachable challenge for beginners to practice ML skills on tabular data.\n    * **Key Points:**\n        * Dataset is synthetic but based on real-world email spam detection features.\n        * Intended as a learning-focused competition between beginner and advanced levels.\n        * Team size limited to 3 to encourage broader participation.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with anonymized continuous features derived from email content (originally for spam detection).\n* **Data Files:**\n    * `train.csv` (contains `target` column)\n    * `test.csv` (no `target` column)\n    * `sample_submission.csv` (submission format example)\n* **Features:** \n    * 100 anonymized continuous feature columns\n    * Binary `target` variable (exact labels not specified)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model performance in ranking positive class probabilities\n    * Submission requires predicted probabilities for the binary `target`",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary `target` variable (likely indicating spam vs. non-spam) based on anonymized email features. The competition is designed as an approachable challenge for beginners to practice ML skills on tabular data.\n    * **Key Points:**\n        * Dataset is synthetic but based on real-world email spam detection features.\n        * Intended as a learning-focused competition between beginner and advanced levels.\n        * Team size limited to 3 to encourage broader participation.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with anonymized continuous features derived from email content (originally for spam detection).\n* **Data Files:**\n    * `train.csv` (contains `target` column)\n    * `test.csv` (no `target` column)\n    * `sample_submission.csv` (submission format example)\n* **Features:** \n    * 100 anonymized continuous feature columns\n    * Binary `target` variable (exact labels not specified)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model performance in ranking positive class probabilities\n    * Submission requires predicted probabilities for the binary `target`"
    },
    "file_path": "kaggle_datasets/465/problem_summary.md"
  },
  "491": {
    "problem_id": "491",
    "title": "Sorghum Cultivar Identification from RGB Images",
    "problem_type": "Multi-class Classification (Computer Vision - Fine-Grained Visual Categorization)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Sorghum Cultivar Identification from RGB Images\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective:**  \n    * Predict the cultivar (specific varietal) of sorghum plants from RGB images taken in a field setting.  \n    * The task focuses on identifying subtle visual differences between 100 different sorghum cultivars.\n* **Key Points:**  \n    * High inter-class similarity between cultivars and intra-class variability due to changing imaging conditions.  \n    * Designed to detect planting errors (e.g., mislabeled seeds or non-uniform planting).  \n    * Test set uses images from separate field plots to prevent memorization of non-phenotypic features (e.g., soil patterns).  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * RGB images of sorghum plants captured by a vertical field scanner in June 2017 (mid-growing season).  \n    * Part of the TERRA-REF dataset, focusing on plant phenotyping.  \n* **Data Files:**  \n    * `train_images/`: Folder containing training images.  \n    * `train_cultivar_mapping.csv`: Maps training image filenames to cultivar labels.  \n    * `test/`: Folder containing test images (no labels provided).  \n    * `sample_submission.csv`: Example submission file with `filename` and `cultivar` columns.  \n* **Key Features:**  \n    * 48,106 total images (100 cultivars).  \n    * Images show plants under varying lighting/field conditions.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Mean Classification Accuracy  \n    * Percentage of test images where the predicted cultivar matches the ground truth.  \n* **Submission Format:**  \n    * CSV with `filename` and `cultivar` columns (25,472 test predictions).",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective:**  \n    * Predict the cultivar (specific varietal) of sorghum plants from RGB images taken in a field setting.  \n    * The task focuses on identifying subtle visual differences between 100 different sorghum cultivars.\n* **Key Points:**  \n    * High inter-class similarity between cultivars and intra-class variability due to changing imaging conditions.  \n    * Designed to detect planting errors (e.g., mislabeled seeds or non-uniform planting).  \n    * Test set uses images from separate field plots to prevent memorization of non-phenotypic features (e.g., soil patterns).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * RGB images of sorghum plants captured by a vertical field scanner in June 2017 (mid-growing season).  \n    * Part of the TERRA-REF dataset, focusing on plant phenotyping.  \n* **Data Files:**  \n    * `train_images/`: Folder containing training images.  \n    * `train_cultivar_mapping.csv`: Maps training image filenames to cultivar labels.  \n    * `test/`: Folder containing test images (no labels provided).  \n    * `sample_submission.csv`: Example submission file with `filename` and `cultivar` columns.  \n* **Key Features:**  \n    * 48,106 total images (100 cultivars).  \n    * Images show plants under varying lighting/field conditions.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Mean Classification Accuracy  \n    * Percentage of test images where the predicted cultivar matches the ground truth.  \n* **Submission Format:**  \n    * CSV with `filename` and `cultivar` columns (25,472 test predictions)."
    },
    "file_path": "kaggle_datasets/491/problem_summary.md"
  },
  "496": {
    "problem_id": "496",
    "title": "Semantic Similarity Matching for U.S. Patent Phrases",
    "problem_type": "NLP - Semantic Textual Similarity (Regression)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Semantic Similarity Matching for U.S. Patent Phrases\n\n**Problem Description:**\n* **Problem Type:** NLP - Semantic Textual Similarity (Regression)\n* **Objective:**  \n  Build a model to predict the semantic similarity score between pairs of patent phrases, scored within the context of their Cooperative Patent Classification (CPC). The goal is to assist patent examiners in identifying related inventions by matching key phrases with varying linguistic forms (e.g., synonyms, abbreviations, hypernyms) while accounting for domain-specific meanings.\n* **Key Points:**\n  * Similarity is context-dependent (CPC classification defines the technical domain).\n  * Scores are ordinal (0, 0.25, 0.5, 0.75, 1.0) with defined semantic meanings (e.g., 0.75 = close synonyms like \"mobile phone\" vs. \"cellphone\").\n  * Extends beyond paraphrase detection to include domain-specific equivalence (e.g., \"strong material\" ≈ \"steel\" in certain contexts).\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  Tabular text data containing pairs of patent phrases (`anchor` and `target`) with their CPC classification (`context`) and similarity scores.\n* **Data Files:**\n  * `train.csv`: Phrase pairs, CPC contexts, and manually rated similarity scores.\n  * `test.csv`: Phrase pairs without scores (competition test set).\n  * `sample_submission.csv`: Submission template with `id` and predicted `score`.\n* **Key Features:**\n  * `anchor`/`target`: Phrases to compare (e.g., \"television set\" vs. \"TV set\").\n  * `context`: CPC code (e.g., \"F04D29/30\") indicating the technical domain.\n  * `score`: Similarity rating (0–1 in 0.25 increments).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Pearson correlation coefficient between predicted and actual similarity scores.\n  * Measures linear relationship between model predictions and ground truth.\n  * Sensitive to both magnitude and direction of errors (unlike RMSE/MAE).",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Semantic Textual Similarity (Regression)\n* **Objective:**  \n  Build a model to predict the semantic similarity score between pairs of patent phrases, scored within the context of their Cooperative Patent Classification (CPC). The goal is to assist patent examiners in identifying related inventions by matching key phrases with varying linguistic forms (e.g., synonyms, abbreviations, hypernyms) while accounting for domain-specific meanings.\n* **Key Points:**\n  * Similarity is context-dependent (CPC classification defines the technical domain).\n  * Scores are ordinal (0, 0.25, 0.5, 0.75, 1.0) with defined semantic meanings (e.g., 0.75 = close synonyms like \"mobile phone\" vs. \"cellphone\").\n  * Extends beyond paraphrase detection to include domain-specific equivalence (e.g., \"strong material\" ≈ \"steel\" in certain contexts).",
      "Dataset Overview": "* **Data Type & Context:**  \n  Tabular text data containing pairs of patent phrases (`anchor` and `target`) with their CPC classification (`context`) and similarity scores.\n* **Data Files:**\n  * `train.csv`: Phrase pairs, CPC contexts, and manually rated similarity scores.\n  * `test.csv`: Phrase pairs without scores (competition test set).\n  * `sample_submission.csv`: Submission template with `id` and predicted `score`.\n* **Key Features:**\n  * `anchor`/`target`: Phrases to compare (e.g., \"television set\" vs. \"TV set\").\n  * `context`: CPC code (e.g., \"F04D29/30\") indicating the technical domain.\n  * `score`: Similarity rating (0–1 in 0.25 increments).",
      "Evaluation Metrics": "* **Primary Metric:** Pearson correlation coefficient between predicted and actual similarity scores.\n  * Measures linear relationship between model predictions and ground truth.\n  * Sensitive to both magnitude and direction of errors (unlike RMSE/MAE)."
    },
    "file_path": "kaggle_datasets/496/problem_summary.md"
  },
  "462": {
    "problem_id": "462",
    "title": "Ventilator Pressure Prediction in Mechanical Lung Simulation",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Ventilator Pressure Prediction in Mechanical Lung Simulation\n\n## Problem Description\n- **Problem Type**: Time Series Regression\n- **Objective**: Predict the airway pressure in a respiratory circuit during the inspiratory phase of a mechanical ventilation breath, given control inputs and lung attributes. The goal is to simulate a ventilator connected to a sedated patient's lung.\n- **Key Points**:\n  - Focus on generalizing across varying lung characteristics (compliance and resistance)\n  - Only the inspiratory phase is evaluated (expiratory phase is ignored)\n  - Aims to improve upon current PID controller standards in ventilator systems\n  - Potential medical applications for adaptive ventilator algorithms\n\n## Dataset Overview\n- **Data Type**: Time series data of mechanical ventilator operations\n- **Context**: Simulated data from a ventilator connected to an artificial bellows test lung\n- **Data Files**:\n  - train.csv (training set)\n  - test.csv (test set)\n  - sample_submission.csv (submission format example)\n- **Key Features**:\n  - Control inputs: `u_in` (inspiratory valve opening %), `u_out` (binary expiratory valve state)\n  - Lung attributes: `R` (airway resistance), `C` (lung compliance)\n  - Time-based features: `time_step`, `breath_id`\n  - Target variable: `pressure` (airway pressure in cmH2O)\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Absolute Error (MAE)\n- **Calculation**:\n  - Computed only during inspiratory phases (`u_out = 0`)\n  - Formula: |𝑋−𝑌| where 𝑋 is predicted pressure and 𝑌 is actual pressure\n  - Averaged across all breaths in test set\n  - Lower values indicate better performance",
    "sections": {},
    "file_path": "kaggle_datasets/462/problem_summary.md"
  },
  "234": {
    "problem_id": "234",
    "title": "Error: Competition Details Not Found",
    "problem_type": "Unknown (No competition details available)",
    "objective": "Unable to determine due to missing competition context.",
    "evaluation_metric": null,
    "full_content": "# Error: Competition Details Not Found\n\n**Problem Description:**\n*   **Problem Type:** Unknown (No competition details available)\n*   **Objective:** Unable to determine due to missing competition context.\n*   **Key Points:**  \n    *   The provided links/context return a server error page, indicating the competition details are inaccessible.\n\n**Dataset Overview:**\n*   **Data Type:** Unknown (No dataset information available)  \n*   **Data Files:** Not specified (No files listed due to error)  \n*   **Features:** Unable to determine (No dataset description provided)  \n\n**Evaluation Metrics:**\n*   **Evaluation Metric:** Unknown (No evaluation criteria available)  \n*   **Components:** N/A (No metric details provided)  \n\n**Note:** This summary reflects the lack of retrievable competition data due to the encountered server error. Further investigation or corrected links are needed to proceed.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Unknown (No competition details available)\n*   **Objective:** Unable to determine due to missing competition context.\n*   **Key Points:**  \n    *   The provided links/context return a server error page, indicating the competition details are inaccessible.",
      "Dataset Overview": "*   **Data Type:** Unknown (No dataset information available)  \n*   **Data Files:** Not specified (No files listed due to error)  \n*   **Features:** Unable to determine (No dataset description provided)",
      "Evaluation Metrics": "*   **Evaluation Metric:** Unknown (No evaluation criteria available)  \n*   **Components:** N/A (No metric details provided)  \n\n**Note:** This summary reflects the lack of retrievable competition data due to the encountered server error. Further investigation or corrected links are needed to proceed."
    },
    "file_path": "kaggle_datasets/234/problem_summary.md"
  },
  "346": {
    "problem_id": "346",
    "title": "Steel Defect Detection and Classification",
    "problem_type": "Computer Vision - Semantic Segmentation & Multi-class Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Steel Defect Detection and Classification\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Semantic Segmentation & Multi-class Classification\n* **Objective:** \n    * Localize and classify surface defects in steel sheet images.\n    * Predict pixel-level segmentation masks for 4 defect classes (ClassId = 1,2,3,4).\n    * Each image may contain zero, one, or multiple types of defects.\n* **Key Points:**\n    * Defects must be segmented even if non-contiguous within an image.\n    * Submission requires predictions for all 4 classes per image (even if no defect exists for a class).\n    * Efficiency Prize was awarded for fastest inference among top solutions.\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * High-frequency camera images of steel sheets during manufacturing.\n    * Images show various surface defects needing segmentation and classification.\n* **Data Files:**\n    * `train_images/` - Folder containing training images.\n    * `test_images/` - Folder containing test images for prediction.\n    * `train.csv` - Contains run-length encoded segmentation masks for training images.\n    * `sample_submission.csv` - Demonstrates submission format.\n* **Features:**\n    * Images are the primary input data.\n    * `train.csv` contains `ImageId`, `ClassId`, and `EncodedPixels` (RLE-format segmentation masks).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Dice Coefficient (Sørensen-Dice Index)\n    * Measures pixel-wise agreement between predicted and ground truth masks.\n    * Formula: \\( \\frac{2 \\times |X \\cap Y|}{|X| + |Y|} \\), where:\n        * \\( X \\) = predicted pixels\n        * \\( Y \\) = ground truth pixels\n    * Special case: Score = 1 when both X and Y are empty.\n* **Submission Format:**\n    * Requires run-length encoded (RLE) pixel segments.\n    * Each row represents one `ImageId_ClassId` pair with space-delimited RLE values.\n    * Example: `004f40c73.jpg_1,1 1` indicates a single pixel at position 1.",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Semantic Segmentation & Multi-class Classification\n* **Objective:** \n    * Localize and classify surface defects in steel sheet images.\n    * Predict pixel-level segmentation masks for 4 defect classes (ClassId = 1,2,3,4).\n    * Each image may contain zero, one, or multiple types of defects.\n* **Key Points:**\n    * Defects must be segmented even if non-contiguous within an image.\n    * Submission requires predictions for all 4 classes per image (even if no defect exists for a class).\n    * Efficiency Prize was awarded for fastest inference among top solutions.",
      "Dataset Overview": "* **Data Type & Context:** \n    * High-frequency camera images of steel sheets during manufacturing.\n    * Images show various surface defects needing segmentation and classification.\n* **Data Files:**\n    * `train_images/` - Folder containing training images.\n    * `test_images/` - Folder containing test images for prediction.\n    * `train.csv` - Contains run-length encoded segmentation masks for training images.\n    * `sample_submission.csv` - Demonstrates submission format.\n* **Features:**\n    * Images are the primary input data.\n    * `train.csv` contains `ImageId`, `ClassId`, and `EncodedPixels` (RLE-format segmentation masks).",
      "Evaluation Metrics": "* **Primary Metric:** Mean Dice Coefficient (Sørensen-Dice Index)\n    * Measures pixel-wise agreement between predicted and ground truth masks.\n    * Formula: \\( \\frac{2 \\times |X \\cap Y|}{|X| + |Y|} \\), where:\n        * \\( X \\) = predicted pixels\n        * \\( Y \\) = ground truth pixels\n    * Special case: Score = 1 when both X and Y are empty.\n* **Submission Format:**\n    * Requires run-length encoded (RLE) pixel segments.\n    * Each row represents one `ImageId_ClassId` pair with space-delimited RLE values.\n    * Example: `004f40c73.jpg_1,1 1` indicates a single pixel at position 1."
    },
    "file_path": "kaggle_datasets/346/problem_summary.md"
  },
  "174": {
    "problem_id": "174",
    "title": "Predicting Hourly Rainfall from Polarimetric Radar Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Hourly Rainfall from Polarimetric Radar Data\n\n## Problem Description\n* **Problem Type:** Regression (Continuous Value Prediction)\n* **Objective:** Predict hourly rainfall totals (in mm) at specific gauge locations using polarimetric radar observations. The core challenge is to overcome the spatial and temporal variability of rainfall measurements by leveraging improved radar data.\n    * **Key Points:**\n        * Focuses on improving rainfall estimation compared to traditional Doppler radar methods.\n        * Uses polarimetric radar data, which provides dual-polarization measurements (horizontal and vertical pulses) for better precipitation characterization.\n        * Training data contains implausible gauge values (e.g., due to clogged gauges), requiring robustness to noise.\n        * Time and location information are censored, and data is shuffled to prevent temporal/geospatial ordering.\n\n## Dataset Overview\n* **Data Type:** Tabular data (time-series snapshots of radar observations aggregated hourly).\n* **Context:** Radar and gauge data collected in midwestern U.S. corn-growing regions (Apr-Aug 2014), with censored timestamps and shuffled order.\n* **Data Files:**\n    * `train.zip`: Radar observations and corresponding hourly gauge measurements (20 days of data).\n    * `test.zip`: Radar observations for remaining days (10-11 days/month) requiring hourly rainfall predictions.\n    * `sample_solution.zip`: Example submission format.\n* **Key Features:**\n    * Radar metrics: Reflectivity (`Ref`), correlation coefficient (`RhoHV`), differential reflectivity (`Zdr`), specific differential phase (`Kdp`).\n    * Neighborhood statistics (10th/50th/90th percentiles) for each metric in a 5x5 grid around gauges.\n    * `radardist_km`: Distance from radar to gauge.\n    * `minutes_past`: Time offset of radar snapshots within each hour.\n    * Target: `Expected` (hourly gauge observation in mm).\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE).\n    * **Rationale:** Chosen over RMSE to avoid over-penalizing large errors (per referenced meteorological research).\n    * **Calculation:** \n        * For each hourly gauge prediction, compute absolute difference between predicted and actual rainfall.\n        * Average these differences across all test samples.",
    "sections": {},
    "file_path": "kaggle_datasets/174/problem_summary.md"
  },
  "510": {
    "problem_id": "510",
    "title": "Image Classification of Stroke Blood Clot Origin",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Image Classification of Stroke Blood Clot Origin\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Medical Image Analysis)\n* **Objective:**  \n    * Classify the origin of blood clots in ischemic stroke patients using whole-slide digital pathology images into two subtypes:\n        * **CE (Cardioembolic)**\n        * **LAA (Large Artery Atherosclerosis)**\n    * Goal is to assist healthcare providers in determining stroke etiology for better therapeutic management and reduced risk of recurrent strokes.\n* **Key Points:**\n    * Focus on high-resolution pathology slides from real-world stroke patients.\n    * Aims to address challenges in AI-based stroke etiology classification due to unique data formats and large image sizes.\n    * Medical impact: Second-leading cause of death worldwide, with 23% of strokes being recurrent.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Whole-slide digital pathology images (TIFF format) of blood clots from ischemic stroke patients.\n    * Accompanying metadata includes patient IDs, medical center IDs, and image enumeration.\n* **Data Files:**\n    * `train/`: Folder of TIFF images for training (~1,000+ high-resolution slides)\n    * `test/`: Folder of TIFF images for testing (~280 images)\n    * `other/`: Supplemental images with unknown/other etiologies\n    * `train.csv`: Annotations with image IDs and labels (CE/LAA)\n    * `test.csv`: Test set annotations (without labels)\n    * `other.csv`: Supplemental image annotations\n* **Key Features:**\n    * High-resolution pathology slides (each image is several GB)\n    * Multi-patient, multi-center dataset\n    * Each patient may have multiple clot images\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Multi-class Logarithmic Loss\n    * Formula:  \n        `Log Loss = −(∑(w_i * (∑(y_ij/N_i) * ln(p_ij)))/∑w_i)`\n    * Components:\n        * N = number of images in class set\n        * M = number of classes (2: CE/LAA)\n        * y_ij = 1 if observation i belongs to class j (else 0)\n        * p_ij = predicted probability for class j\n        * Probabilities clipped to [1e-15, 1-1e-15] to avoid",
    "sections": {},
    "file_path": "kaggle_datasets/510/problem_summary.md"
  },
  "180": {
    "problem_id": "180",
    "title": "Predicting Stock Returns with Noisy Financial Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Stock Returns with Noisy Financial Data\n\n## Problem Description\n* **Problem Type**: Time Series Regression (Financial Forecasting)\n* **Objective**: Predict future stock returns using historical performance data and masked features. The task involves:\n  * Forecasting intraday returns (minutes 121-180) from partial intraday data (minutes 1-120)\n  * Predicting next-day (Ret_PlusOne) and two-day (Ret_PlusTwo) returns\n* **Key Points**:\n  * Data is intentionally noisy and non-stationary to mimic real-world conditions\n  * Each prediction window covers a 5-day period with partial observations\n  * Features are anonymized (Feature_1 to Feature_25) with undefined relationships to targets\n\n## Dataset Overview\n* **Data Type**: Tabular time series data of stock returns\n* **Context**: Arbitrary stocks across arbitrary 5-day windows in financial markets\n* **Data Files**:\n  * train.csv (features + full 180-min intraday returns + 2-day future returns)\n  * test.csv (features + only first 120-min intraday returns)\n  * sample_submission.csv\n* **Key Features**:\n  * 25 anonymized features (Feature_1 to Feature_25)\n  * Historical returns (Ret_MinusTwo, Ret_MinusOne)\n  * Partial intraday returns (Ret_2 to Ret_120 in test set)\n  * Weight columns (Weight_Intraday, Weight_Daily) for evaluation\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Mean Absolute Error (WMAE)\n* **Calculation**:\n  * WMAE = (1/n) * Σ(w_i * |y_i - ŷ_i|)\n  * Two weight types applied:\n    * Weight_Intraday: For intraday return predictions (Ret_121 to Ret_180)\n    * Weight_Daily: For daily return predictions (Ret_PlusOne and Ret_PlusTwo)\n  * Test set weights are undisclosed (only training weights provided)",
    "sections": {},
    "file_path": "kaggle_datasets/180/problem_summary.md"
  },
  "379": {
    "problem_id": "379",
    "title": "Fine-Grained Plant Species Classification from Herbarium Specimens",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Plant Species Classification from Herbarium Specimens\n\n## Problem Description\n* **Problem Type:** Multi-class Image Classification (Fine-Grained Visual Categorization)\n* **Objective:** \n    * Identify vascular plant species from herbarium specimen images\n    * Classify specimens among 32,000+ species with a long-tailed distribution\n    * Key application: Assist NYBG in classifying unnamed specimens in their collection\n* **Key Points:**\n    * Long-tailed dataset (some species have >100 samples while others have minimum 3)\n    * Focus on vascular plants (lycophytes, ferns, gymnosperms, flowering plants)\n    * Real-world application for botanical research and conservation\n    * Test set distribution differs from training (capped at 10 samples per species)\n\n## Dataset Overview\n* **Data Type & Context:**\n    * High-resolution JPEG images of pressed herbarium specimens\n    * Metadata includes taxonomic hierarchy (family, genus, species) and collection regions\n    * Text/barcode labels blurred to prevent information leakage\n* **Data Files:**\n    * `train/` folder with subfolders containing specimen images\n    * `test/` folder with subfolders containing test images\n    * `train/metadata.json` with annotations, categories, and regions\n    * `test/metadata.json` (without annotations)\n    * `sample_submission.csv` for submission format\n* **Key Features:**\n    * Variable image dimensions (max 1000px in larger dimension)\n    * COCO-format metadata with taxonomic hierarchy\n    * Region information about specimen collection locations\n\n## Evaluation Metrics\n* **Primary Metric:** Macro F1-Score\n    * Calculated per species then averaged\n    * Formula: F1 = 2 * (precision * recall) / (precision + recall)\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n* **Key Characteristics:**\n    * Macro averaging gives equal weight to all species regardless of frequency\n    * Particularly suitable for imbalanced, long-tailed distribution\n    * Evaluates both precision and recall simultaneously",
    "sections": {},
    "file_path": "kaggle_datasets/379/problem_summary.md"
  },
  "20": {
    "problem_id": "20",
    "title": "Predicting Bodily Injury Liability Insurance Claims",
    "problem_type": "Regression (predicting claim payment amounts).",
    "objective": "Predict the dollar amount of Bodily Injury Liability Insurance claim payments based on characteristics of the insured customer’s vehicle. The goal is to improve risk assessment by accurately estimating claim likelihood and severity using vehicle-related factors.",
    "evaluation_metric": null,
    "full_content": "# Predicting Bodily Injury Liability Insurance Claims\n\n**Problem Description:**\n*   **Problem Type:** Regression (predicting claim payment amounts).\n*   **Objective:** Predict the dollar amount of Bodily Injury Liability Insurance claim payments based on characteristics of the insured customer’s vehicle. The goal is to improve risk assessment by accurately estimating claim likelihood and severity using vehicle-related factors.\n    *   **Key Points:**\n        *   Focuses on **Bodily Injury Liability Insurance**, which covers injuries/deaths of others caused by the insured.\n        *   Claims data has been adjusted to control for known **non-vehicle effects** (e.g., driver behavior, location), though some non-vehicle policy characteristics are included as potential interaction terms.\n        *   Training data spans 2005–2007; test data is split into public (2008) and private (2009) leaderboard evaluation sets.\n\n**Dataset Overview:**\n*   **Data Type & Context:** Tabular data containing yearly records of insured vehicles, with anonymized vehicle/policy features and adjusted claim payment amounts.\n    *   **Data Files:**\n        *   `train_set.zip` (2005–2007 data).\n        *   `test_set.zip` (2008–2009 data).\n        *   `data_dictionary.txt` (feature descriptions).\n    *   **Key Features:**\n        *   `Household_ID` (for tracking households across years).\n        *   `Vehicle` identifier (not persistent year-to-year).\n        *   Vehicle attributes: make, model, submodel (coded), model year.\n        *   Miscellaneous policy/vehicle characteristics (anonymized).\n        *   Missing values coded as \"?\" (present only in training data).\n\n**Evaluation Metrics:**\n*   **Primary Metric:** Normalized Gini coefficient.\n    *   **Calculation Logic:**\n        1.  Predictions are sorted from highest to lowest.\n        2.  For each percentile of sorted predictions, compute the cumulative percentage of **actual observed loss** captured.\n        3.  The Gini coefficient is the area between this curve and the \"null model\" line (where loss accumulates linearly).\n        4.  Normalization: Divide by the Gini coefficient of a perfect model (max achievable area).\n    *   **Interpretation:** Higher values indicate better ranking of high-risk claims. Only the **order of predictions** affects the score.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Regression (predicting claim payment amounts).\n*   **Objective:** Predict the dollar amount of Bodily Injury Liability Insurance claim payments based on characteristics of the insured customer’s vehicle. The goal is to improve risk assessment by accurately estimating claim likelihood and severity using vehicle-related factors.\n    *   **Key Points:**\n        *   Focuses on **Bodily Injury Liability Insurance**, which covers injuries/deaths of others caused by the insured.\n        *   Claims data has been adjusted to control for known **non-vehicle effects** (e.g., driver behavior, location), though some non-vehicle policy characteristics are included as potential interaction terms.\n        *   Training data spans 2005–2007; test data is split into public (2008) and private (2009) leaderboard evaluation sets.",
      "Dataset Overview": "*   **Data Type & Context:** Tabular data containing yearly records of insured vehicles, with anonymized vehicle/policy features and adjusted claim payment amounts.\n    *   **Data Files:**\n        *   `train_set.zip` (2005–2007 data).\n        *   `test_set.zip` (2008–2009 data).\n        *   `data_dictionary.txt` (feature descriptions).\n    *   **Key Features:**\n        *   `Household_ID` (for tracking households across years).\n        *   `Vehicle` identifier (not persistent year-to-year).\n        *   Vehicle attributes: make, model, submodel (coded), model year.\n        *   Miscellaneous policy/vehicle characteristics (anonymized).\n        *   Missing values coded as \"?\" (present only in training data).",
      "Evaluation Metrics": "*   **Primary Metric:** Normalized Gini coefficient.\n    *   **Calculation Logic:**\n        1.  Predictions are sorted from highest to lowest.\n        2.  For each percentile of sorted predictions, compute the cumulative percentage of **actual observed loss** captured.\n        3.  The Gini coefficient is the area between this curve and the \"null model\" line (where loss accumulates linearly).\n        4.  Normalization: Divide by the Gini coefficient of a perfect model (max achievable area).\n    *   **Interpretation:** Higher values indicate better ranking of high-risk claims. Only the **order of predictions** affects the score."
    },
    "file_path": "kaggle_datasets/20/problem_summary.md"
  },
  "187": {
    "problem_id": "187",
    "title": "Automated Cardiac MRI Volume Measurement for Heart Disease Diagnosis",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Automated Cardiac MRI Volume Measurement for Heart Disease Diagnosis\n\n## Problem Description\n* **Problem Type:** Computer Vision - Medical Image Analysis (Regression on Volumetric Data)\n* **Objective:** \n    * Develop an algorithm to automatically measure end-systolic and end-diastolic volumes from cardiac MRI scans to calculate ejection fraction (EF), a key indicator of heart disease.\n    * Replace the manual cardiologist process (taking ~20 minutes per case) with an automated solution.\n* **Key Points:**\n    * Must predict volumes at two cardiac cycle points: maximum contraction (systole) and maximum expansion (diastole)\n    * Requires analysis of dynamic MRI sequences (~30 frames per cardiac cycle)\n    * Must handle anatomical variations, image quality differences, and acquisition artifacts\n    * Clinical application: Early diagnosis of cardiovascular disease through efficient EF calculation\n\n## Dataset Overview\n* **Data Type:** Medical Imaging (DICOM format cardiac MRI scans)\n* **Context:** \n    * 2D cine MRI images from 1,000+ patients (largest released cardiac MRI dataset at time)\n    * Collected by National Institutes of Health and Children's National Medical Center\n    * Includes normal and abnormal cardiac function cases across age ranges\n* **Data Files:**\n    * `train.zip`: DICOM images with ground truth volumes (train.csv)\n    * `validate.zip`: Validation set for stage 1 evaluation\n    * `test.zip`: Final test set (released in stage 2)\n    * Sample submission files for both stages\n* **Key Features:**\n    * Short-axis view images (prefix \"sax_\") - primary view for ventricle measurement\n    * ~30 temporal frames per cardiac cycle\n    * Multi-slice acquisitions (imperfect slice-to-slice registration)\n    * Varied image resolutions and quality\n\n## Evaluation Metrics\n* **Primary Metric:** Continuous Ranked Probability Score (CRPS)\n    * Requires predicting full cumulative distribution functions (CDFs) for both systolic and diastolic volumes (600 probability values each from 0-599mL)\n* **CRPS Calculation:**\n    * For each case, compares predicted CDF (P) against actual volume (V) using Heaviside step function\n    * Formula: \n    ```\n    C = 1/(600N) Σ_m Σ_n [P(y≤n) - H(n-V_m)]²\n    ```\n    * N = number of test cases",
    "sections": {},
    "file_path": "kaggle_datasets/187/problem_summary.md"
  },
  "18": {
    "problem_id": "18",
    "title": "Predicting Wikipedia Editor Participation",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Wikipedia Editor Participation\n\n## Problem Description\n* **Problem Type**: Regression (Time Series Forecasting)\n* **Objective**: Predict the number of edits a Wikipedia editor will make in a 5-month future period (September 1, 2010 - February 1, 2011) based on their historical edit patterns from January 2001 - August 2010.\n* **Key Points**:\n  * Focuses on understanding factors that influence editor retention and activity levels\n  * Aims to help Wikimedia Foundation forecast long-term editing trends\n  * Models must be reusable by Wikimedia for analytics purposes\n  * Performance/runtime tradeoff considered (faster models preferred)\n\n## Dataset Overview\n* **Data Type**: Tabular time-series data of Wikipedia editor activities\n* **Context**: Sampled edit history from English Wikipedia (2001-2010)\n* **Data Files**:\n  * `training.tsv`: Primary dataset with editor histories\n  * `comments.tsv`: Revision comments\n  * `articles.tsv`: Article metadata\n  * `namespaces.tsv`, `categories.tsv`: Reference tables\n* **Key Features**:\n  * Editor IDs and revision timestamps\n  * Article IDs and namespaces (0-5 only)\n  * Revision metadata (size changes, revert status)\n  * Article categories (featured articles, lists, etc.)\n  * Edit content hashes (MD5)\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Logarithmic Error (RMSLE)\n* **Calculation**:\n  * 𝜖 = √[1/n Σ(log(pᵢ+1) - log(aᵢ+1))²]\n  * Where:\n    * pᵢ = predicted edits for editor i\n    * aᵢ = actual edits for editor i\n    * n = total editors in evaluation set\n  * Uses natural logarithm (base e)\n  * +1 adjustment handles zero-value cases\n* **Baseline**: WMF's internal model scored 1.47708 RMSLE (must be beaten)",
    "sections": {},
    "file_path": "kaggle_datasets/18/problem_summary.md"
  },
  "517": {
    "problem_id": "517",
    "title": "Predicting Multimodal Single-Cell Integration of DNA, RNA, and Protein Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Multimodal Single-Cell Integration of DNA, RNA, and Protein Data\n\n## Problem Description\n* **Problem Type**: Multimodal Regression (Tabular Data)\n* **Objective**: Predict how DNA, RNA, and protein measurements co-vary in single cells as bone marrow stem cells develop into mature blood cells. Specifically:\n  * For Multiome samples: Predict gene expression (RNA) from chromatin accessibility (DNA)\n  * For CITEseq samples: Predict surface protein levels from gene expression (RNA)\n* **Key Points**:\n  * Test data comes from a later time point than any training data (temporal generalization challenge)\n  * Data comes from CD34+ hematopoietic stem/progenitor cells (HSPCs) from 4 human donors across 5 time points\n  * Must account for different feature spaces between modalities\n  * Must handle sparse, noisy single-cell measurements and batch effects\n\n## Dataset Overview\n* **Data Type**: Tabular single-cell multiomics data (HDF5 format)\n* **Context**: Measurements from 300,000-cell time course of blood cell differentiation\n* **Key Files**:\n  * `train/test_multi_inputs.h5`: Chromatin accessibility (DNA) data\n  * `train_multi_targets.h5`: Corresponding gene expression (RNA) data\n  * `train/test_cite_inputs.h5`: Gene expression (RNA) data\n  * `train_cite_targets.h5`: Corresponding surface protein data\n  * `metadata.csv`: Cell identifiers, donor, day, technology, and cell type\n* **Key Features**:\n  * Multiome: TF-IDF transformed ATAC-seq peak counts (chromatin accessibility)\n  * CITEseq: Library-size normalized and log1p transformed RNA counts\n  * Targets: Normalized gene expression/protein levels\n  * 7 annotated cell types in differentiation hierarchy\n\n## Evaluation Metrics\n* **Primary Metric**: Pearson correlation coefficient (averaged across samples)\n  * For Multiome: Correlation between predicted and true gene expressions\n  * For CITEseq: Correlation between predicted and true protein levels\n  * Overall score is average of all sample correlations\n  * Constant predictions score -1.0 for that sample\n* **Scoring Notes**:\n  * Only a subset of Multiome predictions are evaluated (30% of rows, 15% varying columns per row)\n  * All C",
    "sections": {},
    "file_path": "kaggle_datasets/517/problem_summary.md"
  },
  "173": {
    "problem_id": "173",
    "title": "Predicting Western Australia Rental Prices",
    "problem_type": "Regression",
    "objective": "Predict the weekly market rental value (REN_BASE_RENT) for residential properties across Western Australia. The goal is to improve upon existing automated valuation models by incorporating diverse factors affecting rental prices.",
    "evaluation_metric": null,
    "full_content": "# Predicting Western Australia Rental Prices\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the weekly market rental value (REN_BASE_RENT) for residential properties across Western Australia. The goal is to improve upon existing automated valuation models by incorporating diverse factors affecting rental prices.\n* **Key Points:**\n  * Rental prices are influenced by both objective factors (proximity to amenities) and subjective factors (aesthetic value).\n  * The Western Australia rental market is particularly challenging due to:\n    * Varied landscape\n    * Sparse, widely distributed population\n    * Lack of comparable pricing information in rural areas\n  * Real-world data quality challenges expected (imperfect joins, missing values, nonsensical entries).\n\n**Dataset Overview:**\n* **Data Type & Context:** Relational database containing:\n  * Tabular property data (1990-2015)\n  * Geographic land information\n  * Administrative zoning data\n* **Data Files:**\n  * Core files: train.csv, test.csv\n  * Property tables: valuation_entities*.csv (3 files)\n  * Land tables: land*.csv (6 files including pins, restrictions, urban)\n  * Joining table: land_valuation_key.csv\n* **Key Features:**\n  * Property characteristics (rooms, size, pools etc.)\n  * Land information (zoning, restrictions, urban features)\n  * Geographic boundaries\n  * Administrative classifications\n  * Test set stratified by postcode to ensure rural representation\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n* **Calculation:**\n  * `RMSLE = sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))^2)`\n  * Where:\n    * n = number of test rentals\n    * p = predicted price\n    * a = actual price\n    * log = natural logarithm\n  * +1 terms prevent undefined values for zero prices\n  * Logarithmic transformation emphasizes relative errors over absolute differences",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the weekly market rental value (REN_BASE_RENT) for residential properties across Western Australia. The goal is to improve upon existing automated valuation models by incorporating diverse factors affecting rental prices.\n* **Key Points:**\n  * Rental prices are influenced by both objective factors (proximity to amenities) and subjective factors (aesthetic value).\n  * The Western Australia rental market is particularly challenging due to:\n    * Varied landscape\n    * Sparse, widely distributed population\n    * Lack of comparable pricing information in rural areas\n  * Real-world data quality challenges expected (imperfect joins, missing values, nonsensical entries).",
      "Dataset Overview": "* **Data Type & Context:** Relational database containing:\n  * Tabular property data (1990-2015)\n  * Geographic land information\n  * Administrative zoning data\n* **Data Files:**\n  * Core files: train.csv, test.csv\n  * Property tables: valuation_entities*.csv (3 files)\n  * Land tables: land*.csv (6 files including pins, restrictions, urban)\n  * Joining table: land_valuation_key.csv\n* **Key Features:**\n  * Property characteristics (rooms, size, pools etc.)\n  * Land information (zoning, restrictions, urban features)\n  * Geographic boundaries\n  * Administrative classifications\n  * Test set stratified by postcode to ensure rural representation",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n* **Calculation:**\n  * `RMSLE = sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))^2)`\n  * Where:\n    * n = number of test rentals\n    * p = predicted price\n    * a = actual price\n    * log = natural logarithm\n  * +1 terms prevent undefined values for zero prices\n  * Logarithmic transformation emphasizes relative errors over absolute differences"
    },
    "file_path": "kaggle_datasets/173/problem_summary.md"
  },
  "341": {
    "problem_id": "341",
    "title": "Open Images 2019 - Object Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Open Images 2019 - Object Detection\n\n## Problem Description\n* **Problem Type:** Computer Vision - Object Detection\n* **Objective:**  \n    * Predict tight bounding boxes around object instances in diverse and complex images.\n    * The task involves detecting objects across 500 categories, with an average of 7 objects per image.\n* **Key Points:**\n    * The dataset includes 12.2M manually annotated bounding boxes for high accuracy.\n    * Images are highly diverse, often containing complex scenes.\n    * Part of a larger challenge with 3 tracks (Object Detection, Visual Relationship Detection, Instance Segmentation).\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Image data with bounding box annotations for object detection.\n    * Contains 1.7M training images and 99,999 test images.\n* **Data Files:**\n    * `test.zip` - Test set images.\n    * `sample_submission.csv` - Submission template.\n* **Features:**\n    * Images are in JPG format.\n    * Annotations include bounding box coordinates (`XMin`, `YMin`, `XMax`, `YMax`) and class labels.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Mean Average Precision (mAP)  \n    * Modified to align with the Open Images annotation process.\n    * Calculated as the average of per-class Average Precision (AP) across 500 classes.\n* **Components:**\n    * AP is computed for each class independently.\n    * Final score is the mean of all per-class APs.\n    * Implemented using TensorFlow Object Detection API.",
    "sections": {},
    "file_path": "kaggle_datasets/341/problem_summary.md"
  },
  "27": {
    "problem_id": "27",
    "title": "Predicting Student Test Question Accuracy",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Student Test Question Accuracy\n\n## Problem Description\n- **Problem Type:** Binary Classification  \n- **Objective:** Predict whether a student will answer a test question correctly (binary outcome: 0 or 1) based on their interaction history and question metadata. The goal is to improve personalized student evaluation by identifying knowledge gaps.  \n- **Key Points:**  \n  - Focus on standardized tests (GMAT, SAT, ACT).  \n  - Test data distribution differs from training data:  \n    - No \"skipped\" or \"timeout\" outcomes in test set.  \n    - Biased toward users with more training questions and later answers.  \n  - Questions are labeled with test areas, but latent structure may exist.  \n\n## Dataset Overview  \n- **Data Type:** Tabular data of student-question interactions with timestamps and categorical features.  \n- **Data Files:**  \n  - `training.csv.gz`: Historical student answers with labels.  \n  - `test.csv.gz`: Questions to predict (no `correct` or `outcome` fields).  \n  - `valid_training.csv.gz`/`valid_test.csv.gz`: Suggested validation split.  \n  - `category_labels.csv`: Mappings for categorical codes (e.g., question types, test groups).  \n- **Key Features:**  \n  - **Student/Question IDs:** Anonymized identifiers (`user_id`, `question_id`).  \n  - **Metadata:** Question type (multiple-choice/free response), test group (ACT/GMAT/SAT), tags, timestamps.  \n  - **Behavioral:** Response time (`round_started_at`, `answered_at`), game context (`num_players`, `game_type`).  \n\n## Evaluation Metrics  \n- **Primary Metric:** Capped Binomial Deviance  \n  - **Components:**  \n    - Penalizes probabilistic predictions deviating from true binary outcomes.  \n    - Caps extreme values to limit influence of high-confidence errors.  \n    - Detailed formula: [Kaggle evaluation link](http://www.kaggle.com/c/ChessRatings2/Details/Evaluation).",
    "sections": {},
    "file_path": "kaggle_datasets/27/problem_summary.md"
  },
  "9": {
    "problem_id": "9",
    "title": "Predicting Edges in an Online Social Network",
    "problem_type": "Binary Classification (Edge Prediction in Graphs)",
    "objective": "Predict whether potential edges (connections) in a social network are true (1) or false (0) based on given training data of existing edges. The goal is to develop algorithms that could power friend suggestion systems in online social networks.",
    "evaluation_metric": null,
    "full_content": "# Predicting Edges in an Online Social Network\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Edge Prediction in Graphs)\n* **Objective:** Predict whether potential edges (connections) in a social network are true (1) or false (0) based on given training data of existing edges. The goal is to develop algorithms that could power friend suggestion systems in online social networks.\n* **Key Points:**\n  * Focus on predicting connections among 8,960 test edges (balanced with 4,480 true and 4,480 false edges).\n  * The social network is represented as a directed graph where relationships are not necessarily mutual.\n  * Participants are encouraged to explore graph-based techniques and explain their approaches.\n\n**Dataset Overview:**\n* **Data Type:** Graph/Tabular data representing a directed social network (nodes = users, edges = connections).\n* **Data Files:**\n  * `social_train.zip`: Contains 7,237,983 training edges (outbound node, inbound node pairs).\n  * `social_test.txt`: Contains 8,960 test edges to predict.\n  * `sample_submission.csv`: Example submission file format.\n* **Features:**\n  * Nodes are anonymized with IDs ranging from 1 to 1,133,547.\n  * Training data includes 37,689 outbound nodes and 1,133,518 inbound nodes (most outbound nodes are also inbound nodes).\n  * The dataset was sampled to ensure a roughly \"closed\" network universe.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the Receiver Operating Characteristic Curve (AUC)\n* **Metric Details:**\n  * AUC evaluates model performance across all classification thresholds.\n  * Advantages:\n    * Insensitive to class imbalance (balanced test set in this case).\n    * Measures how well the model separates the two classes (true vs. false edges).\n  * Interpretation:\n    * 1.0 = Perfect classifier\n    * 0.5 = Random guessing\n    * >0.5 = Better than random\n  * Calculation based on:\n    * True Positive Rate (Recall) = True Positives / Total Positives\n    * False Positive Rate = False Positives / Total Negatives\n    * AUC is the area under the curve plotting TPR vs. FPR at all thresholds.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Edge Prediction in Graphs)\n* **Objective:** Predict whether potential edges (connections) in a social network are true (1) or false (0) based on given training data of existing edges. The goal is to develop algorithms that could power friend suggestion systems in online social networks.\n* **Key Points:**\n  * Focus on predicting connections among 8,960 test edges (balanced with 4,480 true and 4,480 false edges).\n  * The social network is represented as a directed graph where relationships are not necessarily mutual.\n  * Participants are encouraged to explore graph-based techniques and explain their approaches.",
      "Dataset Overview": "* **Data Type:** Graph/Tabular data representing a directed social network (nodes = users, edges = connections).\n* **Data Files:**\n  * `social_train.zip`: Contains 7,237,983 training edges (outbound node, inbound node pairs).\n  * `social_test.txt`: Contains 8,960 test edges to predict.\n  * `sample_submission.csv`: Example submission file format.\n* **Features:**\n  * Nodes are anonymized with IDs ranging from 1 to 1,133,547.\n  * Training data includes 37,689 outbound nodes and 1,133,518 inbound nodes (most outbound nodes are also inbound nodes).\n  * The dataset was sampled to ensure a roughly \"closed\" network universe.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the Receiver Operating Characteristic Curve (AUC)\n* **Metric Details:**\n  * AUC evaluates model performance across all classification thresholds.\n  * Advantages:\n    * Insensitive to class imbalance (balanced test set in this case).\n    * Measures how well the model separates the two classes (true vs. false edges).\n  * Interpretation:\n    * 1.0 = Perfect classifier\n    * 0.5 = Random guessing\n    * >0.5 = Better than random\n  * Calculation based on:\n    * True Positive Rate (Recall) = True Positives / Total Positives\n    * False Positive Rate = False Positives / Total Negatives\n    * AUC is the area under the curve plotting TPR vs. FPR at all thresholds."
    },
    "file_path": "kaggle_datasets/9/problem_summary.md"
  },
  "528": {
    "problem_id": "528",
    "title": "Binary Classification with a Tabular Stroke Prediction Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Tabular Stroke Prediction Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict the probability of a stroke occurrence based on patient characteristics. The task is to classify whether a patient is likely to have a stroke (binary target: `stroke`).  \n* **Key Points:**  \n  * Dataset is synthetically generated from a real-world stroke prediction dataset, with similar but not identical feature distributions.  \n  * Participants are encouraged to explore differences between synthetic and original data for potential performance improvements.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular data containing anonymized patient health metrics for stroke prediction.  \n* **Data Files:**  \n  * `train.csv` - Training data with target label (`stroke`).  \n  * `test.csv` - Test data (no target label).  \n  * `sample_submission.csv` - Example submission file in required format.  \n* **Features:** Includes patient attributes (e.g., demographics, medical history) relevant to stroke risk. Exact features are anonymized but derived from real-world clinical data.  \n\n## Evaluation Metrics  \n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).  \n* **Components:**  \n  * Measures the model's ability to distinguish between positive (stroke) and negative (no stroke) cases.  \n  * Higher AUC indicates better classification performance (range: 0.5 to 1.0).  \n  * Predictions must be probabilities (not hard labels) for the target class.",
    "sections": {},
    "file_path": "kaggle_datasets/528/problem_summary.md"
  },
  "383": {
    "problem_id": "383",
    "title": "Abstraction and Reasoning Challenge",
    "problem_type": "Abstract Reasoning / Few-Shot Learning",
    "objective": "Develop an AI capable of solving novel reasoning tasks by generalizing from only 3-5 demonstration input/output pairs per task. The goal is to predict the correct output grid(s) for unseen test input grid(s) by identifying underlying patterns or rules.",
    "evaluation_metric": null,
    "full_content": "# Abstraction and Reasoning Challenge\n\n**Problem Description:**\n* **Problem Type:** Abstract Reasoning / Few-Shot Learning\n* **Objective:** Develop an AI capable of solving novel reasoning tasks by generalizing from only 3-5 demonstration input/output pairs per task. The goal is to predict the correct output grid(s) for unseen test input grid(s) by identifying underlying patterns or rules.\n    * **Key Points:**\n        * Tasks require human-like abstraction and reasoning abilities.\n        * Each task is self-contained with minimal training examples (few-shot learning).\n        * Output must exactly match the ground truth (all cells correct).\n        * Grids are small (1x1 to 30x30) with integer values (0-9) representing colors.\n\n**Dataset Overview:**\n* **Data Type:** JSON files containing 2D grids (lists of lists) representing visual reasoning tasks.\n* **Context:** Tasks involve transforming input grids to output grids based on abstract rules (e.g., pattern completion, object manipulation).\n* **Data Files:**\n    * `training/`: 400 tasks with full input/output pairs for development.\n    * `evaluation/`: 400 tasks with full pairs for validation.\n    * `test/`: 100 hidden tasks (only inputs provided) for scoring.\n* **Features:**\n    * Each task contains:\n        * `train`: 3-5 demonstration input/output pairs.\n        * `test`: 1-2 input grids requiring output prediction.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Top-3 Error Rate (MeanBestErrorAtK)\n    * **Components:**\n        * For each test input, submit up to 3 output predictions.\n        * Error is 0 if any prediction exactly matches the ground truth, else 1.\n        * Final score = average error across all test inputs.\n    * **Prediction Format:**\n        * Output grids must be flattened to strings (e.g., `[[1,2],[3,4]]` → `|12|34|`).\n        * Multiple predictions per task are space-delimited.",
    "sections": {
      "Problem Description": "* **Problem Type:** Abstract Reasoning / Few-Shot Learning\n* **Objective:** Develop an AI capable of solving novel reasoning tasks by generalizing from only 3-5 demonstration input/output pairs per task. The goal is to predict the correct output grid(s) for unseen test input grid(s) by identifying underlying patterns or rules.\n    * **Key Points:**\n        * Tasks require human-like abstraction and reasoning abilities.\n        * Each task is self-contained with minimal training examples (few-shot learning).\n        * Output must exactly match the ground truth (all cells correct).\n        * Grids are small (1x1 to 30x30) with integer values (0-9) representing colors.",
      "Dataset Overview": "* **Data Type:** JSON files containing 2D grids (lists of lists) representing visual reasoning tasks.\n* **Context:** Tasks involve transforming input grids to output grids based on abstract rules (e.g., pattern completion, object manipulation).\n* **Data Files:**\n    * `training/`: 400 tasks with full input/output pairs for development.\n    * `evaluation/`: 400 tasks with full pairs for validation.\n    * `test/`: 100 hidden tasks (only inputs provided) for scoring.\n* **Features:**\n    * Each task contains:\n        * `train`: 3-5 demonstration input/output pairs.\n        * `test`: 1-2 input grids requiring output prediction.",
      "Evaluation Metrics": "* **Evaluation Metric:** Top-3 Error Rate (MeanBestErrorAtK)\n    * **Components:**\n        * For each test input, submit up to 3 output predictions.\n        * Error is 0 if any prediction exactly matches the ground truth, else 1.\n        * Final score = average error across all test inputs.\n    * **Prediction Format:**\n        * Output grids must be flattened to strings (e.g., `[[1,2],[3,4]]` → `|12|34|`).\n        * Multiple predictions per task are space-delimited."
    },
    "file_path": "kaggle_datasets/383/problem_summary.md"
  },
  "145": {
    "problem_id": "145",
    "title": "Predicting Popularity of New York Times Blog Articles",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Popularity of New York Times Blog Articles\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict whether a New York Times blog article will receive 25 or more comments (popular) based on features available at the time of publication.\n  - **Key Points:**\n    - The task is to help prioritize article placement by predicting popularity.\n    - Focuses on articles published between September-December 2014.\n    - Popularity is defined as a binary outcome (1 if ≥25 comments, 0 otherwise).\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing metadata and text features from New York Times blog articles.\n- **Data Files:**\n  - `NYTimesBlogTrain.csv` (6,532 articles)\n  - `NYTimesBlogTest.csv` (1,870 articles)\n  - `SampleSubmission.csv` (example submission format)\n- **Key Features:**\n  - Categorical: NewsDesk, SectionName, SubsectionName\n  - Text: Headline, Snippet, Abstract\n  - Numerical: WordCount\n  - Temporal: PubDate (publication timestamp)\n  - Target: Popular (binary label)\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n  - **Interpretation:** \n    - Measures model's ability to distinguish between popular and non-popular articles\n    - Represents probability that a random positive example ranks higher than a random negative example\n    - Range: 0.5 (random) to 1.0 (perfect)\n  - **Advantage:** Less sensitive to class imbalance than accuracy",
    "sections": {},
    "file_path": "kaggle_datasets/145/problem_summary.md"
  },
  "377": {
    "problem_id": "377",
    "title": "Flower Classification with TPUs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Flower Classification with TPUs\n\n## Problem Description\n* **Problem Type:** Multi-class Image Classification\n* **Objective:**  \n    * Build a machine learning model to classify images of flowers into one of 104 distinct categories.  \n    * The competition specifically encourages the use of Tensor Processing Units (TPUs) for model training and inference.  \n* **Key Points:**  \n    * Focuses on leveraging TPU hardware acceleration for deep learning tasks.  \n    * Classes vary in granularity (some are broad like \"wild roses,\" others are specific like \"pink primroses\").  \n    * Submissions must use TPUs to be eligible for prizes.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Image data (flowers) stored in TFRecord format, optimized for TensorFlow/TPU processing.  \n    * Images are sourced from five public datasets, resized to multiple resolutions (192x192, 224x224, 331x331, 512x512).  \n* **Data Files:**  \n    * `train/*.tfrec`: Training images with labels.  \n    * `val/*.tfrec`: Pre-split validation set (stratified by label).  \n    * `test/*.tfrec`: Test images (no labels).  \n    * `sample_submission.csv`: Submission template with `id` and predicted `label` columns.  \n* **Features:**  \n    * Each record contains:  \n        * `id`: Unique image identifier.  \n        * `label`: Class index (0–103) for training/validation data.  \n        * `img`: Pixel array of the flower image.  \n\n## Evaluation Metrics\n* **Primary Metric:** Macro F1-Score  \n    * **Calculation:**  \n        * F1-score is computed per class as the harmonic mean of precision and recall:  \n            * Precision = TP / (TP + FP)  \n            * Recall = TP / (TP + FN)  \n            * F1 = 2 * (Precision * Recall) / (Precision + Recall)  \n        * Final score is the **unweighted average** of F1-scores across all 104 classes.  \n    * **Nuance:** \"Macro\" averaging ensures equal weight for all classes, regardless of class imbalance.",
    "sections": {},
    "file_path": "kaggle_datasets/377/problem_summary.md"
  },
  "521": {
    "problem_id": "521",
    "title": "Scrabble Player Rating Prediction",
    "problem_type": "Regression",
    "objective": "Predict the pre-game ratings of human Scrabble players based on their gameplay data against bots. The goal is to estimate a player's skill level (rating) before a specific game was played, using metadata about the game and turn-by-turn gameplay details.",
    "evaluation_metric": null,
    "full_content": "# Scrabble Player Rating Prediction\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the pre-game ratings of human Scrabble players based on their gameplay data against bots. The goal is to estimate a player's skill level (rating) before a specific game was played, using metadata about the game and turn-by-turn gameplay details.\n    * **Key Points:**\n        * Ratings are specific to game variants (lexicon/time control combinations).\n        * The model must generalize to new players not seen in the training set (no overlap between train/test human players).\n        * Predictions are based on gameplay performance against three distinct bot difficulty levels.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing Scrabble gameplay metadata and turn-by-turn actions from Woogles.io platform.\n* **Data Files:**\n    * `games.csv`: Game metadata (time controls, winner, lexicon, duration, etc.)\n    * `turns.csv`: Detailed turn data for all games (racks, moves, scores, turn types)\n    * `train.csv`: Human player scores and pre-game ratings (for training)\n    * `test.csv`: Human player scores with missing ratings (for prediction)\n    * `sample_submission.csv`: Submission format example\n* **Key Features:**\n    * Gameplay features: Turn actions, points scored, racks, move types\n    * Game context: Time controls, lexicons, game duration\n    * Player performance: Final scores, rating history (in training set)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * Measures the square root of the average squared differences between predicted and actual ratings\n        * Lower values indicate better performance (perfect prediction = 0)\n        * Sensitive to large errors due to squaring of differences",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the pre-game ratings of human Scrabble players based on their gameplay data against bots. The goal is to estimate a player's skill level (rating) before a specific game was played, using metadata about the game and turn-by-turn gameplay details.\n    * **Key Points:**\n        * Ratings are specific to game variants (lexicon/time control combinations).\n        * The model must generalize to new players not seen in the training set (no overlap between train/test human players).\n        * Predictions are based on gameplay performance against three distinct bot difficulty levels.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing Scrabble gameplay metadata and turn-by-turn actions from Woogles.io platform.\n* **Data Files:**\n    * `games.csv`: Game metadata (time controls, winner, lexicon, duration, etc.)\n    * `turns.csv`: Detailed turn data for all games (racks, moves, scores, turn types)\n    * `train.csv`: Human player scores and pre-game ratings (for training)\n    * `test.csv`: Human player scores with missing ratings (for prediction)\n    * `sample_submission.csv`: Submission format example\n* **Key Features:**\n    * Gameplay features: Turn actions, points scored, racks, move types\n    * Game context: Time controls, lexicons, game duration\n    * Player performance: Final scores, rating history (in training set)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:**\n        * Measures the square root of the average squared differences between predicted and actual ratings\n        * Lower values indicate better performance (perfect prediction = 0)\n        * Sensitive to large errors due to squaring of differences"
    },
    "file_path": "kaggle_datasets/521/problem_summary.md"
  },
  "11": {
    "problem_id": "11",
    "title": "Time Series Forecasting for Freeway Travel Time Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Time Series Forecasting for Freeway Travel Time Prediction\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict travel times on Sydney's M4 freeway at multiple future time horizons (15 mins to 24 hours ahead) using historical travel time observations. The goal is to improve road safety, efficiency, and commuter decision-making.\n* **Key Points:**\n  * Multi-horizon prediction required for 10 different time intervals\n  * Must produce a general algorithm that can generate predictions from any input timestamp\n  * Focus on improving functionality for NSW government's live traffic website\n  * Results must be replicable by the Roads and Traffic Authority (RTA)\n\n## Dataset Overview\n* **Data Type & Context:** Time series data of freeway travel times collected from road loops at 3-minute intervals\n* **Data Files:**\n  * RTAData.csv (primary dataset, March-July 2010 with prediction gaps August-November 2010)\n  * RTAHistorical.csv (additional data from November 2008-February 2010)\n  * RTAError.csv (loop sensor accuracy indicators)\n  * RouteLengthApprox.csv (approximate route lengths)\n  * m4-map.pdf (freeway layout)\n  * sampleEntry.csv (submission format example)\n* **Features:**\n  * Route IDs (40010-40150 westbound, 41010-41160 eastbound)\n  * Timestamps (3-minute intervals)\n  * Travel times (in deciseconds)\n  * Sensor error proportions\n  * Route length approximations\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Error (RMSE) across all routes and prediction horizons\n* **Evaluation Details:**\n  * Predictions evaluated at 29 cut-off times\n  * Each submission requires 290 forecasts (10 horizons × 29 cut-offs)\n  * Data withheld for 18 hours after last forecast to prevent lookahead\n  * Final score based on best of five nominated submissions",
    "sections": {},
    "file_path": "kaggle_datasets/11/problem_summary.md"
  },
  "348": {
    "problem_id": "348",
    "title": "RSNA Intracranial Hemorrhage Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# RSNA Intracranial Hemorrhage Detection\n\n## Problem Description\n* **Problem Type**: Multi-label Classification (Computer Vision - Medical Imaging)\n* **Objective**:  \n  * Develop an algorithm to detect acute intracranial hemorrhage (ICH) and classify its subtypes from head CT scans.  \n  * Predict the presence/absence of hemorrhage (`any` label) and 5 specific subtypes:  \n    * Epidural  \n    * Intraparenchymal  \n    * Intraventricular  \n    * Subarachnoid  \n    * Subdural  \n* **Key Points**:  \n  * Medical urgency: Rapid detection is critical for treatment.  \n  * Multi-label nature: A single image may contain multiple hemorrhage subtypes.  \n  * The `any` label (indicating any hemorrhage presence) is weighted more heavily in evaluation.  \n\n## Dataset Overview\n* **Data Type**: DICOM medical images (CT scans) with tabular labels.  \n* **Context**: De-identified head CT scans from 4 research institutions, annotated by neuroradiologists.  \n* **Data Files**:  \n  * `stage_2_train.csv`: Image IDs with multi-label annotations (6 rows per image).  \n  * `stage_2_sample_submission.csv`: Submission format template.  \n  * DICOM image files (458.97 GB total).  \n* **Features**:  \n  * DICOM metadata (e.g., `PatientID`, `StudyInstanceUID`).  \n  * Pixel data from CT scans (grayscale images).  \n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Multi-label Logarithmic Loss.  \n* **Components**:  \n  * Log loss calculated separately for each hemorrhage subtype (6 labels per image).  \n  * The `any` label is given higher weight than subtype labels.  \n  * Predictions clipped to `[10^-15, 1-10^-15]` to avoid log(0).  \n  * Final score: Average log loss across all samples.  \n* **Submission Format**:  \n  * 6 rows per test image (one per label), with predicted probabilities.  \n  * Example: `1_subdural,0.2` indicates a 20% probability of subdural hemorrhage in image `1`.",
    "sections": {},
    "file_path": "kaggle_datasets/348/problem_summary.md"
  },
  "526": {
    "problem_id": "526",
    "title": "Evaluating Linemen Performance in NFL Pass Plays",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Evaluating Linemen Performance in NFL Pass Plays\n\n## Problem Description\n- **Problem Type**: Sports Analytics / Metric Development\n- **Objective**: Develop novel metrics and actionable insights to evaluate offensive and defensive linemen performance during NFL pass plays using player tracking data. The competition focuses on creating innovative statistical measures that can be used by NFL teams to assess linemen contributions.\n- **Key Points**:\n  - Participants must choose one of three tracks: Undergraduate (students only), Metric Development, or Coaching Presentation.\n  - Solutions should address the complex dynamics of football data and provide practical week-to-week utility for NFL teams.\n  - Emphasis on creativity and novelty in analyzing player tracking data.\n\n## Dataset Overview\n- **Data Type**: Multi-source sports tracking and scouting data\n- **Context**: NFL player movements and play outcomes during 2021 season (Weeks 1-8)\n- **Data Files**:\n  - `games.csv`: Game metadata (teams, dates, etc.)\n  - `plays.csv`: Play-level information (down, distance, formation, etc.)\n  - `players.csv`: Player demographics and positions\n  - `pffScoutingData.csv`: Professional scouting assessments\n  - `week[1-8].csv`: Player tracking data (position, speed, acceleration per frame)\n- **Key Features**:\n  - Player tracking coordinates (x,y), speed, acceleration, orientation\n  - Play context (down, distance, formation, coverage scheme)\n  - Scouting evaluations (block types, pressures allowed, sacks)\n  - Play outcomes (pass results, yards gained)\n\n## Evaluation Metrics\n- **Evaluation Method**: Multi-component scoring by NFL analytics experts\n- **Scoring Components** (each weighted equally, scored 0-10):\n  - **Innovation**: Novelty and creativity of approach\n  - **Accuracy**: Statistical validity and data support\n  - **Relevance**: Practical utility for NFL teams\n  - **Clarity**: Explanation quality and communication\n  - **Data Visualization**: Effectiveness of visual presentation\n- **Submission Format**: Notebooks (≤2000 words, ≤10 figures) demonstrating analysis on the provided tracking data",
    "sections": {},
    "file_path": "kaggle_datasets/526/problem_summary.md"
  },
  "370": {
    "problem_id": "370",
    "title": "Binary Classification with Categorical Feature Encoding Challenge II",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of a binary target variable using a dataset composed entirely of categorical features. The challenge focuses on evaluating different encoding strategies for categorical variables, including handling missing data and feature interactions.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Categorical Feature Encoding Challenge II\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary target variable using a dataset composed entirely of categorical features. The challenge focuses on evaluating different encoding strategies for categorical variables, including handling missing data and feature interactions.\n    * **Key Points:**\n        * Dataset contains **only** categorical features, including:\n            * Binary features (`bin_*`)\n            * Low- and high-cardinality nominal features (`nom_*`)\n            * Low- and high-cardinality ordinal features (`ord_*`)\n            * Potentially cyclical features (e.g., `day` of week, `month`)\n        * Additional complexities compared to the first iteration:\n            * Feature interactions\n            * Missing data\n        * String ordinal features (`ord_{3-5}`) are lexically ordered according to `string.ascii_letters`.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with exclusively categorical features, designed to test encoding strategies for binary classification.\n* **Data Files:**\n    * `train.csv` - Training set\n    * `test.csv` - Test set for predictions\n    * `sample_submission.csv` - Example submission file in correct format\n* **Features:**\n    * Binary (`bin_*`), nominal (`nom_*`), and ordinal (`ord_*`) features\n    * Cyclical features like `day` and `month`\n    * Missing values and feature interactions present\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between the two classes\n        * Ranges from 0 to 1, where 1 indicates perfect classification and 0.5 indicates random guessing\n        * Calculated by plotting the True Positive Rate against the False Positive Rate at various threshold settings",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary target variable using a dataset composed entirely of categorical features. The challenge focuses on evaluating different encoding strategies for categorical variables, including handling missing data and feature interactions.\n    * **Key Points:**\n        * Dataset contains **only** categorical features, including:\n            * Binary features (`bin_*`)\n            * Low- and high-cardinality nominal features (`nom_*`)\n            * Low- and high-cardinality ordinal features (`ord_*`)\n            * Potentially cyclical features (e.g., `day` of week, `month`)\n        * Additional complexities compared to the first iteration:\n            * Feature interactions\n            * Missing data\n        * String ordinal features (`ord_{3-5}`) are lexically ordered according to `string.ascii_letters`.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with exclusively categorical features, designed to test encoding strategies for binary classification.\n* **Data Files:**\n    * `train.csv` - Training set\n    * `test.csv` - Test set for predictions\n    * `sample_submission.csv` - Example submission file in correct format\n* **Features:**\n    * Binary (`bin_*`), nominal (`nom_*`), and ordinal (`ord_*`) features\n    * Cyclical features like `day` and `month`\n    * Missing values and feature interactions present",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between the two classes\n        * Ranges from 0 to 1, where 1 indicates perfect classification and 0.5 indicates random guessing\n        * Calculated by plotting the True Positive Rate against the False Positive Rate at various threshold settings"
    },
    "file_path": "kaggle_datasets/370/problem_summary.md"
  },
  "142": {
    "problem_id": "142",
    "title": "NCAA Basketball Tournament Outcome Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NCAA Basketball Tournament Outcome Prediction\n\n## Problem Description\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. The competition involves two stages:\n  * **Stage 1:** Build and test models against historical tournament data (2011-2014)\n  * **Stage 2:** Predict outcomes for the 2015 tournament before it begins\n* **Key Points:**\n  * Participants must predict probabilities for all possible matchups (not just bracket predictions)\n  * Play-in games are not scored in evaluation\n  * External data sources are encouraged (e.g., player stats, social media sentiment)\n  * Predictions are bounded away from 0 and 1 to prevent infinite log loss\n\n## Dataset Overview\n* **Data Type:** Tabular data with historical basketball game results and team statistics\n* **Context:** NCAA Division I Men's Basketball tournaments (1985-2014)\n* **Data Files:**\n  * `teams.csv` - Team identifiers\n  * `seasons.csv` - Season metadata\n  * `regular_season_compact_results.csv` - Basic game results (1985-2014)\n  * `regular_season_detailed_results.csv` - Detailed game stats (2003-2014)\n  * `tourney_compact_results.csv` - Tournament game results\n  * `tourney_detailed_results.csv` - Detailed tournament stats (2003-2014)\n  * `tourney_seeds.csv` - Tournament seeding information\n  * `tourney_slots.csv` - Tournament bracket structure\n* **Key Features:**\n  * Team identifiers and seeds\n  * Game outcomes (win/loss, scores)\n  * Detailed game statistics (field goals, rebounds, assists, etc.)\n  * Temporal information (season, day numbers)\n\n## Evaluation Metrics\n* **Primary Metric:** Logarithmic Loss (Log Loss)\n  * Formula: \n    ```\n    LogLoss = −1/n ∑[y_i log(ŷ_i) + (1−y_i) log(1−ŷ_i)]\n    ```\n  * Where:\n    * n = number of games played\n    * ŷ_i = predicted probability of team 1 beating team 2\n    * y_i = 1 if team 1 wins, 0 if team 2 wins\n  * Characteristics:\n    *",
    "sections": {},
    "file_path": "kaggle_datasets/142/problem_summary.md"
  },
  "384": {
    "problem_id": "384",
    "title": "Fine-Grained Attribute Recognition in Artwork Images",
    "problem_type": "Multi-label Classification (Computer Vision - Fine-Grained Visual Categorization)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Attribute Recognition in Artwork Images\n\n**Problem Description:**\n*   **Problem Type:** Multi-label Classification (Computer Vision - Fine-Grained Visual Categorization)\n*   **Objective:**  \n    * Predict multiple visual attributes for artwork images from The Metropolitan Museum of Art (The Met).  \n    * Attributes describe both visual characteristics (what one \"sees\") and inferred utility of the objects.  \n    * Goal is to enhance search capabilities for visually related museum objects.\n*   **Key Points:**  \n    * Attributes are noisy (single annotator, no verification step).  \n    * Annotations include both structured labels from an ontology and free-form text.  \n    * Competition focuses on recall over precision due to label noise.  \n\n**Dataset Overview:**\n*   **Data Type & Context:**  \n    * Image dataset (PNG format) of digitized museum objects (29.46 GB total).  \n    * Includes centered object photos and scenic room images.  \n    * Camera sources and modalities are unknown/varied.  \n*   **Data Files:**  \n    * `train.csv` (image IDs + attribute IDs)  \n    * `/train` (training images)  \n    * `/test` (test images)  \n    * `labels.csv` (attribute descriptions)  \n    * `sample_submission.csv`  \n*   **Features:**  \n    * Image IDs correspond to filenames.  \n    * Attributes cover art-historical and visual characteristics (e.g., medium, culture, visual patterns).  \n\n**Evaluation Metrics:**\n*   **Primary Metric:** Micro-averaged F1 score  \n    * Favors recall over precision to handle noisy labels.  \n    * Calculated by:  \n        1. Aggregating true positives, false positives, and false negatives across all labels.  \n        2. Computing F1 from these global counts (harmonic mean of precision and recall).  \n    * Submission format requires space-separated attribute IDs for each test image.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Multi-label Classification (Computer Vision - Fine-Grained Visual Categorization)\n*   **Objective:**  \n    * Predict multiple visual attributes for artwork images from The Metropolitan Museum of Art (The Met).  \n    * Attributes describe both visual characteristics (what one \"sees\") and inferred utility of the objects.  \n    * Goal is to enhance search capabilities for visually related museum objects.\n*   **Key Points:**  \n    * Attributes are noisy (single annotator, no verification step).  \n    * Annotations include both structured labels from an ontology and free-form text.  \n    * Competition focuses on recall over precision due to label noise.",
      "Dataset Overview": "*   **Data Type & Context:**  \n    * Image dataset (PNG format) of digitized museum objects (29.46 GB total).  \n    * Includes centered object photos and scenic room images.  \n    * Camera sources and modalities are unknown/varied.  \n*   **Data Files:**  \n    * `train.csv` (image IDs + attribute IDs)  \n    * `/train` (training images)  \n    * `/test` (test images)  \n    * `labels.csv` (attribute descriptions)  \n    * `sample_submission.csv`  \n*   **Features:**  \n    * Image IDs correspond to filenames.  \n    * Attributes cover art-historical and visual characteristics (e.g., medium, culture, visual patterns).",
      "Evaluation Metrics": "*   **Primary Metric:** Micro-averaged F1 score  \n    * Favors recall over precision to handle noisy labels.  \n    * Calculated by:  \n        1. Aggregating true positives, false positives, and false negatives across all labels.  \n        2. Computing F1 from these global counts (harmonic mean of precision and recall).  \n    * Submission format requires space-separated attribute IDs for each test image."
    },
    "file_path": "kaggle_datasets/384/problem_summary.md"
  },
  "7": {
    "problem_id": "7",
    "title": "Predicting Chess Game Outcomes with Alternative Rating Systems",
    "problem_type": "Regression (Probability Prediction)",
    "objective": "Develop a rating system that predicts the outcome of chess games more accurately than the Elo rating system. Participants must predict the expected score (between 0 and 1) for the white player in each game.",
    "evaluation_metric": null,
    "full_content": "# Predicting Chess Game Outcomes with Alternative Rating Systems\n\n**Problem Description:**\n* **Problem Type:** Regression (Probability Prediction)\n* **Objective:** Develop a rating system that predicts the outcome of chess games more accurately than the Elo rating system. Participants must predict the expected score (between 0 and 1) for the white player in each game.\n    * **Key Points:**\n        * Focus on improving upon established rating systems (Elo, Glicko, Chessmetrics, etc.).\n        * Must account for chess-specific dynamics (e.g., white's first-move advantage).\n        * Predictions are aggregated by player and month for evaluation.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data of chess game results.\n* **Context:** Contains 105 months of game results among 8,631 top chess players.\n* **Data Files:**\n    * `training_data.csv`: 65,053 games (months 1-100) with columns: Month #, White Player #, Black Player #, Score (0, 0.5, or 1).\n    * `test_data.csv`: 7,809 games (months 101-105) with same columns (Score withheld for prediction).\n    * `cross_validation_dataset.csv`: Subset of months 96-100 for validation.\n    * `example_submission.csv`: Sample submission file.\n* **Features:**\n    * Player IDs (anonymized).\n    * Month number (temporal feature).\n    * Game outcome (Score) in training data.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Root Mean Squared Error (RMSE) on aggregated player-month scores.\n    * **Components:**\n        1. Predict expected score for each game in test set.\n        2. Aggregate predictions by player and month (sum predicted and actual scores).\n        3. Calculate squared error for each player-month: (actual - predicted)^2.\n        4. Compute RMSE as square root of average squared error.\n    * **Note:** Public leaderboard uses 20% of test data; final evaluation uses remaining 80%.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Probability Prediction)\n* **Objective:** Develop a rating system that predicts the outcome of chess games more accurately than the Elo rating system. Participants must predict the expected score (between 0 and 1) for the white player in each game.\n    * **Key Points:**\n        * Focus on improving upon established rating systems (Elo, Glicko, Chessmetrics, etc.).\n        * Must account for chess-specific dynamics (e.g., white's first-move advantage).\n        * Predictions are aggregated by player and month for evaluation.",
      "Dataset Overview": "* **Data Type:** Tabular data of chess game results.\n* **Context:** Contains 105 months of game results among 8,631 top chess players.\n* **Data Files:**\n    * `training_data.csv`: 65,053 games (months 1-100) with columns: Month #, White Player #, Black Player #, Score (0, 0.5, or 1).\n    * `test_data.csv`: 7,809 games (months 101-105) with same columns (Score withheld for prediction).\n    * `cross_validation_dataset.csv`: Subset of months 96-100 for validation.\n    * `example_submission.csv`: Sample submission file.\n* **Features:**\n    * Player IDs (anonymized).\n    * Month number (temporal feature).\n    * Game outcome (Score) in training data.",
      "Evaluation Metrics": "* **Evaluation Metric:** Root Mean Squared Error (RMSE) on aggregated player-month scores.\n    * **Components:**\n        1. Predict expected score for each game in test set.\n        2. Aggregate predictions by player and month (sum predicted and actual scores).\n        3. Calculate squared error for each player-month: (actual - predicted)^2.\n        4. Compute RMSE as square root of average squared error.\n    * **Note:** Public leaderboard uses 20% of test data; final evaluation uses remaining 80%."
    },
    "file_path": "kaggle_datasets/7/problem_summary.md"
  },
  "29": {
    "problem_id": "29",
    "title": "Biometric Identification from Eye Movement Patterns",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Biometric Identification from Eye Movement Patterns\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (with probabilistic outputs)\n* **Objective**: Develop a model to identify individuals based on their unique eye movement characteristics. The task requires predicting which subject (from 37 possible individuals) generated a given eye movement sample.\n* **Key Points**:\n  * Each sample represents a sequence of eye movements in response to visual stimuli.\n  * The problem has biometric identification applications.\n  * Models must output probability distributions across all subjects for each sample.\n\n## Dataset Overview\n* **Data Type**: Time-series tabular data (eye movement recordings)\n* **Context**: Eye tracking data collected during controlled visual stimulus experiments\n* **Data Files**:\n  * train.csv (labeled samples)\n  * test.csv (unlabeled samples for submission)\n  * Benchmark files (rf_benchmark.csv, svm_benchmark.csv, uniform_benchmark.csv)\n* **Features**:\n  * 2048-time-step recordings for each eye (left/right X/Y positions)\n  * Stimulus was a jumping point on a 3x3 grid\n  * Values represent gaze positions (0=center, positive=right/upper, negative=left/lower)\n\n## Evaluation Metrics\n* **Primary Metric**: Multi-class logarithmic loss (log loss)\n* **Metric Components**:\n  * Calculated as: $-\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^M y_{i,j}\\log(\\hat{y}_{i,j})$\n  * Where:\n    * N = number of samples\n    * M = number of subjects (37)\n    * $\\hat{y}_{i,j}$ = predicted probability that subject j generated sample i\n    * $y_{i,j}$ = ground truth indicator (1 if correct subject, 0 otherwise)\n  * Predictions must form valid probability distributions (sum to 1 per sample)",
    "sections": {},
    "file_path": "kaggle_datasets/29/problem_summary.md"
  },
  "519": {
    "problem_id": "519",
    "title": "Predicting English Language Proficiency from Student Essays",
    "problem_type": "Regression (Multi-target)",
    "objective": "Predict six analytic measures of English language proficiency (cohesion, syntax, vocabulary, phraseology, grammar, conventions) for essays written by 8th-12th grade English Language Learners (ELLs). Each measure is scored from 1.0 to 5.0 in 0.5 increments.",
    "evaluation_metric": null,
    "full_content": "# Predicting English Language Proficiency from Student Essays\n\n**Problem Description:**\n* **Problem Type:** Regression (Multi-target)\n* **Objective:** Predict six analytic measures of English language proficiency (cohesion, syntax, vocabulary, phraseology, grammar, conventions) for essays written by 8th-12th grade English Language Learners (ELLs). Each measure is scored from 1.0 to 5.0 in 0.5 increments.\n* **Key Points:**\n  * Focus on providing accurate feedback tailored to ELLs' language development stage\n  * Aims to improve automated feedback tools for educational contexts\n  * Essays are argumentative in nature\n  * Some essays overlap with previous Feedback Prize competitions' datasets\n\n**Dataset Overview:**\n* **Data Type:** Text data (student essays) with associated numerical scores\n* **Context:** Argumentative essays from grades 8-12 ELL students (ELLIPSE corpus)\n* **Data Files:**\n  * train.csv (contains full_text, text_id, and six target scores)\n  * test.csv (contains full_text and text_id for prediction)\n  * sample_submission.csv (example submission format)\n* **Features:**\n  * Primary feature: full_text (essay content)\n  * Targets: Six numerical scores representing different language proficiency aspects\n\n**Evaluation Metrics:**\n* **Primary Metric:** MCRMSE (Mean Columnwise Root Mean Squared Error)\n  * Calculated as: \n    ```\n    MCRMSE = 1/Nt * ∑[√(1/n * ∑(y_ij - ŷ_ij)²)]\n    ```\n    Where:\n    * Nt = number of target columns (6)\n    * n = number of observations\n    * y = actual values\n    * ŷ = predicted values\n* **Efficiency Prize Metric:** Combines MCRMSE performance with runtime (CPU-only)\n  * Formula: \n    ```\n    Efficiency = (1/(Base - minMCRMSE)) * MCRMSE + (1/32400) * RuntimeSeconds\n    ```\n    Where Base is the baseline score (mean prediction)",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Multi-target)\n* **Objective:** Predict six analytic measures of English language proficiency (cohesion, syntax, vocabulary, phraseology, grammar, conventions) for essays written by 8th-12th grade English Language Learners (ELLs). Each measure is scored from 1.0 to 5.0 in 0.5 increments.\n* **Key Points:**\n  * Focus on providing accurate feedback tailored to ELLs' language development stage\n  * Aims to improve automated feedback tools for educational contexts\n  * Essays are argumentative in nature\n  * Some essays overlap with previous Feedback Prize competitions' datasets",
      "Dataset Overview": "* **Data Type:** Text data (student essays) with associated numerical scores\n* **Context:** Argumentative essays from grades 8-12 ELL students (ELLIPSE corpus)\n* **Data Files:**\n  * train.csv (contains full_text, text_id, and six target scores)\n  * test.csv (contains full_text and text_id for prediction)\n  * sample_submission.csv (example submission format)\n* **Features:**\n  * Primary feature: full_text (essay content)\n  * Targets: Six numerical scores representing different language proficiency aspects",
      "Evaluation Metrics": "* **Primary Metric:** MCRMSE (Mean Columnwise Root Mean Squared Error)\n  * Calculated as: \n    ```\n    MCRMSE = 1/Nt * ∑[√(1/n * ∑(y_ij - ŷ_ij)²)]\n    ```\n    Where:\n    * Nt = number of target columns (6)\n    * n = number of observations\n    * y = actual values\n    * ŷ = predicted values\n* **Efficiency Prize Metric:** Combines MCRMSE performance with runtime (CPU-only)\n  * Formula: \n    ```\n    Efficiency = (1/(Base - minMCRMSE)) * MCRMSE + (1/32400) * RuntimeSeconds\n    ```\n    Where Base is the baseline score (mean prediction)"
    },
    "file_path": "kaggle_datasets/519/problem_summary.md"
  },
  "16": {
    "problem_id": "16",
    "title": "Binary Classification with High-Dimensional Simulated Data to Prevent Overfitting",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with High-Dimensional Simulated Data to Prevent Overfitting\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Develop a predictive model that generalizes well to unseen data despite having far more features (200) than training samples (250).  \n    * The core challenge is to avoid overfitting while achieving high predictive accuracy on hidden test cases (19,750 samples).  \n* **Key Points:**  \n    * Simulates real-world scenarios like medical data analysis with limited cases.  \n    * Three distinct target sets (Practice, Leaderboard, Evaluation) generated from the same data but with different underlying \"equations\" to test robustness.  \n    * Includes a secondary task (Part B) to identify which of the 200 variables were used in the target-generating equation.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data with 200 anonymized numerical features (`var_1` to `var_200`) and a binary target.  \n    * Simulated dataset with 20,000 total cases, but only 250 have labeled targets for training.  \n* **Data Files:**  \n    * `overfitting.csv` containing:  \n        * `case_id`: Unique row identifier (1-20,000).  \n        * `train`: Flag indicating training rows (1 for first 250 samples).  \n        * Three target columns:  \n            * `Target_Practice` (all 20,000 labeled for offline development).  \n            * `Target_Leaderboard` (only 250 labeled for public leaderboard submissions).  \n            * `Target_Evaluate` (only 250 labeled for final evaluation).  \n* **Features:**  \n    * 200 randomly generated numerical variables.  \n    * Targets are binary (0/1), with unlabeled values represented as `-99`.  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC) on the hidden evaluation set.  \n* **Competition Phases:**  \n    1. **Leaderboard Phase:**  \n        * Submissions evaluated on `Target_Leaderboard` (AUC).  \n        * Must beat a dynamic \"Benchmark\" AUC to qualify for final evaluation.  \n    2. **Evaluation Phase:**  \n        * Qualifiers submit predictions for `Target_Evaluate` (AUC determines winner).  \n        * **Part B Scoring:** Variable selection accuracy:",
    "sections": {},
    "file_path": "kaggle_datasets/16/problem_summary.md"
  },
  "189": {
    "problem_id": "189",
    "title": "Multi-label Classification of Restaurant Attributes from User-Submitted Photos",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Restaurant Attributes from User-Submitted Photos\n\n## Problem Description\n* **Problem Type:** Multi-label Image Classification  \n* **Objective:**  \n  * Predict multiple attribute labels for restaurants based on user-submitted photos.  \n  * Automate the tagging process currently done manually by Yelp users during reviews.  \n* **Key Points:**  \n  * Labels represent business attributes like dining suitability (lunch/dinner), amenities (alcohol, reservations), and ambiance (classy, kid-friendly).  \n  * Real-world challenges include duplicate photos from users or chain businesses.  \n  * Test set contains ignored businesses (not scored) to deter hand-labeling.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n  * Image data (user-submitted restaurant photos) with multi-label annotations.  \n  * Business-photo mapping and label metadata provided in CSV files.  \n* **Data Files:**  \n  * `train_photos.tgz` / `test_photos.tgz` – Training and test photo sets.  \n  * `train_photo_to_biz_ids.csv` / `test_photo_to_biz_ids.csv` – Photo-to-business mappings.  \n  * `train.csv` – Business IDs and their 9 attribute labels (space-delimited).  \n  * `sample_submission.csv` – Submission template with `business_id` and predicted labels.  \n* **Features:**  \n  * **9 Labels:** `good_for_lunch`, `good_for_dinner`, `takes_reservations`, `outdoor_seating`, `restaurant_is_expensive`, `has_alcohol`, `has_table_service`, `ambience_is_classy`, `good_for_kids`.  \n\n## Evaluation Metrics\n* **Primary Metric:** Mean F1-Score (example-based F-measure for multi-label tasks).  \n* **Components:**  \n  * **Precision (p):** `tp / (tp + fp)` – Ratio of correct positive predictions.  \n  * **Recall (r):** `tp / (tp + fn)` – Ratio of actual positives identified.  \n  * **F1 Score:** Harmonic mean of precision and recall: `2 * (p * r) / (p + r)`.  \n  * **Final Score:** Averages F1-scores across all labels and examples.",
    "sections": {},
    "file_path": "kaggle_datasets/189/problem_summary.md"
  },
  "129": {
    "problem_id": "129",
    "title": "Predicting Soil Properties from Infrared Spectroscopy Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Soil Properties from Infrared Spectroscopy Data\n\n## Problem Description\n* **Problem Type:** Multi-target Regression\n* **Objective:** Predict five key soil functional properties (Ca, P, pH, SOC, and Sand) using mid-infrared absorbance spectra measurements. The goal is to develop models that can accurately estimate these properties from rapid, low-cost spectral measurements as an alternative to conventional soil tests.\n* **Key Points:**\n  * Focus on digital mapping of soil properties for sustainable agriculture in Africa\n  * Uses diffuse reflectance infrared spectroscopy as primary input\n  * Targets include both chemical (Ca, P, pH) and physical (SOC, Sand) properties\n  * Data comes from spatially stratified sampling across Africa\n\n## Dataset Overview\n* **Data Type:** Tabular data combining spectral measurements with spatial predictors\n* **Context:** Soil samples collected from across Africa with associated spectral and remote sensing data\n* **Data Files:**\n  * train.csv (1,158 samples)\n  * test.csv (728 samples)\n  * sample_submission.csv\n* **Key Features:**\n  * 3,578 mid-infrared absorbance measurements (columns m7497.96 to m599.76)\n  * 5 target variables (Ca, P, pH, SOC, Sand)\n  * Spatial predictors from remote sensing (BSA, CTI, ELEV, EVI, LST, Ref, Reli, TMAP, TMFI)\n  * Sample depth (Topsoil/Subsoil)\n  * PIDN (unique sample identifier)\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Columnwise Root Mean Squared Error (MCRMSE)\n* **Metric Components:**\n  * Calculated as the average of RMSE across all 5 target variables\n  * Formula: MCRMSE = (1/5) * Σ[sqrt(mean((y_ij - ŷ_ij)^2))] for j=1 to 5\n  * Each target variable contributes equally to the final score\n  * Lower values indicate better performance (closer to zero is better)",
    "sections": {},
    "file_path": "kaggle_datasets/129/problem_summary.md"
  },
  "42": {
    "problem_id": "42",
    "title": "Music Recommendation Prediction with the Million Song Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Music Recommendation Prediction with the Million Song Dataset\n\n## Problem Description\n- **Problem Type**: Recommendation System (Ranking/Prediction Task)\n- **Objective**: Predict the missing half of listening histories for 110K users, given:\n  - Full listening histories for 1M users (training data)\n  - Half of listening histories for 110K users (validation/test sets)\n- **Key Points**:\n  - Open-ended approach: Participants can use collaborative filtering, content-based methods, or hybrid techniques\n  - Leverages the open Million Song Dataset (MSD) with extensive metadata/features\n  - Focuses on predicting listened songs rather than explicit ratings\n\n## Dataset Overview\n- **Data Type**: Tabular data (user-song interactions) with optional audio/metadata features\n- **Context**: Music listening histories from The Echo Nest's Taste Profile dataset\n- **Data Files**:\n  - `kaggle_songs.txt`: Official song indexing (starting at 1)\n  - `kaggle_users.txt`: User IDs for submission ordering\n  - `kaggle_visible_evaluation_triplets.zip`: Partial listening histories for 110K evaluation users\n  - `taste_profile_song_to_tracks.txt`: Mapping between song IDs and track IDs\n- **Features**:\n  - Core data: (user ID, song ID, play count) triplets\n  - Additional available features (from MSD):\n    - Audio features\n    - Track-level tags and similarities\n    - Lyrics (bag-of-words format)\n    - Artist/title metadata\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Average Precision (MAP) truncated at 500\n- **Components**:\n  - For each user, calculate average precision of recommended songs\n  - Average these values across all users\n  - Only top 500 predictions per user are considered\n- **Additional Post-Competition Analysis**:\n  - Will include metrics like precision@K, AUC, and average rank\n  - Human evaluation of top predictions for some users",
    "sections": {},
    "file_path": "kaggle_datasets/42/problem_summary.md"
  },
  "89": {
    "problem_id": "89",
    "title": "Predicting 311 Issue Engagement Metrics",
    "problem_type": "Multi-target Regression (predicting multiple continuous variables).",
    "objective": "Predict three engagement metrics for 311 service requests (issues) submitted through SeeClickFix:",
    "evaluation_metric": null,
    "full_content": "# Predicting 311 Issue Engagement Metrics\n\n**Problem Description:**\n* **Problem Type:** Multi-target Regression (predicting multiple continuous variables).\n* **Objective:** Predict three engagement metrics for 311 service requests (issues) submitted through SeeClickFix:\n    * Number of views (`num_views`)\n    * Number of votes (`num_votes`)\n    * Number of comments (`num_comments`)\n* **Key Points:**\n    * Data spans four cities (Oakland, Richmond, New Haven, Chicago) from 2012 onward.\n    * Challenges include:\n        * Temporal dynamics (older issues have more engagement time but may become less relevant).\n        * Platform evolution (changing user base and input sources).\n        * Data quality issues (duplicates, incomplete descriptions, systematic differences across sources).\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with geospatial and text features.\n* **Data Files:**\n    * `train.csv` (contains features + target variables)\n    * `test.csv` (contains only features for prediction)\n* **Key Features:**\n    * Geographic coordinates (`latitude`, `longitude`)\n    * Text fields (`summary`, `description`)\n    * Temporal feature (`created_time`)\n    * Categorical features (`source`, `tag_type`)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Columnwise Root Mean Squared Error (MCRMSE)\n    * **Calculation:**\n        1. Compute RMSE separately for each target variable (`num_views`, `num_votes`, `num_comments`)\n        2. Average the three RMSE values\n    * **Purpose:** Equally weights prediction accuracy across all three engagement metrics.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-target Regression (predicting multiple continuous variables).\n* **Objective:** Predict three engagement metrics for 311 service requests (issues) submitted through SeeClickFix:\n    * Number of views (`num_views`)\n    * Number of votes (`num_votes`)\n    * Number of comments (`num_comments`)\n* **Key Points:**\n    * Data spans four cities (Oakland, Richmond, New Haven, Chicago) from 2012 onward.\n    * Challenges include:\n        * Temporal dynamics (older issues have more engagement time but may become less relevant).\n        * Platform evolution (changing user base and input sources).\n        * Data quality issues (duplicates, incomplete descriptions, systematic differences across sources).",
      "Dataset Overview": "* **Data Type:** Tabular data with geospatial and text features.\n* **Data Files:**\n    * `train.csv` (contains features + target variables)\n    * `test.csv` (contains only features for prediction)\n* **Key Features:**\n    * Geographic coordinates (`latitude`, `longitude`)\n    * Text fields (`summary`, `description`)\n    * Temporal feature (`created_time`)\n    * Categorical features (`source`, `tag_type`)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Columnwise Root Mean Squared Error (MCRMSE)\n    * **Calculation:**\n        1. Compute RMSE separately for each target variable (`num_views`, `num_votes`, `num_comments`)\n        2. Average the three RMSE values\n    * **Purpose:** Equally weights prediction accuracy across all three engagement metrics."
    },
    "file_path": "kaggle_datasets/89/problem_summary.md"
  },
  "572": {
    "problem_id": "572",
    "title": "Predicting Horse Health Outcomes",
    "problem_type": "Multiclass Classification",
    "objective": "Predict the health outcomes of horses based on various medical indicators. The target variable is categorical, representing different possible health outcomes (e.g., \"lived\", \"died\").",
    "evaluation_metric": null,
    "full_content": "# Predicting Horse Health Outcomes\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the health outcomes of horses based on various medical indicators. The target variable is categorical, representing different possible health outcomes (e.g., \"lived\", \"died\").\n* **Key Points:**\n  * Dataset is synthetically generated from real-world data to balance realism with competition integrity\n  * Participants can optionally incorporate the original Horse Survival Dataset to improve model performance\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing medical indicators and health outcomes for horses\n* **Data Files:**\n  * train.csv (contains target variable `outcome`)\n  * test.csv\n  * sample_submission.csv\n* **Features:** Medical indicators (59 total columns, specific features not named in description)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Micro-averaged F1-Score\n  * Calculates F1-score by counting total true positives, false negatives and false positives across all classes\n  * Appropriate for multiclass classification with potentially imbalanced classes\n  * Focuses on overall model performance across all categories rather than per-class performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the health outcomes of horses based on various medical indicators. The target variable is categorical, representing different possible health outcomes (e.g., \"lived\", \"died\").\n* **Key Points:**\n  * Dataset is synthetically generated from real-world data to balance realism with competition integrity\n  * Participants can optionally incorporate the original Horse Survival Dataset to improve model performance",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing medical indicators and health outcomes for horses\n* **Data Files:**\n  * train.csv (contains target variable `outcome`)\n  * test.csv\n  * sample_submission.csv\n* **Features:** Medical indicators (59 total columns, specific features not named in description)",
      "Evaluation Metrics": "* **Primary Metric:** Micro-averaged F1-Score\n  * Calculates F1-score by counting total true positives, false negatives and false positives across all classes\n  * Appropriate for multiclass classification with potentially imbalanced classes\n  * Focuses on overall model performance across all categories rather than per-class performance"
    },
    "file_path": "kaggle_datasets/572/problem_summary.md"
  },
  "116": {
    "problem_id": "116",
    "title": "Predicting Insurance Policy Purchases from Transaction History",
    "problem_type": "Multi-class Classification (with sequence prediction elements)",
    "objective": "Predict the exact combination of 7 coverage options (A-G) that a customer will purchase, based on their partial quote history and customer characteristics. The goal is to enable earlier prediction in the shopping process to reduce customer drop-off.",
    "evaluation_metric": null,
    "full_content": "# Predicting Insurance Policy Purchases from Transaction History\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (with sequence prediction elements)\n* **Objective:** Predict the exact combination of 7 coverage options (A-G) that a customer will purchase, based on their partial quote history and customer characteristics. The goal is to enable earlier prediction in the shopping process to reduce customer drop-off.\n    * **Key Points:**\n        * Predictions must be made from truncated transaction histories (simulating different stages of the shopping process)\n        * All 7 coverage options must be predicted correctly for a customer to count as accurate\n        * Coverage options have ordinal values (2-4 possible values per option)\n        * Customer characteristics may change during shopping process\n\n**Dataset Overview:**\n* **Data Type:** Tabular time-series data of insurance quote transactions\n* **Context:** Customer shopping histories for auto insurance policies from Allstate\n* **Data Files:**\n    * train.csv - Full quote histories with final purchased options\n    * test_v2.csv - Partial quote histories without final purchases\n    * sampleSubmission.csv - Submission format example\n* **Key Features:**\n    * Temporal features: shopping_pt, day, time\n    * Customer characteristics: group_size, homeowner, car_age, car_value, risk_factor, age_oldest, age_youngest, married_couple\n    * Coverage options: A-G (target variables)\n    * Cost: quoted price for each configuration\n    * Shopping context: state, location\n    * Previous coverage: C_previous, duration_previous\n\n**Evaluation Metrics:**\n* **Primary Metric:** All-or-none accuracy\n    * **Components:**\n        * For each customer: 1 point if all 7 predicted coverage options exactly match the true purchase\n        * 0 points for any incorrect prediction (even if only 1 option is wrong)\n        * Final score = (Number of perfect predictions) / (Total customers) × 100",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (with sequence prediction elements)\n* **Objective:** Predict the exact combination of 7 coverage options (A-G) that a customer will purchase, based on their partial quote history and customer characteristics. The goal is to enable earlier prediction in the shopping process to reduce customer drop-off.\n    * **Key Points:**\n        * Predictions must be made from truncated transaction histories (simulating different stages of the shopping process)\n        * All 7 coverage options must be predicted correctly for a customer to count as accurate\n        * Coverage options have ordinal values (2-4 possible values per option)\n        * Customer characteristics may change during shopping process",
      "Dataset Overview": "* **Data Type:** Tabular time-series data of insurance quote transactions\n* **Context:** Customer shopping histories for auto insurance policies from Allstate\n* **Data Files:**\n    * train.csv - Full quote histories with final purchased options\n    * test_v2.csv - Partial quote histories without final purchases\n    * sampleSubmission.csv - Submission format example\n* **Key Features:**\n    * Temporal features: shopping_pt, day, time\n    * Customer characteristics: group_size, homeowner, car_age, car_value, risk_factor, age_oldest, age_youngest, married_couple\n    * Coverage options: A-G (target variables)\n    * Cost: quoted price for each configuration\n    * Shopping context: state, location\n    * Previous coverage: C_previous, duration_previous",
      "Evaluation Metrics": "* **Primary Metric:** All-or-none accuracy\n    * **Components:**\n        * For each customer: 1 point if all 7 predicted coverage options exactly match the true purchase\n        * 0 points for any incorrect prediction (even if only 1 option is wrong)\n        * Final score = (Number of perfect predictions) / (Total customers) × 100"
    },
    "file_path": "kaggle_datasets/116/problem_summary.md"
  },
  "324": {
    "problem_id": "324",
    "title": "Fine-Grained Attribute Recognition in Artwork Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Attribute Recognition in Artwork Images\n\n## Problem Description\n* **Problem Type:** Multi-label Image Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective:**  \n  Predict multiple fine-grained visual attributes for artwork images from The Metropolitan Museum of Art (The Met). The goal is to enhance visual understanding of museum objects by automatically tagging them with attributes that may not be captured in traditional art historical cataloging.\n* **Key Points:**\n  * Attributes describe both visual appearance (\"what one sees\") and inferred utility of the artwork\n  * Annotations are noisy (single annotator, no verification step)\n  * Competition uses a two-stage evaluation with a significantly larger private test set\n\n## Dataset Overview\n* **Data Type & Context:**  \n  PNG images of museum artwork/artifacts with CSV metadata files. Images vary from centered object photos to scenic room shots.\n* **Data Files:**\n  * `/train/` - Training images\n  * `/test/` - Test images\n  * `train.csv` - Image IDs with space-delimited attribute IDs\n  * `labels.csv` - Attribute ID descriptions\n  * `sample_submission.csv` - Submission format example\n* **Key Features:**\n  * Images named by unique ID\n  * Multiple possible attributes per image (multi-label)\n  * Attributes come from Met's ontology plus free-form text annotations\n\n## Evaluation Metrics\n* **Primary Metric:** Mean F2 Score (micro-averaged across all labels)\n* **Metric Components:**\n  * Fβ score formula: (1+β²)pr/(β²p + r) where β=2\n  * p (precision) = tp/(tp + fp)\n  * r (recall) = tp/(tp + fn)\n  * Specifically weights recall higher than precision (β=2)\n  * Final score is average of F2 scores for each test image",
    "sections": {},
    "file_path": "kaggle_datasets/324/problem_summary.md"
  },
  "586": {
    "problem_id": "586",
    "title": "Detecting Sleep States from Wrist-Worn Accelerometer Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Detecting Sleep States from Wrist-Worn Accelerometer Data\n\n## Problem Description\n* **Problem Type**: Time Series Event Detection (Binary Event Classification)\n* **Objective**: \n    * Detect two key sleep-related events (`onset` and `wakeup`) from continuous wrist-worn accelerometer data.\n    * The goal is to identify precise timestamps when sleep begins and ends based on movement patterns.\n* **Key Points**:\n    * Focus on detecting the longest single period of inactivity (≥30 minutes) per night.\n    * Must handle periods where the device was removed (no predictions should be made during these intervals).\n    * Events are annotated following specific sleep study conventions (e.g., no more than one sleep window per night).\n\n## Dataset Overview\n* **Data Type**: Time series accelerometer data with sleep event annotations.\n* **Context**: Multi-day recordings from wrist-worn devices (~500 series) collected for sleep research.\n* **Data Files**:\n    * `train_series.parquet`: Continuous accelerometer recordings (anglez, ENMO features) with timestamps.\n    * `train_events.csv`: Annotated sleep events (`onset`/`wakeup`) with corresponding timestamps.\n    * `test_series.parquet`: Hidden test set for evaluation (format same as training).\n* **Key Features**:\n    * `anglez`: Arm angle relative to vertical axis (sleep detection metric).\n    * `enmo`: Euclidean Norm Minus One of accelerometer signals (movement metric).\n\n## Evaluation Metrics\n* **Primary Metric**: **Average Precision (AP)** with event-specific error tolerances.\n* **Components**:\n    1. **Matching**: Predictions matched to ground-truth within tolerance thresholds (1-30 minutes).\n    2. **Scoring**: AP calculated per `(event × tolerance × series_id)` group.\n        * TP: Correctly matched predictions.\n        * FP: Unmatched predictions.\n        * FN: Unmatched ground-truth events.\n    3. **Reduction**: Final score averages AP across all tolerances, then across both event types.",
    "sections": {},
    "file_path": "kaggle_datasets/586/problem_summary.md"
  },
  "45": {
    "problem_id": "45",
    "title": "Kinect Gesture Recognition Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Kinect Gesture Recognition Challenge\n\n## Problem Description\n- **Problem Type**: Multi-class Sequence Classification (Gesture Recognition)\n- **Objective**: Develop a gesture recognizer capable of learning from a single training example per gesture class (\"one-shot learning\") using Microsoft Kinect data. The system must classify sequences of 1-5 gestures from vocabularies of 8-15 unique gestures across different batches.\n- **Key Points**:\n  - Focus on one-shot learning (single training example per gesture class)\n  - Each data batch has its own unique gesture vocabulary\n  - Must handle both RGB and depth video data\n  - Sequences may contain 1-5 gestures\n  - User-dependent recognition (same performer in training and test)\n\n## Dataset Overview\n- **Data Type**: Paired video data (RGB + depth) from Kinect sensor\n- **Context**: Upper body gestures performed by individuals in controlled settings\n- **Data Files**:\n  - Development sets (devel01-40): Fully labeled training data\n  - Validation sets (valid01-xx): Partially labeled for leaderboard feedback\n  - Final evaluation sets: Released at competition end\n  - Sample submission file\n- **Features**:\n  - RGB videos (M_yy.avi)\n  - Depth videos (K_yy.avi)\n  - Label files indicating gesture sequences\n  - Normalization constants for depth data restoration\n\n## Evaluation Metrics\n- **Evaluation Metric**: Normalized Levenshtein Distance (Edit Distance)\n- **Components**:\n  - For each video, compare predicted gesture sequence (R) to true sequence (T)\n  - Compute minimum edit operations (insertions, deletions, substitutions) to transform R→T\n  - Sum distances across all test sequences\n  - Normalize by total number of gestures in ground truth\n  - Lower scores indicate better performance (0 = perfect)\n  - Metric accounts for both classification errors and sequence length errors",
    "sections": {},
    "file_path": "kaggle_datasets/45/problem_summary.md"
  },
  "581": {
    "problem_id": "581",
    "title": "Predicting AI Model Runtime Optimization Configurations",
    "problem_type": "Regression with Ranking (Hybrid Task)",
    "objective": "Predict the optimal compiler configurations (layout and tile settings) for AI models to minimize runtime on TPUs. The goal is to:",
    "evaluation_metric": null,
    "full_content": "# Predicting AI Model Runtime Optimization Configurations\n\n**Problem Description:**\n* **Problem Type:** Regression with Ranking (Hybrid Task)\n* **Objective:** Predict the optimal compiler configurations (layout and tile settings) for AI models to minimize runtime on TPUs. The goal is to:\n    * For `tile:xla` data: Identify the top-5 fastest configurations per model graph.\n    * For `layout:*` data: Rank all configurations by runtime (fastest to slowest) for each graph.\n* **Key Points:**\n    * Two distinct optimization types:\n        * **Layout configurations:** Control tensor memory layout (node-level optimizations).\n        * **Tile configurations:** Control fused subgraph tile sizes (graph-level optimizations).\n    * Real-world impact: Improving compiler heuristics for AI model efficiency.\n\n**Dataset Overview:**\n* **Data Type:** Graph-structured data (AI model computation graphs) with configuration features.\n* **Context:** XLA HLO graphs executed on Tensor Processing Units (TPUs).\n* **Data Files:**\n    * `.npz` files organized by collection (`tile:xla`, `layout:xla:random`, `layout:xla:default`, `layout:nlp:random`, `layout:nlp:default`).\n    * Each file contains:\n        * Graph structure (nodes/edges)\n        * Node features (140-dim vectors)\n        * Configuration features (tile: 24-dim, layout: 18-dim per node)\n        * Runtime measurements\n* **Key Features:**\n    * **Node features:** Opcode types, tensor shapes, windowing/padding parameters, convolution dimensions.\n    * **Configuration features:** Tile sizes (kernel/output/input bounds) or layout orders (minor-to-major dimensions).\n\n**Evaluation Metrics:**\n* **Composite Metric:** Average of two sub-metrics:\n    1. **For `tile:xla`:** (1 - slowdown) of top-5 predictions:\n        * `2 - (min(runtime_top5) / min(runtime_all_configs))`\n    2. **For `layout:*`:** Kendall Tau Correlation between predicted and true runtime rankings.\n* **Submission Format:**\n    * CSV with `ID,TopConfigs` columns.\n    * For `tile:xla`: First 5 entries used (indices of predicted fastest configs).\n    * For `layout:*`: Full permutation of all config indices required.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression with Ranking (Hybrid Task)\n* **Objective:** Predict the optimal compiler configurations (layout and tile settings) for AI models to minimize runtime on TPUs. The goal is to:\n    * For `tile:xla` data: Identify the top-5 fastest configurations per model graph.\n    * For `layout:*` data: Rank all configurations by runtime (fastest to slowest) for each graph.\n* **Key Points:**\n    * Two distinct optimization types:\n        * **Layout configurations:** Control tensor memory layout (node-level optimizations).\n        * **Tile configurations:** Control fused subgraph tile sizes (graph-level optimizations).\n    * Real-world impact: Improving compiler heuristics for AI model efficiency.",
      "Dataset Overview": "* **Data Type:** Graph-structured data (AI model computation graphs) with configuration features.\n* **Context:** XLA HLO graphs executed on Tensor Processing Units (TPUs).\n* **Data Files:**\n    * `.npz` files organized by collection (`tile:xla`, `layout:xla:random`, `layout:xla:default`, `layout:nlp:random`, `layout:nlp:default`).\n    * Each file contains:\n        * Graph structure (nodes/edges)\n        * Node features (140-dim vectors)\n        * Configuration features (tile: 24-dim, layout: 18-dim per node)\n        * Runtime measurements\n* **Key Features:**\n    * **Node features:** Opcode types, tensor shapes, windowing/padding parameters, convolution dimensions.\n    * **Configuration features:** Tile sizes (kernel/output/input bounds) or layout orders (minor-to-major dimensions).",
      "Evaluation Metrics": "* **Composite Metric:** Average of two sub-metrics:\n    1. **For `tile:xla`:** (1 - slowdown) of top-5 predictions:\n        * `2 - (min(runtime_top5) / min(runtime_all_configs))`\n    2. **For `layout:*`:** Kendall Tau Correlation between predicted and true runtime rankings.\n* **Submission Format:**\n    * CSV with `ID,TopConfigs` columns.\n    * For `tile:xla`: First 5 entries used (indices of predicted fastest configs).\n    * For `layout:*`: Full permutation of all config indices required."
    },
    "file_path": "kaggle_datasets/581/problem_summary.md"
  },
  "323": {
    "problem_id": "323",
    "title": "Wildlife Animal Classification in Camera Trap Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Wildlife Animal Classification in Camera Trap Images\n\n## Problem Description\n- **Problem Type:** Multi-class Image Classification\n- **Objective:** Classify animals in camera trap images from different geographic regions (American Southwest for training, American Northwest for testing), where species overlap but are not identical. The challenge involves handling domain shift between regions and unseen species.\n- **Key Points:**\n  - Training and test data come from distinct geographic regions (Southern California vs. Idaho)\n  - Species distributions differ between regions\n  - Participants can use supplemental data from iNaturalist (2017-2019) and Microsoft AirSim simulations\n  - Includes an \"empty\" class (0) for images with no animals\n  - Some classes may not appear in either train or test sets\n\n## Dataset Overview\n- **Data Type:** Image data from camera traps (wildlife monitoring cameras)\n- **Context:** Biodiversity monitoring through automated image classification\n- **Data Files:**\n  - train.csv (with image labels)\n  - train_images.zip (196,157 training images from 138 Southern California locations)\n  - test.csv\n  - test_images.zip (153,730 test images from 100 Idaho locations)\n  - sample_submission.csv\n- **Features:**\n  - Raw camera trap images\n  - Provided animal detection bounding boxes (optional)\n  - 23 animal classes + empty class (0)\n  - Location metadata for images\n\n## Evaluation Metrics\n- **Evaluation Metric:** Macro F1 Score\n  - F1 score calculated separately for each class (including \"empty\")\n  - Final score is the unweighted mean of all class F1 scores\n  - Handles class imbalance by giving equal weight to each class",
    "sections": {},
    "file_path": "kaggle_datasets/323/problem_summary.md"
  },
  "111": {
    "problem_id": "111",
    "title": "NCAA Basketball Tournament Outcome Prediction",
    "problem_type": "Binary Classification (with probabilistic outputs)",
    "objective": "Predict the probability of one team beating another in NCAA basketball tournament matchups. The competition has two stages:",
    "evaluation_metric": null,
    "full_content": "# NCAA Basketball Tournament Outcome Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. The competition has two stages:\n  * Stage 1: Predict outcomes for historical tournaments (past 5 years)\n  * Stage 2: Predict outcomes for the 2014 tournament before it begins\n* **Key Points:**\n  * Requires predicting probabilities for all possible team pairings (n*(n-1)/2 predictions for n teams)\n  * Participants are encouraged to incorporate external data sources beyond provided historical data\n  * Predictions must be submitted before tournament begins (no updates allowed)\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with historical basketball game results and team information\n* **Data Files:**\n  * `teams.csv` - Team IDs and names (356 teams)\n  * `seasons.csv` - Season identifiers and metadata\n  * `regular_season_results.csv` - Game results from regular seasons (1995-2013)\n  * `tourney_results.csv` - NCAA tournament game results (1995-2013)\n  * `tourney_seeds.csv` - Tournament seeding information\n  * `tourney_slots.csv` - Tournament bracket structure and pairing rules\n* **Key Features:**\n  * Team identifiers\n  * Game scores and outcomes\n  * Season and day numbers (for temporal analysis)\n  * Tournament seeds and bracket positions\n  * Game locations (home/away/neutral)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss (Logarithmic Loss)\n* **Metric Components:**\n  * Calculated as: \n    ```\n    LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]\n    ```\n    Where:\n    * n = number of games played\n    * ŷ_i = predicted probability of team 1 beating team 2\n    * y_i = actual outcome (1 if team 1 wins, 0 otherwise)\n  * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n  * Only scored on games that are actually played\n  * Lower scores indicate better performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. The competition has two stages:\n  * Stage 1: Predict outcomes for historical tournaments (past 5 years)\n  * Stage 2: Predict outcomes for the 2014 tournament before it begins\n* **Key Points:**\n  * Requires predicting probabilities for all possible team pairings (n*(n-1)/2 predictions for n teams)\n  * Participants are encouraged to incorporate external data sources beyond provided historical data\n  * Predictions must be submitted before tournament begins (no updates allowed)",
      "Dataset Overview": "* **Data Type:** Tabular data with historical basketball game results and team information\n* **Data Files:**\n  * `teams.csv` - Team IDs and names (356 teams)\n  * `seasons.csv` - Season identifiers and metadata\n  * `regular_season_results.csv` - Game results from regular seasons (1995-2013)\n  * `tourney_results.csv` - NCAA tournament game results (1995-2013)\n  * `tourney_seeds.csv` - Tournament seeding information\n  * `tourney_slots.csv` - Tournament bracket structure and pairing rules\n* **Key Features:**\n  * Team identifiers\n  * Game scores and outcomes\n  * Season and day numbers (for temporal analysis)\n  * Tournament seeds and bracket positions\n  * Game locations (home/away/neutral)",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss (Logarithmic Loss)\n* **Metric Components:**\n  * Calculated as: \n    ```\n    LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]\n    ```\n    Where:\n    * n = number of games played\n    * ŷ_i = predicted probability of team 1 beating team 2\n    * y_i = actual outcome (1 if team 1 wins, 0 otherwise)\n  * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n  * Only scored on games that are actually played\n  * Lower scores indicate better performance"
    },
    "file_path": "kaggle_datasets/111/problem_summary.md"
  },
  "575": {
    "problem_id": "575",
    "title": "RSNA 2023 Abdominal Trauma Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# RSNA 2023 Abdominal Trauma Detection\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Medical Image Analysis - CT Scans)\n* **Objective:** Develop an AI model to detect and classify traumatic abdominal injuries in CT scans, specifically:\n    * Identify injuries to organs (liver, spleen, kidneys, bowel)\n    * Detect active internal bleeding (extravasation)\n    * Grade injury severity (low/high for solid organs)\n* **Key Points:**\n    * Focus on rapid diagnosis for emergency trauma care\n    * Must handle multi-phase CT scans (different contrast timings)\n    * Includes both organ-level and image-level injury localization\n    * Dataset contains incomplete scans (marked in metadata)\n\n## Dataset Overview\n* **Data Type:** Medical Imaging (CT Scans in DICOM format) with tabular metadata\n* **Data Files:**\n    * `train.csv`/`test.csv` - Patient-level injury labels\n    * `[train/test]_images/` - DICOM CT scan slices (organized by patient/series)\n    * `[train/test]_series_meta.csv` - Scan phase and completeness metadata\n    * `image_level_labels.csv` - Frame-specific injury annotations (train only)\n    * `segmentations/` - Organ segmentation masks (NIFTI format, subset of train)\n* **Key Features:**\n    * 3D volumetric CT data with varying resolutions/parameters\n    * Multi-phase scans (via aortic_hu values)\n    * Binary and ternary classification targets per organ\n    * Pixel-level segmentations available for some scans\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Multi-class Log Loss\n    * **Weighting Scheme:**\n        * 1x: Healthy cases\n        * 2x: Low-grade solid organ injuries\n        * 4x: High-grade solid organ injuries\n        * 2x: Bowel injuries\n        * 6x: Extravasation (active bleeding)\n        * 6x: Any_injury composite label\n* **Submission Format:**\n    * Probability predictions for all injury classes per patient\n    * Required columns: `patient_id` + 13 injury probability columns (e.g., `bowel_healthy`, `bowel_injury`, `kidney_low`, etc.)",
    "sections": {},
    "file_path": "kaggle_datasets/575/problem_summary.md"
  },
  "588": {
    "problem_id": "588",
    "title": "Protein Function Prediction with Gene Ontology Terms",
    "problem_type": "Multi-label Classification (Biological Function Prediction)",
    "objective": "Predict the biological function of proteins by associating them with Gene Ontology (GO) terms across three subontologies:",
    "evaluation_metric": null,
    "full_content": "# Protein Function Prediction with Gene Ontology Terms\n\n**Problem Description:**\n* **Problem Type:** Multi-label Classification (Biological Function Prediction)\n* **Objective:** Predict the biological function of proteins by associating them with Gene Ontology (GO) terms across three subontologies:\n  * Molecular Function (MF) - What the protein does at a molecular level\n  * Biological Process (BP) - Biological processes the protein participates in\n  * Cellular Component (CC) - Location of the protein within the cell\n* **Key Points:**\n  * Prospective evaluation: Test set consists of proteins that gain experimental annotations after submission deadline\n  * Hierarchical classification: GO terms form a directed acyclic graph with parent-child relationships\n  * Multi-faceted predictions: Proteins may have multiple functions across subontologies\n  * Evidence-based training: Uses only experimentally validated GO term annotations\n\n**Dataset Overview:**\n* **Data Type:** Biological sequence data (protein amino-acid sequences) with hierarchical functional annotations\n* **Data Files:**\n  * `train_sequences.fasta`: Protein sequences for training (FASTA format)\n  * `train_terms.tsv`: GO term annotations for training proteins\n  * `train_taxonomy.tsv`: Species taxonomy IDs for training proteins\n  * `go-basic.obo`: Gene Ontology graph structure\n  * `testsuperset.fasta`: Protein sequences for prediction\n  * `IA.txt`: Term weights (information accretion) for evaluation\n* **Key Features:**\n  * Protein sequences as amino-acid strings\n  * Hierarchical GO term annotations (over 40,000 terms)\n  * Taxonomic information for evolutionary context\n  * Evidence codes distinguishing experimental vs predicted annotations\n\n**Evaluation Metrics:**\n* **Primary Metric:** Arithmetic mean of three maximum F-measures (one per subontology)\n* **Metric Components:**\n  * Weighted precision and recall using term-specific information accretion (IA) weights\n  * IA weights reflect term specificity in GO hierarchy (rare terms weighted higher)\n  * Final score = (F_max(MF) + F_max(BP) + F_max(CC)) / 3\n  * F-measure calculation based on information-theoretic evaluation of ontological annotations",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Classification (Biological Function Prediction)\n* **Objective:** Predict the biological function of proteins by associating them with Gene Ontology (GO) terms across three subontologies:\n  * Molecular Function (MF) - What the protein does at a molecular level\n  * Biological Process (BP) - Biological processes the protein participates in\n  * Cellular Component (CC) - Location of the protein within the cell\n* **Key Points:**\n  * Prospective evaluation: Test set consists of proteins that gain experimental annotations after submission deadline\n  * Hierarchical classification: GO terms form a directed acyclic graph with parent-child relationships\n  * Multi-faceted predictions: Proteins may have multiple functions across subontologies\n  * Evidence-based training: Uses only experimentally validated GO term annotations",
      "Dataset Overview": "* **Data Type:** Biological sequence data (protein amino-acid sequences) with hierarchical functional annotations\n* **Data Files:**\n  * `train_sequences.fasta`: Protein sequences for training (FASTA format)\n  * `train_terms.tsv`: GO term annotations for training proteins\n  * `train_taxonomy.tsv`: Species taxonomy IDs for training proteins\n  * `go-basic.obo`: Gene Ontology graph structure\n  * `testsuperset.fasta`: Protein sequences for prediction\n  * `IA.txt`: Term weights (information accretion) for evaluation\n* **Key Features:**\n  * Protein sequences as amino-acid strings\n  * Hierarchical GO term annotations (over 40,000 terms)\n  * Taxonomic information for evolutionary context\n  * Evidence codes distinguishing experimental vs predicted annotations",
      "Evaluation Metrics": "* **Primary Metric:** Arithmetic mean of three maximum F-measures (one per subontology)\n* **Metric Components:**\n  * Weighted precision and recall using term-specific information accretion (IA) weights\n  * IA weights reflect term specificity in GO hierarchy (rare terms weighted higher)\n  * Final score = (F_max(MF) + F_max(BP) + F_max(CC)) / 3\n  * F-measure calculation based on information-theoretic evaluation of ontological annotations"
    },
    "file_path": "kaggle_datasets/588/problem_summary.md"
  },
  "73": {
    "problem_id": "73",
    "title": "Handwriting Stroke Recovery from Offline Signature Images",
    "problem_type": "Time Series Regression (Trajectory Prediction)",
    "objective": "Predict the online trajectory (x, y coordinates over time) of handwritten signatures from their offline scanned images. The goal is to reconstruct the dynamic writing process (online data) from static images (offline data).",
    "evaluation_metric": null,
    "full_content": "# Handwriting Stroke Recovery from Offline Signature Images\n\n**Problem Description:**\n* **Problem Type:** Time Series Regression (Trajectory Prediction)\n* **Objective:** Predict the online trajectory (x, y coordinates over time) of handwritten signatures from their offline scanned images. The goal is to reconstruct the dynamic writing process (online data) from static images (offline data).\n    * **Key Points:**\n        * Focuses on forensic applications and handwriting recognition enhancement\n        * Requires normalization of predicted coordinates to (0,1) range\n        * Pressure information is not considered in this task\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * Offline data: Scanned signature images (JPEG format)\n    * Online data: Time-series coordinates (x,y) of writing trajectories\n* **Data Files:**\n    * `train.csv`: Contains online trajectory data for 605 signatures (time, normalized x/y coordinates)\n    * `test.csv`: Contains signature metadata for 476 signatures requiring trajectory prediction\n    * `images_gender.zip`: Contains all offline signature images\n    * `unnormalized_data.csv`: Unnormalized training coordinates\n* **Key Features:**\n    * Signature metadata (writer_id, occurrence_id)\n    * Time-series coordinates (time, x, y)\n    * All coordinates are normalized to (0,1) range\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Error (RMSE)\n    * Calculated between predicted and ground truth trajectories\n    * Evaluates both x and y coordinate predictions simultaneously\n    * Requires predictions to be normalized to (0,1) range before evaluation",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Regression (Trajectory Prediction)\n* **Objective:** Predict the online trajectory (x, y coordinates over time) of handwritten signatures from their offline scanned images. The goal is to reconstruct the dynamic writing process (online data) from static images (offline data).\n    * **Key Points:**\n        * Focuses on forensic applications and handwriting recognition enhancement\n        * Requires normalization of predicted coordinates to (0,1) range\n        * Pressure information is not considered in this task",
      "Dataset Overview": "* **Data Type & Context:** \n    * Offline data: Scanned signature images (JPEG format)\n    * Online data: Time-series coordinates (x,y) of writing trajectories\n* **Data Files:**\n    * `train.csv`: Contains online trajectory data for 605 signatures (time, normalized x/y coordinates)\n    * `test.csv`: Contains signature metadata for 476 signatures requiring trajectory prediction\n    * `images_gender.zip`: Contains all offline signature images\n    * `unnormalized_data.csv`: Unnormalized training coordinates\n* **Key Features:**\n    * Signature metadata (writer_id, occurrence_id)\n    * Time-series coordinates (time, x, y)\n    * All coordinates are normalized to (0,1) range",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Error (RMSE)\n    * Calculated between predicted and ground truth trajectories\n    * Evaluates both x and y coordinate predictions simultaneously\n    * Requires predictions to be normalized to (0,1) range before evaluation"
    },
    "file_path": "kaggle_datasets/73/problem_summary.md"
  },
  "118": {
    "problem_id": "118",
    "title": "Predicting Repeat Shoppers from Transactional Data",
    "problem_type": "Binary Classification",
    "objective": "Predict which shoppers, after redeeming an initial promotional offer, will become repeat buyers of the same product. The goal is to identify loyal customers *before* their first incentivized purchase.",
    "evaluation_metric": null,
    "full_content": "# Predicting Repeat Shoppers from Transactional Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict which shoppers, after redeeming an initial promotional offer, will become repeat buyers of the same product. The goal is to identify loyal customers *before* their first incentivized purchase.\n    * **Key Points:**\n        * Focuses on predicting customer loyalty *prior to initial offer redemption*\n        * Uses pre-offer shopping history to forecast post-offer behavior\n        * One of Kaggle's largest datasets at time of competition (~350M rows)\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular transactional data from retail promotions, containing:\n    * Complete anonymized shopping histories (basket-level)\n    * Incentive offer details\n    * Post-incentive purchase behavior (for training set)\n* **Data Files:**\n    * `transactions.csv`: 1+ year of pre-offer purchase history for all customers\n    * `trainHistory.csv`: Incentive details and repeat purchase outcomes (training labels)\n    * `testHistory.csv`: Incentive details without outcomes (prediction target)\n    * `offers.csv`: Metadata about promotional offers\n* **Key Features:**\n    * Anonymized transactional features (product categories, brands, purchase quantities/amounts)\n    * Store chain and geographical markers\n    * Offer characteristics (required purchase quantity, discount value)\n    * Negative values indicate product returns\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures how well the model ranks customers by predicted repeat-purchase probability\n    * Evaluated on predicted probabilities (`repeatProbability`) vs. binary `repeater` outcomes in test set",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict which shoppers, after redeeming an initial promotional offer, will become repeat buyers of the same product. The goal is to identify loyal customers *before* their first incentivized purchase.\n    * **Key Points:**\n        * Focuses on predicting customer loyalty *prior to initial offer redemption*\n        * Uses pre-offer shopping history to forecast post-offer behavior\n        * One of Kaggle's largest datasets at time of competition (~350M rows)",
      "Dataset Overview": "* **Data Type & Context:** Tabular transactional data from retail promotions, containing:\n    * Complete anonymized shopping histories (basket-level)\n    * Incentive offer details\n    * Post-incentive purchase behavior (for training set)\n* **Data Files:**\n    * `transactions.csv`: 1+ year of pre-offer purchase history for all customers\n    * `trainHistory.csv`: Incentive details and repeat purchase outcomes (training labels)\n    * `testHistory.csv`: Incentive details without outcomes (prediction target)\n    * `offers.csv`: Metadata about promotional offers\n* **Key Features:**\n    * Anonymized transactional features (product categories, brands, purchase quantities/amounts)\n    * Store chain and geographical markers\n    * Offer characteristics (required purchase quantity, discount value)\n    * Negative values indicate product returns",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures how well the model ranks customers by predicted repeat-purchase probability\n    * Evaluated on predicted probabilities (`repeatProbability`) vs. binary `repeater` outcomes in test set"
    },
    "file_path": "kaggle_datasets/118/problem_summary.md"
  },
  "87": {
    "problem_id": "87",
    "title": "Yelp Business Rating Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Yelp Business Rating Prediction\n\n## Problem Description\n* **Problem Type**: Regression (Rating Prediction)\n* **Objective**: Predict the star rating a user would assign to a business on Yelp, based on historical data. The goal is to build a recommender system that accurately forecasts user ratings for businesses they haven't reviewed yet.\n* **Key Points**:\n  * Focuses on personalized business recommendations\n  * Requires predicting ratings for (user_id, business_id) pairs in the test set\n  * Must handle cold-start problems (new users/businesses not in training data)\n\n## Dataset Overview\n* **Data Type**: JSON-structured user-business interaction data with tabular submission format\n* **Context**: Yelp reviews, businesses, users, and check-ins from Phoenix, AZ metropolitan area\n* **Data Files**:\n  * `yelp_training_set.zip`: Contains business, review, user, and checkin JSON files\n  * `yelp_test_set.zip`: Test set with same structure (but missing target ratings)\n  * `sample_submission.csv`: Submission format template\n* **Key Features**:\n  * Business attributes (location, categories, review count)\n  * User profiles (review history, average ratings)\n  * Review text and metadata (votes, dates)\n  * Temporal check-in patterns (hourly/daily frequencies)\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Error (RMSE)\n* **Calculation**:\n  * RMSE = √[Σ(p_i - a_i)² / n]\n  * Where:\n    * n = total number of review ratings to predict\n    * p_i = predicted rating for review i\n    * a_i = actual rating for review i\n* **Submission Format**:\n  * CSV with columns: RecommendationId, Stars\n  * Must predict ratings for all (user_id, business_id) pairs in test set",
    "sections": {},
    "file_path": "kaggle_datasets/87/problem_summary.md"
  },
  "543": {
    "problem_id": "543",
    "title": "NCAA Basketball Tournament Outcome Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NCAA Basketball Tournament Outcome Prediction\n\n## Problem Description\n- **Problem Type**: Binary Probability Prediction (Sports Outcome Forecasting)\n- **Objective**: Predict the probability of one NCAA Division I basketball team defeating another in the 2023 Men's and Women's tournaments. Participants must forecast outcomes for all possible team matchups before the tournaments begin.\n- **Key Points**:\n  - Combined prediction task for both Men's and Women's tournaments (previously separate competitions)\n  - Requires predictions for all possible team pairs (not just tournament-qualified teams)\n  - Historical data provided for model training (1985-2022 for men, 1998-2022 for women)\n  - Changed evaluation metric from log loss to Brier score for this edition\n\n## Dataset Overview\n- **Data Type**: Tabular sports performance data with temporal components\n- **Context**: NCAA Division I basketball game results and team statistics\n- **Key Data Files**:\n  - Team metadata (MTeams.csv, WTeams.csv)\n  - Seasonal results (MRegularSeasonCompactResults.csv, WRegularSeasonCompactResults.csv)\n  - Tournament results (MNCAATourneyCompactResults.csv, WNCAATourneyCompactResults.csv)\n  - Detailed box scores (2003+ for men, 2010+ for women)\n  - Tournament seeds and bracket structure\n  - Team rankings and conference affiliations\n- **Important Features**:\n  - Team identifiers (with gender-specific ID ranges)\n  - Game outcomes (scores, locations, overtime periods)\n  - Team performance statistics (FG%, rebounds, assists etc.)\n  - Tournament seeding information\n  - Temporal features (DayNum alignment across seasons)\n\n## Evaluation Metrics\n- **Primary Metric**: Brier score (equivalent to mean squared error for probability predictions)\n- **Metric Components**:\n  - Calculated as mean squared difference between predicted probabilities and actual outcomes (1 for win, 0 for loss)\n  - Penalizes both overconfidence (e.g. predicting 0.9 for a loss) and underconfidence\n  - Replaced log loss to reduce focus on extreme probability boundaries (0/1 predictions)\n  - Scores calculated separately for Men's and Women's tournaments then combined",
    "sections": {},
    "file_path": "kaggle_datasets/543/problem_summary.md"
  },
  "315": {
    "problem_id": "315",
    "title": "Gendered Pronoun Resolution",
    "problem_type": "NLP - Coreference Resolution (Multi-class Classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Gendered Pronoun Resolution\n\n**Problem Description:**\n* **Problem Type:** NLP - Coreference Resolution (Multi-class Classification)\n* **Objective:**  \n    * Build a pronoun resolution system that correctly pairs ambiguous pronouns (e.g., \"he\"/\"she\") to their referring entities (Name A, Name B, or NEITHER) in text passages.  \n    * Key focus: **Gender fairness** – the system must perform equally well on masculine and feminine pronouns, as the test set's gender distribution is intentionally undisclosed to prevent bias.  \n* **Key Points:**  \n    * Uses the **Gendered Ambiguous Pronouns (GAP) dataset**, explicitly designed with 50% masculine and 50% feminine pronoun examples.  \n    * Two-stage competition:  \n        * Stage 1: Public test set with known labels.  \n        * Stage 2: Private test set (withheld gender ratio) determines final rankings.  \n\n**Dataset Overview:**  \n* **Data Type:** Text data (Wikipedia passages) with annotated pronouns and candidate entities.  \n* **Data Files:**  \n    * `test_stage_1.tsv` / `test_stage_2.tsv` (test sets for each stage)  \n    * `sample_submission_stage_1.csv` (submission format example)  \n* **Key Features:**  \n    * `Text`: Paragraph containing the pronoun and candidate names.  \n    * `Pronoun`, `A`, `B`: Target pronoun and candidate names.  \n    * Offsets (`Pronoun-offset`, `A-offset`, `B-offset`): Character positions of entities in the text.  \n    * `URL`: Source Wikipedia page for context.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Multi-class logarithmic loss (log loss).  \n    * **Calculation:**  \n        * For each pronoun, predict probabilities for classes A, B, or NEITHER.  \n        * Log loss formula:  \n            ```  \n            logloss = −1/N ∑(i=1 to N) ∑(j=1 to 3) y_ij * log(p_ij)  \n            ```  \n            * `N`: Number of test samples.  \n            * `y_ij`: 1 if true class is `j`, else 0.  \n            * `p_ij`: Predicted probability for class `j` (clipped to [1e-15,",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Coreference Resolution (Multi-class Classification)\n* **Objective:**  \n    * Build a pronoun resolution system that correctly pairs ambiguous pronouns (e.g., \"he\"/\"she\") to their referring entities (Name A, Name B, or NEITHER) in text passages.  \n    * Key focus: **Gender fairness** – the system must perform equally well on masculine and feminine pronouns, as the test set's gender distribution is intentionally undisclosed to prevent bias.  \n* **Key Points:**  \n    * Uses the **Gendered Ambiguous Pronouns (GAP) dataset**, explicitly designed with 50% masculine and 50% feminine pronoun examples.  \n    * Two-stage competition:  \n        * Stage 1: Public test set with known labels.  \n        * Stage 2: Private test set (withheld gender ratio) determines final rankings.  \n\n**Dataset Overview:**  \n* **Data Type:** Text data (Wikipedia passages) with annotated pronouns and candidate entities.  \n* **Data Files:**  \n    * `test_stage_1.tsv` / `test_stage_2.tsv` (test sets for each stage)  \n    * `sample_submission_stage_1.csv` (submission format example)  \n* **Key Features:**  \n    * `Text`: Paragraph containing the pronoun and candidate names.  \n    * `Pronoun`, `A`, `B`: Target pronoun and candidate names.  \n    * Offsets (`Pronoun-offset`, `A-offset`, `B-offset`): Character positions of entities in the text.  \n    * `URL`: Source Wikipedia page for context.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Multi-class logarithmic loss (log loss).  \n    * **Calculation:**  \n        * For each pronoun, predict probabilities for classes A, B, or NEITHER.  \n        * Log loss formula:  \n            ```  \n            logloss = −1/N ∑(i=1 to N) ∑(j=1 to 3) y_ij * log(p_ij)  \n            ```  \n            * `N`: Number of test samples.  \n            * `y_ij`: 1 if true class is `j`, else 0.  \n            * `p_ij`: Predicted probability for class `j` (clipped to [1e-15,"
    },
    "file_path": "kaggle_datasets/315/problem_summary.md"
  },
  "127": {
    "problem_id": "127",
    "title": "Predicting Click-Through Rates on Display Ads",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that a user will click on a given display ad based on user and page context. The goal is to benchmark the most accurate machine learning algorithms for click-through rate (CTR) estimation.",
    "evaluation_metric": null,
    "full_content": "# Predicting Click-Through Rates on Display Ads\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a user will click on a given display ad based on user and page context. The goal is to benchmark the most accurate machine learning algorithms for click-through rate (CTR) estimation.\n    * **Key Points:**\n        * Focus on probabilistic prediction (not just binary classification)\n        * Uses real-world advertising data typically kept private\n        * Winning models were released as open source\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing anonymized display ad impressions with click labels\n* **Data Files:**\n    * `train.csv` - Chronologically ordered training data (7 days of subsampled click data)\n    * `test.csv` - Test data from the day following the training period\n    * `random_submission.csv` - Sample submission file\n* **Features:**\n    * Target: `Label` (1 for clicked, 0 for not clicked)\n    * 13 integer features (`I1-I13`, mostly count features)\n    * 26 categorical features (`C1-C26`, hashed for anonymization)\n    * Missing values represented by empty fields\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (Log Loss)\n    * **Key Characteristics:**\n        * Penalizes confident wrong predictions more heavily\n        * Suitable for probabilistic predictions\n        * Smaller values indicate better performance\n        * Calculated as: -1/N * Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)] where y_i is the true label and p_i is the predicted probability",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a user will click on a given display ad based on user and page context. The goal is to benchmark the most accurate machine learning algorithms for click-through rate (CTR) estimation.\n    * **Key Points:**\n        * Focus on probabilistic prediction (not just binary classification)\n        * Uses real-world advertising data typically kept private\n        * Winning models were released as open source",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing anonymized display ad impressions with click labels\n* **Data Files:**\n    * `train.csv` - Chronologically ordered training data (7 days of subsampled click data)\n    * `test.csv` - Test data from the day following the training period\n    * `random_submission.csv` - Sample submission file\n* **Features:**\n    * Target: `Label` (1 for clicked, 0 for not clicked)\n    * 13 integer features (`I1-I13`, mostly count features)\n    * 26 categorical features (`C1-C26`, hashed for anonymization)\n    * Missing values represented by empty fields",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (Log Loss)\n    * **Key Characteristics:**\n        * Penalizes confident wrong predictions more heavily\n        * Suitable for probabilistic predictions\n        * Smaller values indicate better performance\n        * Calculated as: -1/N * Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)] where y_i is the true label and p_i is the predicted probability"
    },
    "file_path": "kaggle_datasets/127/problem_summary.md"
  },
  "80": {
    "problem_id": "80",
    "title": "Multi-class Audio Classification of Bird Species",
    "problem_type": "Multi-class Classification (Audio Signal Processing)",
    "objective": "Identify which of 35 bird species are present in continuous 150-second audio recordings from three different forest locations. Participants must predict probabilities for each species' presence in test recordings.",
    "evaluation_metric": null,
    "full_content": "# Multi-class Audio Classification of Bird Species\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Audio Signal Processing)\n* **Objective:** Identify which of 35 bird species are present in continuous 150-second audio recordings from three different forest locations. Participants must predict probabilities for each species' presence in test recordings.\n* **Key Points:**\n  * Challenging due to background noise, bird sound variability, and overlapping songs.\n  * Focus on bioacoustic pattern recognition in continuous recordings.\n  * Optional use of phylogenetic relationships and weather data for context.\n\n**Dataset Overview:**\n* **Data Type & Context:** Audio recordings (.wav files) of bird songs with optional pre-extracted MFCC features. Recordings collected in natural forest environments with varying vegetation density.\n* **Data Files:**\n  * `species_numbers.csv` (species ID mapping)\n  * `phylogenetic_distance.txt` (optional evolutionary relationships)\n  * `weather.txt` (optional recording conditions)\n  * Training: `train_set.zip` (35 species-specific 30-sec clips)\n  * Testing: `test_set.zip` (90 continuous 150-sec recordings)\n  * Pre-processed features: `*_set_features.zip` (MFCCs in .mat/.csv)\n* **Key Features:**\n  * 44.1kHz 16-bit audio samples\n  * Three distinct forest environments (mature/young/open)\n  * Recordings taken at sunrise in Vallée Chevreuse (Paris)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under ROC Curve (AUC)\n  * Calculated per species, then aggregated\n  * Implementation examples provided for MATLAB, R, and Python\n* **Submission Format:**\n  * Requires probability predictions for all 35 species per test clip\n  * File structure: `clip, species, probability` with header row",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Audio Signal Processing)\n* **Objective:** Identify which of 35 bird species are present in continuous 150-second audio recordings from three different forest locations. Participants must predict probabilities for each species' presence in test recordings.\n* **Key Points:**\n  * Challenging due to background noise, bird sound variability, and overlapping songs.\n  * Focus on bioacoustic pattern recognition in continuous recordings.\n  * Optional use of phylogenetic relationships and weather data for context.",
      "Dataset Overview": "* **Data Type & Context:** Audio recordings (.wav files) of bird songs with optional pre-extracted MFCC features. Recordings collected in natural forest environments with varying vegetation density.\n* **Data Files:**\n  * `species_numbers.csv` (species ID mapping)\n  * `phylogenetic_distance.txt` (optional evolutionary relationships)\n  * `weather.txt` (optional recording conditions)\n  * Training: `train_set.zip` (35 species-specific 30-sec clips)\n  * Testing: `test_set.zip` (90 continuous 150-sec recordings)\n  * Pre-processed features: `*_set_features.zip` (MFCCs in .mat/.csv)\n* **Key Features:**\n  * 44.1kHz 16-bit audio samples\n  * Three distinct forest environments (mature/young/open)\n  * Recordings taken at sunrise in Vallée Chevreuse (Paris)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under ROC Curve (AUC)\n  * Calculated per species, then aggregated\n  * Implementation examples provided for MATLAB, R, and Python\n* **Submission Format:**\n  * Requires probability predictions for all 35 species per test clip\n  * File structure: `clip, species, probability` with header row"
    },
    "file_path": "kaggle_datasets/80/problem_summary.md"
  },
  "74": {
    "problem_id": "74",
    "title": "Predicting Product Launch Success with Early Sales Data",
    "problem_type": "Regression (Time Series Forecasting)",
    "objective": "Predict the number of units sold in week 26 for new product launches, using only sales data from the first 13 weeks.",
    "evaluation_metric": null,
    "full_content": "# Predicting Product Launch Success with Early Sales Data\n\n**Problem Description:**\n* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the number of units sold in week 26 for new product launches, using only sales data from the first 13 weeks.\n* **Key Points:**\n  * Focuses on early-stage prediction of long-term product success.\n  * Uses cumulative sales metrics and customer segmentation data.\n  * Requires handling of time-series patterns in weekly sales data.\n  * Must account for store distribution (known for full 26 weeks).\n\n**Dataset Overview:**\n* **Data Type:** Tabular time-series data of product sales metrics.\n* **Context:** Retail product launches with weekly sales tracking.\n* **Data Files:**\n  * Training set (2768 product launches with full 26-week data)\n  * Question set (1089 product launches with only 13-week data)\n* **Features:**\n  * Product category\n  * Weekly stores selling the product\n  * Weekly units sold\n  * Cumulative distinct customers (total and repeat buyers)\n  * Cumulative units sold to 11 different customer segments (e.g., Price Sensitive, Family Focussed)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Logarithmic Error (RMSLE)\n* **Components:**\n  * Uses natural logarithm of predicted vs actual week 26 sales\n  * Formula: sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))²)\n  * Designed to minimize impact of outliers\n  * Penalizes proportional errors rather than absolute differences",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the number of units sold in week 26 for new product launches, using only sales data from the first 13 weeks.\n* **Key Points:**\n  * Focuses on early-stage prediction of long-term product success.\n  * Uses cumulative sales metrics and customer segmentation data.\n  * Requires handling of time-series patterns in weekly sales data.\n  * Must account for store distribution (known for full 26 weeks).",
      "Dataset Overview": "* **Data Type:** Tabular time-series data of product sales metrics.\n* **Context:** Retail product launches with weekly sales tracking.\n* **Data Files:**\n  * Training set (2768 product launches with full 26-week data)\n  * Question set (1089 product launches with only 13-week data)\n* **Features:**\n  * Product category\n  * Weekly stores selling the product\n  * Weekly units sold\n  * Cumulative distinct customers (total and repeat buyers)\n  * Cumulative units sold to 11 different customer segments (e.g., Price Sensitive, Family Focussed)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Logarithmic Error (RMSLE)\n* **Components:**\n  * Uses natural logarithm of predicted vs actual week 26 sales\n  * Formula: sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))²)\n  * Designed to minimize impact of outliers\n  * Penalizes proportional errors rather than absolute differences"
    },
    "file_path": "kaggle_datasets/74/problem_summary.md"
  },
  "120": {
    "problem_id": "120",
    "title": "Predicting Exciting Education Projects on DonorsChoose.org",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Exciting Education Projects on DonorsChoose.org\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict whether a project posted on DonorsChoose.org will be \"exciting\" from a business perspective at the time of posting. Exciting projects are those that meet all of five specific criteria related to funding success, donor engagement, and donation quality.\n- **Key Points:**\n  - Projects must meet all five criteria to be considered exciting:\n    * Fully funded\n    * At least one teacher-referred donor\n    * Higher-than-average percentage of donors leaving original messages\n    * At least one \"green\" donation\n    * One or more additional quality indicators (multiple non-teacher donors, large donation, or donation from \"thoughtful\" donor)\n  - Requires analysis of both structured data (donation patterns, school characteristics) and unstructured text data (project essays, descriptions)\n  - Goal is early identification of high-potential projects to improve funding outcomes\n\n## Dataset Overview\n- **Data Type:** Relational dataset containing both tabular and text data\n- **Context:** Education project proposals and donation records from DonorsChoose.org\n- **Data Files:**\n  - `donations.csv`: Donation records (training set only)\n  - `essays.csv`: Project text (titles, descriptions, need statements)\n  - `projects.csv`: Project metadata (school info, resource requests, costs)\n  - `resources.csv`: Detailed resource requests\n  - `outcomes.csv`: Funding outcomes and exciting status labels (training set)\n  - `sampleSubmission.csv`: Submission format template\n- **Key Features:**\n  - Project characteristics (subject, grade level, resource type, cost)\n  - School demographics (location, poverty level, charter status)\n  - Teacher attributes (gender, program participation)\n  - Donation patterns (amounts, sources, payment methods)\n  - Textual content (project essays, need statements)\n  - Temporal features (posting dates, donation timestamps)\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n  - Measures the model's ability to distinguish between exciting and non-exciting projects\n  - Evaluates the ranking of predicted probabilities against the binary outcomes\n  - Particularly suitable for imbalanced classification problems where the positive class (exciting projects) is rare",
    "sections": {},
    "file_path": "kaggle_datasets/120/problem_summary.md"
  },
  "312": {
    "problem_id": "312",
    "title": "Pet Adoption Speed Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Pet Adoption Speed Prediction\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Ordinal)\n* **Objective:** Predict the speed at which a pet is adopted based on its online profile features. The goal is to help shelters optimize pet profiles to reduce euthanization rates by improving adoption appeal.\n    * **Key Points:**\n        * Predict one of 5 ordinal classes representing adoption speed (0 = same day, 4 = not adopted within 100 days).\n        * Profiles may represent groups of pets (adoption speed reflects when ALL pets are adopted).\n        * Uses multimodal data: tabular metadata, text descriptions, and pet images.\n\n## Dataset Overview\n* **Data Type & Context:** Multimodal dataset from Malaysian animal welfare platform PetFinder.my (2008-2018), containing:\n    * **Tabular data:** 23 features including pet attributes (age, breed, color), health status, and profile metrics (photo/video counts)\n    * **Text data:** Pet description fields (English/Malay/Chinese)\n    * **Image data:** Photos of pets (faces/contact info redacted)\n* **Key Files:**\n    * `train.csv`/`test.csv`: Core tabular data (23 features + target)\n    * `breed_labels.csv`, `color_labels.csv`, `state_labels.csv`: Metadata dictionaries\n    * Image files (`PetID-ImageNumber.jpg`) and JSON metadata from Google Vision API\n    * Sentiment JSON files from Google Natural Language API (description analysis)\n* **Notable Features:**\n    * `AdoptionSpeed`: Target variable (0-4 ordinal categories)\n    * `PhotoAmt`/`VideoAmt`: Profile media metrics\n    * `Description`: Free-text profile write-ups\n    * Image metadata: Face/label/text annotations from Vision API\n\n## Evaluation Metrics\n* **Primary Metric:** Quadratic Weighted Kappa (QWK)\n    * **Components:**\n        1. Constructs NxN histogram matrix O (actual vs predicted ratings)\n        2. Computes weight matrix w where w[i,j] = (i-j)²/(N-1)²\n        3. Generates expected ratings matrix E under null correlation\n        4. Calculates: κ = 1 - (sum(w[i,j]*O[i,j]) / sum(w[i,j]*E[i,j]))\n    * **Interpretation:**\n        *",
    "sections": {},
    "file_path": "kaggle_datasets/312/problem_summary.md"
  },
  "544": {
    "problem_id": "544",
    "title": "Binary Classification with a Tabular Kidney Stone Prediction Dataset",
    "problem_type": "Binary Classification",
    "objective": "Predict the likelihood of the presence of a kidney stone based on urine analysis features. Participants are tasked with building a model that outputs probabilities for the binary target variable (presence/absence of kidney stone).",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Tabular Kidney Stone Prediction Dataset\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the likelihood of the presence of a kidney stone based on urine analysis features. Participants are tasked with building a model that outputs probabilities for the binary target variable (presence/absence of kidney stone).\n* **Key Points:**\n  * Dataset is synthetically generated from a real-world kidney stone prediction dataset to balance realism with competition integrity.\n  * Encourages exploration of feature engineering and model iteration due to lightweight nature.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing urine analysis measurements and derived features for kidney stone prediction.\n* **Data Files:**\n  * `train.csv` - Contains features and binary target (`target`) for model training.\n  * `test.csv` - Contains features for which predictions must be made.\n  * `sample_submission.csv` - Demonstrates submission format with `id` and predicted probability columns.\n* **Features:** \n  * Features are anonymized but derived from real urine analysis measurements (e.g., pH, chemical concentrations).\n  * Synthetic generation aims to mimic original distributions while introducing controlled variations.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n  * Measures model's ability to distinguish between positive (kidney stone present) and negative classes.\n  * Higher values indicate better classification performance (range: 0.5 to 1.0).\n* **Submission Format:** Requires probability predictions for the positive class (`target`) for each test sample.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the likelihood of the presence of a kidney stone based on urine analysis features. Participants are tasked with building a model that outputs probabilities for the binary target variable (presence/absence of kidney stone).\n* **Key Points:**\n  * Dataset is synthetically generated from a real-world kidney stone prediction dataset to balance realism with competition integrity.\n  * Encourages exploration of feature engineering and model iteration due to lightweight nature.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing urine analysis measurements and derived features for kidney stone prediction.\n* **Data Files:**\n  * `train.csv` - Contains features and binary target (`target`) for model training.\n  * `test.csv` - Contains features for which predictions must be made.\n  * `sample_submission.csv` - Demonstrates submission format with `id` and predicted probability columns.\n* **Features:** \n  * Features are anonymized but derived from real urine analysis measurements (e.g., pH, chemical concentrations).\n  * Synthetic generation aims to mimic original distributions while introducing controlled variations.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n  * Measures model's ability to distinguish between positive (kidney stone present) and negative classes.\n  * Higher values indicate better classification performance (range: 0.5 to 1.0).\n* **Submission Format:** Requires probability predictions for the positive class (`target`) for each test sample."
    },
    "file_path": "kaggle_datasets/544/problem_summary.md"
  },
  "6": {
    "problem_id": "6",
    "title": "Predicting Short-Term Stock Price Movements",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Short-Term Stock Price Movements\n\n## Problem Description\n- **Problem Type:** Binary Classification (Time Series)\n- **Objective:** Predict whether a stock price will increase or decrease over the next 60 minutes (12 five-minute intervals) using intraday trading data.\n- **Key Points:**\n  - Focus on high-frequency trading applications\n  - Uses anonymized stock data to prevent lookup of real-world prices\n  - Predictions made at 5-minute intervals\n  - Combines multiple data sources: stock prices, sectoral data, economic indicators, expert predictions, and indices\n\n## Dataset Overview\n- **Data Type:** Time series tabular data\n- **Context:** Intraday financial trading data at 5-minute intervals\n- **Data Files:**\n  - TrainingData.zip (5922 observations)\n  - TestData.zip (2539 observations)\n- **Features:**\n  - 609 anonymized explanatory variables (named Variable...)\n  - Includes: stock prices, sectoral data, economic indicators, expert predictions, and indices\n  - First column is timestamp\n  - Target variable is binary (TargetVariable)\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n- **Metric Components:**\n  - Calculated using trapezoid method on ROC curve\n  - ROC curve plots sensitivity (true positive rate) vs. 1-specificity (false positive rate)\n  - Sensitivity = tp/(tp+fn)\n  - Specificity = tn/(tn+fp)\n  - Submissions can contain any real number (higher values indicate higher confidence in price increase)\n  - AUC is related to Gini index used in finance (Gini = 2*AUC - 1)",
    "sections": {},
    "file_path": "kaggle_datasets/6/problem_summary.md"
  },
  "385": {
    "problem_id": "385",
    "title": "TREC-COVID Information Retrieval for Pandemic Document Search",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# TREC-COVID Information Retrieval for Pandemic Document Search\n\n## Problem Description\n* **Problem Type:** NLP - Information Retrieval (Document Ranking)\n* **Objective:**  \n    * Build a retrieval system that ranks biomedical literature articles from the CORD-19 dataset based on their relevance to COVID-19 research topics.  \n    * Participants must generate ranked lists of documents for 40 topics (including new Round 3 topics and residual rankings for Rounds 1-2).  \n* **Key Points:**  \n    * Focus on dynamic search systems for pandemic response.  \n    * Documents are judged on a 3-level relevance scale (Relevant/Partially Relevant/Not Relevant).  \n    * Private leaderboard uses post-submission human annotations (partial assessment).  \n\n## Dataset Overview  \n* **Data Type & Context:**  \n    * Text data (biomedical research articles from CORD-19 corpus, last updated May 19, 2020).  \n    * Metadata and full-text JSON documents provided.  \n* **Key Files:**  \n    * `CORD-19/`: Document collection (12.73 GB).  \n    * `topics-rnd3.csv`: Topic IDs, queries, questions, and narratives.  \n    * `docids-rnd3.txt`: Eligible documents for Round 3 predictions.  \n    * `qrels.csv`: Pre-existing relevance judgments for prior rounds.  \n* **Features:**  \n    * `cord_uid` (document ID), article text/metadata, topic-specific queries.  \n\n## Evaluation Metrics  \n* **Primary Metric:** Normalized Discounted Cumulative Gain (NDCG).  \n* **Relevance Judgments:**  \n    * Human-annotated 3-level scale:  \n        * **Relevant**: Fully answers the topic question.  \n        * **Partially Relevant**: Provides partial answer.  \n        * **Not Relevant**: No relevant information.  \n    * Submissions ranked by predicted relevance per topic (`topicid-docid` pairs).",
    "sections": {},
    "file_path": "kaggle_datasets/385/problem_summary.md"
  },
  "28": {
    "problem_id": "28",
    "title": "One-Shot-Learning Gesture Recognition with Kinect Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# One-Shot-Learning Gesture Recognition with Kinect Data\n\n## Problem Description\n- **Problem Type:** Multi-class Sequence Classification (Gesture Recognition)\n- **Objective:** Develop a gesture recognizer capable of learning new gesture categories from a single training example per class (one-shot-learning) using Microsoft Kinect data. The system must classify sequences of 1-5 gestures from small vocabularies (8-15 unique gestures per batch).\n- **Key Points:**\n  - Focus on **user-dependent recognition** (same user in training and test)\n  - Gestures are performed with return to resting position between motions\n  - Emphasis on **generalizable feature learning** across different gesture lexicons\n  - Applications include sign language translation, appliance control, and gaming interfaces\n\n## Dataset Overview\n- **Data Type:** Paired RGB and Depth video sequences from Kinect camera\n- **Context:** Upper-body gestures performed by individuals in controlled settings\n- **Data Files:**\n  - Per batch: 47 RGB videos (`M_yy.avi`), 47 depth videos (`K_yy.avi`)\n  - Label files (`*_train.csv`, `*_test.csv`) with gesture sequences\n- **Features:**\n  - Time-series of 3D skeletal/hand positions (from depth data)\n  - Quasi-lossless compressed AVI format\n  - Normalized depth values (8-bit integers with provided normalization factors)\n  - Batches organized by unique gesture lexicons (hidden during competition)\n\n## Evaluation Metrics\n- **Primary Metric:** Normalized Levenshtein Distance (Edit Distance)\n  - Computed between predicted gesture sequence and ground truth\n  - Summed across all test sequences then divided by total number of gestures\n  - **Components:**\n    - Counts minimum edit operations (substitutions, insertions, deletions)\n    - Penalizes both incorrect classifications and sequence length errors\n    - Analogous to error rate (can exceed 1.0 for very poor predictions)",
    "sections": {},
    "file_path": "kaggle_datasets/28/problem_summary.md"
  },
  "527": {
    "problem_id": "527",
    "title": "Regression with a Tabular California Housing Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Regression with a Tabular California Housing Dataset\n\n## Problem Description\n* **Problem Type:** Regression\n* **Objective:** Predict the median house value (`MedHouseVal`) for California housing districts using synthetic tabular data derived from the original California Housing Dataset.\n    * **Key Points:**\n        * Dataset is synthetically generated to mimic real-world data while keeping test labels private.\n        * Participants are encouraged to explore differences between synthetic and original data.\n        * Lightweight challenge aimed at skill-building in feature engineering and model iteration.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing housing district attributes in California.\n    * **Data Files:**\n        * `train.csv` - Training data with target `MedHouseVal`.\n        * `test.csv` - Test data for which predictions must be made.\n        * `sample_submission.csv` - Example submission file.\n    * **Features:** Derived from the original California Housing Dataset (e.g., likely includes location, income, population, and housing characteristics). Exact features are anonymized but follow similar distributions to the original.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Root Mean Squared Error (RMSE)\n    * **Components:**\n        * RMSE = √(1/N * Σ(y_i - ŷ_i)^2), where:\n            * `y_i` = true value for instance `i`.\n            * `ŷ_i` = predicted value for instance `i`.\n            * `N` = total number of instances.\n    * Submissions must predict `MedHouseVal` for each `id` in the test set.",
    "sections": {},
    "file_path": "kaggle_datasets/527/problem_summary.md"
  },
  "143": {
    "problem_id": "143",
    "title": "Malware Family Classification Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Malware Family Classification Challenge\n\n## Problem Description\n- **Problem Type:** Multiclass Classification\n- **Objective:** Classify malware files into one of 9 predefined families based on their binary content and metadata. The goal is to develop a robust mechanism for grouping polymorphic malware variants into their respective families, which is crucial for anti-malware systems to efficiently analyze and detect malicious files.\n- **Key Points:**\n  - Malware files are polymorphic, meaning they are constantly modified to evade detection while maintaining the same malicious behavior.\n  - The challenge involves processing large volumes of data (hundreds of GBs of raw binary files).\n  - The dataset includes both raw hexadecimal representations of file content and metadata extracted using IDA disassembler.\n\n## Dataset Overview\n- **Data Type:** \n  - Raw binary files represented in hexadecimal format (without PE headers)\n  - Metadata logs containing function calls, strings, and other disassembler-extracted features\n- **Data Files:**\n  - `train.7z` - Raw training data files\n  - `trainLabels.csv` - Class labels for training set\n  - `test.7z` - Raw test data files\n  - `sampleSubmission.csv` - Example submission format\n  - `dataSample.csv` - Preview of dataset\n- **Features:**\n  - File ID (20-character hash)\n  - Hexadecimal representation of binary content\n  - Metadata features from disassembler (function calls, strings, etc.)\n  - 9 malware family classes (Ramnit, Lollipop, Kelihos_ver3, Vundo, Simda, Tracur, Kelihos_ver1, Obfuscator.ACY, Gatak)\n\n## Evaluation Metrics\n- **Evaluation Metric:** Multi-class Logarithmic Loss (Log Loss)\n- **Metric Components:**\n  - Formula: `logloss = -1/N * Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)`\n    - N = number of files in test set\n    - M = number of labels (9)\n    - y_ij = 1 if observation i is in class j, 0 otherwise\n    - p_ij = predicted probability that observation i belongs to class j\n  - Submitted probabilities are rescaled (each row divided by row sum)\n  - Probabilities are clipped to [10^-15",
    "sections": {},
    "file_path": "kaggle_datasets/143/problem_summary.md"
  },
  "371": {
    "problem_id": "371",
    "title": "COVID-19 Global Forecasting (Week 2)",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# COVID-19 Global Forecasting (Week 2)\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting\n* **Objective**: \n    * Forecast the cumulative number of confirmed COVID-19 cases and fatalities globally for future dates (April 1 to April 30, 2020). \n    * Secondary goal: Identify factors impacting COVID-19 transmission rates (e.g., policy interventions, environmental conditions, demographics).\n* **Key Points**:\n    * Focus on both accuracy and interpretability of factors influencing transmission.\n    * Participants are encouraged to incorporate external datasets (e.g., policy actions, climate data, healthcare capacity).\n    * Public/private leaderboard split based on time periods (March 19-April 1 for public, April 2-30 for private).\n\n## Dataset Overview\n* **Data Type**: Tabular time-series data with geographic granularity.\n* **Context**: Daily cumulative COVID-19 cases and fatalities by region, sourced from Johns Hopkins CSSE.\n* **Data Files**:\n    * `train.csv`: Historical data for model training.\n    * `test.csv`: Dates and regions for forecasting.\n    * `submission.csv`: Sample submission format (cumulative predictions).\n* **Features**:\n    * Geographic identifiers (e.g., region/country).\n    * Date fields.\n    * Target variables: `ConfirmedCases` and `Fatalities` (cumulative counts).\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n* **Components**:\n    * Calculated per target column (`ConfirmedCases`, `Fatalities`):\n        $$ \\text{RMSLE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2} $$\n        * \\( p_i \\): Predicted value.\n        * \\( a_i \\): Actual value.\n        * \\( n \\): Number of observations.\n    * Final score: Average RMSLE across both columns.",
    "sections": {},
    "file_path": "kaggle_datasets/371/problem_summary.md"
  },
  "17": {
    "problem_id": "17",
    "title": "Measuring Galaxy Ellipticity to Map Dark Matter",
    "problem_type": "Image Analysis / Regression (Ellipticity Estimation)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Measuring Galaxy Ellipticity to Map Dark Matter\n\n**Problem Description:**\n* **Problem Type:** Image Analysis / Regression (Ellipticity Estimation)\n* **Objective:**  \n    * Develop algorithms to measure tiny distortions (ellipticity) in simulated galaxy images caused by gravitational lensing from dark matter.\n    * Account for noise, blurring (Point Spread Function), and pixelization effects in the images.\n* **Key Points:**\n    * Galaxies have intrinsic ellipticity; dark matter induces additional small distortions.\n    * Participants must estimate two ellipticity components (e1, e2) for each galaxy.\n    * The challenge involves deconvolving the blurring effect using paired star images (representing the PSF).\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Image data (PNG files) of simulated galaxies and stars, representing noisy astronomical observations.\n    * Each galaxy image is paired with a star image showing the convolution kernel (PSF) applied to it.\n* **Data Files:**  \n    * Training: 40,000 galaxy-star image pairs (`mdm_galaxy_*.png`, `mdm_star_*.png`).\n    * Test: 60,000 galaxy-star image pairs.\n    * Auxiliary files: Training solutions (`mdm_training_solution.csv`), example submissions, and a PDF explaining image degradation effects.\n* **Features:**  \n    * Galaxy images: Noisy, pixelated 2D brightness profiles of elliptical objects.\n    * Star images: Pixelated versions of the PSF kernels used to blur galaxies.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square (RMS) of the difference between true and predicted ellipticity values (e1, e2).\n    * Calculated separately for e1 and e2, then combined via RMS.\n    * The mean ellipticity is not assumed to be zero; absolute values matter.",
    "sections": {
      "Problem Description": "* **Problem Type:** Image Analysis / Regression (Ellipticity Estimation)\n* **Objective:**  \n    * Develop algorithms to measure tiny distortions (ellipticity) in simulated galaxy images caused by gravitational lensing from dark matter.\n    * Account for noise, blurring (Point Spread Function), and pixelization effects in the images.\n* **Key Points:**\n    * Galaxies have intrinsic ellipticity; dark matter induces additional small distortions.\n    * Participants must estimate two ellipticity components (e1, e2) for each galaxy.\n    * The challenge involves deconvolving the blurring effect using paired star images (representing the PSF).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Image data (PNG files) of simulated galaxies and stars, representing noisy astronomical observations.\n    * Each galaxy image is paired with a star image showing the convolution kernel (PSF) applied to it.\n* **Data Files:**  \n    * Training: 40,000 galaxy-star image pairs (`mdm_galaxy_*.png`, `mdm_star_*.png`).\n    * Test: 60,000 galaxy-star image pairs.\n    * Auxiliary files: Training solutions (`mdm_training_solution.csv`), example submissions, and a PDF explaining image degradation effects.\n* **Features:**  \n    * Galaxy images: Noisy, pixelated 2D brightness profiles of elliptical objects.\n    * Star images: Pixelated versions of the PSF kernels used to blur galaxies.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square (RMS) of the difference between true and predicted ellipticity values (e1, e2).\n    * Calculated separately for e1 and e2, then combined via RMS.\n    * The mean ellipticity is not assumed to be zero; absolute values matter."
    },
    "file_path": "kaggle_datasets/17/problem_summary.md"
  },
  "188": {
    "problem_id": "188",
    "title": "NCAA Basketball Tournament Outcome Prediction",
    "problem_type": "Binary Classification (with probabilistic outputs)",
    "objective": "Predict the probability of one team beating another in NCAA basketball tournament matchups. Participants must forecast outcomes for:",
    "evaluation_metric": null,
    "full_content": "# NCAA Basketball Tournament Outcome Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. Participants must forecast outcomes for:\n    * **Stage 1:** Historical tournaments (2012-2015) for model validation\n    * **Stage 2:** All possible matchups in the 2016 tournament before it begins\n* **Key Points:**\n    * Requires predicting every possible team pairing (n*(n-1)/2 predictions)\n    * Play-in games are included in submissions but not scored\n    * Encourages incorporation of external data sources beyond provided datasets\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data covering 31 seasons (1985-2015) of NCAA basketball statistics, including:\n    * Team identifiers and seasonal properties\n    * Regular season and tournament game results\n    * Detailed box score metrics (2003-2015)\n    * Tournament seeding and bracket structure\n* **Key Data Files:**\n    * `Teams.csv` - Team IDs and metadata\n    * `Seasons.csv` - Yearly tournament configurations\n    * `RegularSeasonCompactResults.csv`/`TourneyCompactResults.csv` - Game outcomes\n    * `RegularSeasonDetailedResults.csv`/`TourneyDetailedResults.csv` - Advanced stats\n    * `TourneySeeds.csv`/`TourneySlots.csv` - Bracket positioning rules\n* **Notable Features:**\n    * Team IDs, scores, and game locations\n    * Seasonal day numbering system for temporal alignment\n    * Offensive/defensive basketball metrics (FG%, rebounds, etc.)\n    * Seed numbers and regional bracket positions\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n    * Formula: \n      ```\n      LogLoss = −1/n ∑[y_i*log(ŷ_i) + (1−y_i)*log(1−ŷ_i)]\n      ```\n    * Where:\n        * n = number of scored games\n        * ŷ_i = predicted probability of team 1 winning\n        * y_i = 1 if team 1 wins, 0 otherwise\n    * **Key Properties:**\n        * Heavily penalizes confident incorrect predictions\n        * Predictions clipped near 0/1 to avoid infinite penalties\n        * Only",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team beating another in NCAA basketball tournament matchups. Participants must forecast outcomes for:\n    * **Stage 1:** Historical tournaments (2012-2015) for model validation\n    * **Stage 2:** All possible matchups in the 2016 tournament before it begins\n* **Key Points:**\n    * Requires predicting every possible team pairing (n*(n-1)/2 predictions)\n    * Play-in games are included in submissions but not scored\n    * Encourages incorporation of external data sources beyond provided datasets",
      "Dataset Overview": "* **Data Type & Context:** Tabular data covering 31 seasons (1985-2015) of NCAA basketball statistics, including:\n    * Team identifiers and seasonal properties\n    * Regular season and tournament game results\n    * Detailed box score metrics (2003-2015)\n    * Tournament seeding and bracket structure\n* **Key Data Files:**\n    * `Teams.csv` - Team IDs and metadata\n    * `Seasons.csv` - Yearly tournament configurations\n    * `RegularSeasonCompactResults.csv`/`TourneyCompactResults.csv` - Game outcomes\n    * `RegularSeasonDetailedResults.csv`/`TourneyDetailedResults.csv` - Advanced stats\n    * `TourneySeeds.csv`/`TourneySlots.csv` - Bracket positioning rules\n* **Notable Features:**\n    * Team IDs, scores, and game locations\n    * Seasonal day numbering system for temporal alignment\n    * Offensive/defensive basketball metrics (FG%, rebounds, etc.)\n    * Seed numbers and regional bracket positions",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (LogLoss)\n    * Formula: \n      ```\n      LogLoss = −1/n ∑[y_i*log(ŷ_i) + (1−y_i)*log(1−ŷ_i)]\n      ```\n    * Where:\n        * n = number of scored games\n        * ŷ_i = predicted probability of team 1 winning\n        * y_i = 1 if team 1 wins, 0 otherwise\n    * **Key Properties:**\n        * Heavily penalizes confident incorrect predictions\n        * Predictions clipped near 0/1 to avoid infinite penalties\n        * Only"
    },
    "file_path": "kaggle_datasets/188/problem_summary.md"
  },
  "518": {
    "problem_id": "518",
    "title": "Analyzing the 2022 Kaggle Machine Learning & Data Science Survey",
    "problem_type": "Survey Analysis / Data Storytelling",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Analyzing the 2022 Kaggle Machine Learning & Data Science Survey\n\n**Problem Description:**\n* **Problem Type:** Survey Analysis / Data Storytelling\n* **Objective:**  \n    * Participants are tasked with creating a data-driven narrative exploring a specific subset of the data science and machine learning community represented in the 2022 Kaggle survey. The goal is to uncover insights about the impact, priorities, or concerns of a defined group (e.g., Python users, female data science students, etc.) through a combination of narrative text and data exploration.\n    * **Key Points:**\n        * Focus on a well-defined community within the broader survey respondents.\n        * Support the narrative with data visualizations and clear analysis.\n        * Submissions should be informative, original, and thought-provoking.\n\n**Dataset Overview:**\n* **Data Type:** Tabular survey data with mixed question types (multiple-choice and multiple-selection).\n* **Context:** Annual survey capturing the state of the data science and ML industry (23,997 responses, 43 questions).\n* **Data Files:**\n    * `kaggle_survey_2022_responses.csv`: Contains all survey responses (296 columns due to multi-selection questions).\n    * Supplementary files: `kaggle_survey_2022_answer_choices.pdf` (answer choices) and `kaggle_survey_2022_methodology.pdf` (survey methodology).\n* **Features:**  \n    * Responses cover demographics, tools, frameworks, industry trends, education, and more (e.g., programming languages used, job titles, company size).\n\n**Evaluation Metrics:**\n* **Evaluation Criteria:** Submissions are judged holistically on:\n    * **Composition:** Clarity and coherence of the narrative, supported by data and visualizations.\n    * **Originality:** Novelty of insights or perspective offered.\n    * **Documentation:** Reproducibility and clarity of code/data sources.\n* **Note:** No single quantitative metric is used; judging is qualitative based on the above criteria.",
    "sections": {
      "Problem Description": "* **Problem Type:** Survey Analysis / Data Storytelling\n* **Objective:**  \n    * Participants are tasked with creating a data-driven narrative exploring a specific subset of the data science and machine learning community represented in the 2022 Kaggle survey. The goal is to uncover insights about the impact, priorities, or concerns of a defined group (e.g., Python users, female data science students, etc.) through a combination of narrative text and data exploration.\n    * **Key Points:**\n        * Focus on a well-defined community within the broader survey respondents.\n        * Support the narrative with data visualizations and clear analysis.\n        * Submissions should be informative, original, and thought-provoking.",
      "Dataset Overview": "* **Data Type:** Tabular survey data with mixed question types (multiple-choice and multiple-selection).\n* **Context:** Annual survey capturing the state of the data science and ML industry (23,997 responses, 43 questions).\n* **Data Files:**\n    * `kaggle_survey_2022_responses.csv`: Contains all survey responses (296 columns due to multi-selection questions).\n    * Supplementary files: `kaggle_survey_2022_answer_choices.pdf` (answer choices) and `kaggle_survey_2022_methodology.pdf` (survey methodology).\n* **Features:**  \n    * Responses cover demographics, tools, frameworks, industry trends, education, and more (e.g., programming languages used, job titles, company size).",
      "Evaluation Metrics": "* **Evaluation Criteria:** Submissions are judged holistically on:\n    * **Composition:** Clarity and coherence of the narrative, supported by data and visualizations.\n    * **Originality:** Novelty of insights or perspective offered.\n    * **Documentation:** Reproducibility and clarity of code/data sources.\n* **Note:** No single quantitative metric is used; judging is qualitative based on the above criteria."
    },
    "file_path": "kaggle_datasets/518/problem_summary.md"
  },
  "376": {
    "problem_id": "376",
    "title": "COVID-19 Daily Case and Fatality Forecasting",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# COVID-19 Daily Case and Fatality Forecasting\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting with Quantile Regression\n* **Objective**: \n    * Predict daily COVID-19 confirmed cases and fatalities across global regions from May 12 to June 7, 2020\n    * Generate quantile estimates (0.05, 0.50, 0.95) for both targets\n    * Secondary goal: Identify factors impacting COVID-19 transmission rates\n* **Key Points**:\n    * Companion challenge to COVID-19 Open Research Dataset (CORD-19) initiative\n    * Participants are encouraged to incorporate external datasets\n    * Includes US county-level data (new in Week 5)\n    * Focus on both forecast accuracy and scientific insights\n\n## Dataset Overview\n* **Data Type**: Tabular time series data with geographical attributes\n* **Context**: Daily reported COVID-19 cases and fatalities by region\n* **Data Files**:\n    * `train.csv`: Historical daily case/fatality records\n    * `test.csv`: Future dates requiring predictions\n    * `submission.csv`: Sample submission format\n* **Features**:\n    * Temporal features (date)\n    * Geographical features (region/country, US counties)\n    * Target variables: ConfirmedCases and Fatalities\n    * Population data used for weighting\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Pinball Loss\n* **Metric Components**:\n    * Calculated separately for each quantile (0.05, 0.50, 0.95)\n    * Loss function for each quantile τ:\n        * Lτ(y,ŷ) = (y-ŷ)τ if y ≥ ŷ\n        * Lτ(y,ŷ) = (ŷ-y)(1-τ) if ŷ > y\n    * Weighting scheme:\n        * ConfirmedCases: log(population+1)⁻¹\n        * Fatalities: 10·log(population+1)⁻¹\n    * Final score averages across all quantiles and forecast days",
    "sections": {},
    "file_path": "kaggle_datasets/376/problem_summary.md"
  },
  "144": {
    "problem_id": "144",
    "title": "NLP - Missing Word Imputation in Billion Word Corpus",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NLP - Missing Word Imputation in Billion Word Corpus\n\n## Problem Description\n* **Problem Type:**  \n  * Natural Language Processing (NLP) - Word Imputation\n* **Objective:**  \n  * Predict and insert the correct missing word at the correct location in sentences from a large English corpus. Each test sentence has exactly one word removed (never the first or last word).\n* **Key Points:**  \n  * This is a variation on traditional language modeling, adapted for Kaggle's supervised ML setting.\n  * The task focuses on precise word-level imputation rather than full sentence probability scoring.\n  * The train/test split is unique to this competition and differs from the original corpus version.\n\n## Dataset Overview\n* **Data Type & Context:**  \n  * Text data consisting of English language sentences from the Billion Word Benchmark corpus.\n* **Data Files:**  \n  * `train.txt`: Large collection of complete sentences for training.\n  * `test.txt`: Sentences with one word removed per sentence (used for evaluation).\n* **Features:**  \n  * Raw text sentences with standard English punctuation.\n  * In test set: Sentences are missing exactly one non-terminal word (position chosen uniformly at random).\n\n## Evaluation Metrics\n* **Primary Metric:**  \n  * Mean Levenshtein distance between submitted sentences and original (complete) sentences.\n* **Metric Components:**  \n  * Levenshtein distance measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into another.\n  * The mean is taken across all test sentences to produce the final score.\n  * Partial credit is inherently given through the edit distance calculation.",
    "sections": {},
    "file_path": "kaggle_datasets/144/problem_summary.md"
  },
  "520": {
    "problem_id": "520",
    "title": "Binary Classification with Model Blending Practice",
    "problem_type": "Binary Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Model Blending Practice\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Participants are tasked with improving binary classification predictions by blending (ensembling) multiple model submissions. The goal is to create a combined prediction that outperforms individual models.\n    * The competition focuses on practicing ensemble techniques to achieve better predictive performance through blending strategies.\n* **Key Points:**\n    * The ground truth labels are only available for the first half of the prediction rows, allowing participants to evaluate their blending strategies before submission.\n    * The competition is designed to be beginner-friendly, serving as practice for ensemble methods.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Tabular data consisting of binary classification predictions from multiple models.\n    * The dataset is structured as a collection of model submission files with corresponding ground truth labels for validation.\n* **Data Files:**\n    * `submission_files/` - Folder containing binary model predictions (5002 files).\n    * `train_labels.csv` - Ground truth labels for the first half of rows in the submission files.\n    * `sample_submission.csv` - Sample submission file with row IDs for the second half of rows (used for final submission).\n* **Features:**\n    * Each submission file contains predicted probabilities for binary classification.\n    * The `train_labels.csv` provides the true binary labels (0 or 1) for validation.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Log Loss (Logarithmic Loss)\n    * The metric is calculated as:  \n      `LogLoss = −1/n * Σ[y_i * log(ŷ_i) + (1−y_i) * log(1−ŷ_i)]`\n    * Where:\n        * `n` = number of observations\n        * `ŷ_i` = predicted probability\n        * `y_i` = true binary label (0 or 1)\n    * Key characteristics:\n        * Heavily penalizes confident but wrong predictions\n        * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n        * Lower values indicate better performance (perfect prediction = 0)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Participants are tasked with improving binary classification predictions by blending (ensembling) multiple model submissions. The goal is to create a combined prediction that outperforms individual models.\n    * The competition focuses on practicing ensemble techniques to achieve better predictive performance through blending strategies.\n* **Key Points:**\n    * The ground truth labels are only available for the first half of the prediction rows, allowing participants to evaluate their blending strategies before submission.\n    * The competition is designed to be beginner-friendly, serving as practice for ensemble methods.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Tabular data consisting of binary classification predictions from multiple models.\n    * The dataset is structured as a collection of model submission files with corresponding ground truth labels for validation.\n* **Data Files:**\n    * `submission_files/` - Folder containing binary model predictions (5002 files).\n    * `train_labels.csv` - Ground truth labels for the first half of rows in the submission files.\n    * `sample_submission.csv` - Sample submission file with row IDs for the second half of rows (used for final submission).\n* **Features:**\n    * Each submission file contains predicted probabilities for binary classification.\n    * The `train_labels.csv` provides the true binary labels (0 or 1) for validation.",
      "Evaluation Metrics": "* **Evaluation Metric:** Log Loss (Logarithmic Loss)\n    * The metric is calculated as:  \n      `LogLoss = −1/n * Σ[y_i * log(ŷ_i) + (1−y_i) * log(1−ŷ_i)]`\n    * Where:\n        * `n` = number of observations\n        * `ŷ_i` = predicted probability\n        * `y_i` = true binary label (0 or 1)\n    * Key characteristics:\n        * Heavily penalizes confident but wrong predictions\n        * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n        * Lower values indicate better performance (perfect prediction = 0)"
    },
    "file_path": "kaggle_datasets/520/problem_summary.md"
  },
  "382": {
    "problem_id": "382",
    "title": "Plant Pathology Classification in Apple Leaves",
    "problem_type": "Multi-class Image Classification (Computer Vision)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Plant Pathology Classification in Apple Leaves\n\n**Problem Description:**\n* **Problem Type:** Multi-class Image Classification (Computer Vision)\n* **Objective:** \n    * Accurately classify images of apple leaves into disease categories: healthy, rust, scab, or multiple diseases (combinations).\n    * Address challenges in agricultural disease diagnosis through computer vision.\n* **Key Points:**\n    * Must handle variance in symptoms due to:\n        * Age of infected tissues\n        * Genetic variations\n        * Lighting conditions (angle, shade)\n        * Physiological age of leaves\n    * Requires distinguishing between multiple diseases on a single leaf\n    * Must deal with rare classes and novel symptoms\n    * Incorporates expert knowledge in feature identification\n\n**Dataset Overview:**\n* **Data Type:** Image data (leaf photographs) with tabular labels\n* **Context:** Agricultural plant pathology - apple foliar diseases\n* **Data Files:**\n    * `train.csv`: Contains image IDs and multi-label classifications (healthy, rust, scab, combinations)\n    * `test.csv`: Contains image IDs for prediction\n    * `sample_submission.csv`: Example submission format\n    * `images/`: Folder containing JPG images of apple leaves (train and test sets)\n* **Features:**\n    * High-resolution images of apple leaves under varying conditions\n    * Multi-label classification targets (not mutually exclusive)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean column-wise ROC AUC (Average of individual AUCs for each predicted class)\n    * Calculated separately for each target class (healthy, rust, scab, combinations)\n    * Final score is the average of these individual AUC scores\n* **Submission Format:**\n    * Requires probability predictions for all four classes per test image\n    * Format: `image_id, healthy_prob, rust_prob, scab_prob, combinations_prob`",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Image Classification (Computer Vision)\n* **Objective:** \n    * Accurately classify images of apple leaves into disease categories: healthy, rust, scab, or multiple diseases (combinations).\n    * Address challenges in agricultural disease diagnosis through computer vision.\n* **Key Points:**\n    * Must handle variance in symptoms due to:\n        * Age of infected tissues\n        * Genetic variations\n        * Lighting conditions (angle, shade)\n        * Physiological age of leaves\n    * Requires distinguishing between multiple diseases on a single leaf\n    * Must deal with rare classes and novel symptoms\n    * Incorporates expert knowledge in feature identification",
      "Dataset Overview": "* **Data Type:** Image data (leaf photographs) with tabular labels\n* **Context:** Agricultural plant pathology - apple foliar diseases\n* **Data Files:**\n    * `train.csv`: Contains image IDs and multi-label classifications (healthy, rust, scab, combinations)\n    * `test.csv`: Contains image IDs for prediction\n    * `sample_submission.csv`: Example submission format\n    * `images/`: Folder containing JPG images of apple leaves (train and test sets)\n* **Features:**\n    * High-resolution images of apple leaves under varying conditions\n    * Multi-label classification targets (not mutually exclusive)",
      "Evaluation Metrics": "* **Primary Metric:** Mean column-wise ROC AUC (Average of individual AUCs for each predicted class)\n    * Calculated separately for each target class (healthy, rust, scab, combinations)\n    * Final score is the average of these individual AUC scores\n* **Submission Format:**\n    * Requires probability predictions for all four classes per test image\n    * Format: `image_id, healthy_prob, rust_prob, scab_prob, combinations_prob`"
    },
    "file_path": "kaggle_datasets/382/problem_summary.md"
  },
  "1": {
    "problem_id": "1",
    "title": "Forecasting Eurovision Song Contest Voting Patterns",
    "problem_type": "Time Series Forecasting / Multi-output Regression",
    "objective": "Predict the voting matrix for the 2010 Eurovision Song Contest final, where each voting country allocates specific scores (1-8, 10, 12) to competing countries. The task involves:",
    "evaluation_metric": null,
    "full_content": "# Forecasting Eurovision Song Contest Voting Patterns\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting / Multi-output Regression\n* **Objective:** Predict the voting matrix for the 2010 Eurovision Song Contest final, where each voting country allocates specific scores (1-8, 10, 12) to competing countries. The task involves:\n  * Predicting which 25 countries will qualify for the final (with 5 countries automatically qualifying)\n  * Forecasting the complete voting pattern matrix for all voting countries\n* **Key Points:**\n  * Must adhere to Eurovision's positional voting rules (each voting country assigns each score exactly once)\n  * Historical voting patterns, regional alliances, and cultural factors are known to influence results\n  * Betting market data can be used as a proxy for performance quality\n\n**Dataset Overview:**\n* **Data Type:** Tabular data containing historical voting patterns, song/artist metadata, and betting odds\n* **Data Files:**\n  * `Final data by year.csv` (1998-2009 finals data)\n  * `Semi-final data by year.csv` (2004-2009 semi-finals data)\n  * `2010 Data.csv` (current year's competitor information)\n* **Key Features:**\n  * Historical voting matrices (country-to-country scores)\n  * Song/artist metadata (language, gender, group/solo)\n  * Regional classifications (Scandinavia, Western Europe, etc.)\n  * Betting odds data\n  * Host country information\n  * Performance outcomes (placement, total points)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Absolute Error\n  * Calculated as the sum of absolute differences between predicted and actual votes across the entire matrix\n  * Example: For each cell in the voting matrix, compute |predicted - actual| and sum all values\n  * Lower scores indicate better performance (smaller deviations from actual votes)\n  * Matrix must comply with competition rules (proper score allocation per voting country)",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting / Multi-output Regression\n* **Objective:** Predict the voting matrix for the 2010 Eurovision Song Contest final, where each voting country allocates specific scores (1-8, 10, 12) to competing countries. The task involves:\n  * Predicting which 25 countries will qualify for the final (with 5 countries automatically qualifying)\n  * Forecasting the complete voting pattern matrix for all voting countries\n* **Key Points:**\n  * Must adhere to Eurovision's positional voting rules (each voting country assigns each score exactly once)\n  * Historical voting patterns, regional alliances, and cultural factors are known to influence results\n  * Betting market data can be used as a proxy for performance quality",
      "Dataset Overview": "* **Data Type:** Tabular data containing historical voting patterns, song/artist metadata, and betting odds\n* **Data Files:**\n  * `Final data by year.csv` (1998-2009 finals data)\n  * `Semi-final data by year.csv` (2004-2009 semi-finals data)\n  * `2010 Data.csv` (current year's competitor information)\n* **Key Features:**\n  * Historical voting matrices (country-to-country scores)\n  * Song/artist metadata (language, gender, group/solo)\n  * Regional classifications (Scandinavia, Western Europe, etc.)\n  * Betting odds data\n  * Host country information\n  * Performance outcomes (placement, total points)",
      "Evaluation Metrics": "* **Primary Metric:** Absolute Error\n  * Calculated as the sum of absolute differences between predicted and actual votes across the entire matrix\n  * Example: For each cell in the voting matrix, compute |predicted - actual| and sum all values\n  * Lower scores indicate better performance (smaller deviations from actual votes)\n  * Matrix must comply with competition rules (proper score allocation per voting country)"
    },
    "file_path": "kaggle_datasets/1/problem_summary.md"
  },
  "349": {
    "problem_id": "349",
    "title": "Cloud Structure Segmentation from Satellite Images",
    "problem_type": "Computer Vision - Semantic Segmentation",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Cloud Structure Segmentation from Satellite Images\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Semantic Segmentation\n* **Objective:**  \n    * Build a model to classify and segment cloud organization patterns (`Fish`, `Flower`, `Gravel`, `Sugar`) in satellite images.  \n    * The goal is to identify pixel-wise regions of each cloud formation type to aid climate research and improve climate models.  \n* **Key Points:**  \n    * Multi-label segmentation: Each image may contain 1-4 cloud formation types.  \n    * Human-labeled ground truth derived from crowd-sourcing by climate scientists.  \n    * Predictions must be scaled down to 25% of original image dimensions (350x525 px).  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Satellite images (true-color) from NASA's TERRA and AQUA satellites, covering 21° longitude × 14° latitude regions.  \n    * Images may contain black bands due to orbital stitching artifacts.  \n* **Data Files:**  \n    * `train_images.zip`: Folder of training images (1400x2100 px).  \n    * `train.csv`: Run-length encoded (RLE) segmentation masks for each image-label pair.  \n    * `test_images.zip`: Test images for prediction.  \n    * `sample_submission.csv`: Submission template with `Image_Label` and `EncodedPixels` columns.  \n* **Key Features:**  \n    * Images: High-resolution (scaled to 350x525 px for predictions).  \n    * Labels: Four cloud formation classes with RLE-encoded pixel masks.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Mean Dice Coefficient (Sørensen-Dice index).  \n    * Formula: \\( \\frac{2 \\times |X \\cap Y|}{|X| + |Y|} \\), where:  \n        * \\( X \\): Predicted pixel set.  \n        * \\( Y \\): Ground truth pixel set.  \n    * Handles empty masks (score = 1 if both empty).  \n* **Submission Format:**  \n    * CSV with `Image_Label` (e.g., `002f507.jpg_Fish`) and RLE-encoded `EncodedPixels`.  \n    * RLE pairs must be sorted, positive, and non-duplicated (e.g., `start_pixel run_length`).",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Semantic Segmentation\n* **Objective:**  \n    * Build a model to classify and segment cloud organization patterns (`Fish`, `Flower`, `Gravel`, `Sugar`) in satellite images.  \n    * The goal is to identify pixel-wise regions of each cloud formation type to aid climate research and improve climate models.  \n* **Key Points:**  \n    * Multi-label segmentation: Each image may contain 1-4 cloud formation types.  \n    * Human-labeled ground truth derived from crowd-sourcing by climate scientists.  \n    * Predictions must be scaled down to 25% of original image dimensions (350x525 px).  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Satellite images (true-color) from NASA's TERRA and AQUA satellites, covering 21° longitude × 14° latitude regions.  \n    * Images may contain black bands due to orbital stitching artifacts.  \n* **Data Files:**  \n    * `train_images.zip`: Folder of training images (1400x2100 px).  \n    * `train.csv`: Run-length encoded (RLE) segmentation masks for each image-label pair.  \n    * `test_images.zip`: Test images for prediction.  \n    * `sample_submission.csv`: Submission template with `Image_Label` and `EncodedPixels` columns.  \n* **Key Features:**  \n    * Images: High-resolution (scaled to 350x525 px for predictions).  \n    * Labels: Four cloud formation classes with RLE-encoded pixel masks.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Mean Dice Coefficient (Sørensen-Dice index).  \n    * Formula: \\( \\frac{2 \\times |X \\cap Y|}{|X| + |Y|} \\), where:  \n        * \\( X \\): Predicted pixel set.  \n        * \\( Y \\): Ground truth pixel set.  \n    * Handles empty masks (score = 1 if both empty).  \n* **Submission Format:**  \n    * CSV with `Image_Label` (e.g., `002f507.jpg_Fish`) and RLE-encoded `EncodedPixels`.  \n    * RLE pairs must be sorted, positive, and non-duplicated (e.g., `start_pixel run_length`)."
    },
    "file_path": "kaggle_datasets/349/problem_summary.md"
  },
  "10": {
    "problem_id": "10",
    "title": "R Package Recommendation Engine for R Programmers",
    "problem_type": "Binary Classification (Recommendation System)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# R Package Recommendation Engine for R Programmers\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Recommendation System)\n* **Objective:**  \n    Develop a recommendation engine to predict whether a given R package (P) will be installed by a specific user (U). The goal is to help new R programmers identify high-value packages to learn by modeling installation patterns from metadata and user behavior.\n    * **Key Points:**\n        * Predict installation probability (binary: installed/not installed) for package-user pairs.\n        * Leverage package metadata (e.g., maintainer quality, dependency graphs) and user behavior.\n        * Generalize predictions to unseen test data (33,125 rows) after training on 99,640 rows.\n        * Participants are encouraged to explore advanced techniques (e.g., nearest neighbors, regularization, graph metrics) beyond the provided logistic regression baseline.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    Tabular data containing R package installation records and metadata for 1,865 packages across 52 users.\n* **Data Files:**\n    * `training_data.csv`: Installation records (99,640 rows) with package metadata features.\n    * `test.csv` / `test_data.csv`: Test set (33,125 rows) for evaluation.\n    * `example_submission.csv`: Submission format template.\n* **Features:**  \n    * Package metadata (e.g., maintainer info, dependency graphs, in-degree/out-degree metrics).\n    * User-package interaction labels (binary installation status in training data).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n    * **Why AUC?**  \n        * Robust to class imbalance (unequal installed/not-installed ratios).\n        * Evaluates model performance across all classification thresholds.\n    * **Components:**  \n        * **ROC Curve:** Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at varying thresholds.\n        * **AUC Calculation:** Area under the ROC curve (1 = perfect, 0.5 = random).",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Recommendation System)\n* **Objective:**  \n    Develop a recommendation engine to predict whether a given R package (P) will be installed by a specific user (U). The goal is to help new R programmers identify high-value packages to learn by modeling installation patterns from metadata and user behavior.\n    * **Key Points:**\n        * Predict installation probability (binary: installed/not installed) for package-user pairs.\n        * Leverage package metadata (e.g., maintainer quality, dependency graphs) and user behavior.\n        * Generalize predictions to unseen test data (33,125 rows) after training on 99,640 rows.\n        * Participants are encouraged to explore advanced techniques (e.g., nearest neighbors, regularization, graph metrics) beyond the provided logistic regression baseline.",
      "Dataset Overview": "* **Data Type & Context:**  \n    Tabular data containing R package installation records and metadata for 1,865 packages across 52 users.\n* **Data Files:**\n    * `training_data.csv`: Installation records (99,640 rows) with package metadata features.\n    * `test.csv` / `test_data.csv`: Test set (33,125 rows) for evaluation.\n    * `example_submission.csv`: Submission format template.\n* **Features:**  \n    * Package metadata (e.g., maintainer info, dependency graphs, in-degree/out-degree metrics).\n    * User-package interaction labels (binary installation status in training data).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC).  \n    * **Why AUC?**  \n        * Robust to class imbalance (unequal installed/not-installed ratios).\n        * Evaluates model performance across all classification thresholds.\n    * **Components:**  \n        * **ROC Curve:** Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at varying thresholds.\n        * **AUC Calculation:** Area under the ROC curve (1 = perfect, 0.5 = random)."
    },
    "file_path": "kaggle_datasets/10/problem_summary.md"
  },
  "516": {
    "problem_id": "516",
    "title": "Horse Racing Tactics and Path Efficiency Analysis",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Horse Racing Tactics and Path Efficiency Analysis\n\n## **Problem Description:**\n*   **Problem Type:** Data Analytics / Sports Performance Modeling\n*   **Objective:** Analyze horse racing tracking data to derive actionable insights for improving equine welfare, racing strategies, and path efficiency. Participants are tasked with exploring novel ways to interpret X/Y coordinate data of horses during races to address key questions in the sport.\n*   **Key Points:**\n    *   Develop metrics or models to evaluate:\n        *   Horse performance (expected vs. actual finish position)\n        *   Optimal racing strategies (venue/surface/distance-specific)\n        *   Jockey path efficiency ratings\n        *   Surface fairness (impact of track paths on outcomes)\n        *   Drafting benefits and gait pattern optimization\n    *   Emphasis on practical applications for owners, trainers, and veterinarians\n    *   Submissions should prioritize depth over breadth (focused analysis preferred)\n\n## **Dataset Overview:**\n*   **Data Type & Context:** Tabular data containing horse racing tracking information, including real-time positional coordinates and race metadata from NYRA tracks (Aqueduct, Belmont, Saratoga).\n*   **Data Files:**\n    *   `nyra_start_table.csv` - Horse/jockey race data (weight, odds, finish position)\n    *   `nyra_race_table.csv` - Race details (distance, course type, conditions)\n    *   `nyra_tracking_table.csv` - High-frequency positional data (latitude/longitude per 0.25s)\n    *   `nyra_2019_complete.csv` - Combined dataset of all three files\n*   **Key Features:**\n    *   Temporal-spatial tracking: `trakus_index`, `latitude`, `longitude`\n    *   Race context: `track_condition`, `course_type`, `distance_id`\n    *   Performance metrics: `position_at_finish`, `odds`, `weight_carried`\n\n## **Evaluation Metrics:**\n*   **Evaluation Method:** Multi-criteria scoring rubric (0-100 points) assessing:\n    *   **Innovation (25 pts):**\n        *   Novelty of approach (10)\n        *   Methodological rigor (5)\n        *   Potential to challenge status quo (10)\n    *   **Relevance (30 pts):**\n        *   Impact on welfare/performance (10)\n        *   Scalability",
    "sections": {},
    "file_path": "kaggle_datasets/516/problem_summary.md"
  },
  "340": {
    "problem_id": "340",
    "title": "Visual Relationship Detection in Open Images",
    "problem_type": "Computer Vision - Visual Relationship Detection",
    "objective": "Detect pairs of objects and the relationships that connect them in images. The task involves:",
    "evaluation_metric": null,
    "full_content": "# Visual Relationship Detection in Open Images\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Visual Relationship Detection\n* **Objective:** Detect pairs of objects and the relationships that connect them in images. The task involves:\n    * Identifying bounding boxes for two related objects\n    * Classifying the relationship between them\n    * Handling various relationship types including:\n        * Human-object relationships (e.g., \"woman playing guitar\")\n        * Object-object relationships (e.g., \"beer on table\")\n        * Object-attribute relationships (e.g., \"handbag is made of leather\")\n\n**Dataset Overview:**\n* **Data Type:** Image data with relationship annotations\n* **Context:** Large-scale dataset of diverse images with visual relationship annotations\n* **Data Files:**\n    * test.zip - 99,999 test images\n    * VRD_sample_submission.csv - sample submission file\n* **Key Features:**\n    * 329 relationship triplets in training set\n    * 375k training samples\n    * Bounding box coordinates for object pairs\n    * Relationship labels between objects\n\n**Evaluation Metrics:**\n* Weighted combination of three metrics:\n    * 40% weight: mAP (mean Average Precision) on relationships detection\n    * 20% weight: Recall@50\n    * 40% weight: mAP on phrase detection\n* Implementation:\n    * Uses TensorFlow Object Detection API\n    * Mean is taken over per-relationship APs for mAP components",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Visual Relationship Detection\n* **Objective:** Detect pairs of objects and the relationships that connect them in images. The task involves:\n    * Identifying bounding boxes for two related objects\n    * Classifying the relationship between them\n    * Handling various relationship types including:\n        * Human-object relationships (e.g., \"woman playing guitar\")\n        * Object-object relationships (e.g., \"beer on table\")\n        * Object-attribute relationships (e.g., \"handbag is made of leather\")",
      "Dataset Overview": "* **Data Type:** Image data with relationship annotations\n* **Context:** Large-scale dataset of diverse images with visual relationship annotations\n* **Data Files:**\n    * test.zip - 99,999 test images\n    * VRD_sample_submission.csv - sample submission file\n* **Key Features:**\n    * 329 relationship triplets in training set\n    * 375k training samples\n    * Bounding box coordinates for object pairs\n    * Relationship labels between objects",
      "Evaluation Metrics": "* Weighted combination of three metrics:\n    * 40% weight: mAP (mean Average Precision) on relationships detection\n    * 20% weight: Recall@50\n    * 40% weight: mAP on phrase detection\n* Implementation:\n    * Uses TensorFlow Object Detection API\n    * Mean is taken over per-relationship APs for mAP components"
    },
    "file_path": "kaggle_datasets/340/problem_summary.md"
  },
  "172": {
    "problem_id": "172",
    "title": "Predicting Direct Mail Response for Loan Marketing",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a customer will respond to a direct mail offer for personal/auto loans, helping Springleaf optimize their targeted marketing efforts.",
    "evaluation_metric": null,
    "full_content": "# Predicting Direct Mail Response for Loan Marketing\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a customer will respond to a direct mail offer for personal/auto loans, helping Springleaf optimize their targeted marketing efforts.\n    * **Key Points:**\n        * Focus on identifying customers likely to respond *and* be good candidates for loans.\n        * Challenge involves handling a high-dimensional dataset with many anonymized features.\n        * Requires constructing meta-variables and performing feature selection due to data complexity.\n        * Dataset intentionally contains messy features with placeholder values for missing data.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing anonymized customer information (mix of continuous and categorical features).\n* **Data Files:**\n    * `train.csv` - Training data with target labels\n    * `test.csv` - Test data for prediction\n    * `sample_submission.csv` - Submission format example\n* **Features:**\n    * All features anonymized to protect privacy\n    * Contains placeholder values representing missing data\n    * Mix of continuous and categorical variables\n    * Target variable is binary (\"target\")\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between responders and non-responders\n        * Evaluates predicted probabilities against observed binary target\n        * Score ranges from 0.5 (random) to 1.0 (perfect separation)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a customer will respond to a direct mail offer for personal/auto loans, helping Springleaf optimize their targeted marketing efforts.\n    * **Key Points:**\n        * Focus on identifying customers likely to respond *and* be good candidates for loans.\n        * Challenge involves handling a high-dimensional dataset with many anonymized features.\n        * Requires constructing meta-variables and performing feature selection due to data complexity.\n        * Dataset intentionally contains messy features with placeholder values for missing data.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing anonymized customer information (mix of continuous and categorical features).\n* **Data Files:**\n    * `train.csv` - Training data with target labels\n    * `test.csv` - Test data for prediction\n    * `sample_submission.csv` - Submission format example\n* **Features:**\n    * All features anonymized to protect privacy\n    * Contains placeholder values representing missing data\n    * Mix of continuous and categorical variables\n    * Target variable is binary (\"target\")",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between responders and non-responders\n        * Evaluates predicted probabilities against observed binary target\n        * Score ranges from 0.5 (random) to 1.0 (perfect separation)"
    },
    "file_path": "kaggle_datasets/172/problem_summary.md"
  },
  "186": {
    "problem_id": "186",
    "title": "Predicting Telstra Network Fault Severity",
    "problem_type": "Multi-class Classification",
    "objective": "Predict the severity of service disruptions on Telstra's telecommunications network, categorized as:",
    "evaluation_metric": null,
    "full_content": "# Predicting Telstra Network Fault Severity\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the severity of service disruptions on Telstra's telecommunications network, categorized as:\n    * 0: No fault\n    * 1: Few faults\n    * 2: Many faults\n* **Key Points:**\n    * The task simulates real-world network fault prediction to improve customer experience.\n    * Focuses on translating log data into actionable predictions about service disruption impact.\n    * Requires distinguishing between warning messages (features) and actual fault reports (target).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data derived from network service logs, with features extracted from various system components.\n* **Data Files:**\n    * `train.csv` - Training set with fault severity labels\n    * `test.csv` - Test set for predictions\n    * `sample_submission.csv` - Submission format example\n    * Auxiliary files: `event_type.csv`, `log_feature.csv`, `resource_type.csv`, `severity_type.csv`\n* **Key Features:**\n    * Location-time identifiers (`id` column)\n    * Log-derived features (event types, resource types, warning severity types)\n    * Note: `severity_type` (feature) is distinct from `fault_severity` (target)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n* **Metric Components:**\n    * Penalizes divergence between predicted probabilities and true class labels\n    * Formula: \n        ```\n        logloss = -1/N * Σ[i=1 to N] Σ[j=1 to M] y_ij * log(p_ij)\n        ```\n        Where:\n        - N = number of test rows\n        - M = number of classes (3)\n        - y_ij = 1 if observation i is class j, else 0\n        - p_ij = predicted probability for class j\n    * Probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes\n    * Row probabilities normalized (sum to 1) before scoring",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the severity of service disruptions on Telstra's telecommunications network, categorized as:\n    * 0: No fault\n    * 1: Few faults\n    * 2: Many faults\n* **Key Points:**\n    * The task simulates real-world network fault prediction to improve customer experience.\n    * Focuses on translating log data into actionable predictions about service disruption impact.\n    * Requires distinguishing between warning messages (features) and actual fault reports (target).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data derived from network service logs, with features extracted from various system components.\n* **Data Files:**\n    * `train.csv` - Training set with fault severity labels\n    * `test.csv` - Test set for predictions\n    * `sample_submission.csv` - Submission format example\n    * Auxiliary files: `event_type.csv`, `log_feature.csv`, `resource_type.csv`, `severity_type.csv`\n* **Key Features:**\n    * Location-time identifiers (`id` column)\n    * Log-derived features (event types, resource types, warning severity types)\n    * Note: `severity_type` (feature) is distinct from `fault_severity` (target)",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (log loss)\n* **Metric Components:**\n    * Penalizes divergence between predicted probabilities and true class labels\n    * Formula: \n        ```\n        logloss = -1/N * Σ[i=1 to N] Σ[j=1 to M] y_ij * log(p_ij)\n        ```\n        Where:\n        - N = number of test rows\n        - M = number of classes (3)\n        - y_ij = 1 if observation i is class j, else 0\n        - p_ij = predicted probability for class j\n    * Probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes\n    * Row probabilities normalized (sum to 1) before scoring"
    },
    "file_path": "kaggle_datasets/186/problem_summary.md"
  },
  "19": {
    "problem_id": "19",
    "title": "Predicting Supermarket Shopper Visits and Spend",
    "problem_type": "Time Series Forecasting with Regression Components",
    "objective": "Predict two key outcomes for supermarket shoppers:",
    "evaluation_metric": null,
    "full_content": "# Predicting Supermarket Shopper Visits and Spend\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting with Regression Components\n* **Objective:** Predict two key outcomes for supermarket shoppers:\n    * The first date on or after April 1, 2011 that each customer will next visit the store\n    * The dollar amount the customer will spend during that predicted visit\n* **Key Points:**\n    * Predictions must be exact for both date and spend to be considered correct\n    * Spend predictions have a ±$10 tolerance window\n    * No partial credit - predictions are either fully correct or incorrect\n\n**Dataset Overview:**\n* **Data Type:** Tabular time series data of customer visits\n* **Context:** Supermarket shopping behavior of 100,000 customers over one year (April 2010-March 2011)\n* **Data Files:** (Specific files not listed, but dataset contained)\n    * Historical visit records with timestamps and spend amounts\n    * Customer identifiers\n* **Features:**\n    * Customer ID\n    * Visit dates (time series)\n    * Visit spend amounts\n    * Next visit information (for evaluation)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Count of Correct Visits (binary classification of predictions)\n    * A Correct Visit requires:\n        * Exact prediction of next visit date\n        * Spend prediction within ±$10 of actual amount\n    * No partial credit or accuracy scaling\n* **Tiebreaker:** Submission timestamp (for leaderboard ranking)",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting with Regression Components\n* **Objective:** Predict two key outcomes for supermarket shoppers:\n    * The first date on or after April 1, 2011 that each customer will next visit the store\n    * The dollar amount the customer will spend during that predicted visit\n* **Key Points:**\n    * Predictions must be exact for both date and spend to be considered correct\n    * Spend predictions have a ±$10 tolerance window\n    * No partial credit - predictions are either fully correct or incorrect",
      "Dataset Overview": "* **Data Type:** Tabular time series data of customer visits\n* **Context:** Supermarket shopping behavior of 100,000 customers over one year (April 2010-March 2011)\n* **Data Files:** (Specific files not listed, but dataset contained)\n    * Historical visit records with timestamps and spend amounts\n    * Customer identifiers\n* **Features:**\n    * Customer ID\n    * Visit dates (time series)\n    * Visit spend amounts\n    * Next visit information (for evaluation)",
      "Evaluation Metrics": "* **Primary Metric:** Count of Correct Visits (binary classification of predictions)\n    * A Correct Visit requires:\n        * Exact prediction of next visit date\n        * Spend prediction within ±$10 of actual amount\n    * No partial credit or accuracy scaling\n* **Tiebreaker:** Submission timestamp (for leaderboard ranking)"
    },
    "file_path": "kaggle_datasets/19/problem_summary.md"
  },
  "529": {
    "problem_id": "529",
    "title": "Robotic Arm Path Optimization for Image Printing",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Robotic Arm Path Optimization for Image Printing\n\n## Problem Description\n* **Problem Type**: Optimization (Path Planning with Constraints)\n* **Objective**: \n    * Find the most efficient sequence of configurations for an 8-axis robotic arm to print a 257×257 pixel Christmas card image.\n    * Minimize total cost, which combines:\n        * **Reconfiguration cost**: Square root of the number of arm links moved per step\n        * **Color cost**: Sum of absolute RGB differences between consecutive pixels (scaled by 3.0)\n* **Key Points**:\n    * The arm must visit every pixel in the image exactly once\n    * Configuration constraints:\n        * Each link's displacement vector must satisfy max(|x|,|y|) = link length\n        * First/last configurations must be in a specific \"home\" position\n    * All movements must be integer-valued and change by at most 1 unit per step\n\n## Dataset Overview\n* **Data Type**: \n    * Tabular data (Cartesian coordinates with RGB values)\n    * Image data (PNG representation)\n* **Data Files**:\n    * `image.csv`: Canonical source with (x,y) coordinates and associated (r,g,b) values (0-1 range)\n    * `image.png`: Visual representation of the target image\n    * `sample_submission.csv`: Example submission file format\n* **Key Features**:\n    * 257×257 grid of pixels (66,049 points total)\n    * Each point has:\n        * Cartesian coordinates (x,y)\n        * Normalized RGB values (three floating-point numbers)\n\n## Evaluation Metrics\n* **Primary Metric**: Total cost minimization\n    * Combined cost of:\n        1. **Reconfiguration cost**: √(number of links changed per step)\n        2. **Color cost**: 3.0 × Σ(|Δr| + |Δg| + |Δb|) between consecutive pixels\n* **Validation Requirements**:\n    * Submission must:\n        1. Start and end with specified home configuration\n        2. Cover all image points exactly once\n        3. Use integer coordinates satisfying vector constraints\n        4. Make valid 1-unit moves between configurations",
    "sections": {},
    "file_path": "kaggle_datasets/529/problem_summary.md"
  },
  "26": {
    "problem_id": "26",
    "title": "Getting Started Competition",
    "problem_type": "Regression (based on evaluation metric).",
    "objective": "The competition appears to be a beginner-friendly challenge aimed at helping new users get started with Kaggle. While the exact problem statement is not explicitly provided, the use of Root Mean Squared Error (RMSE) as the evaluation metric suggests a regression task where participants predict a continuous target variable.",
    "evaluation_metric": null,
    "full_content": "# Getting Started Competition\n\n**Problem Description:**\n*   **Problem Type:** Regression (based on evaluation metric).\n*   **Objective:** The competition appears to be a beginner-friendly challenge aimed at helping new users get started with Kaggle. While the exact problem statement is not explicitly provided, the use of Root Mean Squared Error (RMSE) as the evaluation metric suggests a regression task where participants predict a continuous target variable.\n*   **Key Points:**\n    *   Designed as an introductory competition for new Kaggle users.\n    *   Focus on foundational machine learning concepts, likely involving tabular data.\n\n**Dataset Overview:**\n*   **Data Type & Context:** The specific data type is not described in the provided context, but given the competition's beginner-friendly nature, it likely involves structured (tabular) data.\n*   **Data Files:** Not explicitly listed, but standard competition files like `train.csv`, `test.csv`, and `sample_submission.csv` are likely provided.\n*   **Features:** No specific features are mentioned, but the dataset is presumably simple and suitable for beginners.\n\n**Evaluation Metrics:**\n*   **Evaluation Metric:** Root Mean Squared Error (RMSE).\n    *   **Components:** RMSE is calculated as the square root of the average of squared differences between predicted and actual values. Lower values indicate better performance.\n    *   **Purpose:** Commonly used for regression tasks to measure the magnitude of prediction errors.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Regression (based on evaluation metric).\n*   **Objective:** The competition appears to be a beginner-friendly challenge aimed at helping new users get started with Kaggle. While the exact problem statement is not explicitly provided, the use of Root Mean Squared Error (RMSE) as the evaluation metric suggests a regression task where participants predict a continuous target variable.\n*   **Key Points:**\n    *   Designed as an introductory competition for new Kaggle users.\n    *   Focus on foundational machine learning concepts, likely involving tabular data.",
      "Dataset Overview": "*   **Data Type & Context:** The specific data type is not described in the provided context, but given the competition's beginner-friendly nature, it likely involves structured (tabular) data.\n*   **Data Files:** Not explicitly listed, but standard competition files like `train.csv`, `test.csv`, and `sample_submission.csv` are likely provided.\n*   **Features:** No specific features are mentioned, but the dataset is presumably simple and suitable for beginners.",
      "Evaluation Metrics": "*   **Evaluation Metric:** Root Mean Squared Error (RMSE).\n    *   **Components:** RMSE is calculated as the square root of the average of squared differences between predicted and actual values. Lower values indicate better performance.\n    *   **Purpose:** Commonly used for regression tasks to measure the magnitude of prediction errors."
    },
    "file_path": "kaggle_datasets/26/problem_summary.md"
  },
  "8": {
    "problem_id": "8",
    "title": "Tourism Time Series Forecasting Competition",
    "problem_type": "Time Series Forecasting (Monthly and Quarterly)",
    "objective": "Predict future values for 793 tourism-related time series:",
    "evaluation_metric": null,
    "full_content": "# Tourism Time Series Forecasting Competition\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (Monthly and Quarterly)\n* **Objective:** Predict future values for 793 tourism-related time series:\n    * Forecast **24 monthly observations** for the first 366 time series\n    * Forecast **8 quarterly observations** for the remaining 427 time series\n* **Key Points:**\n    * Winning methodology must outperform thresholds from prior research (MASE < 1.38 for monthly, < 1.43 for quarterly)\n    * Winner contributes a discussion paper to the International Journal of Forecasting\n    * Combines results with Part One of the competition for final evaluation\n\n**Dataset Overview:**\n* **Data Type:** Time series data (tabular format with each column representing a separate time series)\n* **Context:** Tourism industry metrics (monthly and quarterly measurements)\n* **Data Files:**\n    * `tourism_data2_revision2.csv` (main dataset with 793 time series)\n    * `example_submission.csv` (submission format template)\n* **Features:**\n    * 366 monthly time series (first 366 columns)\n    * 427 quarterly time series (remaining columns)\n    * Each row represents a time period observation across all series\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Scaled Error (MASE)\n    * Calculated separately for each time series then averaged\n* **MASE Calculation:**\n    * For each series: \n        * Numerator: Mean absolute error of forecasts\n        * Denominator: Mean absolute seasonal difference in training data\n        * Scaled by seasonal frequency (m) and forecast horizon (h)\n    * Final score: Average MASE across all 793 series\n* **Special Considerations:**\n    * Different seasonal frequencies (monthly vs quarterly)\n    * Combined evaluation with Part One results for final ranking",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (Monthly and Quarterly)\n* **Objective:** Predict future values for 793 tourism-related time series:\n    * Forecast **24 monthly observations** for the first 366 time series\n    * Forecast **8 quarterly observations** for the remaining 427 time series\n* **Key Points:**\n    * Winning methodology must outperform thresholds from prior research (MASE < 1.38 for monthly, < 1.43 for quarterly)\n    * Winner contributes a discussion paper to the International Journal of Forecasting\n    * Combines results with Part One of the competition for final evaluation",
      "Dataset Overview": "* **Data Type:** Time series data (tabular format with each column representing a separate time series)\n* **Context:** Tourism industry metrics (monthly and quarterly measurements)\n* **Data Files:**\n    * `tourism_data2_revision2.csv` (main dataset with 793 time series)\n    * `example_submission.csv` (submission format template)\n* **Features:**\n    * 366 monthly time series (first 366 columns)\n    * 427 quarterly time series (remaining columns)\n    * Each row represents a time period observation across all series",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Scaled Error (MASE)\n    * Calculated separately for each time series then averaged\n* **MASE Calculation:**\n    * For each series: \n        * Numerator: Mean absolute error of forecasts\n        * Denominator: Mean absolute seasonal difference in training data\n        * Scaled by seasonal frequency (m) and forecast horizon (h)\n    * Final score: Average MASE across all 793 series\n* **Special Considerations:**\n    * Different seasonal frequencies (monthly vs quarterly)\n    * Combined evaluation with Part One results for final ranking"
    },
    "file_path": "kaggle_datasets/8/problem_summary.md"
  },
  "181": {
    "problem_id": "181",
    "title": "Predicting Cervical Cancer Screening Compliance from Medical Records",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a patient (aged 25-65) will receive regular cervical cancer screening (pap smear) within a 5-year period, based on de-identified medical records. The goal is to identify at-risk populations who may not comply with recommended screening schedules.",
    "evaluation_metric": null,
    "full_content": "# Predicting Cervical Cancer Screening Compliance from Medical Records\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a patient (aged 25-65) will receive regular cervical cancer screening (pap smear) within a 5-year period, based on de-identified medical records. The goal is to identify at-risk populations who may not comply with recommended screening schedules.\n    * **Key Points:**\n        * Focuses on preventive healthcare behavior prediction rather than direct disease diagnosis.\n        * Targets women without cervical cancer diagnoses or hysterectomies.\n        * Data may contain sampling bias (not fully representative of general population).\n        * Intended application: Improve targeted education campaigns for underserved populations.\n\n**Dataset Overview:**\n* **Data Type & Context:** Relational medical records (tabular data) for over 3 million US women, including:\n    * Patient demographics\n    * Diagnosis records\n    * Procedure histories\n    * Prescription data\n    * Surgical records\n* **Key Data Files:**\n    * `patient_train.csv`, `patient_test.csv` (core patient records)\n    * `diagnosis_head.csv.gz`, `procedure_head.csv.gz`, `surgical_head.csv.gz` (medical event details)\n    * `drugs.csv`, `physicians.csv` (supplemental data)\n    * `sample_submission.csv` (format example)\n* **Notable Features:**\n    * Multi-table relational schema (11 tables total)\n    * Contains temporal healthcare interaction data (service dates)\n    * Features anonymized claim IDs for linking related medical events\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model's ability to distinguish between screeners and non-screeners\n    * Robust to class imbalance (important given potential data biases)\n    * Threshold-independent evaluation of probabilistic predictions (submissions require probabilities between 0-1)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a patient (aged 25-65) will receive regular cervical cancer screening (pap smear) within a 5-year period, based on de-identified medical records. The goal is to identify at-risk populations who may not comply with recommended screening schedules.\n    * **Key Points:**\n        * Focuses on preventive healthcare behavior prediction rather than direct disease diagnosis.\n        * Targets women without cervical cancer diagnoses or hysterectomies.\n        * Data may contain sampling bias (not fully representative of general population).\n        * Intended application: Improve targeted education campaigns for underserved populations.",
      "Dataset Overview": "* **Data Type & Context:** Relational medical records (tabular data) for over 3 million US women, including:\n    * Patient demographics\n    * Diagnosis records\n    * Procedure histories\n    * Prescription data\n    * Surgical records\n* **Key Data Files:**\n    * `patient_train.csv`, `patient_test.csv` (core patient records)\n    * `diagnosis_head.csv.gz`, `procedure_head.csv.gz`, `surgical_head.csv.gz` (medical event details)\n    * `drugs.csv`, `physicians.csv` (supplemental data)\n    * `sample_submission.csv` (format example)\n* **Notable Features:**\n    * Multi-table relational schema (11 tables total)\n    * Contains temporal healthcare interaction data (service dates)\n    * Features anonymized claim IDs for linking related medical events",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model's ability to distinguish between screeners and non-screeners\n    * Robust to class imbalance (important given potential data biases)\n    * Threshold-independent evaluation of probabilistic predictions (submissions require probabilities between 0-1)"
    },
    "file_path": "kaggle_datasets/181/problem_summary.md"
  },
  "175": {
    "problem_id": "175",
    "title": "Time Series Forecasting for Rossmann Store Sales",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Time Series Forecasting for Rossmann Store Sales\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict 6 weeks of daily sales for 1,115 Rossmann drug stores across Germany. The goal is to help store managers create effective staff schedules by providing accurate sales forecasts.\n* **Key Points:**\n  * Sales are influenced by multiple factors: promotions, competition, school/state holidays, seasonality, and locality.\n  * Some stores may be temporarily closed for refurbishment (zero sales days).\n  * The competition focuses on practical business impact - improving workforce planning.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular time series data containing historical sales records from Rossmann stores with supplemental store metadata.\n* **Data Files:**\n  * `train.csv` - Historical sales data (target variable included)\n  * `test.csv` - Data for making predictions (target variable excluded)\n  * `sample_submission.csv` - Submission format example\n  * `store.csv` - Supplemental store information\n* **Key Features:**\n  * Temporal features: Date, StateHoliday, SchoolHoliday\n  * Store characteristics: StoreType, Assortment\n  * Promotional data: Promo, Promo2, PromoInterval\n  * Competition data: CompetitionDistance, CompetitionOpenSince[Month/Year]\n  * Operational status: Open/Closed indicator\n  * Sales and Customers (target and related metric)\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Square Percentage Error (RMSPE)\n* **Metric Calculation:**\n  * RMSPE = √(1/n Σ((y_i - ŷ_i)/y_i)²)\n  * Where:\n    * y_i = actual sales for store-day combination\n    * ŷ_i = predicted sales\n    * n = number of valid predictions\n  * Special handling:\n    * Days with zero sales (y_i = 0) are excluded from scoring\n    * Percentage-based error metric emphasizes relative accuracy across different sales volumes",
    "sections": {},
    "file_path": "kaggle_datasets/175/problem_summary.md"
  },
  "347": {
    "problem_id": "347",
    "title": "3D Object Detection for Autonomous Vehicles",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 3D Object Detection for Autonomous Vehicles\n\n## Problem Description\n* **Problem Type:** Computer Vision - 3D Object Detection\n* **Objective:**  \n  Participants are tasked with developing algorithms to detect and localize 3D objects (e.g., cars, buses) in autonomous vehicle sensor data. The goal is to predict 3D bounding volumes (boxes) and their class labels from raw sensor inputs (LIDAR, images) to advance self-driving technology.\n  * **Key Points:**\n    * Focus on **3D context** (ground bounding box + height) rather than 2D detection.\n    * Predictions must include object class, spatial dimensions, orientation (`yaw`), and confidence scores.\n    * Dataset includes multi-modal sensor data (LIDAR, cameras, maps) from Lyft's autonomous fleet.\n\n## Dataset Overview\n* **Data Type & Context:**  \n  Multi-modal autonomous vehicle data, including:\n  * **LIDAR point clouds** (`.bin` files)\n  * **Camera images** (`.jpeg`)\n  * **Semantic maps** (geographic context)\n  * **Tabular metadata** (JSON files linking samples to sensor data).\n* **Data Files:**  \n  * `train_data.zip` / `test_data.zip` (JSON metadata)\n  * `train_images.zip` / `test_images.zip` (camera images)\n  * `train_lidar.zip` / `test_lidar.zip` (LIDAR scans)\n  * `train_maps.zip` / `test_maps.zip` (semantic maps)\n  * `train.csv` (annotations: `center_x, center_y, center_z, width, length, height, yaw, class_name`)\n  * `sample_submission.csv` (test sample IDs).\n* **Key Features:**  \n  * **3D bounding box parameters**: Center coordinates (`x,y,z`), dimensions (`width, length, height`), and orientation (`yaw`).\n  * **Class labels**: Object categories (e.g., `car`, `bus`).\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Average Precision (mAP) at varying Intersection-over-Union (IoU) thresholds.\n  * **IoU Calculation:**  \n    $$ IoU(A,B) = \\frac{A \\cap B}{A \\cup B} $$\n    * For 3D",
    "sections": {},
    "file_path": "kaggle_datasets/347/problem_summary.md"
  },
  "511": {
    "problem_id": "511",
    "title": "Predicting Stock Returns for the Tokyo Stock Exchange",
    "problem_type": "Time Series Forecasting (Financial Markets)",
    "objective": "Predict future returns of ~2,000 Japanese stocks to construct optimal portfolios. Participants must rank stocks daily from highest to lowest expected returns.",
    "evaluation_metric": null,
    "full_content": "# Predicting Stock Returns for the Tokyo Stock Exchange\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (Financial Markets)\n* **Objective:** Predict future returns of ~2,000 Japanese stocks to construct optimal portfolios. Participants must rank stocks daily from highest to lowest expected returns.\n    * **Key Points:**\n        * Focus on quantitative trading strategy development using historical Japanese market data.\n        * Models are evaluated on their ability to create portfolios with high returns and low risk (via Sharpe Ratio).\n        * Uses a time-series API to prevent lookahead bias in predictions.\n        * Winning models are made public to encourage transparency and learning.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular financial data (daily stock prices, options, trades, financials) from the Tokyo Stock Exchange.\n* **Key Data Files:**\n    * `stock_prices.csv`: Daily closing prices and target returns (primary data).\n    * `options.csv`: Market options data (indirect predictors).\n    * `financials.csv`: Quarterly earnings reports.\n    * `trades.csv`: Weekly trading volumes.\n    * `stock_list.csv`: Metadata (company names, industries).\n* **Features:** \n    * Time-series of stock prices, returns (`Target` column), trading volumes, and derived financial metrics.\n    * Options data for broader market context.\n    * Supplemental data for less liquid stocks (`secondary_stock_prices.csv`).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Sharpe Ratio of daily spread returns.\n    * **Components:**\n        * Each day, the top 200 ranked stocks are treated as \"long\" positions, and the bottom 200 as \"short.\"\n        * Stocks are weighted by rank (higher ranks = larger allocations).\n        * Returns are calculated assuming stocks are bought the next day and sold the day after.\n        * The Sharpe Ratio measures risk-adjusted returns (mean return divided by volatility).\n    * **Constraints:**\n        * Ranks must be unique and within valid range (0 to number of stocks - 1).\n        * Submissions must use the provided Python API to ensure no future data leakage.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (Financial Markets)\n* **Objective:** Predict future returns of ~2,000 Japanese stocks to construct optimal portfolios. Participants must rank stocks daily from highest to lowest expected returns.\n    * **Key Points:**\n        * Focus on quantitative trading strategy development using historical Japanese market data.\n        * Models are evaluated on their ability to create portfolios with high returns and low risk (via Sharpe Ratio).\n        * Uses a time-series API to prevent lookahead bias in predictions.\n        * Winning models are made public to encourage transparency and learning.",
      "Dataset Overview": "* **Data Type & Context:** Tabular financial data (daily stock prices, options, trades, financials) from the Tokyo Stock Exchange.\n* **Key Data Files:**\n    * `stock_prices.csv`: Daily closing prices and target returns (primary data).\n    * `options.csv`: Market options data (indirect predictors).\n    * `financials.csv`: Quarterly earnings reports.\n    * `trades.csv`: Weekly trading volumes.\n    * `stock_list.csv`: Metadata (company names, industries).\n* **Features:** \n    * Time-series of stock prices, returns (`Target` column), trading volumes, and derived financial metrics.\n    * Options data for broader market context.\n    * Supplemental data for less liquid stocks (`secondary_stock_prices.csv`).",
      "Evaluation Metrics": "* **Primary Metric:** Sharpe Ratio of daily spread returns.\n    * **Components:**\n        * Each day, the top 200 ranked stocks are treated as \"long\" positions, and the bottom 200 as \"short.\"\n        * Stocks are weighted by rank (higher ranks = larger allocations).\n        * Returns are calculated assuming stocks are bought the next day and sold the day after.\n        * The Sharpe Ratio measures risk-adjusted returns (mean return divided by volatility).\n    * **Constraints:**\n        * Ranks must be unique and within valid range (0 to number of stocks - 1).\n        * Submissions must use the provided Python API to ensure no future data leakage."
    },
    "file_path": "kaggle_datasets/511/problem_summary.md"
  },
  "21": {
    "problem_id": "21",
    "title": "Semi-Supervised Feature Learning for High-Dimensional Data",
    "problem_type": "Semi-Supervised Feature Learning + Binary Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Semi-Supervised Feature Learning for High-Dimensional Data\n\n**Problem Description:**\n* **Problem Type:** Semi-Supervised Feature Learning + Binary Classification\n* **Objective:**  \n    * Learn a dense feature representation (≤100 features) from high-dimensional, sparse input data (~1M features) using both labeled and unlabeled data.  \n    * Transform the original data into this new feature space to improve the performance of a downstream linear classifier.  \n* **Key Points:**  \n    * Focus on large-scale, high-dimensional learning tasks.  \n    * Any method for feature transformation is allowed (e.g., deep learning, matrix factorization, clustering).  \n    * Must use a provided linear SVM (C=1.0) for final classification on the transformed features.  \n    * Evaluation is based on the classifier's performance, not the feature learning method itself.  \n\n**Dataset Overview:**  \n* **Data Type:** Sparse, anonymized tabular data (originally from web classification).  \n* **Context:**  \n    * Extremely high-dimensional (~1M features), with most features being sparse (few non-zero values).  \n    * Contains both labeled (small) and unlabeled (large) subsets.  \n* **Data Files:**  \n    * `public_train_data.svmlight.dat` / `public_test_data.svmlight.dat`: Training/test data in SVMLight format.  \n    * `public_train.labels.dat`: Labels for the training set.  \n    * Additional files in `matlab_format_data.tgz` and `semisupervised_feature_learning.tgz`.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** AUC (Area Under the ROC Curve) of a linear SVM trained on the transformed features.  \n* **Evaluation Process:**  \n    1. Participants submit transformed test data (≤100 features) + SVM predictions.  \n    2. Organizers train a linear SVM (C=1.0) on the transformed training data and evaluate AUC on the test set.  \n    3. Leaderboard uses 30% of test data; final evaluation uses the remaining 70%.  \n    4. Top submissions are validated via 10-fold cross-validation to ensure reproducibility.",
    "sections": {
      "Problem Description": "* **Problem Type:** Semi-Supervised Feature Learning + Binary Classification\n* **Objective:**  \n    * Learn a dense feature representation (≤100 features) from high-dimensional, sparse input data (~1M features) using both labeled and unlabeled data.  \n    * Transform the original data into this new feature space to improve the performance of a downstream linear classifier.  \n* **Key Points:**  \n    * Focus on large-scale, high-dimensional learning tasks.  \n    * Any method for feature transformation is allowed (e.g., deep learning, matrix factorization, clustering).  \n    * Must use a provided linear SVM (C=1.0) for final classification on the transformed features.  \n    * Evaluation is based on the classifier's performance, not the feature learning method itself.  \n\n**Dataset Overview:**  \n* **Data Type:** Sparse, anonymized tabular data (originally from web classification).  \n* **Context:**  \n    * Extremely high-dimensional (~1M features), with most features being sparse (few non-zero values).  \n    * Contains both labeled (small) and unlabeled (large) subsets.  \n* **Data Files:**  \n    * `public_train_data.svmlight.dat` / `public_test_data.svmlight.dat`: Training/test data in SVMLight format.  \n    * `public_train.labels.dat`: Labels for the training set.  \n    * Additional files in `matlab_format_data.tgz` and `semisupervised_feature_learning.tgz`.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** AUC (Area Under the ROC Curve) of a linear SVM trained on the transformed features.  \n* **Evaluation Process:**  \n    1. Participants submit transformed test data (≤100 features) + SVM predictions.  \n    2. Organizers train a linear SVM (C=1.0) on the transformed training data and evaluate AUC on the test set.  \n    3. Leaderboard uses 30% of test data; final evaluation uses the remaining 70%.  \n    4. Top submissions are validated via 10-fold cross-validation to ensure reproducibility."
    },
    "file_path": "kaggle_datasets/21/problem_summary.md"
  },
  "378": {
    "problem_id": "378",
    "title": "Predicting Ion Channel Openings from Electrophysiological Signals",
    "problem_type": "Time Series Classification (Discrete Value Prediction)",
    "objective": "Predict the number of open ion channels (`open_channels`) at each time point based on electrophysiological signal data. The goal is to automate the detection of ion channel state changes, which is currently a slow and laborious manual process in laboratory research.",
    "evaluation_metric": null,
    "full_content": "# Predicting Ion Channel Openings from Electrophysiological Signals\n\n**Problem Description:**\n* **Problem Type:** Time Series Classification (Discrete Value Prediction)\n* **Objective:** Predict the number of open ion channels (`open_channels`) at each time point based on electrophysiological signal data. The goal is to automate the detection of ion channel state changes, which is currently a slow and laborious manual process in laboratory research.\n    * **Key Points:**\n        * Data represents simulated ion channel activity injected with real-world noise to emulate laboratory conditions.\n        * Successful models could accelerate research in cell health, disease mechanisms (e.g., cancer), and climate impact studies on plants.\n        * The time series appears continuous but consists of discrete 50-second batches (500,000 samples per batch at 10 kHz), with discontinuities between batches.\n\n**Dataset Overview:**\n* **Data Type & Context:** Time series data of electrophysiological signals (electric currents) generated by ion channel activity in cells.\n* **Data Files:**\n    * `train.csv` - Training set with signal and corresponding `open_channels` labels.\n    * `test.csv` - Test set where participants must predict `open_channels` from signal data.\n    * `sample_submission.csv` - Example submission file in required format.\n* **Features:**\n    * Primary feature: `signal` (electrophysiological measurement at each time point).\n    * Target variable: `open_channels` (integer count of open channels at each time point).\n    * Time values (`time`) are provided but note the batch discontinuities.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Macro F1-Score\n    * **Components:**\n        * F1 = 2 * (precision * recall) / (precision + recall)\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * \"Macro\" averaging: Separate F1 scores calculated for each `open_channels` class, then averaged.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Classification (Discrete Value Prediction)\n* **Objective:** Predict the number of open ion channels (`open_channels`) at each time point based on electrophysiological signal data. The goal is to automate the detection of ion channel state changes, which is currently a slow and laborious manual process in laboratory research.\n    * **Key Points:**\n        * Data represents simulated ion channel activity injected with real-world noise to emulate laboratory conditions.\n        * Successful models could accelerate research in cell health, disease mechanisms (e.g., cancer), and climate impact studies on plants.\n        * The time series appears continuous but consists of discrete 50-second batches (500,000 samples per batch at 10 kHz), with discontinuities between batches.",
      "Dataset Overview": "* **Data Type & Context:** Time series data of electrophysiological signals (electric currents) generated by ion channel activity in cells.\n* **Data Files:**\n    * `train.csv` - Training set with signal and corresponding `open_channels` labels.\n    * `test.csv` - Test set where participants must predict `open_channels` from signal data.\n    * `sample_submission.csv` - Example submission file in required format.\n* **Features:**\n    * Primary feature: `signal` (electrophysiological measurement at each time point).\n    * Target variable: `open_channels` (integer count of open channels at each time point).\n    * Time values (`time`) are provided but note the batch discontinuities.",
      "Evaluation Metrics": "* **Primary Metric:** Macro F1-Score\n    * **Components:**\n        * F1 = 2 * (precision * recall) / (precision + recall)\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * \"Macro\" averaging: Separate F1 scores calculated for each `open_channels` class, then averaged."
    },
    "file_path": "kaggle_datasets/378/problem_summary.md"
  },
  "75": {
    "problem_id": "75",
    "title": "Multi-modal Image and Text Tag Association Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-modal Image and Text Tag Association Challenge\n\n## Problem Description\n* **Problem Type:** Multi-modal Learning (Image-Text Association)\n* **Objective:**  \n  Train a system to associate images with their correct word tags from two provided options. The core task is to determine which of two given text annotations better describes an image.\n* **Key Points:**\n  * Test data consists of triplets: an image + two text annotations (one correct, one incorrect).\n  * Incorrect annotations are derived from other test images' correct labels.\n  * Strict rules against manual labeling of test data (final test set released only 1 week before competition close).\n  * Systems must handle scale variations (training images vary in size, test images are standardized to 300px on the larger dimension).\n\n## Dataset Overview\n* **Data Type & Context:**  \n  Multi-modal dataset pairing images with text tags, derived from the Small ESP Game Dataset (crowdsourced image labeling via online game).\n* **Key Data Files:**\n  * `ESPGame100k.tar.gz` - Training set (100,000 images with single correct tags).\n  * `public_test_images.tgz` / `private_test_images.tar.gz` - Test images (1000 total).\n  * `public_test_options.tgz` / `private_test_options.tar.gz` - Paired text annotations for test images (correct vs. incorrect).\n* **Features:**\n  * Image data: Varied resolutions in training, standardized to 300px in test.\n  * Text data: Crowdsourced word tags reflecting game-based labeling biases.\n\n## Evaluation Metrics\n* **Primary Metric:** Accuracy  \n  * Percentage of correct predictions when choosing between two text annotations for each test image.\n* **Scoring Process:**\n  * For each image, system predicts 0 (option 0 matches) or 1 (option 1 matches).\n  * Final score = (# correct predictions) / (total test images).",
    "sections": {},
    "file_path": "kaggle_datasets/75/problem_summary.md"
  },
  "81": {
    "problem_id": "81",
    "title": "Author-Paper Identification Challenge (Binary Classification with Ranking)",
    "problem_type": "Binary Classification (with Ranking Emphasis)",
    "objective": "Resolve author-name ambiguity in academic databases by correctly assigning papers to authors, filtering out incorrect assignments due to name variations or shared names.",
    "evaluation_metric": null,
    "full_content": "# Author-Paper Identification Challenge (Binary Classification with Ranking)\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with Ranking Emphasis)\n    * Core task is to determine whether a given author wrote a specific paper (YES/NO classification).\n    * **Objective:** Resolve author-name ambiguity in academic databases by correctly assigning papers to authors, filtering out incorrect assignments due to name variations or shared names.\n    * **Key Points:**\n        * Focus on ranking papers by likelihood of authorship (YES instances should rank higher than NO instances).\n        * Real-world noise in the data: duplicate paper-author pairs, inconsistent metadata, and unconfirmed authorship labels.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with relational metadata (authors, papers, conferences/journals).\n* **Data Files:**\n    * `Author.csv`: Author IDs, names, and affiliations (may contain duplicates due to name variations).\n    * `Paper.csv`: Paper IDs, titles, publication years, venue IDs (conference/journal), and keywords.\n    * `PaperAuthor.csv`: Noisy paper-author pairs with potential incorrect assignments.\n    * `Conference.csv`/`Journal.csv`: Venue metadata (short names, full names, URLs).\n    * `Train.csv`/`Valid.csv`: Ground truth labels (confirmed/deleted papers per author).\n* **Key Features:**\n    * Author names and affiliations (with variations).\n    * Paper titles, keywords, and publication venues.\n    * Co-authorship derived from `PaperAuthor.csv`.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision (MAP).\n    * **Calculation Logic:**\n        1. For each author, rank their candidate papers by predicted likelihood of authorship.\n        2. Compute Average Precision (AP) for each author:\n            * At each position in the ranked list where a true YES instance occurs, calculate precision (fraction of YES instances up to that point).\n            * Average these precision values across all YES instances.\n        3. Final score is the mean of AP values across all authors.\n    * **Example:** If an author has 3 YES papers out of 10, and they are ranked 1st, 4th, and 7th, the AP is (1/1 + 2/4 + 3/7)/3. MAP averages this across all authors.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with Ranking Emphasis)\n    * Core task is to determine whether a given author wrote a specific paper (YES/NO classification).\n    * **Objective:** Resolve author-name ambiguity in academic databases by correctly assigning papers to authors, filtering out incorrect assignments due to name variations or shared names.\n    * **Key Points:**\n        * Focus on ranking papers by likelihood of authorship (YES instances should rank higher than NO instances).\n        * Real-world noise in the data: duplicate paper-author pairs, inconsistent metadata, and unconfirmed authorship labels.",
      "Dataset Overview": "* **Data Type:** Tabular data with relational metadata (authors, papers, conferences/journals).\n* **Data Files:**\n    * `Author.csv`: Author IDs, names, and affiliations (may contain duplicates due to name variations).\n    * `Paper.csv`: Paper IDs, titles, publication years, venue IDs (conference/journal), and keywords.\n    * `PaperAuthor.csv`: Noisy paper-author pairs with potential incorrect assignments.\n    * `Conference.csv`/`Journal.csv`: Venue metadata (short names, full names, URLs).\n    * `Train.csv`/`Valid.csv`: Ground truth labels (confirmed/deleted papers per author).\n* **Key Features:**\n    * Author names and affiliations (with variations).\n    * Paper titles, keywords, and publication venues.\n    * Co-authorship derived from `PaperAuthor.csv`.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision (MAP).\n    * **Calculation Logic:**\n        1. For each author, rank their candidate papers by predicted likelihood of authorship.\n        2. Compute Average Precision (AP) for each author:\n            * At each position in the ranked list where a true YES instance occurs, calculate precision (fraction of YES instances up to that point).\n            * Average these precision values across all YES instances.\n        3. Final score is the mean of AP values across all authors.\n    * **Example:** If an author has 3 YES papers out of 10, and they are ranked 1st, 4th, and 7th, the AP is (1/1 + 2/4 + 3/7)/3. MAP averages this across all authors."
    },
    "file_path": "kaggle_datasets/81/problem_summary.md"
  },
  "313": {
    "problem_id": "313",
    "title": "Santander Customer Transaction Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Santander Customer Transaction Prediction\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a customer will make a specific transaction in the future, irrespective of the transaction amount. The goal is to identify customers who will transact (target=1) vs. those who will not (target=0).\n* **Key Points:**\n  * The problem focuses on binary classification, a common challenge in banking/finance.\n  * The dataset is anonymized, with no contextual information about the features.\n  * The competition aims to improve Santander's ability to solve similar business problems.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing anonymized numerical features representing customer attributes or transaction-related variables.\n* **Data Files:**\n  * `train.csv`: Training set with features and binary target column.\n  * `test.csv`: Test set (some rows are not scored).\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:**\n  * `ID_code`: Unique identifier for each customer (string).\n  * `target`: Binary label (0 or 1) indicating whether the customer will transact.\n  * 200 anonymized numerical features (names not disclosed).\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n  * Measures the model's ability to distinguish between positive (target=1) and negative (target=0) classes.\n  * Higher AUC indicates better classification performance.\n  * Submission requires predicted probabilities (not hard binary predictions).",
    "sections": {},
    "file_path": "kaggle_datasets/313/problem_summary.md"
  },
  "121": {
    "problem_id": "121",
    "title": "Schizophrenia Diagnosis from Multimodal MRI Features",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Schizophrenia Diagnosis from Multimodal MRI Features\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:**  \n    * Automatically diagnose schizophrenia in subjects using multimodal features derived from functional and structural MRI (fMRI/sMRI) scans.  \n    * Key challenge involves optimally combining multimodal neuroimaging data (FNC and SBM features) to enhance diagnostic accuracy.\n* **Key Points:**  \n    * Schizophrenia lacks objective biomarkers, making diagnosis subjective and challenging.  \n    * Features are derived from state-of-the-art neuroimaging techniques (ICA-based FNC and SBM).  \n    * Participants must handle multimodal data fusion and feature selection.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data derived from MRI scans (functional and structural modalities).  \n    * Features represent brain connectivity (FNC) and gray-matter concentration patterns (SBM).  \n* **Data Files:**  \n    * `train_labels.csv`: Binary labels (0 = Healthy, 1 = Schizophrenic).  \n    * `train_FNC.csv`: Functional Network Connectivity (FNC) features (correlation values between brain maps).  \n    * `train_SBM.csv`: Source-Based Morphometry (SBM) features (ICA-derived gray-matter weights).  \n    * Test set (`test_FNC.csv`, `test_SBM.csv`) with inflated rows to deter cheating.  \n    * Optional NIfTI files of ICA brain maps for deeper analysis.  \n* **Key Features:**  \n    * FNC: Pairwise correlations of fMRI timecourses (functional connectivity).  \n    * SBM: ICA loadings of gray-matter concentration (structural patterns).  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n* **Calculation:**  \n    * ROC curve generated from true labels vs. predicted probabilities.  \n    * AUC computed as the integral under the curve (higher values indicate better discrimination).  \n    * Supported in MATLAB (`perfcurve`), R (`roc.area`), and Python (`sklearn.metrics.roc_curve` + `auc`).",
    "sections": {},
    "file_path": "kaggle_datasets/121/problem_summary.md"
  },
  "545": {
    "problem_id": "545",
    "title": "Neutrino Direction Reconstruction with IceCube Data",
    "problem_type": "Regression (Direction Prediction)",
    "objective": "Predict the direction of origin (azimuth and zenith angles) of neutrino particles detected by the IceCube Neutrino Observatory. The goal is to improve the speed and accuracy of neutrino event reconstruction to enable real-time analysis of cosmic neutrino sources.",
    "evaluation_metric": null,
    "full_content": "# Neutrino Direction Reconstruction with IceCube Data\n\n**Problem Description:**\n* **Problem Type:** Regression (Direction Prediction)\n* **Objective:** Predict the direction of origin (azimuth and zenith angles) of neutrino particles detected by the IceCube Neutrino Observatory. The goal is to improve the speed and accuracy of neutrino event reconstruction to enable real-time analysis of cosmic neutrino sources.\n* **Key Points:**\n  * Focus on overcoming computational bottlenecks in existing methods (trade-off between speed and accuracy).\n  * Applications include identifying astrophysical sources like supernovae, gamma-ray bursts, and black hole phenomena.\n  * Requires processing high-volume data (~3000 events/second in real-world operation).\n\n**Dataset Overview:**\n* **Data Type & Context:** \n  * Tabular data representing neutrino detection events from the IceCube detector (a cubic kilometer of instrumented ice at the South Pole).\n  * Each event consists of multiple pulses recorded by photomultiplier sensors.\n* **Data Files:**\n  * `train/test_meta.parquet`: Event metadata including target angles (azimuth/zenith) for training.\n  * `train/test/batch_[n].parquet`: Pulse-level data for each event (time, sensor ID, charge, auxiliary flag).\n  * `sensor_geometry.csv`: 3D coordinates of all 5160 IceCube sensors.\n* **Key Features:**\n  * Pulse timing (relative nanoseconds within event).\n  * Sensor positions (enables geometric reconstruction).\n  * Charge measurements (photoelectrons indicating light intensity).\n  * Auxiliary flags (indicating data quality/noise).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Angular Error (in radians)\n  * Calculates the angular distance between predicted and true direction vectors.\n  * Direction vectors are derived from predicted (azimuth, zenith) pairs.\n  * Metric accounts for spherical geometry (unlike simple Euclidean distance).\n* **Implementation Note:** \n  * The metric uses arccos of dot products between true and predicted unit vectors.\n  * Lower values indicate better performance (perfect prediction = 0 error).",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Direction Prediction)\n* **Objective:** Predict the direction of origin (azimuth and zenith angles) of neutrino particles detected by the IceCube Neutrino Observatory. The goal is to improve the speed and accuracy of neutrino event reconstruction to enable real-time analysis of cosmic neutrino sources.\n* **Key Points:**\n  * Focus on overcoming computational bottlenecks in existing methods (trade-off between speed and accuracy).\n  * Applications include identifying astrophysical sources like supernovae, gamma-ray bursts, and black hole phenomena.\n  * Requires processing high-volume data (~3000 events/second in real-world operation).",
      "Dataset Overview": "* **Data Type & Context:** \n  * Tabular data representing neutrino detection events from the IceCube detector (a cubic kilometer of instrumented ice at the South Pole).\n  * Each event consists of multiple pulses recorded by photomultiplier sensors.\n* **Data Files:**\n  * `train/test_meta.parquet`: Event metadata including target angles (azimuth/zenith) for training.\n  * `train/test/batch_[n].parquet`: Pulse-level data for each event (time, sensor ID, charge, auxiliary flag).\n  * `sensor_geometry.csv`: 3D coordinates of all 5160 IceCube sensors.\n* **Key Features:**\n  * Pulse timing (relative nanoseconds within event).\n  * Sensor positions (enables geometric reconstruction).\n  * Charge measurements (photoelectrons indicating light intensity).\n  * Auxiliary flags (indicating data quality/noise).",
      "Evaluation Metrics": "* **Primary Metric:** Mean Angular Error (in radians)\n  * Calculates the angular distance between predicted and true direction vectors.\n  * Direction vectors are derived from predicted (azimuth, zenith) pairs.\n  * Metric accounts for spherical geometry (unlike simple Euclidean distance).\n* **Implementation Note:** \n  * The metric uses arccos of dot products between true and predicted unit vectors.\n  * Lower values indicate better performance (perfect prediction = 0 error)."
    },
    "file_path": "kaggle_datasets/545/problem_summary.md"
  },
  "119": {
    "problem_id": "119",
    "title": "Greek Media Monitoring Multilabel Classification (WISE 2014)",
    "problem_type": "Multi-label Classification (Text)",
    "objective": "Automatically categorize Greek printed media articles into one or more of 203 predefined topics (labels). The goal is to support or automate human annotation for media monitoring, improving efficiency in identifying relevant topics.",
    "evaluation_metric": null,
    "full_content": "# Greek Media Monitoring Multilabel Classification (WISE 2014)\n\n**Problem Description:**\n* **Problem Type:** Multi-label Classification (Text)\n* **Objective:** Automatically categorize Greek printed media articles into one or more of 203 predefined topics (labels). The goal is to support or automate human annotation for media monitoring, improving efficiency in identifying relevant topics.\n    * **Key Points:**\n        * Raw text data is noisy due to OCR extraction from scanned print media.\n        * Topics range from specific entities (persons, products) to broad semantic concepts (economy, environment).\n        * Solutions should prioritize both precision and recall to balance topic relevance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Text data (Greek articles) represented as TF-IDF vectors (bag-of-words model) with 301,561 numerical features.\n    * **Data Files:**\n        * Training set (`wise2014-arff-train`, `wise2014-libsvm-train`): 64,857 articles.\n        * Test set (`wise2014-arff-test`, `wise2014-libsvm-test`): 34,923 articles.\n        * Sample submission file (`sampleSubmission`).\n    * **Features:** \n        * Sparse TF-IDF values for tokens (unit-normalized).\n        * Labels: 203 binary attributes indicating topic associations.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean F1-Score (example-based F-measure for multi-label tasks).\n    * **Components:**\n        * Precision (*p*): `tp / (tp + fp)`.\n        * Recall (*r*): `tp / (tp + fn)`.\n        * F1-score: Harmonic mean of precision and recall: `2 * (p * r) / (p + r)`.\n    * **Submission Format:** Space-delimited topic IDs per article (e.g., `ArticleId,Topics\\n1,12 2`).",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Classification (Text)\n* **Objective:** Automatically categorize Greek printed media articles into one or more of 203 predefined topics (labels). The goal is to support or automate human annotation for media monitoring, improving efficiency in identifying relevant topics.\n    * **Key Points:**\n        * Raw text data is noisy due to OCR extraction from scanned print media.\n        * Topics range from specific entities (persons, products) to broad semantic concepts (economy, environment).\n        * Solutions should prioritize both precision and recall to balance topic relevance.",
      "Dataset Overview": "* **Data Type & Context:** Text data (Greek articles) represented as TF-IDF vectors (bag-of-words model) with 301,561 numerical features.\n    * **Data Files:**\n        * Training set (`wise2014-arff-train`, `wise2014-libsvm-train`): 64,857 articles.\n        * Test set (`wise2014-arff-test`, `wise2014-libsvm-test`): 34,923 articles.\n        * Sample submission file (`sampleSubmission`).\n    * **Features:** \n        * Sparse TF-IDF values for tokens (unit-normalized).\n        * Labels: 203 binary attributes indicating topic associations.",
      "Evaluation Metrics": "* **Primary Metric:** Mean F1-Score (example-based F-measure for multi-label tasks).\n    * **Components:**\n        * Precision (*p*): `tp / (tp + fp)`.\n        * Recall (*r*): `tp / (tp + fn)`.\n        * F1-score: Harmonic mean of precision and recall: `2 * (p * r) / (p + r)`.\n    * **Submission Format:** Space-delimited topic IDs per article (e.g., `ArticleId,Topics\\n1,12 2`)."
    },
    "file_path": "kaggle_datasets/119/problem_summary.md"
  },
  "86": {
    "problem_id": "86",
    "title": "Multi-modal Gesture Recognition from Kinect Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-modal Gesture Recognition from Kinect Data\n\n## Problem Description\n- **Problem Type**: Multi-class Classification (Sequence Recognition)\n- **Objective**: Recognize sequences of 20 distinct Italian cultural/anthropological gestures from multi-modal Kinect data (RGB video, depth video, audio, and skeleton tracking). The task involves user-independent learning to classify continuous gesture sequences performed by different individuals.\n- **Key Points**:\n  - Focus on **continuous gesture recognition** in unsegmented video streams.\n  - Must handle **multi-modal data fusion** (RGB, depth, audio, skeleton joints).\n  - **User-independent evaluation**: Models must generalize to unseen users.\n  - Gestures may be interspersed with non-target movements (not part of the 20-class vocabulary).\n\n## Dataset Overview\n- **Data Type**: Multi-modal time-series data from Kinect:\n  - RGB video frames (`SessionID_color`)\n  - Depth maps (`SessionID_depth`)\n  - Audio recordings (`SessionID_audio`)\n  - User segmentation masks (`SessionID_user`)\n  - Skeleton joint positions/orientations (`SessionID_data` Matlab files)\n- **Data Files**:\n  - `training1-4.tar.gz`: 7,754 labeled gesture instances (ZIP files per sequence).\n  - `validation1-3.tar.gz`: 3,362 labeled gestures for validation.\n  - `test.csv`: Template for final evaluation submissions.\n  - Sample CSV files with gesture label mappings.\n- **Key Features**:\n  - Per-frame skeletal data (20 joints with 3D world/pixel positions and quaternion rotations).\n  - Depth values in millimeters.\n  - Manually annotated gesture boundaries (start/end frames) for training data.\n\n## Evaluation Metrics\n- **Primary Metric**: Normalized Levenshtein Distance (Edit Distance)\n  - **Calculation**:\n    1. For each video, compare predicted gesture sequence `R` to ground truth `T`.\n    2. Compute minimum edit operations (insertions/deletions/substitutions) to transform `R` to `T`.\n    3. Sum Levenshtein distances across all test sequences.\n    4. Normalize by total number of gestures in ground truth (can yield scores >1).\n  - **Example**: `L([1 2 4], [3 2]) = 2` (1 substitution + 1 deletion).\n- **Phases**:\n  - **Public",
    "sections": {},
    "file_path": "kaggle_datasets/86/problem_summary.md"
  },
  "72": {
    "problem_id": "72",
    "title": "Predicting Bulldozer Auction Prices",
    "problem_type": "Regression",
    "objective": "Predict the auction sale price of heavy equipment (bulldozers) based on usage, equipment type, and configuration. The goal is to create a \"blue book\" valuation tool for customers to determine the worth of their heavy equipment fleet at auction.",
    "evaluation_metric": null,
    "full_content": "# Predicting Bulldozer Auction Prices\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the auction sale price of heavy equipment (bulldozers) based on usage, equipment type, and configuration. The goal is to create a \"blue book\" valuation tool for customers to determine the worth of their heavy equipment fleet at auction.\n* **Key Points:**\n  * Predictions must be made for specific auction events.\n  * Data includes both machine characteristics and auction details.\n  * Some product types may have missing option data.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data containing auction records and machine specifications\n* **Data Files:**\n  * Train.csv (training data through end of 2011)\n  * Valid.csv (validation data from Jan-Apr 2012)\n  * Test.csv (test data from May-Nov 2012)\n  * Machine_Appendix.csv (manufacturing details)\n* **Key Features:**\n  * SalesID (unique sale identifier)\n  * MachineID (unique machine identifier)\n  * SalePrice (target variable, only in training set)\n  * SaleDate (date of auction)\n  * Machine configuration options (multiple features)\n  * Manufacturing year, make, model (in appendix)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n  * Calculated as the square root of the mean of squared logarithmic errors\n  * Logarithmic transformation helps normalize errors across different price ranges\n  * Formula: RMSLE = √(1/n Σ(log(p_i + 1) - log(a_i + 1))² where p_i is predicted and a_i is actual value",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the auction sale price of heavy equipment (bulldozers) based on usage, equipment type, and configuration. The goal is to create a \"blue book\" valuation tool for customers to determine the worth of their heavy equipment fleet at auction.\n* **Key Points:**\n  * Predictions must be made for specific auction events.\n  * Data includes both machine characteristics and auction details.\n  * Some product types may have missing option data.",
      "Dataset Overview": "* **Data Type:** Tabular data containing auction records and machine specifications\n* **Data Files:**\n  * Train.csv (training data through end of 2011)\n  * Valid.csv (validation data from Jan-Apr 2012)\n  * Test.csv (test data from May-Nov 2012)\n  * Machine_Appendix.csv (manufacturing details)\n* **Key Features:**\n  * SalesID (unique sale identifier)\n  * MachineID (unique machine identifier)\n  * SalePrice (target variable, only in training set)\n  * SaleDate (date of auction)\n  * Machine configuration options (multiple features)\n  * Manufacturing year, make, model (in appendix)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n  * Calculated as the square root of the mean of squared logarithmic errors\n  * Logarithmic transformation helps normalize errors across different price ranges\n  * Formula: RMSLE = √(1/n Σ(log(p_i + 1) - log(a_i + 1))² where p_i is predicted and a_i is actual value"
    },
    "file_path": "kaggle_datasets/72/problem_summary.md"
  },
  "589": {
    "problem_id": "589",
    "title": "Multi-Class Prediction of Cirrhosis Outcomes",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Class Prediction of Cirrhosis Outcomes\n\n## Problem Description\n* **Problem Type:** Multi-Class Classification\n* **Objective:** Predict the probability of three possible outcomes for patients with cirrhosis:\n    * `Status_C`: Patient was alive at `N_Days` (censored)\n    * `Status_CL`: Patient was alive at `N_Days` due to liver transplant\n    * `Status_D`: Patient was deceased at `N_Days`\n* **Key Points:**\n    * Requires probabilistic predictions for each class (not just hard classifications)\n    * Predictions must be made for each patient (`id`) in the test set\n    * Dataset is synthetically generated from real-world cirrhosis survival data\n\n## Dataset Overview\n* **Data Type & Context:** Tabular medical data containing patient records related to cirrhosis outcomes\n* **Data Files:**\n    * `train.csv`: Contains target column `Status` with the three outcome classes\n    * `test.csv`: Requires prediction of probabilities for each status class\n    * `sample_submission.csv`: Demonstrates submission format\n* **Features:**\n    * Includes temporal feature `N_Days` (follow-up period)\n    * Other features likely include medical/clinical indicators (though specific features not enumerated in description)\n    * Data is synthetically generated but based on real cirrhosis patient data\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n* **Metric Components:**\n    * Formula: \n        ```\n        logloss = -1/N * Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n        ```\n        Where:\n        * N = number of test set rows\n        * M = number of classes (3)\n        * y_ij = 1 if row i has true label j, else 0\n        * p_ij = predicted probability for class j in row i\n    * Probabilities are rescaled to sum to 1 per row\n    * Probabilities clipped to [10^-15, 1-10^-15] to avoid log(0)",
    "sections": {},
    "file_path": "kaggle_datasets/589/problem_summary.md"
  },
  "542": {
    "problem_id": "542",
    "title": "Regression with a Tabular Media Campaign Cost Dataset",
    "problem_type": "Regression",
    "objective": "Predict the cost of media campaigns based on various tabular features. The goal is to accurately estimate the `cost` target variable using the provided dataset.",
    "evaluation_metric": null,
    "full_content": "# Regression with a Tabular Media Campaign Cost Dataset\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the cost of media campaigns based on various tabular features. The goal is to accurately estimate the `cost` target variable using the provided dataset.\n* **Key Points:**\n  * Dataset is synthetically generated from a deep learning model trained on real-world media campaign cost data.\n  * Participants are encouraged to explore differences between the synthetic and original datasets, and potentially incorporate the original data to improve model performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing media campaign attributes and costs.\n* **Data Files:**\n  * `train.csv`: Contains features and the target `cost` for training.\n  * `test.csv`: Contains features for which participants must predict `cost`.\n  * `sample_submission.csv`: Example submission file in the correct format.\n* **Features:** The dataset includes 35 columns (features) related to media campaigns, though specific features are not detailed. The target variable is `cost`.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Root Mean Squared Log Error (RMSLE)\n  * **Components:**\n    * Calculated using `mean_squared_log_error` from scikit-learn with `squared=False`.\n    * Measures the logarithmic difference between predicted and actual values, penalizing underestimates more than overestimates.\n    * Formula: RMSLE = √(1/n Σ(log(predicted + 1) - log(actual + 1))²)",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the cost of media campaigns based on various tabular features. The goal is to accurately estimate the `cost` target variable using the provided dataset.\n* **Key Points:**\n  * Dataset is synthetically generated from a deep learning model trained on real-world media campaign cost data.\n  * Participants are encouraged to explore differences between the synthetic and original datasets, and potentially incorporate the original data to improve model performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing media campaign attributes and costs.\n* **Data Files:**\n  * `train.csv`: Contains features and the target `cost` for training.\n  * `test.csv`: Contains features for which participants must predict `cost`.\n  * `sample_submission.csv`: Example submission file in the correct format.\n* **Features:** The dataset includes 35 columns (features) related to media campaigns, though specific features are not detailed. The target variable is `cost`.",
      "Evaluation Metrics": "* **Evaluation Metric:** Root Mean Squared Log Error (RMSLE)\n  * **Components:**\n    * Calculated using `mean_squared_log_error` from scikit-learn with `squared=False`.\n    * Measures the logarithmic difference between predicted and actual values, penalizing underestimates more than overestimates.\n    * Formula: RMSLE = √(1/n Σ(log(predicted + 1) - log(actual + 1))²)"
    },
    "file_path": "kaggle_datasets/542/problem_summary.md"
  },
  "126": {
    "problem_id": "126",
    "title": "Higgs Boson Signal Detection Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Higgs Boson Signal Detection Challenge\n\n## Problem Description\n* **Problem Type:** Binary Classification (Signal vs. Background)\n* **Objective:**  \n    * Identify Higgs boson particle decay events (\"signal\") from background processes in Large Hadron Collider (LHC) data.\n    * Distinguish between two classes: \"s\" (signal) and \"b\" (background) in high-energy physics collision data.\n* **Key Points:**\n    * Derived from real ATLAS experiment data at CERN.\n    * Requires handling of weighted events (physics significance weighting).\n    * Features include both raw detector measurements (\"PRI\" variables) and derived quantities (\"DER\" variables).\n    * Missing values are represented as -999.0, requiring special preprocessing.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data representing particle collision events from the ATLAS detector at CERN.\n    * Each row represents one collision event with measured/derived physical properties.\n* **Data Files:**\n    * `training.csv` (250,000 events with labels and weights)\n    * `test.csv` (550,000 events without labels)\n    * `random_submission.zip` (sample submission format)\n    * Metric calculation script provided\n* **Key Features:**\n    * 30 features per event (21 \"DER\" derived features, 9 \"PRI\" primitive features)\n    * Special integer feature: `PRI_jet_num` (number of jets in event)\n    * Weight column indicating event significance in training data\n    * Label column (\"s\" or \"b\") in training data\n\n## Evaluation Metrics\n* **Primary Metric:** Approximate Median Significance (AMS) - a custom physics significance metric\n* **Metric Components:**\n    * AMS = √[2((s+b+b_r)ln(1+s/(b+b_r))-s)]\n    * Where:\n        * s = sum of weights for true signal events correctly classified\n        * b = sum of weights for background events misclassified as signal\n        * b_r = regularization term (fixed at 10)\n    * Uses natural logarithm\n    * Incorporates event weights provided in training data\n    * Submission requires both classification (\"s\"/\"b\") and ranking (confidence ordering)",
    "sections": {},
    "file_path": "kaggle_datasets/126/problem_summary.md"
  },
  "314": {
    "problem_id": "314",
    "title": "Floor Surface Classification for Robot Navigation",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Floor Surface Classification for Robot Navigation\n\n## Problem Description\n* **Problem Type:** Multi-class Classification\n* **Objective:** Predict which of nine floor types a robot is standing on using sensor data from Inertial Measurement Units (IMUs). The goal is to improve robot navigation across different surfaces without human assistance.\n* **Key Points:**\n  * The competition focuses on real-world robotics applications\n  * Uses time-series sensor data from mobile robot movements\n  * Nine possible floor surface classes (including carpet, tiles, concrete, etc.)\n\n## Dataset Overview\n* **Data Type:** Time-series tabular data from IMU sensors\n* **Context:** Data collected while driving a small mobile robot over different floor surfaces in university premises\n* **Data Files:**\n  * `X_train.csv` - Training sensor data\n  * `X_test.csv` - Test sensor data\n  * `y_train.csv` - Surface labels for training data\n  * `sample_submission.csv` - Submission format example\n* **Features:**\n  * 10 sensor channels with 128 measurements per time series\n  * Orientation data (X,Y,Z,W quaternion values)\n  * Angular velocity (X,Y,Z components)\n  * Linear acceleration (X,Y,Z components)\n  * Metadata columns: `row_id`, `series_id`, `measurement_number`\n\n## Evaluation Metrics\n* **Primary Metric:** Multiclass Accuracy\n  * Simple average of correct predictions across all classes\n  * Each observation weighted equally in final score calculation",
    "sections": {},
    "file_path": "kaggle_datasets/314/problem_summary.md"
  },
  "44": {
    "problem_id": "44",
    "title": "Binary Classification of Type 2 Diabetes from Electronic Health Records",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Type 2 Diabetes from Electronic Health Records\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:**  \n  * Predict whether a patient has been diagnosed with Type 2 Diabetes Mellitus (T2DM) based on de-identified electronic health records (EHRs).  \n  * The diagnosis is defined by specific ICD9 codes: `250`, `250.0`, `250.*0`, or `250.*2` (e.g., `250.00`, `250.10`, `250.02`).  \n* **Key Points:**  \n  * The dataset excludes patients with diabetes complications but no primary T2DM diagnosis.  \n  * Specific diabetes-related features (e.g., glucose lab results, diabetes medications) were removed to avoid leakage.  \n  * The task focuses on leveraging EHR data (diagnoses, medications, lab results, etc.) for predictive modeling.  \n\n## Dataset Overview  \n* **Data Type & Context:**  \n  * Tabular data from de-identified EHRs, including diagnoses, lab results, medications, allergies, immunizations, and vital signs.  \n* **Data Files:**  \n  * `training_SyncPatient.csv` (includes a T2DM indicator column for training).  \n  * 16 additional tables per dataset (training/test) linked via patient/visit identifiers (e.g., `SyncDiagnosis`, `SyncMedication`, `LabResult`).  \n  * Provided as CSV files and a SQLite database (`compDataAsSQLiteDB`).  \n* **Features:**  \n  * Structured clinical data (ICD9 codes, medication records, lab observations).  \n  * Key tables: Patient demographics (`SyncPatient`), diagnoses (`SyncDiagnosis`), prescriptions (`SyncPrescription`), lab results (`LabResult`).  \n\n## Evaluation Metrics  \n* **Primary Metric:** Log Loss (cross-entropy loss for probabilistic predictions).  \n  * **Calculation:**  \n    * \\\\( \\text{log loss} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i}) \\right] \\\\)  \n    * Where:  \n      * \\\\(N\\\\) = number of patients.  \n      * \\\\(y_i\\\\) = binary ground truth (1 = T2DM, 0",
    "sections": {},
    "file_path": "kaggle_datasets/44/problem_summary.md"
  },
  "110": {
    "problem_id": "110",
    "title": "Galaxy Morphology Classification Challenge",
    "problem_type": "Multi-class Probability Prediction (Image Classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Galaxy Morphology Classification Challenge\n\n**Problem Description:**\n* **Problem Type:** Multi-class Probability Prediction (Image Classification)\n* **Objective:**  \n    * Predict probability distributions for 37 morphological classes of galaxies based on JPG images, replicating human-classified probability distributions from the Galaxy Zoo citizen science project.\n    * The classification follows a hierarchical decision tree with 11 questions (each with 2-7 possible responses), where probabilities are cumulatively multiplied down the tree branches.\n* **Key Points:**\n    * Hierarchical classification: High-level categories (e.g., smooth vs. spiral) must be accurate to ensure downstream subclass probabilities are meaningful.\n    * Focus on reproducing human consensus (aggregated volunteer classifications) rather than binary labels.\n    * Challenge lies in capturing fine-grained features (e.g., spiral arm tightness, bulge prominence) beyond basic galaxy types.\n\n**Dataset Overview:**\n* **Data Type:** Image data (JPG files) of galaxies from the Sloan Digital Sky Survey (SDSS).\n* **Data Files:**\n    * `images_training.zip`: 61,578 JPG images (training set).\n    * `training_solutions_rev1.zip`: Probability distributions for training images (37 classes per galaxy).\n    * `images_test.zip`: 79,975 JPG images (test set).\n    * Benchmark files (`all_ones_benchmark.zip`, `central_pixel_benchmark.zip`, etc.) for submission formatting.\n* **Features:**\n    * Images are named by `GalaxyId` (unique identifier).\n    * No explicit feature engineering; raw pixel data must be used to infer morphological traits.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE) between predicted and actual probabilities.\n    * Formula:  \n      \\[\n      \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (p_i - a_i)^2}\n      \\]\n      where:\n      * \\(N\\) = number of galaxies × total responses (37 per galaxy).\n      * \\(p_i\\) = predicted probability for class \\(i\\).\n      * \\(a_i\\) = actual (human-aggregated) probability for class \\(i\\).\n    * Emphasizes calibration of probabilities across all hierarchical classes.\n    * Lower RMSE indicates better alignment with human consensus.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Probability Prediction (Image Classification)\n* **Objective:**  \n    * Predict probability distributions for 37 morphological classes of galaxies based on JPG images, replicating human-classified probability distributions from the Galaxy Zoo citizen science project.\n    * The classification follows a hierarchical decision tree with 11 questions (each with 2-7 possible responses), where probabilities are cumulatively multiplied down the tree branches.\n* **Key Points:**\n    * Hierarchical classification: High-level categories (e.g., smooth vs. spiral) must be accurate to ensure downstream subclass probabilities are meaningful.\n    * Focus on reproducing human consensus (aggregated volunteer classifications) rather than binary labels.\n    * Challenge lies in capturing fine-grained features (e.g., spiral arm tightness, bulge prominence) beyond basic galaxy types.",
      "Dataset Overview": "* **Data Type:** Image data (JPG files) of galaxies from the Sloan Digital Sky Survey (SDSS).\n* **Data Files:**\n    * `images_training.zip`: 61,578 JPG images (training set).\n    * `training_solutions_rev1.zip`: Probability distributions for training images (37 classes per galaxy).\n    * `images_test.zip`: 79,975 JPG images (test set).\n    * Benchmark files (`all_ones_benchmark.zip`, `central_pixel_benchmark.zip`, etc.) for submission formatting.\n* **Features:**\n    * Images are named by `GalaxyId` (unique identifier).\n    * No explicit feature engineering; raw pixel data must be used to infer morphological traits.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE) between predicted and actual probabilities.\n    * Formula:  \n      \\[\n      \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (p_i - a_i)^2}\n      \\]\n      where:\n      * \\(N\\) = number of galaxies × total responses (37 per galaxy).\n      * \\(p_i\\) = predicted probability for class \\(i\\).\n      * \\(a_i\\) = actual (human-aggregated) probability for class \\(i\\).\n    * Emphasizes calibration of probabilities across all hierarchical classes.\n    * Lower RMSE indicates better alignment with human consensus."
    },
    "file_path": "kaggle_datasets/110/problem_summary.md"
  },
  "322": {
    "problem_id": "322",
    "title": "Landmark Image Retrieval Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Landmark Image Retrieval Challenge\n\n## Problem Description\n* **Problem Type:** Computer Vision - Image Retrieval\n* **Objective:**  \n  Given a query image, retrieve all images from a large database that depict the same landmark. The task involves:\n  * Identifying landmarks in query images\n  * Finding visually similar images from the index set\n  * Ranking retrieved images by relevance\n* **Key Points:**\n  * Two-phase competition with different test/index sets in each phase\n  * Phase 2 dataset contains 700K images with >100K unique landmarks\n  * Some query images may have no matching landmarks in the index set (ignored in scoring)\n  * Linked with Landmark Recognition Challenge (shared test set)\n\n## Dataset Overview\n* **Data Type:** Image data (landmark photographs)\n* **Context:** Web-sourced landmark images with crowdsourced labels\n* **Data Files:**\n  * `test.csv` (query images)\n  * `index.csv` (database images to retrieve from)\n  * `retrieval_sample_submission.csv` (submission format)\n* **Features:**\n  * Image URLs (must be downloaded by participants)\n  * Unique image IDs\n  * Diverse representations per landmark (indoor/outdoor views, different angles)\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Average Precision @ 100 (mAP@100)\n* **Metric Components:**\n  * Calculated only for queries with matching landmarks in index set\n  * Formula:  \n    ```\n    mAP@100 = (1/Q) * Σ [ (1/min(m_q,100)) * Σ [P_q(k) * rel_q(k)] ]\n    ```\n    Where:\n    * Q = Number of valid query images\n    * m_q = Number of relevant index images for query q\n    * P_q(k) = Precision at rank k\n    * rel_q(k) = 1 if k-th prediction is correct, else 0\n  * Predictions ranked by relevance (most relevant first)\n  * Only top 100 predictions considered per query",
    "sections": {},
    "file_path": "kaggle_datasets/322/problem_summary.md"
  },
  "574": {
    "problem_id": "574",
    "title": "Evaluating Student Summaries with NLP",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Evaluating Student Summaries with NLP\n\n## Problem Description\n* **Problem Type**: NLP - Text Regression (Multi-target Regression)\n* **Objective**: \n    * Develop a model to automatically evaluate the quality of student-written summaries (grades 3-12) by predicting two scores:\n        * **Content**: How well the summary represents the main idea and details of the source text\n        * **Wording**: The clarity, precision, and fluency of the language used\n    * Assist teachers in providing faster feedback and enable learning platforms to offer immediate student assessment\n* **Key Points**:\n    * Must consider both student writing AND the source prompt text (unlike standard writing evaluation)\n    * Focuses on real student writing (not machine-generated summaries)\n    * Includes efficiency considerations for educational deployment (separate CPU-only prize track)\n\n## Dataset Overview\n* **Data Type**: Text data (student summaries + prompt texts) with numeric targets\n* **Context**: \n    * ~24,000 summaries from students in grades 3-12\n    * Covers diverse topics and genres\n* **Data Files**:\n    * `summaries_train.csv`: Student summaries with content/wording scores\n    * `prompts_train.csv`: Corresponding source prompt texts (4 total)\n    * Test set files (structure matches training, without targets)\n    * `sample_submission.csv`: Submission template\n* **Key Features**:\n    * Student summary text (free-form)\n    * Prompt metadata (title, question, full text)\n    * Two numeric targets: `content` and `wording` scores\n\n## Evaluation Metrics\n* **Primary Metric**: MCRMSE (Mean Columnwise Root Mean Squared Error)\n    * Calculated separately for content and wording predictions, then averaged\n    * Formula:  \n      ```\n      MCRMSE = 1/Nt * Σ[sqrt(1/n * Σ(y_ij - ŷ_ij)^2)]\n      ```\n      Where:\n      * Nt = number of target columns (2)\n      * n = number of predictions\n      * y = actual values\n      * ŷ = predicted values\n* **Efficiency Prize Metric** (separate track):\n    * Combines prediction accuracy with runtime:  \n      ```\n      Efficiency = (MCRMSE_Base - min_MCRMSE) + (RuntimeSeconds/32400)\n      ```\n    * CPU-only submissions evaluated on both speed and accuracy",
    "sections": {},
    "file_path": "kaggle_datasets/574/problem_summary.md"
  },
  "580": {
    "problem_id": "580",
    "title": "Binary Prediction of Smoker Status using Bio-Signals",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Prediction of Smoker Status using Bio-Signals\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict a patient's smoking status (binary target variable `smoking`) based on various health indicators (bio-signals).  \n* **Key Points:**  \n  * Synthetic dataset derived from real-world bio-signal data.  \n  * Participants may optionally incorporate the original dataset to explore performance improvements.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular data containing anonymized health metrics (bio-signals) related to smoking status prediction.  \n* **Data Files:**  \n  * `train.csv` (training data with target `smoking`)  \n  * `test.csv` (test data for prediction)  \n  * `sample_submission.csv` (submission template)  \n* **Features:**  \n  * 49 columns (exact features not specified, but derived from bio-signals like physiological measurements).  \n\n## Evaluation Metrics  \n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).  \n* **Components:**  \n  * Submissions require predicted probabilities (not hard labels).  \n  * Metric evaluates ranking performance of predicted probabilities against the true binary labels.",
    "sections": {},
    "file_path": "kaggle_datasets/580/problem_summary.md"
  },
  "43": {
    "problem_id": "43",
    "title": "Multi-Class Source Code File Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Class Source Code File Classification\n\n## Problem Description\n* **Problem Type:** Multi-Class Classification\n* **Objective:**  \n    * Classify source code files according to the open-source projects they belong to.\n    * Predict the project affiliation of unseen source code files from the same set of open-source projects.\n* **Key Points:**\n    * Real-world applications include intellectual property protection, Data Loss Prevention (DLP), and automatic categorization of source code repositories.\n    * The challenge involves handling high-dimensional sparse feature data.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Text data (source code files) represented as Term Frequency (TF) features.\n    * Collected from 97 open-source projects, totaling 219,099 files.\n* **Data Files:**\n    * `train_data.csv`, `test_data.csv` (sparse matrices in CSR format)\n    * `train_labels.csv`, `test_labels.csv` (one-line CSV files)\n    * Helper files: `EMC_IO.py`, `EMC_IO.r` (for reading sparse matrices)\n* **Features:**\n    * High-dimensional TF features (592,158 dimensions per file).\n    * Data stored in Compressed Sparse Row (CSR) format for efficiency.\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-Class Log Loss (Cross-Entropy Loss)\n* **Submission Format Requirements:**\n    * CSV file with each row corresponding to a test sample prediction.\n    * First column: sample index.\n    * Remaining columns: certainty measures for each class (automatically rescaled to sum to 1).\n    * Example format:  \n      ```\n      id,class1,class2\n      1,0.5,0.5\n      2,0.1,0.9\n      ",
    "sections": {},
    "file_path": "kaggle_datasets/43/problem_summary.md"
  },
  "128": {
    "problem_id": "128",
    "title": "Multi-class Image Classification on CIFAR-10 Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Image Classification on CIFAR-10 Dataset\n\n## Problem Description\n* **Problem Type:** Computer Vision - Multi-class Image Classification\n* **Objective:**  \n    * Predict the correct object class for 32x32 color images from the CIFAR-10 dataset.\n    * The task involves classifying each image into one of 10 mutually exclusive categories.\n* **Key Points:**\n    * Classes are completely distinct (e.g., no overlap between \"automobile\" and \"truck\").\n    * Test set contains 290,000 junk images mixed with the official 10,000 test images to prevent cheating (only the official images are scored).\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * 60,000 32x32 color images (50,000 training + 10,000 test) across 10 object classes.\n    * Each class has 6,000 images (balanced distribution).\n* **Data Files:**\n    * `train.7z`: Folder of training images in PNG format.\n    * `test.7z`: Folder of test images (including junk images) in PNG format.\n    * `trainLabels.csv`: Labels for the training images.\n* **Features:**\n    * Images are small (32x32 pixels) and low-resolution.\n    * Classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Classification Accuracy  \n    * Percentage of correctly predicted labels over the official 10,000 test images (junk images ignored).\n* **Submission Format:**  \n    * CSV file with `id` and `label` columns, where labels must exactly match the class names (e.g., \"cat\", \"dog\").",
    "sections": {},
    "file_path": "kaggle_datasets/128/problem_summary.md"
  },
  "587": {
    "problem_id": "587",
    "title": "Predicting RNA Structure via Chemical Reactivity Profiles",
    "problem_type": "Regression (Multi-output)",
    "objective": "Predict the chemical reactivity profiles of RNA molecules at each sequence position for two experimental types (`DMS_MaP` and `2A3_MaP`), which indirectly reflect RNA structural properties. The goal is to develop a model that can generalize to novel RNA sequences, including longer sequences (207-457 bases) not seen in training.",
    "evaluation_metric": null,
    "full_content": "# Predicting RNA Structure via Chemical Reactivity Profiles\n\n**Problem Description:**\n* **Problem Type:** Regression (Multi-output)\n* **Objective:** Predict the chemical reactivity profiles of RNA molecules at each sequence position for two experimental types (`DMS_MaP` and `2A3_MaP`), which indirectly reflect RNA structural properties. The goal is to develop a model that can generalize to novel RNA sequences, including longer sequences (207-457 bases) not seen in training.\n* **Key Points:**\n  * The competition aims to address challenges in RNA structure prediction, including limited training data and rigorous train-test splits.\n  * Successful models could enable breakthroughs in RNA-based medicine (e.g., mRNA vaccines, CRISPR therapeutics) and fundamental biology.\n  * A unique aspect is that most private test data will be collected *during* the competition to prevent data leakage.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with RNA sequence information and associated chemical reactivity profiles.\n* **Data Files:**\n  * `train_data.csv`: Contains experimental profiles with reactivity values for each sequence position.\n  * `test_sequences.csv`: Provides test sequences for which predictions must be made.\n  * `sample_submission.csv`: Demonstrates the required prediction format (two reactivity values per sequence position).\n* **Key Features:**\n  * `sequence`: RNA nucleotide sequence (A, C, G, U).\n  * `reactivity_0001...`: Normalized reactivity values (primary prediction targets).\n  * `experiment_type`: Specifies whether reactivity comes from DMS or 2A3 experiments.\n  * Additional metadata includes signal-to-noise ratios and read counts for quality filtering.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)\n  * Calculated separately for `DMS_MaP` and `2A3_MaP` reactivity predictions.\n  * Values are clipped between 0 and 1 before scoring.\n  * Final score aggregates errors across all test sequence positions.\n* **Scoring Notes:**\n  * Only sequences passing quality filters (`SN_filter`) are evaluated.\n  * Private test set includes longer sequences (207-457 bases) to test generalization.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Multi-output)\n* **Objective:** Predict the chemical reactivity profiles of RNA molecules at each sequence position for two experimental types (`DMS_MaP` and `2A3_MaP`), which indirectly reflect RNA structural properties. The goal is to develop a model that can generalize to novel RNA sequences, including longer sequences (207-457 bases) not seen in training.\n* **Key Points:**\n  * The competition aims to address challenges in RNA structure prediction, including limited training data and rigorous train-test splits.\n  * Successful models could enable breakthroughs in RNA-based medicine (e.g., mRNA vaccines, CRISPR therapeutics) and fundamental biology.\n  * A unique aspect is that most private test data will be collected *during* the competition to prevent data leakage.",
      "Dataset Overview": "* **Data Type:** Tabular data with RNA sequence information and associated chemical reactivity profiles.\n* **Data Files:**\n  * `train_data.csv`: Contains experimental profiles with reactivity values for each sequence position.\n  * `test_sequences.csv`: Provides test sequences for which predictions must be made.\n  * `sample_submission.csv`: Demonstrates the required prediction format (two reactivity values per sequence position).\n* **Key Features:**\n  * `sequence`: RNA nucleotide sequence (A, C, G, U).\n  * `reactivity_0001...`: Normalized reactivity values (primary prediction targets).\n  * `experiment_type`: Specifies whether reactivity comes from DMS or 2A3 experiments.\n  * Additional metadata includes signal-to-noise ratios and read counts for quality filtering.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)\n  * Calculated separately for `DMS_MaP` and `2A3_MaP` reactivity predictions.\n  * Values are clipped between 0 and 1 before scoring.\n  * Final score aggregates errors across all test sequence positions.\n* **Scoring Notes:**\n  * Only sequences passing quality filters (`SN_filter`) are evaluated.\n  * Private test set includes longer sequences (207-457 bases) to test generalization."
    },
    "file_path": "kaggle_datasets/587/problem_summary.md"
  },
  "573": {
    "problem_id": "573",
    "title": "LLM Science Exam: Answering Multiple-Choice Science Questions Generated by LLMs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# LLM Science Exam: Answering Multiple-Choice Science Questions Generated by LLMs\n\n## Problem Description\n* **Problem Type:** NLP - Multiple Choice Question Answering  \n* **Objective:**  \n    * Participants must answer difficult science-based multiple-choice questions *written by a Large Language Model (GPT-3.5)*.  \n    * The goal is to predict the top 3 most probable correct answers for each question, given the prompt and options.  \n* **Key Points:**  \n    * Questions were generated by GPT-3.5 using Wikipedia snippets on scientific topics, then filtered to remove easy questions.  \n    * The competition explores whether smaller LLMs (≤10B parameters) can answer questions written by a much larger model (GPT-3.5, 175B parameters).  \n    * There may be a distributional shift between the provided sample questions and the hidden test set (~4000 questions).  \n\n## Dataset Overview  \n* **Data Type:** Text (Multiple-choice QA pairs)  \n* **Context:** Science-focused questions generated by GPT-3.5, covering topics like Physics, Chemistry, Biology, etc.  \n* **Data Files:**  \n    * `train.csv`: 200 questions with prompts, 5 options (A-E), and correct answers.  \n    * `test.csv`: Hidden test set (format mirrors train.csv but without answers).  \n    * `sample_submission.csv`: Example submission file.  \n* **Features:**  \n    * `prompt`: The question text.  \n    * `A`, `B`, `C`, `D`, `E`: Multiple-choice options.  \n    * `answer` (train only): Correct option label (A-E).  \n\n## Evaluation Metrics  \n* **Primary Metric:** Mean Average Precision @ 3 (MAP@3)  \n* **Components of MAP@3:**  \n    * For each question, precision is calculated at each of the top 3 predicted ranks.  \n    * Precision @ k (`P(k)`): Ratio of correct answers in the top *k* predictions.  \n    * `rel(k)`: 1 if the prediction at rank *k* is correct, else 0.  \n    * Correct predictions beyond the first are ignored (e.g., predicting `A` multiple times counts only once).  \n    * Final score is the average precision across all questions.  \n* **Submission Format:**  \n    * Predict up to 3 labels per question (",
    "sections": {},
    "file_path": "kaggle_datasets/573/problem_summary.md"
  },
  "88": {
    "problem_id": "88",
    "title": "Causal Relationship Detection in Variable Pairs",
    "problem_type": "Binary Classification (with ternary output extension)",
    "objective": "Determine whether variable A causes variable B (or vice versa) given joint observational data of paired variables, without temporal or contextual information.",
    "evaluation_metric": null,
    "full_content": "# Causal Relationship Detection in Variable Pairs\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with ternary output extension)\n* **Objective:** Determine whether variable A causes variable B (or vice versa) given joint observational data of paired variables, without temporal or contextual information.\n    * **Key Points:**\n        * Focuses on distinguishing true causation from correlation or common-cause scenarios\n        * Handles four relationship types: A→B (positive class), B→A (negative class), common cause (null class), and independence (null class)\n        * Excludes feedback loops and time-series analysis\n        * Challenge is to rank pairs by confidence in causal directionality\n\n**Dataset Overview:**\n* **Data Type & Context:** Mixed-type paired observational data (numerical, categorical, binary variables) from diverse domains including medicine, climate science, economics, and sociology\n* **Key Data Files:**\n    * `CEdata_train_pairs.csv` / `CEdata_valid_pairs.csv` / `CEfinal_test_pairs.csv` (observation pairs)\n    * `CEdata_xx_publicinfo.csv` (variable type metadata)\n    * `CEdata_train_target.csv` (ground truth labels)\n* **Features:**\n    * Pairs of variables (A,B) with joint observations\n    * Variable types include continuous numerical, discrete categorical, and binary\n    * Semi-artificial pairs created by mixing real variables\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Implementation Details:**\n        * Ternary truth values: +1 (A→B), -1 (B→A), 0 (no causal relationship)\n        * Participants submit continuous scores: \n            * Large positive values → confidence in A→B\n            * Large negative values → confidence in B→A\n            * Near-zero values → no causal relationship\n        * AUC computed by treating A→B as positive class and all other cases as negative",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with ternary output extension)\n* **Objective:** Determine whether variable A causes variable B (or vice versa) given joint observational data of paired variables, without temporal or contextual information.\n    * **Key Points:**\n        * Focuses on distinguishing true causation from correlation or common-cause scenarios\n        * Handles four relationship types: A→B (positive class), B→A (negative class), common cause (null class), and independence (null class)\n        * Excludes feedback loops and time-series analysis\n        * Challenge is to rank pairs by confidence in causal directionality",
      "Dataset Overview": "* **Data Type & Context:** Mixed-type paired observational data (numerical, categorical, binary variables) from diverse domains including medicine, climate science, economics, and sociology\n* **Key Data Files:**\n    * `CEdata_train_pairs.csv` / `CEdata_valid_pairs.csv` / `CEfinal_test_pairs.csv` (observation pairs)\n    * `CEdata_xx_publicinfo.csv` (variable type metadata)\n    * `CEdata_train_target.csv` (ground truth labels)\n* **Features:**\n    * Pairs of variables (A,B) with joint observations\n    * Variable types include continuous numerical, discrete categorical, and binary\n    * Semi-artificial pairs created by mixing real variables",
      "Evaluation Metrics": "* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Implementation Details:**\n        * Ternary truth values: +1 (A→B), -1 (B→A), 0 (no causal relationship)\n        * Participants submit continuous scores: \n            * Large positive values → confidence in A→B\n            * Large negative values → confidence in B→A\n            * Near-zero values → no causal relationship\n        * AUC computed by treating A→B as positive class and all other cases as negative"
    },
    "file_path": "kaggle_datasets/88/problem_summary.md"
  },
  "325": {
    "problem_id": "325",
    "title": "Fine-Grained Species Classification in Natural Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Species Classification in Natural Images\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective:**  \n  The competition aims to advance automatic image classification for real-world biodiversity data by distinguishing between 1,010 highly similar species of plants and animals. The challenge focuses on fine-grained classification where species have subtle visual differences, captured in diverse real-world conditions across global locations.\n* **Key Points:**\n  * Focuses on species with high visual similarity requiring expert-level discrimination\n  * Uses real-world user-submitted images with varied conditions (lighting, angles, backgrounds)\n  * Dataset contains hierarchical taxonomic information (kingdom to genus) for each species\n\n## Dataset Overview\n* **Data Type & Context:**  \n  Image dataset of natural world species (plants and animals) collected through the iNaturalist platform, featuring:\n  * 268,243 training/validation images\n  * JPEG format with max dimension of 800px\n  * Images organized by taxonomic hierarchy\n* **Data Files:**\n  * `train_val2019.tar.gz` - Training/validation images\n  * `train2019.json`/`val2019.json` - Annotations with taxonomic metadata\n  * `test2019.tar.gz` - Test images\n  * `test2019.json` - Test image metadata\n  * Sample submission file\n* **Key Features:**\n  * COCO-style JSON annotations with extended taxonomic fields\n  * Each image annotated with full biological classification hierarchy\n  * Images represent real-world observation scenarios with varied quality\n\n## Evaluation Metrics\n* **Primary Metric:** Top-1 Classification Error (1 - Accuracy)\n  * Error = 0 if predicted label matches ground truth, else 1\n  * Final score = average error across all test images\n* **Additional Analysis:**\n  * While only top-1 prediction is scored, participants are encouraged to provide top-5 predictions for deeper analysis\n  * Submission format requires ordered predictions by confidence",
    "sections": {},
    "file_path": "kaggle_datasets/325/problem_summary.md"
  },
  "117": {
    "problem_id": "117",
    "title": "Predicting Credit Default Risk",
    "problem_type": "Binary Classification (Probability of Default)",
    "objective": "Predict the probability of a customer defaulting on a credit product within the next 18 months. The goal is to accurately rank customers' credit risk based on behavioral and other relevant data.",
    "evaluation_metric": null,
    "full_content": "# Predicting Credit Default Risk\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Probability of Default)\n* **Objective:** Predict the probability of a customer defaulting on a credit product within the next 18 months. The goal is to accurately rank customers' credit risk based on behavioral and other relevant data.\n    * **Key Points:**\n        * Focus on early detection of credit distress signals.\n        * Improve credit risk models for responsible credit line management.\n        * Competition restricted to Kaggle Masters-level participants.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing customer credit-related features (likely anonymized behavioral and demographic data).\n    * **Data Files:** Specific files not listed in the overview, but participants are directed to \"AdditionalCompetitionInformation.pdf\" for details.\n    * **Features:** Likely includes:\n        * Customer behavior metrics (payment history, credit utilization, etc.)\n        * Demographic or financial indicators (if not fully anonymized)\n        * Time-based features related to credit activity.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Not explicitly stated in the overview, but likely a ranking-based metric (e.g., AUC-ROC) given the objective of risk ranking.\n    * **Additional Details:** Participants must refer to \"AdditionalCompetitionInformation.pdf\" for full evaluation criteria, suggesting a potentially custom or nuanced metric.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Probability of Default)\n* **Objective:** Predict the probability of a customer defaulting on a credit product within the next 18 months. The goal is to accurately rank customers' credit risk based on behavioral and other relevant data.\n    * **Key Points:**\n        * Focus on early detection of credit distress signals.\n        * Improve credit risk models for responsible credit line management.\n        * Competition restricted to Kaggle Masters-level participants.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing customer credit-related features (likely anonymized behavioral and demographic data).\n    * **Data Files:** Specific files not listed in the overview, but participants are directed to \"AdditionalCompetitionInformation.pdf\" for details.\n    * **Features:** Likely includes:\n        * Customer behavior metrics (payment history, credit utilization, etc.)\n        * Demographic or financial indicators (if not fully anonymized)\n        * Time-based features related to credit activity.",
      "Evaluation Metrics": "* **Primary Metric:** Not explicitly stated in the overview, but likely a ranking-based metric (e.g., AUC-ROC) given the objective of risk ranking.\n    * **Additional Details:** Participants must refer to \"AdditionalCompetitionInformation.pdf\" for full evaluation criteria, suggesting a potentially custom or nuanced metric."
    },
    "file_path": "kaggle_datasets/117/problem_summary.md"
  },
  "508": {
    "problem_id": "508",
    "title": "Multi-Organ Functional Tissue Unit Segmentation in Histology Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Organ Functional Tissue Unit Segmentation in Histology Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - Semantic Segmentation\n* **Objective**: Develop an algorithm to accurately segment Functional Tissue Units (FTUs) across five human organs (kidney, large intestine, lung, spleen, prostate) from histology images. The goal is to automate the time-consuming manual annotation process for cellular analysis.\n* **Key Points**:\n  * Must generalize across different organs and staining protocols (DAB/hematoxylin vs PAS/H&E)\n  * Needs to handle varying image resolutions (3000x3000 to 160x160 pixels) and pixel sizes (0.2290-6.263 µm)\n  * Robustness to dataset differences between HPA and HuBMAP sources is crucial\n  * Successful models will accelerate creation of a Human Reference Atlas\n\n## Dataset Overview\n* **Data Type**: Histology images (TIFF format) with polygon annotations\n* **Context**: Tissue section images from healthy donors, prepared with different staining protocols\n* **Data Files**:\n  * `train.csv`/`test.csv` - Metadata including organ type, data source, image dimensions\n  * `train_images/` - Training set images\n  * `train_annotations/` - Polygon masks defining FTU boundaries\n  * `test_images/` - Test set images (partial available for download)\n* **Key Features**:\n  * Multi-organ dataset with varying:\n    * Pixel sizes (0.2290-6.263 µm)\n    * Tissue thicknesses (4-10 µm)\n    * Staining protocols (DAB/hematoxylin vs PAS/H&E)\n  * Metadata includes patient age and sex (training only)\n  * All images contain at least one FTU\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Dice Coefficient (Sørensen-Dice coefficient)\n  * Formula: 2∗|𝑋∩𝑌|/(|𝑋|+|𝑌|)\n  * Compares pixel-wise agreement between prediction and ground truth\n  * Defined as 1 when both prediction and ground truth are empty\n* **Submission Format**: Run-length encoded (RLE) binary masks\n  * Space-delimited pairs indicating start position and run length\n  * Pixels numbered top-to-bottom, then left-to-right\n  * Example: '1 3 10 5' → pixels 1,",
    "sections": {},
    "file_path": "kaggle_datasets/508/problem_summary.md"
  },
  "198": {
    "problem_id": "198",
    "title": "Avito Duplicate Ads Detection",
    "problem_type": "Binary Classification (Duplicate Detection)",
    "objective": "Develop a model to automatically identify whether pairs of online classified ads are duplicates, where sellers post the same item multiple times with slight variations (e.g., altered text or photos from different angles). The goal is to improve buyer experience by reducing clutter from deceptive duplicates.",
    "evaluation_metric": null,
    "full_content": "# Avito Duplicate Ads Detection\n\n**Problem Description:**\n*   **Problem Type:** Binary Classification (Duplicate Detection)\n*   **Objective:** Develop a model to automatically identify whether pairs of online classified ads are duplicates, where sellers post the same item multiple times with slight variations (e.g., altered text or photos from different angles). The goal is to improve buyer experience by reducing clutter from deceptive duplicates.\n    *   **Key Points:**\n        *   Focus on **ad pairs** (not individual ads).\n        *   Duplicates may involve subtle variations in text, images, or metadata.\n        *   Training and test data are sampled from different time periods to ensure generalization.\n\n**Dataset Overview:**\n*   **Data Type & Context:** Multimodal data (tabular, text, and images) from Avito, a large online classifieds platform. \n    *   **Data Files:**\n        *   `ItemPairs_train.csv`: Pairs of ads with labels (duplicate or not) and generation method (human/algorithmic).\n        *   `ItemPairs_test.csv`: Pairs of ads to predict (no labels).\n        *   `ItemInfo_train.csv` & `ItemInfo_test.csv`: Ad details (title, description, price, location, images, etc.).\n        *   `Images_X.zip`: 10+ million ad images (organized by ID).\n        *   `Category.csv` & `Location.csv`: Metadata for categories and locations.\n    *   **Key Features:**\n        *   **Text:** Ad titles and descriptions.\n        *   **Images:** Multiple images per ad.\n        *   **Metadata:** Price, location, category, and JSON attributes.\n        *   **Pair Information:** Generation method (human/algorithmic labels in training data).\n\n**Evaluation Metrics:**\n*   **Primary Metric:** AUC (Area Under the ROC Curve)\n    *   **Interpretation:** Measures the model's ability to rank duplicate pairs higher than non-duplicate pairs across all possible thresholds. Higher AUC indicates better discrimination.\n    *   **Submission Format:** Probability scores (0 to 1) for each test pair, where 1 indicates high confidence of being a duplicate.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Binary Classification (Duplicate Detection)\n*   **Objective:** Develop a model to automatically identify whether pairs of online classified ads are duplicates, where sellers post the same item multiple times with slight variations (e.g., altered text or photos from different angles). The goal is to improve buyer experience by reducing clutter from deceptive duplicates.\n    *   **Key Points:**\n        *   Focus on **ad pairs** (not individual ads).\n        *   Duplicates may involve subtle variations in text, images, or metadata.\n        *   Training and test data are sampled from different time periods to ensure generalization.",
      "Dataset Overview": "*   **Data Type & Context:** Multimodal data (tabular, text, and images) from Avito, a large online classifieds platform. \n    *   **Data Files:**\n        *   `ItemPairs_train.csv`: Pairs of ads with labels (duplicate or not) and generation method (human/algorithmic).\n        *   `ItemPairs_test.csv`: Pairs of ads to predict (no labels).\n        *   `ItemInfo_train.csv` & `ItemInfo_test.csv`: Ad details (title, description, price, location, images, etc.).\n        *   `Images_X.zip`: 10+ million ad images (organized by ID).\n        *   `Category.csv` & `Location.csv`: Metadata for categories and locations.\n    *   **Key Features:**\n        *   **Text:** Ad titles and descriptions.\n        *   **Images:** Multiple images per ad.\n        *   **Metadata:** Price, location, category, and JSON attributes.\n        *   **Pair Information:** Generation method (human/algorithmic labels in training data).",
      "Evaluation Metrics": "*   **Primary Metric:** AUC (Area Under the ROC Curve)\n    *   **Interpretation:** Measures the model's ability to rank duplicate pairs higher than non-duplicate pairs across all possible thresholds. Higher AUC indicates better discrimination.\n    *   **Submission Format:** Probability scores (0 to 1) for each test pair, where 1 indicates high confidence of being a duplicate."
    },
    "file_path": "kaggle_datasets/198/problem_summary.md"
  },
  "361": {
    "problem_id": "361",
    "title": "Question Answering on Wikipedia Articles with TensorFlow 2.0",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Question Answering on Wikipedia Articles with TensorFlow 2.0\n\n## Problem Description\n* **Problem Type**: NLP - Question Answering (Open-Domain QA)\n* **Objective**: Predict both short and long answers to real user questions based on Wikipedia article content. The system must:\n  * Extract precise **short answers** (phrases, sentences, or YES/NO responses) from the article.\n  * Identify relevant **long answers** (paragraphs or sections) that contextually address the question.\n* **Key Points**:\n  * Answers must be **directly extracted** from the provided article text (no generation).\n  * The task emulates real-world open-domain QA systems requiring document-level comprehension.\n  * Special emphasis on **succinct and relevant** answers, avoiding overly lengthy responses.\n\n## Dataset Overview\n* **Data Type**: Text data (Wikipedia articles + questions) in JSONL format.\n* **Context**: Simplified version of Google's Natural Questions dataset, with HTML tags removed except for structural markers.\n* **Data Files**:\n  * `simplified-nq-train.jsonl`: Training data with annotations.\n  * `simplified-nq-kaggle-test.jsonl`: Test data (private ground truth).\n  * `sample_submission.csv`: Submission template.\n* **Key Features**:\n  * `document_text`: Full Wikipedia article text (tokenized by whitespace).\n  * `question_text`: User-submitted question.\n  * `long_answer_candidates`: JSON array of plausible answer spans.\n  * `annotations`: Ground truth answers (train only).\n\n## Evaluation Metrics\n* **Primary Metric**: Micro F1-Score (combining both short and long answers).\n* **Scoring Logic**:\n  * **Exact match** required for predicted answer spans (token indices) or YES/NO responses.\n  * **True Positives (TP)**: Predicted indices match any ground truth.\n  * **False Positives (FP)**: Predicted indices mismatch OR prediction made where no answer exists.\n  * **False Negatives (FN)**: No prediction made where an answer exists.\n  * **Micro-averaging**: Precision and recall are computed globally across all predictions (not per-class).",
    "sections": {},
    "file_path": "kaggle_datasets/361/problem_summary.md"
  },
  "153": {
    "problem_id": "153",
    "title": "Predicting Altruism Through Free Pizza Requests",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that a textual request for free pizza on the Reddit community \"Random Acts of Pizza\" will be successful (i.e., result in the requester receiving pizza).",
    "evaluation_metric": null,
    "full_content": "# Predicting Altruism Through Free Pizza Requests\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a textual request for free pizza on the Reddit community \"Random Acts of Pizza\" will be successful (i.e., result in the requester receiving pizza).\n    * **Key Points:**\n        * The task involves analyzing both textual content (request text and title) and meta-data features related to the requester's Reddit activity.\n        * The goal is to understand what factors contribute to successful altruistic requests in an online community.\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * JSON files containing textual data (pizza requests) and associated meta-data from the Reddit community \"Random Acts of Pizza\".\n    * Includes 5,671 requests collected between December 2010 and September 2013.\n* **Data Files:**\n    * `train.json` - Training data with request text, meta-data, and outcome labels\n    * `test.json` - Test data for making predictions\n    * `sampleSubmission.csv` - Example submission file format\n* **Key Features:**\n    * Textual features: `request_text`, `request_title`\n    * Reddit activity features: upvotes/downvotes, number of comments/posts, account age\n    * Community-specific features: days since first RAOP post, number of RAOP comments/posts\n    * Outcome label: `requester_received_pizza` (boolean)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the model's ability to distinguish between successful and unsuccessful pizza requests\n        * Evaluates the ranking of predicted probabilities rather than absolute values\n        * Score ranges from 0.5 (random) to 1.0 (perfect discrimination)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that a textual request for free pizza on the Reddit community \"Random Acts of Pizza\" will be successful (i.e., result in the requester receiving pizza).\n    * **Key Points:**\n        * The task involves analyzing both textual content (request text and title) and meta-data features related to the requester's Reddit activity.\n        * The goal is to understand what factors contribute to successful altruistic requests in an online community.",
      "Dataset Overview": "* **Data Type & Context:** \n    * JSON files containing textual data (pizza requests) and associated meta-data from the Reddit community \"Random Acts of Pizza\".\n    * Includes 5,671 requests collected between December 2010 and September 2013.\n* **Data Files:**\n    * `train.json` - Training data with request text, meta-data, and outcome labels\n    * `test.json` - Test data for making predictions\n    * `sampleSubmission.csv` - Example submission file format\n* **Key Features:**\n    * Textual features: `request_text`, `request_title`\n    * Reddit activity features: upvotes/downvotes, number of comments/posts, account age\n    * Community-specific features: days since first RAOP post, number of RAOP comments/posts\n    * Outcome label: `requester_received_pizza` (boolean)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the model's ability to distinguish between successful and unsuccessful pizza requests\n        * Evaluates the ranking of predicted probabilities rather than absolute values\n        * Score ranges from 0.5 (random) to 1.0 (perfect discrimination)"
    },
    "file_path": "kaggle_datasets/153/problem_summary.md"
  },
  "537": {
    "problem_id": "537",
    "title": "NFL Player Contact Detection from Sensor and Video Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NFL Player Contact Detection from Sensor and Video Data\n\n## Problem Description\n* **Problem Type:** Binary Classification (Contact Detection)\n* **Objective:** Detect moments of external contact between NFL players (player-to-player or player-to-ground) during games using video footage and player tracking data. The goal is to improve player safety by accurately identifying contact events for injury analysis.\n    * **Key Points:**\n        * Focus on both player-to-player contact and player-to-ground contact.\n        * Leverage multiple synchronized video views (sideline, endzone) and player tracking sensor data.\n        * Improve upon existing tracking-data-only solutions by incorporating video analysis.\n        * Labels may have ±10Hz temporal inaccuracy in the training data.\n\n## Dataset Overview\n* **Data Type:** Multimodal (Video + Tabular Sensor Data)\n* **Context:** NFL game footage and player movement tracking during plays.\n* **Data Files:**\n    * `train_labels.csv`: Ground truth contact labels (binary) with timestamps.\n    * `train/test.mp4`: Video footage from sideline, endzone, and All29 views.\n    * `train/test_player_tracking.csv`: 10Hz player movement data (position, speed, acceleration etc.).\n    * `train/test_baseline_helmets.csv`: Pre-computed helmet detection bounding boxes.\n    * `train/test_video_metadata.csv`: Timestamps for video synchronization.\n    * `sample_submission.csv`: Submission format template.\n* **Key Features:**\n    * Video data: Three synchronized views per play (59.94 FPS).\n    * Tracking data: Player position, speed, orientation, acceleration at 10Hz.\n    * Contact labels: Binary indicators for all possible player pairs at each timestep.\n\n## Evaluation Metrics\n* **Primary Metric:** Matthews Correlation Coefficient (Phi Coefficient)\n    * **Properties:**\n        * Balanced measure for binary classification, especially useful with class imbalance.\n        * Ranges from -1 (perfect inverse prediction) to +1 (perfect prediction).\n        * More informative than accuracy for imbalanced problems.\n    * **Implementation:**\n        * Computed between predicted and actual binary contact labels.\n        * Evaluates all possible player pairs (including ground contact) at each timestep.",
    "sections": {},
    "file_path": "kaggle_datasets/537/problem_summary.md"
  },
  "38": {
    "problem_id": "38",
    "title": "Psychopathy Prediction from Twitter Usage",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Psychopathy Prediction from Twitter Usage\n\n## Problem Description\n* **Problem Type**: Binary Classification (with highly imbalanced data)\n* **Objective**: Predict whether Twitter users have a high degree of psychopathy (defined as 2 standard deviations above the mean score of 1.98) based on their Twitter usage patterns and linguistic analysis.\n    * **Key Points**:\n        * Focuses on identifying the top ~3% of psychopathy scores in the sample\n        * Uses anonymized Twitter-derived features to maintain user privacy\n        * Part of research investigating links between social media behavior and \"Dark Triad\" personality traits\n        * Winning models may be cited in academic papers\n\n## Dataset Overview\n* **Data Type**: Tabular data derived from Twitter activity and linguistic analysis\n* **Context**: 337 anonymized features generated from Twitter usage patterns and text analysis of user tweets\n* **Data Files**:\n    * `Pyschopath_Trainingset_v1.csv` (training set with psychopathy scores)\n    * `Pyschopath_Testset_v1.csv` (test set)\n    * `DescriptiveStats.pdf` (statistics of personality dimensions)\n* **Features**:\n    * User identifier (MyID)\n    * Psychopathy score (target variable)\n    * 337 derived variables (categorized as AVar1-13, LVar1-239, FVar1-85) representing:\n        * Twitter activity patterns\n        * Linguistic features from tweet analysis\n        * Usage statistics\n\n## Evaluation Metrics\n* **Primary Metric**: Average Precision\n    * **Calculation Logic**:\n        1. True scores are sorted descending based on submission order\n        2. For each row, compute:\n            * Cumulative \"True Scores Ordered by Submission\" / Cumulative \"True Scores Ordered By True Scores\"\n        3. Final score is the average of these precision values across all rows\n    * **Rationale**: Appropriate for imbalanced binary classification where the order of high-probability predictions matters more than absolute thresholding",
    "sections": {},
    "file_path": "kaggle_datasets/38/problem_summary.md"
  },
  "395": {
    "problem_id": "395",
    "title": "Open Images Object Detection Challenge",
    "problem_type": "Computer Vision - Object Detection",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Open Images Object Detection Challenge\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Detection\n* **Objective:**  \n  * Predict tight bounding boxes around object instances in diverse and complex images.  \n  * The task involves detecting objects across 500 categories in images containing an average of 7 objects per scene.  \n* **Key Points:**  \n  * Part of the Robust Vision Challenge (RVC) 2020, encouraging robustness across multiple datasets.  \n  * Uses the Open Images dataset, known for large-scale and high-quality manual annotations.  \n  * Submission format requires predicting bounding boxes with associated labels and confidence scores.  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  * Image data with bounding box annotations for object detection.  \n  * Images are diverse, containing complex scenes with multiple objects.  \n* **Data Files:**  \n  * `test.zip` - Test set images (99,999 images).  \n  * `sample_submission.csv` - Submission format template.  \n  * Training/validation data must be downloaded separately from the Open Images Challenge page.  \n* **Features:**  \n  * Images in JPG format.  \n  * Annotations include bounding box coordinates (`XMin`, `YMin`, `XMax`, `YMax`) and class labels.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Mean Average Precision (mAP)  \n  * Computed as the average of per-class Average Precision (AP) scores across 500 categories.  \n  * Modified to align with Open Images annotation processes.  \n* **Components:**  \n  * AP measures precision-recall trade-offs for each class.  \n  * Final score averages AP across all classes.  \n  * Implemented via TensorFlow Object Detection API.",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Detection\n* **Objective:**  \n  * Predict tight bounding boxes around object instances in diverse and complex images.  \n  * The task involves detecting objects across 500 categories in images containing an average of 7 objects per scene.  \n* **Key Points:**  \n  * Part of the Robust Vision Challenge (RVC) 2020, encouraging robustness across multiple datasets.  \n  * Uses the Open Images dataset, known for large-scale and high-quality manual annotations.  \n  * Submission format requires predicting bounding boxes with associated labels and confidence scores.",
      "Dataset Overview": "* **Data Type & Context:**  \n  * Image data with bounding box annotations for object detection.  \n  * Images are diverse, containing complex scenes with multiple objects.  \n* **Data Files:**  \n  * `test.zip` - Test set images (99,999 images).  \n  * `sample_submission.csv` - Submission format template.  \n  * Training/validation data must be downloaded separately from the Open Images Challenge page.  \n* **Features:**  \n  * Images in JPG format.  \n  * Annotations include bounding box coordinates (`XMin`, `YMin`, `XMax`, `YMax`) and class labels.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Mean Average Precision (mAP)  \n  * Computed as the average of per-class Average Precision (AP) scores across 500 categories.  \n  * Modified to align with Open Images annotation processes.  \n* **Components:**  \n  * AP measures precision-recall trade-offs for each class.  \n  * Final score averages AP across all classes.  \n  * Implemented via TensorFlow Object Detection API."
    },
    "file_path": "kaggle_datasets/395/problem_summary.md"
  },
  "359": {
    "problem_id": "359",
    "title": "Predicting Vehicle Pose from Single Images for Autonomous Driving",
    "problem_type": "Computer Vision - 6-Degree-of-Freedom (6DoF) Pose Estimation",
    "objective": "Develop an algorithm to estimate the absolute pose (position and rotation) of vehicles from single images in real-world traffic environments. The goal is to predict:",
    "evaluation_metric": null,
    "full_content": "# Predicting Vehicle Pose from Single Images for Autonomous Driving\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - 6-Degree-of-Freedom (6DoF) Pose Estimation\n* **Objective:** Develop an algorithm to estimate the absolute pose (position and rotation) of vehicles from single images in real-world traffic environments. The goal is to predict:\n    * **Translation (x, y, z):** 3D position in meters relative to the camera\n    * **Rotation (pitch, yaw, roll):** Orientation angles in radians\n    * **Confidence Score:** Certainty of each prediction\n* **Key Points:**\n    * Focuses on unmasked cars (ignoring distant/irrelevant vehicles via provided masks)\n    * Uses industry-grade CAD car models as reference\n    * Aims to improve autonomous vehicle perception in traffic scenarios\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * 5,277 real-world street images (camera-mounted car perspective)\n    * 60,000+ labeled 3D car instances with pose annotations\n* **Data Files:**\n    * `train.csv` - Pose strings for training images (model type + 6DoF values)\n    * `train_images.zip`/`test_images.zip` - JPEG images\n    * `train_masks.zip`/`test_masks.zip` - Binary masks for irrelevant cars\n    * `car_models.zip` - 3D CAD models (pickle files) for reference\n    * `camera.zip` - Intrinsic camera parameters\n* **Key Features:**\n    * Image data: Street scenes with multiple vehicles\n    * Pose labels: Model type (train only), yaw/pitch/roll, x/y/z coordinates\n    * Masks: Identify cars to ignore in evaluation\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision (mAP) with pose-specific thresholds\n* **Calculation Components:**\n    1. **Distance Metrics:**\n        * Rotation Distance: Quaternion-based angular difference (degrees)\n        * Translation Distance: Euclidean distance (meters)\n    2. **Thresholding:**\n        * Rotation thresholds: 50° to 5° (10 steps)\n        * Translation thresholds: 0.1m to 0.01m (10 steps)\n    3. **True/False Positives:**\n        * TP if both rotation",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - 6-Degree-of-Freedom (6DoF) Pose Estimation\n* **Objective:** Develop an algorithm to estimate the absolute pose (position and rotation) of vehicles from single images in real-world traffic environments. The goal is to predict:\n    * **Translation (x, y, z):** 3D position in meters relative to the camera\n    * **Rotation (pitch, yaw, roll):** Orientation angles in radians\n    * **Confidence Score:** Certainty of each prediction\n* **Key Points:**\n    * Focuses on unmasked cars (ignoring distant/irrelevant vehicles via provided masks)\n    * Uses industry-grade CAD car models as reference\n    * Aims to improve autonomous vehicle perception in traffic scenarios",
      "Dataset Overview": "* **Data Type & Context:** \n    * 5,277 real-world street images (camera-mounted car perspective)\n    * 60,000+ labeled 3D car instances with pose annotations\n* **Data Files:**\n    * `train.csv` - Pose strings for training images (model type + 6DoF values)\n    * `train_images.zip`/`test_images.zip` - JPEG images\n    * `train_masks.zip`/`test_masks.zip` - Binary masks for irrelevant cars\n    * `car_models.zip` - 3D CAD models (pickle files) for reference\n    * `camera.zip` - Intrinsic camera parameters\n* **Key Features:**\n    * Image data: Street scenes with multiple vehicles\n    * Pose labels: Model type (train only), yaw/pitch/roll, x/y/z coordinates\n    * Masks: Identify cars to ignore in evaluation",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision (mAP) with pose-specific thresholds\n* **Calculation Components:**\n    1. **Distance Metrics:**\n        * Rotation Distance: Quaternion-based angular difference (degrees)\n        * Translation Distance: Euclidean distance (meters)\n    2. **Thresholding:**\n        * Rotation thresholds: 50° to 5° (10 steps)\n        * Translation thresholds: 0.1m to 0.01m (10 steps)\n    3. **True/False Positives:**\n        * TP if both rotation"
    },
    "file_path": "kaggle_datasets/359/problem_summary.md"
  },
  "392": {
    "problem_id": "392",
    "title": "Prostate Cancer Grade Assessment (PANDA) Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Prostate Cancer Grade Assessment (PANDA) Challenge\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (Medical Image Analysis)\n* **Objective**:  \n  Develop models to classify the severity of prostate cancer from whole-slide microscopy images of prostate biopsies using the ISUP grading system (0-5 scale). The task involves:\n  * Detecting cancerous tissue regions in high-resolution images\n  * Classifying tumor growth patterns according to Gleason grading standards\n  * Converting Gleason scores to ISUP grades (0=benign, 1-5=cancer severity)\n* **Key Points**:\n  * Significant inter-pathologist variability in ground truth labels\n  * Largest public whole-slide image dataset available (8x larger than previous benchmarks)\n  * Multi-center data with different scanning protocols and labeling conventions\n  * Imperfect segmentation masks provided for some training images\n\n## Dataset Overview\n* **Data Type**: High-resolution whole-slide TIFF images (H&E-stained prostate biopsies) + tabular metadata\n* **Data Files**:\n  * `train.csv`/`test.csv`: Image IDs, data provider, ISUP grades (train only), Gleason scores (train only)\n  * `train_images/`: ~11,000 multi-level TIFF images (~411GB total)\n  * `train_label_masks/`: Partial segmentation masks (not all images)\n  * `sample_submission.csv`: Submission template\n* **Key Features**:\n  * Images up to 100,000x100,000 pixels (requires specialized processing)\n  * Two data providers with different scanners and labeling protocols:\n    * Radboud: Gland-level annotations (6 classes)\n    * Karolinska: Region-level annotations (3 classes)\n  * ISUP grades derived from Gleason scores via standardized conversion\n\n## Evaluation Metrics\n* **Primary Metric**: Quadratic Weighted Kappa (measures agreement between predicted and pathologist-assigned ISUP grades)\n* **Metric Calculation**:\n  1. Construct NxN histogram matrix O of actual vs predicted grades\n  2. Compute weight matrix w where wᵢⱼ = (i-j)²/(N-1)²\n  3. Calculate expected matrix E as outer product of grade histograms\n  4. Kappa = 1 - (∑wᵢⱼOᵢⱼ)/(∑wᵢⱼ",
    "sections": {},
    "file_path": "kaggle_datasets/392/problem_summary.md"
  },
  "530": {
    "problem_id": "530",
    "title": "Binary Classification with a Tabular Employee Attrition Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Tabular Employee Attrition Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict the probability of employee attrition (whether an employee will leave the company) based on tabular HR data.  \n* **Key Points:**  \n  * The dataset is synthetically generated from real-world employee attrition data, with distributions close to the original.  \n  * Participants are encouraged to explore differences between synthetic and original data for potential performance improvements.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular data containing HR-related features (e.g., employee demographics, job role, satisfaction metrics).  \n* **Data Files:**  \n  * `train.csv` – Contains features and the binary target `Attrition`.  \n  * `test.csv` – Contains features for which predictions must be made.  \n  * `sample_submission.csv` – Example submission file in the required format (`EmployeeNumber, Attrition_probability`).  \n* **Features:** Likely includes anonymized or synthetic versions of real HR attributes (e.g., age, department, job level, performance ratings).  \n\n## Evaluation Metrics  \n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).  \n* **Components:**  \n  * Measures the model’s ability to distinguish between employees who attrit (positive class) and those who do not (negative class).  \n  * Submissions require predicted probabilities (not binary labels).  \n  * Higher AUC-ROC indicates better ranking of positive instances relative to negatives.",
    "sections": {},
    "file_path": "kaggle_datasets/530/problem_summary.md"
  },
  "154": {
    "problem_id": "154",
    "title": "Binary Classification of Online Auction Bidders: Human or Robot?",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Online Auction Bidders: Human or Robot?\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** \n  * Predict whether an online auction bidder is a human or a robot (automated bot) based on bidding behavior and metadata.\n  * The goal is to help auction site owners flag and remove robotic bidders to ensure fair auction activity and improve customer satisfaction.\n* **Key Points:**\n  * The competition focuses on distinguishing between human and automated bidding patterns.\n  * The dataset includes behavioral data like bid frequency, timing, and device information.\n  * The labels (human/robot) are partially hand-labeled and partially based on statistical thresholds, with two types of bot classifications:\n    * Confirmed bots (accounts banned by the auction site).\n    * Suspected bots (statistical outliers without clear proof).\n\n## Dataset Overview\n* **Data Type & Context:** \n  * Tabular data from an online auction platform, containing bidder information and detailed bid records.\n  * Data includes anonymized bidder profiles and timestamped bidding activity.\n* **Data Files:**\n  * `train.csv`: Training set with bidder IDs and labels (human/robot).\n  * `test.csv`: Test set with bidder IDs (no labels).\n  * `bids.csv`: Detailed bid records (7.6M bids) including auction, device, time, and metadata.\n  * `sampleSubmission.csv`: Example submission file in the correct format.\n* **Key Features:**\n  * **Bidder Dataset:**\n    * `bidder_id`, `payment_account`, `address` (obfuscated for privacy), `outcome` (label: 1.0 for robot, 0.0 for human).\n  * **Bid Dataset:**\n    * `bid_id`, `bidder_id`, `auction`, `merchandise` (auction category), `device`, `time` (transformed for privacy), `country`, `ip` (obfuscated), `url` (referral URL, obfuscated).\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).\n* **Components of AUC-ROC:**\n  * Measures the model's ability to distinguish between human and robot bidders across all classification thresholds.\n  * Higher AUC values indicate better performance (1.0 = perfect separation,",
    "sections": {},
    "file_path": "kaggle_datasets/154/problem_summary.md"
  },
  "366": {
    "problem_id": "366",
    "title": "Emissions Factor Calculation Using Remote Sensing Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Emissions Factor Calculation Using Remote Sensing Data\n\n## Problem Description\n- **Problem Type**: Geospatial Regression with Time Series Analysis\n- **Objective**: Develop a methodology to calculate average historical emissions factors for electricity generation in Puerto Rico using remote sensing techniques, as an alternative to traditional data collection methods.\n- **Key Points**:\n  - Focus on sub-national region (Puerto Rico) with isolated energy system\n  - Calculate annual average historical grid-level electricity emissions factor (July 2018-July 2019)\n  - Bonus opportunities:\n    * Monthly emissions factor calculations\n    * Marginal emissions factor methodology\n  - Must provide scalability recommendations for other regions\n  - Requires analysis of conditions affecting emissions factor variability\n\n## Dataset Overview\n- **Data Type**: Multi-source geospatial and temporal data\n- **Context**: Remote sensing and atmospheric data for Puerto Rico\n- **Key Data Files**:\n  - Global Power Plant Database (WRI)\n  - Sentinel-5P NO2 emissions data (Copernicus)\n  - NOAA Global Forecast System atmospheric data\n  - NASA Global Land Data Assimilation System\n- **Features**:\n  - Power plant locations and attributes\n  - Nitrogen dioxide (NO2) concentration measurements\n  - Weather and atmospheric conditions\n  - Temporal coverage: July 2018 - July 2019\n\n## Evaluation Metrics\n- **Evaluation Criteria**:\n  - **Documentation** (30%):\n    * Code reproducibility and clarity\n    * Assumptions documentation\n    * Data visualization quality\n    * Proper dataset citation\n  - **Recommendation** (40%):\n    * Methodology scalability explanation\n    * Model pros/cons analysis\n    * Improvement over current methods\n    * Suggestions for future enhancements\n  - **Accuracy** (30%):\n    * Valid annual emissions factor calculation\n    * Bonus: Monthly calculations\n    * Bonus: Marginal emissions factor methodology",
    "sections": {},
    "file_path": "kaggle_datasets/366/problem_summary.md"
  },
  "36": {
    "problem_id": "36",
    "title": "Predicting Biological Response from Molecular Descriptors",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a molecule will elicit a specific biological response (1) or not (0) based on its chemical properties.",
    "evaluation_metric": null,
    "full_content": "# Predicting Biological Response from Molecular Descriptors\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a molecule will elicit a specific biological response (1) or not (0) based on its chemical properties.\n* **Key Points:**\n  * The task involves relating molecular information to actual biological activity.\n  * The challenge focuses on building optimal predictive models given the provided data constraints.\n  * Participants are restricted to using only the provided dataset (no external data allowed).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing molecular characteristics and their biological responses.\n* **Data Files:**\n  * train.csv (contains both features and target variable)\n  * test.csv (contains only features for prediction)\n  * svm_benchmark.csv (benchmark file)\n* **Features:**\n  * First column: Binary biological response (0/1 target variable)\n  * Remaining 1,775 columns: Molecular descriptors (d1-d1776) representing calculated chemical properties (e.g., size, shape, elemental constitution)\n  * All descriptor values have been normalized\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (Log Loss)\n* **Metric Components:**\n  * Calculated as: $-\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i})]$\n  * Where:\n    * $N$ = number of samples\n    * $y_i$ = ground truth (0 or 1)\n    * $\\hat{y_i}$ = predicted probability of positive class\n  * Penalizes both false positives and false negatives, with stronger penalties for confident wrong predictions",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a molecule will elicit a specific biological response (1) or not (0) based on its chemical properties.\n* **Key Points:**\n  * The task involves relating molecular information to actual biological activity.\n  * The challenge focuses on building optimal predictive models given the provided data constraints.\n  * Participants are restricted to using only the provided dataset (no external data allowed).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing molecular characteristics and their biological responses.\n* **Data Files:**\n  * train.csv (contains both features and target variable)\n  * test.csv (contains only features for prediction)\n  * svm_benchmark.csv (benchmark file)\n* **Features:**\n  * First column: Binary biological response (0/1 target variable)\n  * Remaining 1,775 columns: Molecular descriptors (d1-d1776) representing calculated chemical properties (e.g., size, shape, elemental constitution)\n  * All descriptor values have been normalized",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (Log Loss)\n* **Metric Components:**\n  * Calculated as: $-\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i})]$\n  * Where:\n    * $N$ = number of samples\n    * $y_i$ = ground truth (0 or 1)\n    * $\\hat{y_i}$ = predicted probability of positive class\n  * Penalizes both false positives and false negatives, with stronger penalties for confident wrong predictions"
    },
    "file_path": "kaggle_datasets/36/problem_summary.md"
  },
  "539": {
    "problem_id": "539",
    "title": "Regression with a Tabular Concrete Strength Dataset",
    "problem_type": "Regression",
    "objective": "Predict the compressive strength of concrete based on its compositional and age-related features. The goal is to minimize the error between predicted and actual strength values.",
    "evaluation_metric": null,
    "full_content": "# Regression with a Tabular Concrete Strength Dataset\n\n**Problem Description:**\n* **Problem Type:** Regression  \n* **Objective:** Predict the compressive strength of concrete based on its compositional and age-related features. The goal is to minimize the error between predicted and actual strength values.  \n* **Key Points:**  \n  * Dataset is synthetically generated from a real-world concrete strength prediction dataset, with similar but not identical feature distributions.  \n  * Participants may incorporate the original dataset to explore differences or improve model performance.  \n\n**Dataset Overview:**  \n* **Data Type & Context:** Tabular data representing concrete mixtures, with features like composition ratios (e.g., cement, water, additives) and curing time.  \n* **Data Files:**  \n  * `train.csv`: Contains features and target (`Strength`).  \n  * `test.csv`: Contains features for which predictions are required.  \n  * `sample_submission.csv`: Example submission format.  \n* **Features:** Likely include numerical attributes related to concrete mix proportions (e.g., cement, slag, ash content) and age (e.g., curing time).  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Root Mean Squared Error (RMSE).  \n* **Components:**  \n  * Calculated as the square root of the average squared differences between predicted (`ŷ_i`) and actual (`y_i`) values.  \n  * Formula:  \n    ```  \n    RMSE = sqrt(1/N * Σ(y_i - ŷ_i)^2)  \n    ```  \n  * Lower values indicate better model performance.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression  \n* **Objective:** Predict the compressive strength of concrete based on its compositional and age-related features. The goal is to minimize the error between predicted and actual strength values.  \n* **Key Points:**  \n  * Dataset is synthetically generated from a real-world concrete strength prediction dataset, with similar but not identical feature distributions.  \n  * Participants may incorporate the original dataset to explore differences or improve model performance.  \n\n**Dataset Overview:**  \n* **Data Type & Context:** Tabular data representing concrete mixtures, with features like composition ratios (e.g., cement, water, additives) and curing time.  \n* **Data Files:**  \n  * `train.csv`: Contains features and target (`Strength`).  \n  * `test.csv`: Contains features for which predictions are required.  \n  * `sample_submission.csv`: Example submission format.  \n* **Features:** Likely include numerical attributes related to concrete mix proportions (e.g., cement, slag, ash content) and age (e.g., curing time).  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Root Mean Squared Error (RMSE).  \n* **Components:**  \n  * Calculated as the square root of the average squared differences between predicted (`ŷ_i`) and actual (`y_i`) values.  \n  * Formula:  \n    ```  \n    RMSE = sqrt(1/N * Σ(y_i - ŷ_i)^2)  \n    ```  \n  * Lower values indicate better model performance."
    },
    "file_path": "kaggle_datasets/539/problem_summary.md"
  },
  "196": {
    "problem_id": "196",
    "title": "Satellite Image Chronology Prediction",
    "problem_type": "Computer Vision - Image Sequence Ordering",
    "objective": "Predict the chronological order of satellite images taken at the same location over 5 consecutive days. Participants must analyze subtle changes across images (e.g., moving vehicles, water levels, shadows) to determine the correct sequence.",
    "evaluation_metric": null,
    "full_content": "# Satellite Image Chronology Prediction\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Image Sequence Ordering\n* **Objective:** Predict the chronological order of satellite images taken at the same location over 5 consecutive days. Participants must analyze subtle changes across images (e.g., moving vehicles, water levels, shadows) to determine the correct sequence.\n* **Key Points:**\n  * Images within each set (group of 5) cover approximately the same area but are not perfectly aligned.\n  * Test set images are deliberately scrambled; training set provides correct ordering.\n  * Both manual and computer-based analysis are permitted due to task difficulty.\n  * Intended applications include environmental monitoring, disaster prediction, and humanitarian efforts.\n\n**Dataset Overview:**\n* **Data Type & Context:** High-resolution aerial photographs (simulating satellite imagery) of southern California, grouped into sets of 5 images per location across different days.\n* **Data Files:**\n  * `train/`, `train_sm/`: Training images in TIFF (lossless) and JPEG (compressed) formats\n  * `test/`, `test_sm/`: Test images in same dual formats\n  * `sample_submission.csv`: Submission template\n* **Features:**\n  * Images named as `setId_day` (e.g., `set1_3` = set 1, day 3)\n  * Each set contains subtle temporal changes (e.g., vehicle movement, water/shadow variations)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Spearman's rank correlation coefficient\n  * Calculated per image set (5 images) by comparing predicted vs. true day rankings\n  * Identical predictions receive fractional ranks (average of their positions)\n  * Final score averages correlations across all test sets",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Image Sequence Ordering\n* **Objective:** Predict the chronological order of satellite images taken at the same location over 5 consecutive days. Participants must analyze subtle changes across images (e.g., moving vehicles, water levels, shadows) to determine the correct sequence.\n* **Key Points:**\n  * Images within each set (group of 5) cover approximately the same area but are not perfectly aligned.\n  * Test set images are deliberately scrambled; training set provides correct ordering.\n  * Both manual and computer-based analysis are permitted due to task difficulty.\n  * Intended applications include environmental monitoring, disaster prediction, and humanitarian efforts.",
      "Dataset Overview": "* **Data Type & Context:** High-resolution aerial photographs (simulating satellite imagery) of southern California, grouped into sets of 5 images per location across different days.\n* **Data Files:**\n  * `train/`, `train_sm/`: Training images in TIFF (lossless) and JPEG (compressed) formats\n  * `test/`, `test_sm/`: Test images in same dual formats\n  * `sample_submission.csv`: Submission template\n* **Features:**\n  * Images named as `setId_day` (e.g., `set1_3` = set 1, day 3)\n  * Each set contains subtle temporal changes (e.g., vehicle movement, water/shadow variations)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Spearman's rank correlation coefficient\n  * Calculated per image set (5 images) by comparing predicted vs. true day rankings\n  * Identical predictions receive fractional ranks (average of their positions)\n  * Final score averages correlations across all test sets"
    },
    "file_path": "kaggle_datasets/196/problem_summary.md"
  },
  "162": {
    "problem_id": "162",
    "title": "Predicting Closed Questions on Stack Overflow",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Closed Questions on Stack Overflow\n\n## Problem Description\n* **Problem Type**: Multi-class Classification  \n* **Objective**: Predict whether a new question on Stack Overflow will be closed, and if so, the reason for closure. The target classes are:  \n  - `open` (not closed)  \n  - `not a real question`  \n  - `not constructive`  \n  - `off topic`  \n  - `too localized`  \n* **Key Points**:  \n  - Excludes \"exact duplicate\" closure reason (relies on historical data).  \n  - Requires probabilistic predictions for all classes (summing to 1).  \n  - Uses metadata about the question and user at creation time.  \n\n## Dataset Overview\n* **Data Type**: Tabular + Text (Stack Overflow questions and metadata)  \n* **Data Files**:  \n  - `train.csv`: Full training set (questions, metadata, and closure status).  \n  - `train-sample.csv`: Stratified sample (balanced closed/open questions).  \n  - `test.csv` / `public_leaderboard.csv`: Test data (no closure labels).  \n  - Additional 6GB Stack Overflow data dump (optional, for feature engineering).  \n* **Key Features**:  \n  - Text: `Title`, `BodyMarkdown` (Markdown-formatted question content).  \n  - Metadata: `PostCreationDate`, `OwnerUserId`, `ReputationAtPostCreation`, `Tags` (1-5 tags per question).  \n  - Target: `OpenStatus` (multi-class label).  \n\n## Evaluation Metrics\n* **Primary Metric**: Multiclass Logarithmic Loss (LogLoss)  \n* **Components**:  \n  - Predictions must be probabilities for all 5 classes.  \n  - Each row’s probabilities must sum to 1 (normalized if not provided).  \n  - Penalizes confident incorrect predictions more heavily.  \n  - Formula:  \n    ```\n    LogLoss = -(1/N) Σ Σ y_ij * log(p_ij)\n    ```\n    Where `y_ij` is 1 if observation `i` is class `j`, and `p_ij` is the predicted probability.",
    "sections": {},
    "file_path": "kaggle_datasets/162/problem_summary.md"
  },
  "350": {
    "problem_id": "350",
    "title": "2019 Kaggle Machine Learning & Data Science Survey Analysis Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 2019 Kaggle Machine Learning & Data Science Survey Analysis Challenge\n\n## Problem Description\n* **Problem Type**: Exploratory Data Analysis (EDA) & Data Storytelling\n* **Objective**:  \n    * Participants must create a compelling data-driven narrative about a specific subset of the data science community represented in the 2019 Kaggle survey.  \n    * The goal is to deeply explore the impact, priorities, or concerns of a defined group (e.g., Python users, female students in ML, etc.) using a combination of narrative text and data exploration.  \n* **Key Points**:  \n    * Submissions must tell a **story**—not just present analysis—with a clear narrative supported by data.  \n    * Focus on originality: revealing new insights or challenging existing perspectives.  \n    * The target group can be broad (e.g., Python coders) or niche (e.g., female master’s students in ML).  \n\n## Dataset Overview\n* **Data Type**: Tabular survey data (anonymized responses from 19,717 data science practitioners across 171 countries).  \n* **Context**: Covers demographics, tools, workflows, education, and industry trends in ML/data science.  \n* **Data Files**:  \n    * `multiple_choice_responses.csv`: Primary dataset with single/multiple-choice responses.  \n    * `other_text_responses.csv`: Open-ended text responses (randomized for privacy).  \n    * `questions_only.csv`: Survey questions.  \n    * `survey_schema.csv`: Survey structure (question segments and logic).  \n* **Key Features**:  \n    * Respondent demographics (country, role, experience).  \n    * Tool usage (programming languages, IDEs, ML frameworks).  \n    * Industry-specific trends (company size, ML adoption).  \n    * Free-text responses (encoded for privacy).  \n\n## Evaluation Metrics\nSubmissions are judged holistically on:  \n* **Composition**:  \n    * Clarity and coherence of the narrative.  \n    * Support from data visualizations and analysis.  \n* **Originality**:  \n    * Novelty of insights or perspective.  \n    * Thought-provoking conclusions.  \n* **Documentation**:  \n    * Code reproducibility and clarity.  \n    * Proper citation of external data sources (if used).  \n\n*Note*: No quantitative metric (e.g., AUC) is used; evaluation is qualitative based on the judging criteria above.",
    "sections": {},
    "file_path": "kaggle_datasets/350/problem_summary.md"
  },
  "506": {
    "problem_id": "506",
    "title": "Binary Classification for Product Failure Prediction",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of product failures for a fictional company's absorbent product (\"Super Soaker\") based on lab testing data. The goal is to help the company improve its product prototypes by identifying potential failures.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification for Product Failure Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of product failures for a fictional company's absorbent product (\"Super Soaker\") based on lab testing data. The goal is to help the company improve its product prototypes by identifying potential failures.\n    * **Key Points:**\n        * Focuses on predicting individual product failures in simulated real-world environments.\n        * Uses a combination of fixed product attributes and variable lab measurement values.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing results of a large product testing study for absorbent products.\n    * **Data Files:**\n        * `train.csv` - Contains target variable `failure`\n        * `test.csv` - Requires prediction of failure probabilities\n        * `sample_submission.csv` - Example submission format\n    * **Key Features:**\n        * Product codes with fixed attributes\n        * Multiple measurement values from lab testing\n        * `loading` feature representing absorbed fluid amount\n        * Binary `failure` target variable\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between failing and non-failing products\n        * Evaluates predicted probabilities against observed binary outcomes\n        * Higher values indicate better classification performance (1.0 = perfect prediction)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of product failures for a fictional company's absorbent product (\"Super Soaker\") based on lab testing data. The goal is to help the company improve its product prototypes by identifying potential failures.\n    * **Key Points:**\n        * Focuses on predicting individual product failures in simulated real-world environments.\n        * Uses a combination of fixed product attributes and variable lab measurement values.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing results of a large product testing study for absorbent products.\n    * **Data Files:**\n        * `train.csv` - Contains target variable `failure`\n        * `test.csv` - Requires prediction of failure probabilities\n        * `sample_submission.csv` - Example submission format\n    * **Key Features:**\n        * Product codes with fixed attributes\n        * Multiple measurement values from lab testing\n        * `loading` feature representing absorbed fluid amount\n        * Binary `failure` target variable",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between failing and non-failing products\n        * Evaluates predicted probabilities against observed binary outcomes\n        * Higher values indicate better classification performance (1.0 = perfect prediction)"
    },
    "file_path": "kaggle_datasets/506/problem_summary.md"
  },
  "368": {
    "problem_id": "368",
    "title": "COVID-19 Global Forecasting (Week 1)",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# COVID-19 Global Forecasting (Week 1)\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting\n* **Objective**: Predict the cumulative number of confirmed COVID-19 cases and fatalities for future dates across global regions. The primary goal is not just accurate forecasting but identifying factors impacting COVID-19 transmission rates.\n* **Key Points**:\n  * Focus on understanding transmission-influencing variables (e.g., policy interventions, environmental factors).\n  * Participants are encouraged to incorporate external datasets (e.g., temperature, policy actions) to enrich analysis.\n  * Two evaluation periods: Public Leaderboard (03/12/2020–03/25/2020) and Private Leaderboard (03/26/2020–04/23/2020).\n\n## Dataset Overview\n* **Data Type**: Tabular time-series data (daily cumulative counts).\n* **Context**: Johns Hopkins University CSSE’s global COVID-19 case and fatality records, updated daily.\n* **Data Files**:\n  * `train.csv`: Training data up to 03/18/2020.\n  * `test.csv`: Dates/locations for prediction (includes overlap with training for Public Leaderboard).\n  * `submission.csv`: Sample submission format (cumulative predictions).\n* **Features**:\n  * Region identifiers (e.g., country/state).\n  * Date and cumulative counts of confirmed cases and fatalities.\n\n## Evaluation Metrics\n* **Evaluation Metric**: Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n* **Components**:\n  * RMSLE per target column (ConfirmedCases, Fatalities):\n    $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2}$$\n    where \\(p_i\\) = predicted value, \\(a_i\\) = actual value, \\(n\\) = observations.\n  * Final score: Mean of RMSLE across both columns.\n* **Note**: Predictions must be cumulative values.",
    "sections": {},
    "file_path": "kaggle_datasets/368/problem_summary.md"
  },
  "31": {
    "problem_id": "31",
    "title": "Hourly Air Quality Prediction for Early Warning Systems",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Hourly Air Quality Prediction for Early Warning Systems\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting (Multi-target Regression)\n* **Objective**: Build predictive models to forecast dangerous levels of air pollutants on an hourly basis, creating an early warning system for respiratory disease sufferers. The competition specifically requires:\n  * Predicting 39 normalized target variables representing various air quality measurements\n  * Making forecasts at multiple future time points (1, 2, 3, 4, 5, 10, 17, 24, 48, and 72 hours ahead)\n  * Handling missing data points in both training and evaluation sets\n* **Key Points**:\n  * Data comes in 11-day chunks (8 days training + 3 days prediction)\n  * Predictions must account for site-specific measurements (multiple monitoring locations)\n  * External data use was restricted (required approval)\n  * Time series reconstruction was prohibited (prevented reverse-engineering actual dates)\n\n## Dataset Overview\n* **Data Type**: Time series tabular data with spatial components (multiple monitoring sites)\n* **Context**: Hourly air quality measurements from Cook County, Illinois with meteorological data\n* **Data Files**:\n  * TrainingData.csv (main training set with features and targets)\n  * SiteLocations.csv (latitude/longitude for monitoring sites)\n  * SubmissionZerosExceptNAs.csv (sample submission format)\n* **Key Features**:\n  * Temporal features: hour, weekday, month_most_common\n  * Meteorological measurements: solar radiation, wind speed/direction, temperature, barometric pressure\n  * Site-specific variants of measurements (39 target variables across multiple sites)\n  * Normalized target variables (variance scaled to ~1)\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE)\n* **Scoring Details**:\n  * Missing values were replaced with -1,000,000 in evaluation\n  * Only non-missing values were scored (entries with -1,000,000 ignored)\n  * Multi-target evaluation (39 variables simultaneously)\n  * Predictions required at multiple forecast horizons (1-72 hours)",
    "sections": {},
    "file_path": "kaggle_datasets/31/problem_summary.md"
  },
  "501": {
    "problem_id": "501",
    "title": "Multi-Agent Strategy Game for Kore Mineral Mining",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Agent Strategy Game for Kore Mineral Mining\n\n## Problem Description\n* **Problem Type**: Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective**: \n    * Develop an agent to control a fleet of spaceships competing against opponents to mine \"kore\" mineral.\n    * The goal is to maximize collected kore by turn 400 or eliminate all opponent fleets.\n* **Key Points**:\n    * Turn-based simulation with simultaneous moves\n    * Requires strategic decision-making for:\n        * Fleet movement and mining\n        * Shipyard management and ship production\n        * Resource allocation (kore spending)\n        * Combat tactics against opponents\n    * Complex game mechanics including:\n        * Flight plan navigation\n        * Fleet collisions and combat\n        * Shipyard conversion and control\n        * Kore mining and regeneration dynamics\n\n## Dataset Overview\n* **Data Type**: Game state observations (simulation environment)\n* **Context**: \n    * 21x21 grid world with wrap-around boundaries\n    * Contains kore mineral deposits and player entities\n* **Data Files**:\n    * Python starter agent (main.py)\n* **Important Features**:\n    * Complete game state provided each turn including:\n        * Board kore distribution\n        * All fleet positions, sizes, and flight plans\n        * Shipyard statuses\n        * Player kore reserves\n    * Observation space includes all game entities and their attributes\n\n## Evaluation Metrics\n* **Evaluation Metric**: Skill Rating System (Gaussian N(μ,σ²))\n    * μ represents estimated skill\n    * σ represents rating uncertainty\n* **Components**:\n    * Agents compete in head-to-head matches\n    * Rating updates based on match outcomes:\n        * Winner's μ increases, loser's μ decreases\n        * Draws move both agents' μ toward their mean\n    * Update magnitude depends on:\n        * Deviation from expected result\n        * Current uncertainty (σ)\n    * σ decreases with more matches played\n    * Note: Win/loss margin doesn't affect rating changes",
    "sections": {},
    "file_path": "kaggle_datasets/501/problem_summary.md"
  },
  "357": {
    "problem_id": "357",
    "title": "Santa's Workshop Tour Scheduling Optimization",
    "problem_type": "Combinatorial Optimization with Constraints",
    "objective": "Optimize the assignment of 5,000 families to workshop tour dates (100 days before Christmas) to minimize Santa's total penalty costs while satisfying strict occupancy constraints.",
    "evaluation_metric": null,
    "full_content": "# Santa's Workshop Tour Scheduling Optimization\n\n**Problem Description:**\n* **Problem Type:** Combinatorial Optimization with Constraints\n* **Objective:** Optimize the assignment of 5,000 families to workshop tour dates (100 days before Christmas) to minimize Santa's total penalty costs while satisfying strict occupancy constraints.\n    * **Key Points:**\n        * Each family provides 10 ranked date preferences (choice_0 = best, choice_9 = worst).\n        * Daily occupancy must be between 125-300 people (hard constraint).\n        * Two penalty components:\n            * Preference costs: Consolation gifts based on how far assigned date is from family's preferences (exact monetary values specified per preference level).\n            * Accounting penalty: Complex empirical formula based on daily occupancy fluctuations and absolute values.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with family preferences and sizes\n* **Data Files:**\n    * `family_data.csv`: Contains family ID, number of members (`n_people`), and 10 date preference columns (`choice_0` to `choice_9`).\n    * `sample_submission.csv`: Example submission file with family IDs and assigned_day column.\n* **Key Features:**\n    * Family size (1-8 members)\n    * 10 ranked date preferences per family (values 1-100 representing days before Christmas)\n    * Date assignment must be one of the family's listed preferences or another date (with highest penalty)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Total Penalty Score = Preference Cost + Accounting Penalty\n    * **Preference Cost Components:**\n        * Exact monetary values assigned to each preference level (choice_0 = $0, choice_1 = $50, up to $500 + additional perks for worse choices)\n        * Includes gift cards, buffet discounts, and helicopter ride discounts\n    * **Accounting Penalty Formula:**\n        * Complex daily calculation based on:\n            * Current day's occupancy (N_d)\n            * Previous day's occupancy (N_{d+1})\n            * Formula: ∑[(N_d -125)/400 * N_d^(1/2 + |N_d - N_{d+1}|/50)] for d=100 to 1\n            * Initial condition: N_101 = N_100",
    "sections": {
      "Problem Description": "* **Problem Type:** Combinatorial Optimization with Constraints\n* **Objective:** Optimize the assignment of 5,000 families to workshop tour dates (100 days before Christmas) to minimize Santa's total penalty costs while satisfying strict occupancy constraints.\n    * **Key Points:**\n        * Each family provides 10 ranked date preferences (choice_0 = best, choice_9 = worst).\n        * Daily occupancy must be between 125-300 people (hard constraint).\n        * Two penalty components:\n            * Preference costs: Consolation gifts based on how far assigned date is from family's preferences (exact monetary values specified per preference level).\n            * Accounting penalty: Complex empirical formula based on daily occupancy fluctuations and absolute values.",
      "Dataset Overview": "* **Data Type:** Tabular data with family preferences and sizes\n* **Data Files:**\n    * `family_data.csv`: Contains family ID, number of members (`n_people`), and 10 date preference columns (`choice_0` to `choice_9`).\n    * `sample_submission.csv`: Example submission file with family IDs and assigned_day column.\n* **Key Features:**\n    * Family size (1-8 members)\n    * 10 ranked date preferences per family (values 1-100 representing days before Christmas)\n    * Date assignment must be one of the family's listed preferences or another date (with highest penalty)",
      "Evaluation Metrics": "* **Primary Metric:** Total Penalty Score = Preference Cost + Accounting Penalty\n    * **Preference Cost Components:**\n        * Exact monetary values assigned to each preference level (choice_0 = $0, choice_1 = $50, up to $500 + additional perks for worse choices)\n        * Includes gift cards, buffet discounts, and helicopter ride discounts\n    * **Accounting Penalty Formula:**\n        * Complex daily calculation based on:\n            * Current day's occupancy (N_d)\n            * Previous day's occupancy (N_{d+1})\n            * Formula: ∑[(N_d -125)/400 * N_d^(1/2 + |N_d - N_{d+1}|/50)] for d=100 to 1\n            * Initial condition: N_101 = N_100"
    },
    "file_path": "kaggle_datasets/357/problem_summary.md"
  },
  "165": {
    "problem_id": "165",
    "title": "EEG-Based Hand Motion Detection in Grasp-and-Lift Tasks",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# EEG-Based Hand Motion Detection in Grasp-and-Lift Tasks\n\n## Problem Description\n* **Problem Type:** Multiclass Classification (Time Series)\n* **Objective:** Detect six specific hand motion events from EEG recordings during grasp-and-lift (GAL) trials. The goal is to identify when each event occurs within ±150ms (±75 frames) of the actual event time.\n* **Key Points:**\n  * Events must be detected in sequence: HandStart → FirstDigitTouch → BothStartLoadPhase → LiftOff → Replace → BothReleased\n  * Solution must work in real-time (no future data leakage allowed)\n  * Applications in brain-computer interfaces for patients with neurological disabilities\n  * Challenge involves interpreting complex EEG signals correlated with hand motions\n\n## Dataset Overview\n* **Data Type & Context:** Time series EEG data recorded from 32 electrode channels (500Hz sampling rate) during controlled grasp-and-lift experiments\n* **Data Files:**\n  * Training set: \n    - `*_data.csv` (raw EEG data for each subject/series)\n    - `*_events.csv` (ground truth labels for each event)\n  * Test set: EEG data without labels (must predict all 6 events)\n* **Key Features:**\n  * 32 EEG channels with spatial relationships (electrode positions provided)\n  * Multiple subjects (12 total) with 10 series each (~30 trials per series)\n  * Training uses first 8 series per subject; test uses last 2 series\n\n## Evaluation Metrics\n* **Primary Metric:** Mean column-wise Area Under ROC Curve (AUC)\n  * Calculated as the mean of individual AUC scores for each of the 6 event columns\n  * Requires calibrated probabilities that are comparable across subjects/series\n* **Important Constraints:**\n  * No future data can be used for predictions (strict real-time requirement)\n  * Features must be calculated only from past/present data points\n  * Models will be checked for data leakage violations",
    "sections": {},
    "file_path": "kaggle_datasets/165/problem_summary.md"
  },
  "191": {
    "problem_id": "191",
    "title": "Predicting Search Relevance for Home Depot Products",
    "problem_type": "Regression (with constrained output range).",
    "objective": "Predict the relevance score (1-3) of product search results on Home Depot's website, where:",
    "evaluation_metric": null,
    "full_content": "# Predicting Search Relevance for Home Depot Products\n\n**Problem Description:**\n* **Problem Type:** Regression (with constrained output range).\n* **Objective:** Predict the relevance score (1-3) of product search results on Home Depot's website, where:\n  * 1 = Not relevant\n  * 2 = Mildly relevant  \n  * 3 = Highly relevant\n* **Key Points:**\n  * Goal is to automate relevance scoring to improve search algorithm iterations.\n  * Human raters previously evaluated relevance with access to product images (not provided in competition data).\n  * Test set contains both seen and unseen search terms.\n\n**Dataset Overview:**\n* **Data Type:** Textual data (search queries, product titles/descriptions) + Tabular attributes.\n* **Data Files:**\n  * `train.csv`: Search term-product pairs with relevance scores (average of ≥3 human ratings).\n  * `test.csv`: Search term-product pairs requiring relevance prediction.  \n  * `product_descriptions.csv`: Text descriptions of products.\n  * `attributes.csv`: Technical specifications for a subset of products.\n  * `relevance_instructions.docx`: Human rater guidelines.\n* **Key Features:**\n  * `search_term`: Customer query text.\n  * `product_title`/`product_description`: Product metadata.\n  * `name`/`value` (in attributes): Product specifications (e.g., material, dimensions).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE).\n* **Output Constraints:** Predictions must be real numbers in [1,3] (matching human rating scale).",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (with constrained output range).\n* **Objective:** Predict the relevance score (1-3) of product search results on Home Depot's website, where:\n  * 1 = Not relevant\n  * 2 = Mildly relevant  \n  * 3 = Highly relevant\n* **Key Points:**\n  * Goal is to automate relevance scoring to improve search algorithm iterations.\n  * Human raters previously evaluated relevance with access to product images (not provided in competition data).\n  * Test set contains both seen and unseen search terms.",
      "Dataset Overview": "* **Data Type:** Textual data (search queries, product titles/descriptions) + Tabular attributes.\n* **Data Files:**\n  * `train.csv`: Search term-product pairs with relevance scores (average of ≥3 human ratings).\n  * `test.csv`: Search term-product pairs requiring relevance prediction.  \n  * `product_descriptions.csv`: Text descriptions of products.\n  * `attributes.csv`: Technical specifications for a subset of products.\n  * `relevance_instructions.docx`: Human rater guidelines.\n* **Key Features:**\n  * `search_term`: Customer query text.\n  * `product_title`/`product_description`: Product metadata.\n  * `name`/`value` (in attributes): Product specifications (e.g., material, dimensions).",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE).\n* **Output Constraints:** Predictions must be real numbers in [1,3] (matching human rating scale)."
    },
    "file_path": "kaggle_datasets/191/problem_summary.md"
  },
  "555": {
    "problem_id": "555",
    "title": "Parkinson's Freezing of Gait Detection from Wearable Sensor Data",
    "problem_type": "Multiclass Classification (Event Detection in Time Series)",
    "objective": "Detect freezing of gait (FOG) episodes in Parkinson's disease patients using 3D accelerometer data from a lower-back wearable sensor. Specifically, predict the occurrence of three FOG event types:",
    "evaluation_metric": null,
    "full_content": "# Parkinson's Freezing of Gait Detection from Wearable Sensor Data\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification (Event Detection in Time Series)\n* **Objective:** Detect freezing of gait (FOG) episodes in Parkinson's disease patients using 3D accelerometer data from a lower-back wearable sensor. Specifically, predict the occurrence of three FOG event types:\n  * `StartHesitation`\n  * `Turn` \n  * `Walking`\n* **Key Points:**\n  * Focus on precise detection of FOG start/stop times and event types\n  * Data collected under different conditions (lab vs. home environment)\n  * Includes both annotated (FOG-labeled) and unannotated continuous monitoring data\n  * Must handle varying sampling rates (128Hz vs. 100Hz) across datasets\n\n**Dataset Overview:**\n* **Data Type:** Time series accelerometer data (3D: vertical, mediolateral, anteroposterior)\n* **Context:** Collected from Parkinson's patients during FOG-provoking protocols and daily living\n* **Data Files:**\n  * `train/`: Contains three sub-datasets:\n    * `tdcsfog/` - Lab-collected data (128Hz)\n    * `defog/` - Home-collected data (100Hz) \n    * `notype/` - Home data without event-type labels\n  * `test/`: Hidden test set with same structure\n  * `unlabeled/`: Continuous daily monitoring data\n* **Key Features:**\n  * Time-series acceleration data (`AccV`, `AccML`, `AccAP`)\n  * Event labels (`StartHesitation`, `Turn`, `Walking`)\n  * Metadata including subject characteristics, medication status, and task protocols\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision (mAP)\n  * Computes average precision separately for each event class\n  * Takes average of the three class-wise scores\n* **Key Considerations:**\n  * Only evaluates predictions on annotated portions of series (where `Valid=True` and `Task=True`)\n  * Predictions on unannotated segments are ignored\n  * Submission requires confidence scores for all three event types per timestep",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification (Event Detection in Time Series)\n* **Objective:** Detect freezing of gait (FOG) episodes in Parkinson's disease patients using 3D accelerometer data from a lower-back wearable sensor. Specifically, predict the occurrence of three FOG event types:\n  * `StartHesitation`\n  * `Turn` \n  * `Walking`\n* **Key Points:**\n  * Focus on precise detection of FOG start/stop times and event types\n  * Data collected under different conditions (lab vs. home environment)\n  * Includes both annotated (FOG-labeled) and unannotated continuous monitoring data\n  * Must handle varying sampling rates (128Hz vs. 100Hz) across datasets",
      "Dataset Overview": "* **Data Type:** Time series accelerometer data (3D: vertical, mediolateral, anteroposterior)\n* **Context:** Collected from Parkinson's patients during FOG-provoking protocols and daily living\n* **Data Files:**\n  * `train/`: Contains three sub-datasets:\n    * `tdcsfog/` - Lab-collected data (128Hz)\n    * `defog/` - Home-collected data (100Hz) \n    * `notype/` - Home data without event-type labels\n  * `test/`: Hidden test set with same structure\n  * `unlabeled/`: Continuous daily monitoring data\n* **Key Features:**\n  * Time-series acceleration data (`AccV`, `AccML`, `AccAP`)\n  * Event labels (`StartHesitation`, `Turn`, `Walking`)\n  * Metadata including subject characteristics, medication status, and task protocols",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision (mAP)\n  * Computes average precision separately for each event class\n  * Takes average of the three class-wise scores\n* **Key Considerations:**\n  * Only evaluates predictions on annotated portions of series (where `Valid=True` and `Task=True`)\n  * Predictions on unannotated segments are ignored\n  * Submission requires confidence scores for all three event types per timestep"
    },
    "file_path": "kaggle_datasets/555/problem_summary.md"
  },
  "131": {
    "problem_id": "131",
    "title": "Multi-label Text Classification for Document Text Blocks",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Text Classification for Document Text Blocks\n\n## Problem Description\n* **Problem Type:** Multi-label Classification (Text Data)\n* **Objective:** Predict the probability that a given text block in a document belongs to each of 33 possible classes. The task involves analyzing text blocks with extracted features to determine multiple possible label assignments per sample.\n    * **Key Points:**\n        * Text blocks may belong to multiple labels simultaneously (multi-label problem)\n        * Documents have varying formats and may contain multiple languages\n        * Some documents are OCR-processed, introducing potential noise\n        * Features include content, parsing, spatial, and relational information\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing text blocks extracted from documents (~2.1M samples)\n    * **Data Files:**\n        * `train.csv`: Features for training set (145 features per sample)\n        * `trainLabels.csv`: Multi-label assignments for training set (33 possible labels)\n        * `test.csv`: Features for test set\n        * `sampleSubmission.csv`: Example submission format\n    * **Key Features:**\n        * Content: Cryptographic hash of raw text\n        * Parsing: Text type indicators (number, text, alphanumeric)\n        * Spatial: Box position/size information\n        * Relational: Information about surrounding text blocks\n\n## Evaluation Metrics\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n    * **Components:**\n        * Calculated as average negative log likelihood across all test samples and labels\n        * Formula: \n          ```\n          LogLoss = -1/(N*K) * Σ[y_ij*log(ŷ_ij) + (1-y_ij)*log(1-ŷ_ij)]\n          ```\n          where N = samples, K = labels\n        * Heavily penalizes confident wrong predictions\n        * Predictions bounded to [10^-15, 1-10^-15] to avoid infinite values\n        * Symmetric - equal penalty for false positives/negatives at same confidence levels",
    "sections": {},
    "file_path": "kaggle_datasets/131/problem_summary.md"
  },
  "303": {
    "problem_id": "303",
    "title": "Quora Insincere Questions Classification",
    "problem_type": "Binary Classification (NLP - Text Classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Quora Insincere Questions Classification\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (NLP - Text Classification)\n* **Objective:**  \n  Detect insincere questions on Quora, defined as questions intended to make a statement or spread toxicity rather than seek genuine answers. The goal is to classify each question as insincere (`1`) or sincere (`0`).\n* **Key Points:**\n  * Focus on identifying toxic, inflammatory, or misleading content.\n  * Insincere questions may exhibit:\n    * Non-neutral or exaggerated tone\n    * Disparaging/inflammatory language\n    * False premises or absurd assumptions\n    * Shock-value sexual content\n  * Labels may contain noise (imperfect ground truth).\n\n**Dataset Overview:**\n* **Data Type:** Text data (Quora questions) with binary labels.\n* **Data Files:**\n  * `train.csv`: Contains `qid`, `question_text`, and `target` (label).\n  * `test.csv`: Contains `qid` and `question_text` for prediction.\n  * `sample_submission.csv`: Submission template.\n  * `embeddings/`: Pre-trained word embeddings (Google News, GloVe, etc.).\n* **Features:**\n  * Primary feature: `question_text` (raw text of the question).\n  * Labels: `target` (0 or 1).\n\n**Evaluation Metrics:**\n* **Primary Metric:** F1-Score  \n  * Harmonic mean of precision and recall, calculated as:  \n    \\[\n    F1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n    \\]\n  * Focuses on balancing false positives and false negatives.\n  * Submissions must predict binary labels (`0` or `1`) for each question.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (NLP - Text Classification)\n* **Objective:**  \n  Detect insincere questions on Quora, defined as questions intended to make a statement or spread toxicity rather than seek genuine answers. The goal is to classify each question as insincere (`1`) or sincere (`0`).\n* **Key Points:**\n  * Focus on identifying toxic, inflammatory, or misleading content.\n  * Insincere questions may exhibit:\n    * Non-neutral or exaggerated tone\n    * Disparaging/inflammatory language\n    * False premises or absurd assumptions\n    * Shock-value sexual content\n  * Labels may contain noise (imperfect ground truth).",
      "Dataset Overview": "* **Data Type:** Text data (Quora questions) with binary labels.\n* **Data Files:**\n  * `train.csv`: Contains `qid`, `question_text`, and `target` (label).\n  * `test.csv`: Contains `qid` and `question_text` for prediction.\n  * `sample_submission.csv`: Submission template.\n  * `embeddings/`: Pre-trained word embeddings (Google News, GloVe, etc.).\n* **Features:**\n  * Primary feature: `question_text` (raw text of the question).\n  * Labels: `target` (0 or 1).",
      "Evaluation Metrics": "* **Primary Metric:** F1-Score  \n  * Harmonic mean of precision and recall, calculated as:  \n    \\[\n    F1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n    \\]\n  * Focuses on balancing false positives and false negatives.\n  * Submissions must predict binary labels (`0` or `1`) for each question."
    },
    "file_path": "kaggle_datasets/303/problem_summary.md"
  },
  "91": {
    "problem_id": "91",
    "title": "Appliance-Level Energy Disaggregation Using EMI Signatures",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Appliance-Level Energy Disaggregation Using EMI Signatures\n\n## Problem Description\n* **Problem Type**: Multi-label Classification (Time Series)\n* **Objective**: \n    * Disaggregate household energy consumption into individual appliances using Electromagnetic Interference (EMI) signatures\n    * Predict when specific appliances are active based on high-frequency noise patterns and power measurements\n* **Key Points**:\n    * Must account for appliance signature drift/variation due to operating conditions\n    * Should incorporate usage patterns (temporal context like time of day)\n    * Needs to handle complex scenarios where multiple appliances operate simultaneously\n    * EMI signatures are complex-valued frequency domain data\n\n## Dataset Overview\n* **Data Type**: Time series EMI spectrograms + power measurements\n* **Context**: Residential energy monitoring data from 4 homes (H1-H4)\n* **Data Files**:\n    * `AllTaggingInfo.mat/csv` (appliance ON/OFF labels)\n    * `Tagged_Training_*.mat/csv` (labeled training data)\n    * `Testing_*.mat/csv` (unlabeled test data)\n* **Key Features**:\n    * `Buffer.HF`: 4096xN spectrogram of high-frequency noise (primary EMI signature)\n    * `Buffer.LF1V/LF1I`: Fundamental + 5 harmonics of 60Hz voltage/current (Phase 1)\n    * `Buffer.LF2V/LF2I`: Fundamental + 5 harmonics of 60Hz voltage/current (Phase 2)\n    * UNIX timestamps for all measurements\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Hamming Loss (multi-label classification)\n    * Formula: \n        ```\n        HammingLoss(x_i,y_i) = 1/|D| * Σ_i (xor(x_i,y_i)/|L|)\n        ```\n        Where:\n        * |D| = number of samples\n        * |L| = number of labels (appliances)\n        * y_i = ground truth\n        * x_i = prediction\n    * Final score averages Hamming Loss across all 4 houses",
    "sections": {},
    "file_path": "kaggle_datasets/91/problem_summary.md"
  },
  "65": {
    "problem_id": "65",
    "title": "Binary Classification for Email Spam Detection",
    "problem_type": "Binary Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Binary Classification for Email Spam Detection\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:**  \n  Participants are tasked with building a spam detector that can classify emails as either spam (1) or normal (0) based on 100 extracted features from a corpus of emails.\n* **Key Points:**\n  * This was an extended version of a tutorial competition (\"Just the Basics\") from the Strata 2013 conference.\n  * The competition provided additional time for participants to experiment with different methods beyond the original tutorial timeframe.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  Tabular data containing 100 anonymized features extracted from email content (600 training emails, 4000 test emails).\n* **Data Files:**\n  * `train.csv`: 600 emails × 100 features (training set)\n  * `train_labels.csv`: Corresponding labels (1=spam, 0=normal) for training emails\n  * `test.csv`: 4000 emails × 100 features (test set for predictions)\n* **Features:**  \n  100 anonymized numerical features representing extracted email characteristics (specific feature details not provided in description).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n* **Metric Details:**\n  * Measures the classifier's ability to distinguish between spam and normal emails across all classification thresholds\n  * Implementation examples provided for:\n    * MATLAB (`perfcurve` function)\n    * R (`ROCR` package)\n    * Python (`scikit-learn` metrics module)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:**  \n  Participants are tasked with building a spam detector that can classify emails as either spam (1) or normal (0) based on 100 extracted features from a corpus of emails.\n* **Key Points:**\n  * This was an extended version of a tutorial competition (\"Just the Basics\") from the Strata 2013 conference.\n  * The competition provided additional time for participants to experiment with different methods beyond the original tutorial timeframe.",
      "Dataset Overview": "* **Data Type & Context:**  \n  Tabular data containing 100 anonymized features extracted from email content (600 training emails, 4000 test emails).\n* **Data Files:**\n  * `train.csv`: 600 emails × 100 features (training set)\n  * `train_labels.csv`: Corresponding labels (1=spam, 0=normal) for training emails\n  * `test.csv`: 4000 emails × 100 features (test set for predictions)\n* **Features:**  \n  100 anonymized numerical features representing extracted email characteristics (specific feature details not provided in description).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n* **Metric Details:**\n  * Measures the classifier's ability to distinguish between spam and normal emails across all classification thresholds\n  * Implementation examples provided for:\n    * MATLAB (`perfcurve` function)\n    * R (`ROCR` package)\n    * Python (`scikit-learn` metrics module)"
    },
    "file_path": "kaggle_datasets/65/problem_summary.md"
  },
  "304": {
    "problem_id": "304",
    "title": "Predicting Customer Revenue with Google Analytics Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Customer Revenue with Google Analytics Data\n\n## Problem Description\n* **Problem Type**: Regression (Revenue Prediction)\n* **Objective**: Predict the natural log of the total future revenue per customer for the Google Merchandise Store (GStore) based on their historical visit data. The goal is to identify high-revenue customers to optimize marketing budget allocation.\n    * **Key Points**:\n        * Focuses on the \"80/20 rule\" where a small percentage of customers generate most revenue.\n        * Requires predicting revenue for a future time period (December 2018 - January 2019) using historical data up to October 2018.\n        * Target is aggregated at the user level (`fullVisitorId`), not per visit.\n\n## Dataset Overview\n* **Data Type**: Tabular data with nested JSON fields (user visit logs from Google Analytics)\n* **Context**: Customer behavior data from the Google Merchandise Store (e-commerce)\n* **Data Files**:\n    * `train_v2.csv`: User transactions from August 2016 - April 2018\n    * `test_v2.csv`: User transactions from May 2018 - October 2018 (predict for these users)\n    * `sample_submission_v2.csv`: Template with all test set `fullVisitorId`s\n* **Key Features**:\n    * `fullVisitorId`: Unique user identifier (must be loaded as string)\n    * JSON-blob columns: `device`, `geoNetwork`, `totals`, `trafficSource`, `hits`, `customDimensions`\n    * `totals.transactionRevenue`: Target column (only in training data)\n    * Visit metadata: `channelGrouping`, `date`, `visitId`, `visitNumber`, `visitStartTime`\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Error (RMSE) of log-transformed revenue\n    * **Calculation**:\n        * For each user: Sum all transaction revenue → Apply log(y+1) transform\n        * RMSE formula: \n          ```\n          RMSE = sqrt(1/n * Σ(y_i - ŷ_i)^2)\n          ```\n          where y_i is the true log-revenue and ŷ_i is the predicted log-revenue\n    * **Special Considerations**:\n        * Public LB uses 5/1/18-10/15/18 data\n        * Private LB uses future 12/1",
    "sections": {},
    "file_path": "kaggle_datasets/304/problem_summary.md"
  },
  "136": {
    "problem_id": "136",
    "title": "EEG-Based Error Detection in P300-Speller Brain-Computer Interface",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# EEG-Based Error Detection in P300-Speller Brain-Computer Interface\n\n## Problem Description\n* **Problem Type:** Binary Classification (Time Series Analysis)\n* **Objective:** Detect erroneous feedback events in a P300-Speller BCI system by analyzing EEG signals when the system incorrectly identifies a character the user intended to select.\n* **Key Points:**\n  * Focuses on error potentials (ErrPs) - specific EEG patterns that occur when users perceive incorrect feedback\n  * Requires generalization across subjects (transfer learning)\n  * Involves two experimental conditions:\n    * Fast mode (4 flashes per item, more error-prone)\n    * Slow mode (8 flashes per item, less error-prone)\n  * Must work with EEG data sampled at 200Hz (downsampled from original 600Hz)\n\n## Dataset Overview\n* **Data Type:** Multichannel EEG time series data with associated labels\n* **Context:** Brain-computer interface (BCI) spelling task with visual feedback\n* **Data Files:**\n  * `ChannelsLocation.csv` (electrode positions)\n  * `train.zip` (16 subjects × 5 sessions)\n  * `TrainLabels.csv` (feedback correctness labels)\n  * `test.zip` (10 subjects × 5 sessions)\n  * `SampleSubmission.csv`\n* **Features:**\n  * 56 EEG channels following extended 10-20 system (e.g., Fp1, Fp2, Cz, Pz)\n  * EOG channel for eye movement detection\n  * Feedback event markers (binary vector)\n  * Timestamps for each sample\n  * Subject and session identifiers\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n* **Implementation Details:**\n  * Standard AUC calculation across all test samples\n  * Available implementations provided for:\n    * MATLAB (`perfcurve`)\n    * R (`roc.area` from verification package)\n    * Python (`metrics.roc_curve` and `metrics.auc` from scikit-learn)",
    "sections": {},
    "file_path": "kaggle_datasets/136/problem_summary.md"
  },
  "552": {
    "problem_id": "552",
    "title": "Out-of-Sample Detection in Deep Ocean Imagery",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Out-of-Sample Detection in Deep Ocean Imagery\n\n## Problem Description\n- **Problem Type**:  \n  - **Computer Vision - Multi-label Classification** (identifying multiple species in an image)  \n  - **Out-of-Distribution Detection** (determining if an image is from a different distribution than the training data)  \n\n- **Objective**:  \n  - Predict the presence of 290 fine-grained marine species categories in deep-ocean images.  \n  - Classify whether each test image is \"out-of-sample\" (i.e., drawn from a different depth distribution than the training data).  \n\n- **Key Points**:  \n  - Training data is limited to upper ocean depths (<800m), while test data includes deeper waters (up to 1300m).  \n  - Species distributions overlap but diverge with depth, simulating real-world challenges in marine ecology.  \n  - Participants are encouraged to use external datasets or pretrained models (e.g., ImageNet, COCO).  \n\n## Dataset Overview\n- **Data Type**:  \n  - **Image data** with bounding box annotations (COCO format) and multi-label classifications.  \n  - Collected by MBARI’s Remotely Operated Vehicles in Monterey Bay (0–1300m depth).  \n\n- **Data Files**:  \n  - `multilabel_classification/train.csv` (image IDs and category lists)  \n  - `object_detection/train.json` (COCO-formatted training annotations)  \n  - `object_detection/eval.json` (COCO-formatted evaluation images)  \n  - `category_key.csv` (maps 290 categories to 20 supercategories like \"Fish\" or \"Coral\")  \n\n- **Features**:  \n  - Images contain benthic organisms with taxonomic labels.  \n  - Annotations include fine-grained species IDs and broader supercategories.  \n  - Images must be downloaded via URLs provided in JSON files due to licensing.  \n\n## Evaluation Metrics\n- **Primary Metrics**:  \n  1. **sAUC (Scaled AUC-ROC)**:  \n     - Measures out-of-sample detection performance.  \n     - Scaled as `sAUC = 2 × AUC − 1` to range from 0 to 1.  \n  2. **MAP@20 (Mean Average Precision at 20)**:  \n     - Evaluates multi-label species predictions.  \n     - Precision is calculated for up to",
    "sections": {},
    "file_path": "kaggle_datasets/552/problem_summary.md"
  },
  "62": {
    "problem_id": "62",
    "title": "Visualizing Kaggle Leaderboard Dynamics",
    "problem_type": "Data Visualization Challenge",
    "objective": "Create innovative, engaging visualizations that capture the dynamic nature of Kaggle competition leaderboards, transforming raw leaderboard data into compelling narratives about competition progress and team interactions.",
    "evaluation_metric": "Community voting (top 2 submissions)",
    "full_content": "# Visualizing Kaggle Leaderboard Dynamics\n\n**Problem Description:**\n* **Problem Type:** Data Visualization Challenge\n* **Objective:** Create innovative, engaging visualizations that capture the dynamic nature of Kaggle competition leaderboards, transforming raw leaderboard data into compelling narratives about competition progress and team interactions.\n* **Key Points:**\n  * Focus on conveying the \"story\" behind leaderboard movements (leapfrogging, team mergers, late surges)\n  * Handle edge cases (long team names, 5000+ teams, special characters)\n  * Consider web implementation feasibility\n  * Address limitations of current leapfrog plots (spatial dimension confusion, visibility of lower-ranked teams, small improvements at competition end)\n\n**Dataset Overview:**\n* **Data Type:** Tabular leaderboard data with associated user metadata\n* **Context:** Historical Kaggle competition leaderboards with team information and user profile photos\n* **Data Files:**\n  * Leaderboard files (TeamId, TeamName, SubmissionDate, Score)\n  * teams.csv (TeamId, UserId, RequestDate for mergers)\n  * user_photos.zip (profile photos for visualization)\n* **Features:**\n  * Team identifiers and names\n  * Timestamped submission history\n  * Best public scores over time\n  * Team composition data (user mergers)\n\n**Evaluation Metrics:**\n* **Primary Evaluation Metric:** Community voting (top 2 submissions)\n* **Secondary Kaggle Selection Criteria:**\n  * Web implementation feasibility\n  * Edge case handling\n  * Visual style and clarity\n  * Code quality and reproducibility\n  * Narrative effectiveness in showing competition dynamics",
    "sections": {
      "Problem Description": "* **Problem Type:** Data Visualization Challenge\n* **Objective:** Create innovative, engaging visualizations that capture the dynamic nature of Kaggle competition leaderboards, transforming raw leaderboard data into compelling narratives about competition progress and team interactions.\n* **Key Points:**\n  * Focus on conveying the \"story\" behind leaderboard movements (leapfrogging, team mergers, late surges)\n  * Handle edge cases (long team names, 5000+ teams, special characters)\n  * Consider web implementation feasibility\n  * Address limitations of current leapfrog plots (spatial dimension confusion, visibility of lower-ranked teams, small improvements at competition end)",
      "Dataset Overview": "* **Data Type:** Tabular leaderboard data with associated user metadata\n* **Context:** Historical Kaggle competition leaderboards with team information and user profile photos\n* **Data Files:**\n  * Leaderboard files (TeamId, TeamName, SubmissionDate, Score)\n  * teams.csv (TeamId, UserId, RequestDate for mergers)\n  * user_photos.zip (profile photos for visualization)\n* **Features:**\n  * Team identifiers and names\n  * Timestamped submission history\n  * Best public scores over time\n  * Team composition data (user mergers)",
      "Evaluation Metrics": "* **Primary Evaluation Metric:** Community voting (top 2 submissions)\n* **Secondary Kaggle Selection Criteria:**\n  * Web implementation feasibility\n  * Edge case handling\n  * Visual style and clarity\n  * Code quality and reproducibility\n  * Narrative effectiveness in showing competition dynamics"
    },
    "file_path": "kaggle_datasets/62/problem_summary.md"
  },
  "599": {
    "problem_id": "599",
    "title": "Steel Plate Defect Prediction",
    "problem_type": "Multi-label Binary Classification (7 defect categories)",
    "objective": "Predict the probability of 7 types of defects occurring on steel plates. Each plate can have multiple defects simultaneously (multi-label problem).",
    "evaluation_metric": null,
    "full_content": "# Steel Plate Defect Prediction\n\n**Problem Description:**\n* **Problem Type:** Multi-label Binary Classification (7 defect categories)\n* **Objective:** Predict the probability of 7 types of defects occurring on steel plates. Each plate can have multiple defects simultaneously (multi-label problem).\n    * **Key Points:**\n        * Targets are binary flags for each defect type: `Pastry`, `Z_Scratch`, `K_Scatch`, `Stains`, `Dirtiness`, `Bumps`, `Other_Faults`\n        * Synthetic dataset generated from original UCI Steel Plates Faults data\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing steel plate manufacturing quality measurements\n* **Data Files:**\n    * `train.csv`: Contains features and 7 binary target columns\n    * `test.csv`: Contains features only (requires prediction of all 7 targets)\n    * `sample_submission.csv`: Example submission format\n* **Features:** \n    * Dataset contains 71 columns (64 anonymized numerical features + 7 target columns in training data)\n    * Features represent various manufacturing measurements (exact nature anonymized)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Column-wise ROC AUC (Average of individual AUC scores for each defect category)\n    * **Calculation:**\n        1. Compute ROC AUC separately for each of the 7 defect predictions\n        2. Average all 7 AUC scores to get final evaluation metric\n    * **Submission Format:** Requires probability predictions for all 7 defect types per test sample",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Binary Classification (7 defect categories)\n* **Objective:** Predict the probability of 7 types of defects occurring on steel plates. Each plate can have multiple defects simultaneously (multi-label problem).\n    * **Key Points:**\n        * Targets are binary flags for each defect type: `Pastry`, `Z_Scratch`, `K_Scatch`, `Stains`, `Dirtiness`, `Bumps`, `Other_Faults`\n        * Synthetic dataset generated from original UCI Steel Plates Faults data",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing steel plate manufacturing quality measurements\n* **Data Files:**\n    * `train.csv`: Contains features and 7 binary target columns\n    * `test.csv`: Contains features only (requires prediction of all 7 targets)\n    * `sample_submission.csv`: Example submission format\n* **Features:** \n    * Dataset contains 71 columns (64 anonymized numerical features + 7 target columns in training data)\n    * Features represent various manufacturing measurements (exact nature anonymized)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Column-wise ROC AUC (Average of individual AUC scores for each defect category)\n    * **Calculation:**\n        1. Compute ROC AUC separately for each of the 7 defect predictions\n        2. Average all 7 AUC scores to get final evaluation metric\n    * **Submission Format:** Requires probability predictions for all 7 defect types per test sample"
    },
    "file_path": "kaggle_datasets/599/problem_summary.md"
  },
  "96": {
    "problem_id": "96",
    "title": "Accelerometer-Based User Identification",
    "problem_type": "Binary Classification (User Verification)",
    "objective": "Determine whether sequences of accelerometer data from mobile devices belong to specific users (devices). Participants must verify if test sequences match the claimed device IDs in the question set.",
    "evaluation_metric": null,
    "full_content": "# Accelerometer-Based User Identification\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (User Verification)\n* **Objective:** Determine whether sequences of accelerometer data from mobile devices belong to specific users (devices). Participants must verify if test sequences match the claimed device IDs in the question set.\n    * **Key Points:**\n        * Focus on behavioral biometrics using movement patterns captured by accelerometers.\n        * Test sequences consist of 300 consecutive samples from the same device.\n        * Some questions pair sequences with their true device IDs, while others use false pairings.\n\n**Dataset Overview:**\n* **Data Type & Context:** Time-series accelerometer data (3-axis: X, Y, Z) collected from Android devices during normal usage.\n    * **Data Files:**\n        * `train.zip`: 30M labeled samples (DeviceId, timestamp, X/Y/Z accelerations).\n        * `test.zip`: 30M unlabeled samples grouped into 90k sequences (SequenceId).\n        * `questions.csv`: Maps SequenceIds to claimed DeviceIds (QuizDevice).\n    * **Key Features:**\n        * Unix timestamps (T) and triaxial acceleration values (X, Y, Z in *g* units).\n        * Sequences represent continuous motion samples (~207 ms intervals).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC).\n    * **Components:**\n        * Predictions are real-valued scores ranking the likelihood of a match.\n        * AUC evaluates the ranking quality of true vs. false device claims.\n        * Implementations provided for Matlab, R, and Python (scikit-learn).",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (User Verification)\n* **Objective:** Determine whether sequences of accelerometer data from mobile devices belong to specific users (devices). Participants must verify if test sequences match the claimed device IDs in the question set.\n    * **Key Points:**\n        * Focus on behavioral biometrics using movement patterns captured by accelerometers.\n        * Test sequences consist of 300 consecutive samples from the same device.\n        * Some questions pair sequences with their true device IDs, while others use false pairings.",
      "Dataset Overview": "* **Data Type & Context:** Time-series accelerometer data (3-axis: X, Y, Z) collected from Android devices during normal usage.\n    * **Data Files:**\n        * `train.zip`: 30M labeled samples (DeviceId, timestamp, X/Y/Z accelerations).\n        * `test.zip`: 30M unlabeled samples grouped into 90k sequences (SequenceId).\n        * `questions.csv`: Maps SequenceIds to claimed DeviceIds (QuizDevice).\n    * **Key Features:**\n        * Unix timestamps (T) and triaxial acceleration values (X, Y, Z in *g* units).\n        * Sequences represent continuous motion samples (~207 ms intervals).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC).\n    * **Components:**\n        * Predictions are real-valued scores ranking the likelihood of a match.\n        * AUC evaluates the ranking quality of true vs. false device claims.\n        * Implementations provided for Matlab, R, and Python (scikit-learn)."
    },
    "file_path": "kaggle_datasets/96/problem_summary.md"
  },
  "109": {
    "problem_id": "109",
    "title": "Predicting Pseudorandom Numbers",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Pseudorandom Numbers\n\n## Problem Description\n* **Problem Type:** Regression (Sequence Prediction)\n* **Objective:** Predict a sequence of pseudorandom numbers generated by an unknown algorithm. The challenge is to identify statistical patterns in seemingly random data where no known prediction methods exist.\n* **Key Points:**\n  * Focuses on breaking the perceived randomness of pseudorumber generation\n  * No constraints on model type - encourages creative pattern recognition approaches\n  * Presented as an unsolved problem in computing (\"too random\" per experts)\n\n## Dataset Overview\n* **Data Type:** Tabular data (sequence of numerical values)\n* **Context:** Simulated or cryptographically generated pseudorandom numbers\n* **Data Files:**  \n  * `randomSubmission.csv` (157.77 kB) containing:\n    - `Id`: Sequence identifier\n    - `Prediction`: Target random number value (may be non-integer, real-valued)\n* **Features:**\n  * Pure numerical sequence with no additional metadata\n  * No imaginary components in numbers\n  * No explicit training data provided - appears to be a pure test set prediction challenge\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE)\n* **Calculation:**  \n  MAE = (1/n) * Σ|y_i - ŷ_i|  \n  Where:\n  * y_i = true random number\n  * ŷ_i = predicted value\n  * n = total number of predictions\n* **Submission Format:** CSV with `Id, Prediction` columns",
    "sections": {},
    "file_path": "kaggle_datasets/109/problem_summary.md"
  },
  "590": {
    "problem_id": "590",
    "title": "Ovarian Cancer Subtype Classification from Histopathology Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Ovarian Cancer Subtype Classification from Histopathology Images\n\n## Problem Description\n- **Problem Type:** Multi-class Classification with Outlier Detection\n- **Objective:** Classify ovarian cancer histopathology images into one of five common subtypes (HGSC, CC, EC, LGSC, MC) or identify rare/outlier subtypes (labeled as \"Other\"). The goal is to improve diagnostic accuracy and enable personalized treatment strategies.\n    * **Key Points:**\n        * Addresses challenges in pathologist-based diagnosis (observer disagreement, lack of specialists)\n        * Requires handling diverse image sources (20+ medical centers worldwide)\n        * Must generalize to unseen hospital data in test set\n        * Includes outlier detection challenge (\"Other\" class not present in training)\n\n## Dataset Overview\n- **Data Type:** Whole Slide Images (WSI) and Tissue Microarray (TMA) histopathology images\n- **Context:** Largest ovarian cancer histopathology dataset, collected from international medical centers\n- **Data Files:**\n    * `train_images/`, `test_images/` - High-resolution microscopy scans (550GB total)\n    * `train.csv`, `test.csv` - Contains image IDs and labels (train only)\n    * `supplemental_masks/` - Annotation masks for cancerous regions (separate download)\n- **Key Features:**\n    * Variable image dimensions (up to 100,000×50,000 pixels)\n    * Mixed magnifications (20x for WSI, 40x for TMA)\n    * Metadata columns: `image_id`, `label`, `image_width`, `image_height`, `is_tma`\n\n## Evaluation Metrics\n- **Primary Metric:** Balanced Accuracy Score\n    * **Rationale:** Accounts for class imbalance by averaging recall across classes\n    * **Calculation:** (Recall_Class1 + Recall_Class2 + ... + Recall_ClassN) / N\n    * **Handling Outliers:** \"Other\" class in test set contributes equally despite being unseen in training",
    "sections": {},
    "file_path": "kaggle_datasets/590/problem_summary.md"
  },
  "564": {
    "problem_id": "564",
    "title": "AI Research Essay Competition on ML Advancements",
    "problem_type": "Analytics/Natural Language Processing (Essay Writing)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# AI Research Essay Competition on ML Advancements\n\n**Problem Description:**\n* **Problem Type:** Analytics/Natural Language Processing (Essay Writing)\n* **Objective:**  \n    * Participants are tasked with writing a comprehensive essay summarizing recent advancements (past 2 years) in one of seven predefined AI/ML topics. The goal is to centralize community knowledge and provide accessible insights into rapid innovations in the field.\n* **Key Points:**  \n    * Essays must focus on one of these topics:  \n        * Text data  \n        * Image/video data  \n        * Tabular/time series data  \n        * Kaggle Competitions  \n        * Generative AI  \n        * AI ethics  \n        * Other (catch-all category)  \n    * Requires peer feedback: Participants must review and provide constructive criticism on three other essays.  \n    * Emphasis on accuracy, references, and supplementary materials (tables/figures/code).  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * **Metadata resources** (not mandatory for use):  \n        * Kaggle competition writeups (`kaggle_writeups.csv`)  \n        * arXiv scholarly articles (`arxiv_metadata.json`)  \n    * **Submission file**: `sample_submission.csv` (template for essay URLs and peer feedback links).  \n* **Key Features:**  \n    * **Essay content**: Free-form text adhering to competition guidelines.  \n    * **References**: Expected to cite academic literature, Kaggle solutions, or other credible sources.  \n\n**Evaluation Metrics:**  \n* **Primary Evaluation**: Expert grading panel (Kaggle Grandmasters) using a rubric.  \n* **Rubric Components**:  \n    * **Rules Compliance** (Mandatory):  \n        * Essay adheres to topic, format, and peer-feedback requirements.  \n    * **Narrative Quality** (10 pts): Clarity, conciseness, and absence of inappropriate content.  \n    * **Narrative Background** (10 pts): Sufficient context for non-experts.  \n    * **Narrative Accuracy** (10 pts): Thorough and correct summary of advancements.  \n    * **Narrative References** (10 pts): Depth and appropriateness of citations.  \n    * **Supplementary Materials** (10 pts): Quality of supporting visuals/code.",
    "sections": {
      "Problem Description": "* **Problem Type:** Analytics/Natural Language Processing (Essay Writing)\n* **Objective:**  \n    * Participants are tasked with writing a comprehensive essay summarizing recent advancements (past 2 years) in one of seven predefined AI/ML topics. The goal is to centralize community knowledge and provide accessible insights into rapid innovations in the field.\n* **Key Points:**  \n    * Essays must focus on one of these topics:  \n        * Text data  \n        * Image/video data  \n        * Tabular/time series data  \n        * Kaggle Competitions  \n        * Generative AI  \n        * AI ethics  \n        * Other (catch-all category)  \n    * Requires peer feedback: Participants must review and provide constructive criticism on three other essays.  \n    * Emphasis on accuracy, references, and supplementary materials (tables/figures/code).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * **Metadata resources** (not mandatory for use):  \n        * Kaggle competition writeups (`kaggle_writeups.csv`)  \n        * arXiv scholarly articles (`arxiv_metadata.json`)  \n    * **Submission file**: `sample_submission.csv` (template for essay URLs and peer feedback links).  \n* **Key Features:**  \n    * **Essay content**: Free-form text adhering to competition guidelines.  \n    * **References**: Expected to cite academic literature, Kaggle solutions, or other credible sources.  \n\n**Evaluation Metrics:**  \n* **Primary Evaluation**: Expert grading panel (Kaggle Grandmasters) using a rubric.  \n* **Rubric Components**:  \n    * **Rules Compliance** (Mandatory):  \n        * Essay adheres to topic, format, and peer-feedback requirements.  \n    * **Narrative Quality** (10 pts): Clarity, conciseness, and absence of inappropriate content.  \n    * **Narrative Background** (10 pts): Sufficient context for non-experts.  \n    * **Narrative Accuracy** (10 pts): Thorough and correct summary of advancements.  \n    * **Narrative References** (10 pts): Depth and appropriateness of citations.  \n    * **Supplementary Materials** (10 pts): Quality of supporting visuals/code."
    },
    "file_path": "kaggle_datasets/564/problem_summary.md"
  },
  "332": {
    "problem_id": "332",
    "title": "Predicting Stock Movements Using News Analytics",
    "problem_type": "Time Series Forecasting with Multimodal Data (Financial + NLP)",
    "objective": "Predict 10-day market-adjusted stock returns by analyzing historical market data and news sentiment/features. Participants must output a confidence value between [-1, 1] indicating expected return direction and magnitude.",
    "evaluation_metric": null,
    "full_content": "# Predicting Stock Movements Using News Analytics\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting with Multimodal Data (Financial + NLP)\n* **Objective:** Predict 10-day market-adjusted stock returns by analyzing historical market data and news sentiment/features. Participants must output a confidence value between [-1, 1] indicating expected return direction and magnitude.\n    * **Key Points:**\n        * Focus on market-residualized returns (returns adjusted for broad market movements)\n        * Must handle daily-changing universe of tradable assets (`universe` flag)\n        * Combines structured financial data (prices, volumes) with unstructured news data (headlines, sentiment scores)\n        * Time-sensitive predictions with strict avoidance of future data leakage\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * Tabular market data (daily OHLC prices, volumes, returns) + News metadata/text (Reuters articles with sentiment, relevance, novelty metrics)\n    * Covers US-listed instruments from 2007 onward, with dynamic asset inclusion\n* **Key Files:** \n    * Integrated market/news data accessed via Kaggle Kernels API (no direct file access)\n* **Notable Features:**\n    * **Market Data:** `returnsOpenNextMktres10` (target), `universe` flag, raw/market-residualized returns at 1-day/10-day horizons\n    * **News Data:** Sentiment probabilities (negative/neutral/positive), relevance scores, novelty/volume counts over multiple time windows, article metadata (urgency, provider, subjects)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Sharpe Ratio of daily portfolio returns (`score = mean(x_t) / std(x_t)`)\n    * **Components:**\n        * Daily score `x_t = Σ(confidenceValue * 10-day_market-adjusted_return * universe_flag)` across all assets\n        * Final score is Sharpe Ratio of daily `x_t` values\n        * Zero score if predictions have zero standard deviation (no variability)",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting with Multimodal Data (Financial + NLP)\n* **Objective:** Predict 10-day market-adjusted stock returns by analyzing historical market data and news sentiment/features. Participants must output a confidence value between [-1, 1] indicating expected return direction and magnitude.\n    * **Key Points:**\n        * Focus on market-residualized returns (returns adjusted for broad market movements)\n        * Must handle daily-changing universe of tradable assets (`universe` flag)\n        * Combines structured financial data (prices, volumes) with unstructured news data (headlines, sentiment scores)\n        * Time-sensitive predictions with strict avoidance of future data leakage",
      "Dataset Overview": "* **Data Type & Context:** \n    * Tabular market data (daily OHLC prices, volumes, returns) + News metadata/text (Reuters articles with sentiment, relevance, novelty metrics)\n    * Covers US-listed instruments from 2007 onward, with dynamic asset inclusion\n* **Key Files:** \n    * Integrated market/news data accessed via Kaggle Kernels API (no direct file access)\n* **Notable Features:**\n    * **Market Data:** `returnsOpenNextMktres10` (target), `universe` flag, raw/market-residualized returns at 1-day/10-day horizons\n    * **News Data:** Sentiment probabilities (negative/neutral/positive), relevance scores, novelty/volume counts over multiple time windows, article metadata (urgency, provider, subjects)",
      "Evaluation Metrics": "* **Primary Metric:** Sharpe Ratio of daily portfolio returns (`score = mean(x_t) / std(x_t)`)\n    * **Components:**\n        * Daily score `x_t = Σ(confidenceValue * 10-day_market-adjusted_return * universe_flag)` across all assets\n        * Final score is Sharpe Ratio of daily `x_t` values\n        * Zero score if predictions have zero standard deviation (no variability)"
    },
    "file_path": "kaggle_datasets/332/problem_summary.md"
  },
  "100": {
    "problem_id": "100",
    "title": "Keyword Extraction from Stack Exchange Questions",
    "problem_type": "NLP - Multi-label Text Classification (Keyword/Tag Extraction)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Keyword Extraction from Stack Exchange Questions\n\n**Problem Description:**\n* **Problem Type:** NLP - Multi-label Text Classification (Keyword/Tag Extraction)\n* **Objective:**  \n    * Predict relevant tags/keywords for questions posted on Stack Exchange sites using only the question's title and body text.\n    * The task involves extracting multiple tags per question (space-delimited list) from a large, diverse set of potential tags.\n* **Key Points:**\n    * Dataset contains technical and non-technical questions from various Stack Exchange sites.\n    * Tags must be predicted as exact matches (synonyms are not counted as correct).\n    * Participants must compete as individuals and use only the provided data (no external crawling allowed).\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Text data consisting of question titles and bodies from Stack Exchange sites.\n    * Mixed content: verbose text and technical (math/programming) questions.\n* **Data Files:**  \n    * `Train.csv`: Contains `Id`, `Title`, `Body`, and `Tags` (ground truth labels).\n    * `Test.csv`: Contains `Id`, `Title`, and `Body` (tags must be predicted).\n* **Key Features:**  \n    * `Title`: Short text summarizing the question.\n    * `Body`: Detailed content of the question (may include code, math notation, etc.).\n    * `Tags`: Space-delimited list of lowercase tags (e.g., \"python machine-learning\").\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean F1-Score  \n    * Calculated as the harmonic mean of precision and recall for each tag, then averaged across all predictions.\n    * Precision: Ratio of correctly predicted tags to total predicted tags (`tp / (tp + fp)`).\n    * Recall: Ratio of correctly predicted tags to actual tags (`tp / (tp + fn)`).\n    * F1-Score: `2 * (precision * recall) / (precision + recall)`.\n* **Key Constraints:**  \n    * Predictions must be exact matches to the ground truth tags (no partial credit for synonyms).\n    * Tags must be space-delimited and lowercase in submissions.",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Multi-label Text Classification (Keyword/Tag Extraction)\n* **Objective:**  \n    * Predict relevant tags/keywords for questions posted on Stack Exchange sites using only the question's title and body text.\n    * The task involves extracting multiple tags per question (space-delimited list) from a large, diverse set of potential tags.\n* **Key Points:**\n    * Dataset contains technical and non-technical questions from various Stack Exchange sites.\n    * Tags must be predicted as exact matches (synonyms are not counted as correct).\n    * Participants must compete as individuals and use only the provided data (no external crawling allowed).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Text data consisting of question titles and bodies from Stack Exchange sites.\n    * Mixed content: verbose text and technical (math/programming) questions.\n* **Data Files:**  \n    * `Train.csv`: Contains `Id`, `Title`, `Body`, and `Tags` (ground truth labels).\n    * `Test.csv`: Contains `Id`, `Title`, and `Body` (tags must be predicted).\n* **Key Features:**  \n    * `Title`: Short text summarizing the question.\n    * `Body`: Detailed content of the question (may include code, math notation, etc.).\n    * `Tags`: Space-delimited list of lowercase tags (e.g., \"python machine-learning\").",
      "Evaluation Metrics": "* **Primary Metric:** Mean F1-Score  \n    * Calculated as the harmonic mean of precision and recall for each tag, then averaged across all predictions.\n    * Precision: Ratio of correctly predicted tags to total predicted tags (`tp / (tp + fp)`).\n    * Recall: Ratio of correctly predicted tags to actual tags (`tp / (tp + fn)`).\n    * F1-Score: `2 * (precision * recall) / (precision + recall)`.\n* **Key Constraints:**  \n    * Predictions must be exact matches to the ground truth tags (no partial credit for synonyms).\n    * Tags must be space-delimited and lowercase in submissions."
    },
    "file_path": "kaggle_datasets/100/problem_summary.md"
  },
  "54": {
    "problem_id": "54",
    "title": "Predicting Closed Questions on Stack Overflow",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Closed Questions on Stack Overflow\n\n## Problem Description\n* **Problem Type**: Multi-class Classification\n* **Objective**: Predict whether a new question posted on Stack Overflow will be closed, and if so, the reason for closure. The target classes are:\n  * Open (not closed)\n  * Closed as \"not a real question\"\n  * Closed as \"not constructive\"\n  * Closed as \"off topic\"\n  * Closed as \"too localized\"\n* **Key Points**:\n  * The competition focuses on predicting closure at the time of question submission, using only initial post data and user metadata.\n  * The \"exact duplicate\" closure reason was explicitly excluded from the competition.\n  * Models must output probabilities for all five classes, with rows summing to 1.\n\n## Dataset Overview\n* **Data Type**: Text data (questions) with tabular metadata\n* **Context**: Stack Overflow programming questions with moderation outcomes\n* **Data Files**:\n  * train.csv (primary training set with post text, metadata, and closure status)\n  * train-sample.csv (stratified sample of training data)\n  * public_leaderboard.csv (validation set)\n  * private_leaderboard.csv (final test set)\n* **Key Features**:\n  * Text content: Title, BodyMarkdown (in Markdown format)\n  * Post metadata: CreationDate, Tags (up to 5)\n  * User metadata: ReputationAtPostCreation, OwnerUndeletedAnswerCountAtPostTime, etc.\n  * Target: OpenStatus (closure reason or \"open\")\n\n## Evaluation Metrics\n* **Primary Metric**: Multiclass Logarithmic Loss (LogLoss)\n* **Implementation Details**:\n  * Requires predicted probabilities for all 5 classes\n  * Each row's probabilities must sum to 1 (normalization applied if needed)\n  * Formula: $-\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^M y_{ij}\\log(p_{ij})$\n    * Where N = number of observations, M = 5 classes\n    * $y_{ij}$ = 1 if observation i is in class j, else 0\n    * $p_{ij}$ = predicted probability of observation i being in class j",
    "sections": {},
    "file_path": "kaggle_datasets/54/problem_summary.md"
  },
  "107": {
    "problem_id": "107",
    "title": "Loan Default and Loss Severity Prediction",
    "problem_type": "Regression (with elements of binary classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Loan Default and Loss Severity Prediction\n\n**Problem Description:**\n* **Problem Type:** Regression (with elements of binary classification)\n* **Objective:** \n    * Predict both the occurrence of loan defaults and the severity of financial losses when defaults occur.\n    * Unlike traditional binary classification (good/bad loans), this task requires predicting a continuous loss value (0-100) where 0 indicates no default.\n* **Key Points:**\n    * Bridges banking (economic capital preservation) and asset management (risk optimization) perspectives.\n    * Requires handling both default prediction (implicit) and loss magnitude estimation.\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * Tabular financial data of anonymized loan transactions (~200k observations, ~800 features).\n    * Features are standardized, de-trended, and anonymized with preserved missing values.\n* **Data Files:**\n    * train_v2.csv.zip (training data)\n    * test_v2.csv.zip (test data)\n    * sampleSubmission.csv (submission format)\n* **Key Features:**\n    * Mix of numerical and categorical features (e.g., f776, f777 mentioned as categorical).\n    * Observations ordered chronologically in training set (time dimensionality removed but sequence preserved).\n    * Target variable: Normalized loss value (0-100, where 0 = no default).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)\n    * Calculation: \n        * MAE = (1/n) * Σ|y_i - ŷ_i|\n        * Where:\n            * n = number of predictions\n            * y_i = actual loss value\n            * ŷ_i = predicted loss value",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (with elements of binary classification)\n* **Objective:** \n    * Predict both the occurrence of loan defaults and the severity of financial losses when defaults occur.\n    * Unlike traditional binary classification (good/bad loans), this task requires predicting a continuous loss value (0-100) where 0 indicates no default.\n* **Key Points:**\n    * Bridges banking (economic capital preservation) and asset management (risk optimization) perspectives.\n    * Requires handling both default prediction (implicit) and loss magnitude estimation.",
      "Dataset Overview": "* **Data Type & Context:** \n    * Tabular financial data of anonymized loan transactions (~200k observations, ~800 features).\n    * Features are standardized, de-trended, and anonymized with preserved missing values.\n* **Data Files:**\n    * train_v2.csv.zip (training data)\n    * test_v2.csv.zip (test data)\n    * sampleSubmission.csv (submission format)\n* **Key Features:**\n    * Mix of numerical and categorical features (e.g., f776, f777 mentioned as categorical).\n    * Observations ordered chronologically in training set (time dimensionality removed but sequence preserved).\n    * Target variable: Normalized loss value (0-100, where 0 = no default).",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)\n    * Calculation: \n        * MAE = (1/n) * Σ|y_i - ŷ_i|\n        * Where:\n            * n = number of predictions\n            * y_i = actual loss value\n            * ŷ_i = predicted loss value"
    },
    "file_path": "kaggle_datasets/107/problem_summary.md"
  },
  "335": {
    "problem_id": "335",
    "title": "Predicting Molecular Magnetic Interactions (Scalar Coupling Constants)",
    "problem_type": "Regression (with physics-informed constraints)",
    "objective": "Predict the scalar coupling constant (magnetic interaction strength) between pairs of atoms in a molecule, given molecular structure data. The goal is to replace computationally expensive quantum mechanics calculations with a faster ML-based approach.",
    "evaluation_metric": null,
    "full_content": "# Predicting Molecular Magnetic Interactions (Scalar Coupling Constants)\n\n**Problem Description:**\n* **Problem Type:** Regression (with physics-informed constraints)\n* **Objective:** Predict the scalar coupling constant (magnetic interaction strength) between pairs of atoms in a molecule, given molecular structure data. The goal is to replace computationally expensive quantum mechanics calculations with a faster ML-based approach.\n    * **Key Points:**\n        * Focus on specific atom pairs listed in train/test files (not all possible pairs).\n        * Training and test splits are molecule-wise to prevent data leakage.\n        * Applications in NMR (Nuclear Magnetic Resonance) for molecular structure analysis in pharmaceuticals, materials science, etc.\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * Tabular data derived from molecular structures (XYZ files) with additional quantum chemical properties.\n    * Primary data includes atom pair indices, coupling types, and Cartesian coordinates.\n* **Data Files:**\n    * `train.csv` / `test.csv`: Contain molecule names, atom indices, coupling types, and target (scalar_coupling_constant).\n    * `structures.csv` / `structures.zip`: Atomic elements and XYZ coordinates for all molecules.\n    * Supplemental files (train only): Dipole moments, magnetic shielding tensors, Mulliken charges, potential energy, and scalar coupling contributions (Fermi Contact, Spin-dipolar, etc.).\n* **Key Features:**\n    * Atom types (e.g., H, C) and their spatial coordinates.\n    * Coupling type (e.g., `2JHC` indicating bond topology).\n    * Derived physical properties (dipole moments, shielding tensors) for train molecules.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log of Mean Absolute Error (LogMAE), averaged across coupling types.\n    * **Components:**\n        * MAE is calculated separately for each scalar coupling type.\n        * Final score: Average of log-transformed MAE values across all types:  \n          \\( \\text{score} = \\frac{1}{T} \\sum_{t=1}^{T} \\log\\left(\\frac{1}{n_t} \\sum_{i=1}^{n_t} |y_i - \\hat{y_i}|\\right) \\)\n        * MAE floor: `1e-9` (to avoid log(0)). Best possible score ≈ -20.7232.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (with physics-informed constraints)\n* **Objective:** Predict the scalar coupling constant (magnetic interaction strength) between pairs of atoms in a molecule, given molecular structure data. The goal is to replace computationally expensive quantum mechanics calculations with a faster ML-based approach.\n    * **Key Points:**\n        * Focus on specific atom pairs listed in train/test files (not all possible pairs).\n        * Training and test splits are molecule-wise to prevent data leakage.\n        * Applications in NMR (Nuclear Magnetic Resonance) for molecular structure analysis in pharmaceuticals, materials science, etc.",
      "Dataset Overview": "* **Data Type & Context:** \n    * Tabular data derived from molecular structures (XYZ files) with additional quantum chemical properties.\n    * Primary data includes atom pair indices, coupling types, and Cartesian coordinates.\n* **Data Files:**\n    * `train.csv` / `test.csv`: Contain molecule names, atom indices, coupling types, and target (scalar_coupling_constant).\n    * `structures.csv` / `structures.zip`: Atomic elements and XYZ coordinates for all molecules.\n    * Supplemental files (train only): Dipole moments, magnetic shielding tensors, Mulliken charges, potential energy, and scalar coupling contributions (Fermi Contact, Spin-dipolar, etc.).\n* **Key Features:**\n    * Atom types (e.g., H, C) and their spatial coordinates.\n    * Coupling type (e.g., `2JHC` indicating bond topology).\n    * Derived physical properties (dipole moments, shielding tensors) for train molecules.",
      "Evaluation Metrics": "* **Primary Metric:** Log of Mean Absolute Error (LogMAE), averaged across coupling types.\n    * **Components:**\n        * MAE is calculated separately for each scalar coupling type.\n        * Final score: Average of log-transformed MAE values across all types:  \n          \\( \\text{score} = \\frac{1}{T} \\sum_{t=1}^{T} \\log\\left(\\frac{1}{n_t} \\sum_{i=1}^{n_t} |y_i - \\hat{y_i}|\\right) \\)\n        * MAE floor: `1e-9` (to avoid log(0)). Best possible score ≈ -20.7232."
    },
    "file_path": "kaggle_datasets/335/problem_summary.md"
  },
  "563": {
    "problem_id": "563",
    "title": "Multi-Label Classification with an Enzyme Substrate Dataset",
    "problem_type": "Multi-Label Classification (Binary Targets)",
    "objective": "Predict the probability of two enzyme substrate classes (`EC1` and `EC2`) for each sample in the test set. The dataset involves classifying whether enzymes belong to specific substrate categories.",
    "evaluation_metric": null,
    "full_content": "# Multi-Label Classification with an Enzyme Substrate Dataset\n\n**Problem Description:**\n* **Problem Type:** Multi-Label Classification (Binary Targets)\n* **Objective:** Predict the probability of two enzyme substrate classes (`EC1` and `EC2`) for each sample in the test set. The dataset involves classifying whether enzymes belong to specific substrate categories.\n    * **Key Points:**\n        * Only the first two enzyme classes (`EC1` and `EC2`) need to be predicted, despite additional targets (`EC3-EC6`) being present in the training data.\n        * The dataset is synthetically generated from a real-world enzyme substrate dataset to balance privacy and usability.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing enzyme substrate features and binary multi-label targets.\n* **Data Files:**\n    * `train.csv` - Contains features and binary targets (`EC1-EC6`).\n    * `test.csv` - Contains features only; requires prediction for `EC1` and `EC2`.\n    * `sample_submission.csv` - Example submission file with the correct format.\n* **Features:** \n    * 6 anonymized features derived from the original dataset (selected for high signal).\n    * Targets (`EC1-EC6`) are binary labels indicating enzyme class membership.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Average Area Under the ROC Curve (AUC) for `EC1` and `EC2`.\n    * **Components:**\n        * Compute the ROC AUC score separately for `EC1` and `EC2`.\n        * The final score is the arithmetic mean of the two AUC values.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-Label Classification (Binary Targets)\n* **Objective:** Predict the probability of two enzyme substrate classes (`EC1` and `EC2`) for each sample in the test set. The dataset involves classifying whether enzymes belong to specific substrate categories.\n    * **Key Points:**\n        * Only the first two enzyme classes (`EC1` and `EC2`) need to be predicted, despite additional targets (`EC3-EC6`) being present in the training data.\n        * The dataset is synthetically generated from a real-world enzyme substrate dataset to balance privacy and usability.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing enzyme substrate features and binary multi-label targets.\n* **Data Files:**\n    * `train.csv` - Contains features and binary targets (`EC1-EC6`).\n    * `test.csv` - Contains features only; requires prediction for `EC1` and `EC2`.\n    * `sample_submission.csv` - Example submission file with the correct format.\n* **Features:** \n    * 6 anonymized features derived from the original dataset (selected for high signal).\n    * Targets (`EC1-EC6`) are binary labels indicating enzyme class membership.",
      "Evaluation Metrics": "* **Primary Metric:** Average Area Under the ROC Curve (AUC) for `EC1` and `EC2`.\n    * **Components:**\n        * Compute the ROC AUC score separately for `EC1` and `EC2`.\n        * The final score is the arithmetic mean of the two AUC values."
    },
    "file_path": "kaggle_datasets/563/problem_summary.md"
  },
  "98": {
    "problem_id": "98",
    "title": "Predicting Citizen Engagement with 311 Issues",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Citizen Engagement with 311 Issues\n\n## Problem Description\n- **Problem Type**: Multivariate Regression (predicting three continuous targets: views, votes, and comments)\n- **Objective**: Predict how citizens will engage with specific 311 issues by forecasting:\n  * Number of views\n  * Number of votes\n  * Number of comments\n- **Key Points**:\n  * Focuses on quantifying issue urgency and citizen priorities\n  * Location (geospatial data) is an important factor\n  * Data contains temporal dynamics (older issues have more engagement time)\n  * Challenges include:\n    * Raw, noisy data with quality issues\n    * Evolving platform and community dynamics\n    * Systematic differences based on input sources\n\n## Dataset Overview\n- **Data Type**: Tabular data with geospatial and text features\n- **Context**: 311 service requests from four cities (2012 onwards)\n- **Data Files**:\n  * train.csv (labeled data with targets)\n  * test.csv (unlabeled data for prediction)\n  * sampleSubmission.csv (submission format template)\n- **Key Features**:\n  * Geospatial: latitude, longitude\n  * Temporal: created_time\n  * Text: summary, description\n  * Categorical: source, tag_type\n  * Targets: num_views, num_votes, num_comments\n\n## Evaluation Metrics\n- **Primary Metric**: Root Mean Squared Logarithmic Error (RMSLE)\n- **Calculation**:\n  * Computed across all three targets (views, votes, comments)\n  * Formula: √[1/n Σ(log(p_i + 1) - log(a_i + 1))²]\n  * Where:\n    * n = 3 × number of test issues (summing errors across all three targets)\n    * p_i = predicted value\n    * a_i = actual value\n  * Uses natural logarithm to mitigate scale differences\n  * +1 adjustment prevents log(0) issues",
    "sections": {},
    "file_path": "kaggle_datasets/98/problem_summary.md"
  },
  "597": {
    "problem_id": "597",
    "title": "Multi-Class Prediction of Obesity Risk",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Class Prediction of Obesity Risk  \n\n## Problem Description  \n- **Problem Type:** Multi-Class Classification  \n- **Objective:** Predict the obesity risk category (`NObeyesdad`) of individuals based on various factors related to cardiovascular health.  \n  - **Key Points:**  \n    - The task involves classifying individuals into distinct obesity risk categories.  \n    - The dataset is synthetically generated but closely mirrors real-world obesity risk data.  \n    - Encourages exploratory data analysis (EDA) and visualization due to its suitability for clustering and feature exploration.  \n\n## Dataset Overview  \n- **Data Type & Context:** Tabular data containing anonymized features related to obesity and cardiovascular risk factors.  \n- **Data Files:**  \n  - `train.csv`: Contains features and the target variable (`NObeyesdad`).  \n  - `test.csv`: Contains features for which predictions must be made.  \n  - `sample_submission.csv`: Example submission file with the correct format.  \n- **Features:**  \n  - Includes various health and lifestyle-related attributes (exact features not listed, but likely numerical/categorical).  \n  - Target variable (`NObeyesdad`) represents multiple obesity risk classes (e.g., \"Normal_Weight\").  \n\n## Evaluation Metrics  \n- **Primary Metric:** Accuracy  \n  - **Components:**  \n    - Measures the proportion of correctly predicted class labels over the total predictions.  \n    - Submissions must predict the exact class label for each test instance.",
    "sections": {},
    "file_path": "kaggle_datasets/597/problem_summary.md"
  },
  "138": {
    "problem_id": "138",
    "title": "Driver Telematics Fingerprint Identification",
    "problem_type": "Binary Classification (with anomaly detection aspects)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Driver Telematics Fingerprint Identification\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with anomaly detection aspects)\n* **Objective:**  \n    * Develop a \"telematic fingerprint\" algorithm to distinguish whether a given driving trip belongs to a specific driver or is an imposter trip.\n    * Identify anomalous trips planted in each driver's folder that were actually driven by other drivers.\n    * Create a driver signature based on behavioral patterns (e.g., acceleration, turning, route types).\n* **Key Points:**\n    * Privacy-protected data: Trips are anonymized by centering at origin (0,0) and random rotation.\n    * Unlabeled training data: No explicit labels for true/false trips; assumption is majority in each folder are genuine.\n    * Adversarial aspect: Unknown number of false trips per driver, sourced from external drivers.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Time-series tabular data representing vehicle telematics (position recordings every second).\n    * 50,000+ anonymized driver trips across multiple drivers.\n* **Data Files:**  \n    * Directory structure with folders per driver, each containing 200 CSV files (one per trip).\n    * Each CSV contains `(x,y)` coordinate pairs in meters (privacy-transformed).\n* **Features:**  \n    * Derived features would need to capture driving patterns (e.g., acceleration, speed, turning angles, trip duration).\n    * No pre-extracted features provided—raw coordinate data only.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)  \n    * Global calculation across all driver-trip pairs.\n    * Requires calibrated probability predictions (range [0,1]).\n* **Submission Format:**  \n    * CSV with `driver_trip` IDs (e.g., `1_1` for driver 1, trip 1) and predicted probability `prob` that the trip belongs to the driver.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with anomaly detection aspects)\n* **Objective:**  \n    * Develop a \"telematic fingerprint\" algorithm to distinguish whether a given driving trip belongs to a specific driver or is an imposter trip.\n    * Identify anomalous trips planted in each driver's folder that were actually driven by other drivers.\n    * Create a driver signature based on behavioral patterns (e.g., acceleration, turning, route types).\n* **Key Points:**\n    * Privacy-protected data: Trips are anonymized by centering at origin (0,0) and random rotation.\n    * Unlabeled training data: No explicit labels for true/false trips; assumption is majority in each folder are genuine.\n    * Adversarial aspect: Unknown number of false trips per driver, sourced from external drivers.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Time-series tabular data representing vehicle telematics (position recordings every second).\n    * 50,000+ anonymized driver trips across multiple drivers.\n* **Data Files:**  \n    * Directory structure with folders per driver, each containing 200 CSV files (one per trip).\n    * Each CSV contains `(x,y)` coordinate pairs in meters (privacy-transformed).\n* **Features:**  \n    * Derived features would need to capture driving patterns (e.g., acceleration, speed, turning angles, trip duration).\n    * No pre-extracted features provided—raw coordinate data only.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)  \n    * Global calculation across all driver-trip pairs.\n    * Requires calibrated probability predictions (range [0,1]).\n* **Submission Format:**  \n    * CSV with `driver_trip` IDs (e.g., `1_1` for driver 1, trip 1) and predicted probability `prob` that the trip belongs to the driver."
    },
    "file_path": "kaggle_datasets/138/problem_summary.md"
  },
  "53": {
    "problem_id": "53",
    "title": "Hierarchical Load Forecasting for US Utility Zones",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Hierarchical Load Forecasting for US Utility Zones\n\n## Problem Description\n- **Problem Type**: Time Series Forecasting (Hierarchical)\n- **Objective**: \n  - Backcast missing hourly load data (in kW) for 8 specific weeks across 20 utility zones and their system-level aggregate (21 total series).\n  - Forecast hourly loads for a future week (2008/7/1-2008/7/7) at both zonal and system levels.\n  - Handle hierarchical structure where system-level load is the sum of 20 zonal loads.\n- **Key Points**:\n  - Uses temperature data from 11 stations as exogenous variables (available for backcasting but not forecasting period).\n  - Requires strict adherence to submission format with sorted chronological order and zone IDs.\n\n## Dataset Overview\n- **Data Type**: Tabular time series data with hierarchical structure.\n- **Context**: Hourly energy load data from a US utility with 20 zones (2004-2008).\n- **Data Files**:\n  - `Load_history.csv`: Hourly loads for 20 zones (zone_id + 24 hourly columns)\n  - `Temperature_history.csv`: Hourly temperatures from 11 weather stations\n  - `weights.csv`: Custom weights for evaluation\n  - `submission_template.csv`: Required output format\n  - `Benchmark.csv`: Example predictions from baseline model\n- **Key Features**:\n  - Calendar variables (year, month, day)\n  - 24 hourly load columns per zone\n  - Temperature data from multiple stations\n  - System-level load as sum of zones\n\n## Evaluation Metrics\n- **Primary Metric**: Weighted Root Mean Square Error (WRMSE)\n- **Weighting Scheme**:\n  - Backcasted weeks:\n    - Zonal level: Weight = 1 per hour\n    - System level: Weight = 20 per hour\n  - Forecasted week:\n    - Zonal level: Weight = 8 per hour\n    - System level: Weight = 160 per hour\n- **Calculation**: \n  - Errors are weighted according to predefined importance (system-level predictions carry higher weight)\n  - Final score aggregates all weighted errors across time periods and hierarchy levels",
    "sections": {},
    "file_path": "kaggle_datasets/53/problem_summary.md"
  },
  "30": {
    "problem_id": "30",
    "title": "Arabic Writer Identification from Handwritten Documents",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Arabic Writer Identification from Handwritten Documents\n\n## Problem Description\n- **Problem Type**: Multi-class Classification (with unknown class handling)\n- **Objective**: Identify the writer of Arabic handwritten documents from a set of known writers, or classify as unknown (0) if the writer is not in the training set. The task involves:\n  - Matching test documents to known writers in the training set\n  - Handling cases where test documents may belong to writers not present in training\n- **Key Points**:\n  - Focus on binary images only (no color/gray-level to avoid pen identification bias)\n  - No side information provided (e.g., number of documents per writer)\n  - Significant dataset expansion from previous edition (>200 writers)\n\n## Dataset Overview\n- **Data Type**: Binary images of handwritten Arabic text + pre-extracted geometric features\n- **Context**: Forensic document examination context with documents collected from 200+ writers at Qatar University\n- **Data Files**:\n  - Train images (XXX_Y.png format where XXX=writer ID, Y=paragraph number)\n  - Test images (ZZ.png format)\n  - Pre-extracted feature files (multiple histogram-based features)\n  - Sample submission file (sample_entry.csv)\n- **Features**:\n  - Binary PNG images of handwritten paragraphs\n  - 30+ pre-extracted geometric features including:\n    - Branch length histograms\n    - Tortuosity measures\n    - Chain code histograms\n    - Directional features\n    - Curvature measures\n\n## Evaluation Metrics\n- **Evaluation Metric**: Categorization Accuracy (percentage of correctly identified writers)\n- **Components**:\n  - Correct identification of known writers (non-zero IDs)\n  - Correct rejection of unknown writers (ID=0)\n  - Tie-breaker: Submission time (for private leaderboard ties)",
    "sections": {},
    "file_path": "kaggle_datasets/30/problem_summary.md"
  },
  "369": {
    "problem_id": "369",
    "title": "ImageNet Object Localization Challenge",
    "problem_type": "Computer Vision - Object Localization and Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# ImageNet Object Localization Challenge\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Localization and Classification\n* **Objective:**  \n    * Participants must identify and localize objects within images by predicting both the object class (from 1000 possible categories) and its bounding box coordinates (x_min, y_min, x_max, y_max).  \n    * The goal is to improve the accuracy of object detection systems, reducing classification and localization errors in complex scenes (e.g., overlapping objects or camouflaged subjects).  \n* **Key Points:**  \n    * Requires handling subtle visual differences and occlusions (e.g., overlapping bananas, camouflaged foxes).  \n    * Predictions must include up to 5 bounding boxes per image, each with a class label and coordinates.  \n    * Ground truth annotations follow PASCAL VOC format (XML files).  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Image data (JPEG) with 150,000 photographs from Flickr and search engines, annotated with 1000 non-overlapping object categories.  \n* **Data Files:**  \n    * `ILSVRC/`: Contains training/validation/test images and XML annotations.  \n    * `LOC_sample_submission.csv`: Submission template (ImageId, PredictionString).  \n    * `LOC_train_solution.csv`/`LOC_val_solution.csv`: Ground truth labels in CSV format.  \n    * `LOC_synset_mapping.txt`: Maps synset IDs (e.g., `n01440764`) to class descriptions (e.g., \"tench\").  \n* **Features:**  \n    * Images are organized by synset IDs (e.g., `n02123394_28.JPEG` corresponds to `n02123394_28.xml`).  \n    * Bounding box coordinates and class labels are provided for training/validation.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom error metric based on label matching and bounding box overlap.  \n    * For each image, the error is calculated as:  \n        ```  \n        e = min_i(min_j(max(d_ij, f_ij)))  \n        ```  \n        * `d_ij`: 0 if predicted and ground truth labels match, else 1.  \n        * `f_ij`: 0 if bounding box overlap ≥ 50% (IoU), else 1.  \n    * Final",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Localization and Classification\n* **Objective:**  \n    * Participants must identify and localize objects within images by predicting both the object class (from 1000 possible categories) and its bounding box coordinates (x_min, y_min, x_max, y_max).  \n    * The goal is to improve the accuracy of object detection systems, reducing classification and localization errors in complex scenes (e.g., overlapping objects or camouflaged subjects).  \n* **Key Points:**  \n    * Requires handling subtle visual differences and occlusions (e.g., overlapping bananas, camouflaged foxes).  \n    * Predictions must include up to 5 bounding boxes per image, each with a class label and coordinates.  \n    * Ground truth annotations follow PASCAL VOC format (XML files).  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Image data (JPEG) with 150,000 photographs from Flickr and search engines, annotated with 1000 non-overlapping object categories.  \n* **Data Files:**  \n    * `ILSVRC/`: Contains training/validation/test images and XML annotations.  \n    * `LOC_sample_submission.csv`: Submission template (ImageId, PredictionString).  \n    * `LOC_train_solution.csv`/`LOC_val_solution.csv`: Ground truth labels in CSV format.  \n    * `LOC_synset_mapping.txt`: Maps synset IDs (e.g., `n01440764`) to class descriptions (e.g., \"tench\").  \n* **Features:**  \n    * Images are organized by synset IDs (e.g., `n02123394_28.JPEG` corresponds to `n02123394_28.xml`).  \n    * Bounding box coordinates and class labels are provided for training/validation.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom error metric based on label matching and bounding box overlap.  \n    * For each image, the error is calculated as:  \n        ```  \n        e = min_i(min_j(max(d_ij, f_ij)))  \n        ```  \n        * `d_ij`: 0 if predicted and ground truth labels match, else 1.  \n        * `f_ij`: 0 if bounding box overlap ≥ 50% (IoU), else 1.  \n    * Final"
    },
    "file_path": "kaggle_datasets/369/problem_summary.md"
  },
  "190": {
    "problem_id": "190",
    "title": "Binary Classification for Insurance Claim Acceleration",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification for Insurance Claim Acceleration\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:**  \n  * Predict whether an insurance claim can be approved for accelerated processing (leading to faster payments) or requires additional information before approval.  \n  * The goal is to help BNP Paribas Cardif streamline claims management by identifying claims suitable for fast-tracking early in the process.  \n* **Key Points:**  \n  * Focus on early-stage claim features to make predictions.  \n  * All variables are anonymized, with no ordinal variables present.  \n\n## Dataset Overview  \n* **Data Type & Context:**  \n  * Tabular data containing anonymized insurance claim records with mixed categorical and numerical features.  \n* **Data Files:**  \n  * `train.csv` (includes target variable)  \n  * `test.csv` (excludes target variable)  \n  * `sample_submission.csv` (format reference)  \n* **Features:**  \n  * Categorical (string-type) and numerical variables captured at claim submission.  \n  * Target column (`target=1` indicates claims suitable for accelerated approval).  \n\n## Evaluation Metrics  \n* **Primary Metric:** Log Loss (Cross-Entropy Loss)  \n  * **Calculation:**  \n    * \\\\(logloss = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)]\\\\)  \n    * \\\\(N\\\\): Number of observations  \n    * \\\\(y_i\\\\): Binary target (0 or 1)  \n    * \\\\(p_i\\\\): Predicted probability of class 1  \n  * **Note:** Probabilities clipped to \\\\([10^{-15}, 1-10^{-15}]\\\\) to avoid undefined values.",
    "sections": {},
    "file_path": "kaggle_datasets/190/problem_summary.md"
  },
  "500": {
    "problem_id": "500",
    "title": "Financial Market Return Rate Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Financial Market Return Rate Prediction\n\n## Problem Description\n* **Problem Type:** Time Series Forecasting (Regression)\n* **Objective:** Predict future investment return rates using historical market data. The goal is to improve quantitative researchers' ability to forecast financial returns, enabling better investment decisions.\n* **Key Points:**\n  * Focuses on predicting an obfuscated target metric relevant for trading decisions\n  * Uses anonymized features derived from real market data\n  * Must avoid peeking forward in time (enforced via time-series API)\n  * Competition involves two phases: training period and real-market forecasting period\n\n## Dataset Overview\n* **Data Type & Context:** Tabular time-series data of anonymized financial market features\n* **Data Files:**\n  * `train.csv` - Primary training data with time_id, investment_id, target, and 300 anonymized features (f_0 to f_299)\n  * `example_test.csv` - Sample test data format\n  * `example_sample_submission.csv` - Submission format example\n  * `supplemental_train.csv` - Additional training data (initially noise, later replaced)\n* **Key Features:**\n  * 300 anonymized numerical features (f_0 to f_299) derived from market data\n  * Time-series structure with time_id and investment_id identifiers\n  * Target variable represents the return rate to predict\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Pearson correlation coefficient across all time IDs\n* **Implementation Details:**\n  * Submissions evaluated on correlation between predictions and true values for each time period\n  * Models must use provided Python time-series API to prevent lookahead\n  * Submissions with nulls/infinities or single prediction values are penalized\n  * Test set contains ~1 million rows served in time_id batches",
    "sections": {},
    "file_path": "kaggle_datasets/500/problem_summary.md"
  },
  "164": {
    "problem_id": "164",
    "title": "Predicting Property Hazard Scores for Insurance Inspections",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Property Hazard Scores for Insurance Inspections\n\n## Problem Description\n* **Problem Type:** Regression (predicting a continuous hazard score)\n* **Objective:** Predict a transformed count of hazards or pre-existing damages for residential properties using anonymized property data. The goal is to help Liberty Mutual Insurance identify high-risk homes that may require additional inspection before insuring.\n* **Key Points:**\n  * Hazard scores represent the sum of individual hazards found during property inspections.\n  * Models should use only pre-inspection data to predict hazard scores.\n  * The competition aims to improve risk assessment for home insurance underwriting.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing anonymized property features and hazard scores from home insurance inspections.\n* **Data Files:**\n  * train.csv - Contains hazard scores and predictor variables\n  * test.csv - Contains only predictor variables for prediction\n  * sample_submission.csv - Example submission file format\n* **Features:**\n  * All predictor variables are anonymized\n  * Target variable is \"Hazard\" (continuous score representing property condition)\n  * Features likely represent various property characteristics (though specific details are anonymized)\n\n## Evaluation Metrics\n* **Primary Metric:** Normalized Gini coefficient\n* **Metric Components:**\n  * Predictions are sorted from largest to smallest\n  * Measures how well the model accumulates observed loss in the highest-risk predictions\n  * Compares model performance against:\n    * Null model (straight line representing random accumulation)\n    * Perfect model (maximum achievable area under curve)\n  * Normalization divides model's Gini coefficient by perfect model's Gini coefficient\n  * Only the order of predictions matters, not their absolute values",
    "sections": {},
    "file_path": "kaggle_datasets/164/problem_summary.md"
  },
  "356": {
    "problem_id": "356",
    "title": "Predicting NFL Rushing Yards with Player Tracking Data",
    "problem_type": "Time Series Forecasting (with a focus on probabilistic regression)",
    "objective": "Predict the yardage gained on NFL rushing plays at the moment of handoff, using player tracking and contextual play data. The goal is to model the cumulative probability distribution of possible yardage outcomes (-99 to +99 yards) for each play.",
    "evaluation_metric": null,
    "full_content": "# Predicting NFL Rushing Yards with Player Tracking Data\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (with a focus on probabilistic regression)\n* **Objective:** Predict the yardage gained on NFL rushing plays at the moment of handoff, using player tracking and contextual play data. The goal is to model the cumulative probability distribution of possible yardage outcomes (-99 to +99 yards) for each play.\n* **Key Points:**\n  * Focuses exclusively on rushing plays (handoffs) in American football.\n  * Requires predicting a full probability distribution rather than a single yardage value.\n  * Uses time-series data with strict temporal ordering constraints to prevent look-ahead bias.\n  * Aims to provide better context for evaluating rushing play success beyond traditional metrics like yards per carry.\n\n**Dataset Overview:**\n* **Data Type:** Time-series tracking data (tabular format) with player movement metrics and play context.\n* **Context:** NFL player tracking data from Next Gen Stats, capturing real-time player positions, speeds, and accelerations during rushing plays.\n* **Data Files:**\n  * `train.csv` (primary dataset with tracking data and play outcomes)\n* **Key Features:**\n  * Player tracking: X/Y coordinates, speed (S), acceleration (A), orientation, direction\n  * Play context: down, distance, yard line, time remaining, score differential\n  * Team formations and personnel groupings\n  * Environmental factors: stadium type, turf, weather conditions\n  * Player demographics: position, height, weight, college\n\n**Evaluation Metrics:**\n* **Primary Metric:** Continuous Ranked Probability Score (CRPS)\n* **Metric Components:**\n  * Compares predicted cumulative distribution (CDF) against actual yardage outcome\n  * Calculated as sum of squared differences between predicted CDF and Heaviside step function of actual outcome\n  * Formula: \n    ```\n    C = 1/(199N) * Σ(plays) Σ(yardage) [P(y≤n) - H(n-Y)]²\n    ```\n    where P is predicted CDF, H is Heaviside function, Y is actual yardage\n  * Requires predicted probabilities to be non-decreasing (valid CDF)\n  * Evaluates predictions across all possible yardage outcomes (-99 to 99)",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (with a focus on probabilistic regression)\n* **Objective:** Predict the yardage gained on NFL rushing plays at the moment of handoff, using player tracking and contextual play data. The goal is to model the cumulative probability distribution of possible yardage outcomes (-99 to +99 yards) for each play.\n* **Key Points:**\n  * Focuses exclusively on rushing plays (handoffs) in American football.\n  * Requires predicting a full probability distribution rather than a single yardage value.\n  * Uses time-series data with strict temporal ordering constraints to prevent look-ahead bias.\n  * Aims to provide better context for evaluating rushing play success beyond traditional metrics like yards per carry.",
      "Dataset Overview": "* **Data Type:** Time-series tracking data (tabular format) with player movement metrics and play context.\n* **Context:** NFL player tracking data from Next Gen Stats, capturing real-time player positions, speeds, and accelerations during rushing plays.\n* **Data Files:**\n  * `train.csv` (primary dataset with tracking data and play outcomes)\n* **Key Features:**\n  * Player tracking: X/Y coordinates, speed (S), acceleration (A), orientation, direction\n  * Play context: down, distance, yard line, time remaining, score differential\n  * Team formations and personnel groupings\n  * Environmental factors: stadium type, turf, weather conditions\n  * Player demographics: position, height, weight, college",
      "Evaluation Metrics": "* **Primary Metric:** Continuous Ranked Probability Score (CRPS)\n* **Metric Components:**\n  * Compares predicted cumulative distribution (CDF) against actual yardage outcome\n  * Calculated as sum of squared differences between predicted CDF and Heaviside step function of actual outcome\n  * Formula: \n    ```\n    C = 1/(199N) * Σ(plays) Σ(yardage) [P(y≤n) - H(n-Y)]²\n    ```\n    where P is predicted CDF, H is Heaviside function, Y is actual yardage\n  * Requires predicted probabilities to be non-decreasing (valid CDF)\n  * Evaluates predictions across all possible yardage outcomes (-99 to 99)"
    },
    "file_path": "kaggle_datasets/356/problem_summary.md"
  },
  "538": {
    "problem_id": "538",
    "title": "Regression with a Tabular Gemstone Price Dataset",
    "problem_type": "Regression",
    "objective": "Predict the price of gemstones based on their features. The goal is to build a model that accurately estimates the `price` target variable using the provided tabular dataset.",
    "evaluation_metric": null,
    "full_content": "# Regression with a Tabular Gemstone Price Dataset\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the price of gemstones based on their features. The goal is to build a model that accurately estimates the `price` target variable using the provided tabular dataset.\n* **Key Points:**\n  * Dataset is synthetically generated from a deep learning model trained on real-world gemstone price data.\n  * Participants are encouraged to explore differences between synthetic and original data, and potentially incorporate the original dataset for improved performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing gemstone characteristics and their corresponding prices.\n* **Data Files:**\n  * `train.csv` - Training data with `price` as the target variable\n  * `test.csv` - Test data for which predictions need to be made\n  * `sample_submission.csv` - Example submission file format\n* **Features:** The dataset contains 23 columns (features) related to gemstone characteristics (specific features not listed in description, but presumably include physical properties like dimensions, color, clarity, etc.).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Metric Components:**\n  * RMSE = √(1/N * Σ(y_i - ŷ_i)²)\n  * Where:\n    * N = number of observations\n    * y_i = actual value\n    * ŷ_i = predicted value\n  * Lower RMSE values indicate better model performance.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the price of gemstones based on their features. The goal is to build a model that accurately estimates the `price` target variable using the provided tabular dataset.\n* **Key Points:**\n  * Dataset is synthetically generated from a deep learning model trained on real-world gemstone price data.\n  * Participants are encouraged to explore differences between synthetic and original data, and potentially incorporate the original dataset for improved performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing gemstone characteristics and their corresponding prices.\n* **Data Files:**\n  * `train.csv` - Training data with `price` as the target variable\n  * `test.csv` - Test data for which predictions need to be made\n  * `sample_submission.csv` - Example submission file format\n* **Features:** The dataset contains 23 columns (features) related to gemstone characteristics (specific features not listed in description, but presumably include physical properties like dimensions, color, clarity, etc.).",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Metric Components:**\n  * RMSE = √(1/N * Σ(y_i - ŷ_i)²)\n  * Where:\n    * N = number of observations\n    * y_i = actual value\n    * ŷ_i = predicted value\n  * Lower RMSE values indicate better model performance."
    },
    "file_path": "kaggle_datasets/538/problem_summary.md"
  },
  "37": {
    "problem_id": "37",
    "title": "Personality Prediction from Twitter Usage Data",
    "problem_type": "Multi-output Regression (predicting continuous personality trait scores)",
    "objective": "Predict seven personality traits (Machiavellianism, Narcissism, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) based on Twitter usage patterns and linguistic analysis.",
    "evaluation_metric": null,
    "full_content": "# Personality Prediction from Twitter Usage Data\n\n**Problem Description:**\n* **Problem Type:** Multi-output Regression (predicting continuous personality trait scores)\n* **Objective:** Predict seven personality traits (Machiavellianism, Narcissism, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) based on Twitter usage patterns and linguistic analysis.\n* **Key Points:**\n  * Focus on psychological trait prediction from social media behavior\n  * Includes both standard \"Big 5\" personality traits and \"Dark Triad\" traits\n  * Privacy-preserving approach - uses derived features rather than raw tweets\n  * Intended to study what personal information can be inferred from public social media activity\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with derived Twitter usage features and personality scores\n* **Context:** Anonymized dataset from Twitter users who completed personality assessments\n* **Data Files:**\n  * Personality_Traits_Trainingset_v1.csv (training data with labels)\n  * Personality_Traits_Testset_v1.csv (test data)\n  * DescriptiveStats.pdf (statistical summary)\n* **Features:**\n  * 337 anonymized variables derived from Twitter activity (categorized as AVar, LVar, FVar types)\n  * Includes linguistic analysis features and Twitter usage patterns\n  * Target variables are 7 continuous personality trait scores\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Columnwise Average Precision (custom adaptation of average precision for regression)\n* **Calculation Logic:**\n  1. For each trait column:\n     * Sort true scores in descending order of predicted values\n     * Compute cumulative sum of ordered true scores\n     * Divide by cumulative sum of ideally ordered true scores\n     * Calculate \"precision at n\" for each row\n  2. Average precision scores across all rows for each trait\n  3. Final score is average across all trait columns",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-output Regression (predicting continuous personality trait scores)\n* **Objective:** Predict seven personality traits (Machiavellianism, Narcissism, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) based on Twitter usage patterns and linguistic analysis.\n* **Key Points:**\n  * Focus on psychological trait prediction from social media behavior\n  * Includes both standard \"Big 5\" personality traits and \"Dark Triad\" traits\n  * Privacy-preserving approach - uses derived features rather than raw tweets\n  * Intended to study what personal information can be inferred from public social media activity",
      "Dataset Overview": "* **Data Type:** Tabular data with derived Twitter usage features and personality scores\n* **Context:** Anonymized dataset from Twitter users who completed personality assessments\n* **Data Files:**\n  * Personality_Traits_Trainingset_v1.csv (training data with labels)\n  * Personality_Traits_Testset_v1.csv (test data)\n  * DescriptiveStats.pdf (statistical summary)\n* **Features:**\n  * 337 anonymized variables derived from Twitter activity (categorized as AVar, LVar, FVar types)\n  * Includes linguistic analysis features and Twitter usage patterns\n  * Target variables are 7 continuous personality trait scores",
      "Evaluation Metrics": "* **Primary Metric:** Mean Columnwise Average Precision (custom adaptation of average precision for regression)\n* **Calculation Logic:**\n  1. For each trait column:\n     * Sort true scores in descending order of predicted values\n     * Compute cumulative sum of ordered true scores\n     * Divide by cumulative sum of ideally ordered true scores\n     * Calculate \"precision at n\" for each row\n  2. Average precision scores across all rows for each trait\n  3. Final score is average across all trait columns"
    },
    "file_path": "kaggle_datasets/37/problem_summary.md"
  },
  "351": {
    "problem_id": "351",
    "title": "Binary Classification with Categorical Feature Encoding",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of a binary target variable using a dataset composed entirely of categorical features. The primary challenge is to explore and compare different encoding strategies for categorical variables to maximize predictive performance.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Categorical Feature Encoding\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary target variable using a dataset composed entirely of categorical features. The primary challenge is to explore and compare different encoding strategies for categorical variables to maximize predictive performance.\n    * **Key Points:**\n        * Dataset contains **only** categorical features, including:\n            * Binary features (`bin_*`)\n            * Low- and high-cardinality nominal features (`nom_*`)\n            * Low- and high-cardinality ordinal features (`ord_*`)\n            * Potentially cyclical features (day of week, month)\n        * String ordinal features (`ord_{3-5}`) are lexically ordered according to `string.ascii_letters`.\n        * Simplified data conditions: no missing values, and test set contains no unseen feature values.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data consisting exclusively of categorical features for binary classification.\n* **Data Files:**\n    * `train.csv` - Training set\n    * `test.csv` - Test set for predictions\n    * `sample_submission.csv` - Example submission file\n* **Features:**\n    * Binary features (`bin_*`)\n    * Nominal features (`nom_*`) with varying cardinality\n    * Ordinal features (`ord_*`) with varying cardinality\n    * Cyclical temporal features (day, month)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between classes\n        * Evaluates predicted probabilities against observed target\n        * Higher values indicate better classification performance (perfect model = 1.0)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary target variable using a dataset composed entirely of categorical features. The primary challenge is to explore and compare different encoding strategies for categorical variables to maximize predictive performance.\n    * **Key Points:**\n        * Dataset contains **only** categorical features, including:\n            * Binary features (`bin_*`)\n            * Low- and high-cardinality nominal features (`nom_*`)\n            * Low- and high-cardinality ordinal features (`ord_*`)\n            * Potentially cyclical features (day of week, month)\n        * String ordinal features (`ord_{3-5}`) are lexically ordered according to `string.ascii_letters`.\n        * Simplified data conditions: no missing values, and test set contains no unseen feature values.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data consisting exclusively of categorical features for binary classification.\n* **Data Files:**\n    * `train.csv` - Training set\n    * `test.csv` - Test set for predictions\n    * `sample_submission.csv` - Example submission file\n* **Features:**\n    * Binary features (`bin_*`)\n    * Nominal features (`nom_*`) with varying cardinality\n    * Ordinal features (`ord_*`) with varying cardinality\n    * Cyclical temporal features (day, month)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between classes\n        * Evaluates predicted probabilities against observed target\n        * Higher values indicate better classification performance (perfect model = 1.0)"
    },
    "file_path": "kaggle_datasets/351/problem_summary.md"
  },
  "163": {
    "problem_id": "163",
    "title": "Cross-Device User Identification Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Cross-Device User Identification Challenge\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (with variable-length output classes)\n* **Objective**: Identify individual users across their digital devices by predicting which cookies belong to the same person using a given device. The task involves linking fragmented digital identities as users switch between devices (smartphones, laptops, tablets, etc.).\n* **Key Points**:\n  * Users are represented by non-personally-identifiable `drawbridge_handle` IDs in training data (missing in test set).\n  * Must predict space-delimited lists of associated `cookie_id`s for each test `device_id`.\n  * Real-world marketing application: Improving cross-device targeting while preserving privacy.\n\n## Dataset Overview\n* **Data Type**: Tabular relational data with device/cookie attributes, IP behavior logs, and website/app visitation records.\n* **Data Files**:\n  * `dev_train_basic.csv` / `dev_test_basic.csv` (device attributes)\n  * `cookie_all_basic.csv` (cookie attributes)\n  * `id_all_ip.csv` (IP-level behavior)\n  * `ipagg_all.csv` (IP aggregation stats)\n  * `id_all_property.csv` (website/app visits)\n  * `property_category.csv` (website/app categories)\n* **Key Features**:\n  * Device/cookie attributes: OS type, browser version, country, anonymized features (Boolean/Categorical/Integer)\n  * Behavioral data: IP addresses with frequency counts and 5 anonymized behavior metrics\n  * Property data: Visited websites/apps with category information\n  * Relational structure requiring joins on device/cookie IDs, IPs, and property IDs\n\n## Evaluation Metrics\n* **Primary Metric**: Mean F₀.₅ Score (micro-averaged)\n* **Components**:\n  * Weighted harmonic mean of precision (p) and recall (r) with β=0.5: \n    ```\n    F₀.₅ = (1+0.25)*p*r / (0.25*p + r)\n    ```\n  * Precision-weighted: Emphasizes correct predictions over completeness\n  * Calculated per test row (device) and averaged\n  * Submission format: Space-delimited cookie ID lists for each device",
    "sections": {},
    "file_path": "kaggle_datasets/163/problem_summary.md"
  },
  "507": {
    "problem_id": "507",
    "title": "AI Village Capture the Flag @ DEFCON",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# AI Village Capture the Flag @ DEFCON\n\n## Problem Description\n- **Problem Type**: Adversarial Machine Learning / Security Challenges (Capture the Flag)\n- **Objective**: Solve 22 unique machine learning security challenges by evading, poisoning, stealing, or fooling AI/ML systems to collect flags (128-character strings). Each challenge tests a different aspect of AI security.\n- **Key Points**:\n  - Challenges cover diverse topics: \n    * **Model Evasion**: Bypass defenses (e.g., `salt`, `waf`).\n    * **Data Poisoning**: Manipulate training data (`bad2good`).\n    * **Model Theft**: Extract sensitive information (`theft`, `leakage`).\n    * **Adversarial Attacks**: Modify inputs to deceive models (`honorstudent`, `deepfake`).\n    * **Forensics**: Analyze artifacts for hidden flags (`forensics`).\n  - Flags are unique per participant; sharing leads to disqualification.\n  - Submissions require solving challenges interactively (via APIs or local code).\n\n## Dataset Overview\n- **Data Type**: Mixed (API interactions, code objects, images, text, tabular data).\n- **Data Files**:\n  - `sample_submission.csv`: Template for flag submissions (columns: `challenge_id`, `flag`).\n  - Challenge-specific files (e.g., `bad2good/`, `crop/`, `deepfake/` folders with `.npy`, `.png`, `.py` files).\n- **Features**: \n  - Varies by challenge (e.g., image pixels for `honorstudent`, network requests for `waf`, model weights for `theft`).\n  - No centralized dataset; challenges are self-contained puzzles.\n\n## Evaluation Metrics\n- **Evaluation Metric**: Weighted Classification Accuracy (cumulative score out of 1.0).\n  - Each challenge has a difficulty weight; correct flags contribute proportionally.\n  - Final score = Sum of (weight × correctness) across all 22 challenges.\n- **Submission Format**: \n  - CSV with `challenge_id` and `flag` columns (exactly 22 rows).\n  - Flags are case-sensitive and unique to each participant.",
    "sections": {},
    "file_path": "kaggle_datasets/507/problem_summary.md"
  },
  "197": {
    "problem_id": "197",
    "title": "Predicting Check-Ins in a Simulated Geographic Environment",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Check-Ins in a Simulated Geographic Environment  \n\n## Problem Description  \n- **Problem Type:** Multi-class Classification (Ranking)  \n- **Objective:** Predict the most likely places (businesses) a user would check into based on location data (coordinates, accuracy, timestamp). Participants must return a ranked list of up to 3 place IDs for each check-in event.  \n  - **Key Points:**  \n    - Simulated artificial world with 100,000+ places in a 10 km × 10 km area.  \n    - Data mimics noisy real-world mobile device signals (inconsistent accuracy/timestamps).  \n    - Focus on ranking quality, not just single-label prediction.  \n\n## Dataset Overview  \n- **Data Type & Context:** Tabular data representing fabricated check-in events with geographic and temporal features.  \n- **Data Files:**  \n  - `train.csv`, `test.csv`: Contain `row_id`, coordinates (`x`, `y`), `accuracy`, `time`, and target `place_id`.  \n  - `sample_submission.csv`: Example submission format (space-delimited ranked predictions).  \n- **Key Features:**  \n  - Coordinates (`x`, `y`): Spatial data in a 10 km² grid.  \n  - `accuracy`: Noisy location accuracy metric (definition intentionally vague).  \n  - `time`: Timestamp (exact meaning undisclosed).  \n\n## Evaluation Metrics  \n- **Primary Metric:** Mean Average Precision @3 (**MAP@3**).  \n  - **Components:**  \n    - For each check-in event (`row_id`), precision is calculated at each cutoff (1, 2, 3) in the predicted ranked list.  \n    - Final score averages precision across all events.  \n    - Formula:  \n      ```  \n      MAP@3 = (1/|U|) * Σ[u=1 to |U|] Σ[k=1 to min(3,n)] P(k)  \n      ```  \n      Where `|U|` = total events, `P(k)` = precision at cutoff `k`, `n` = number of predicted places.",
    "sections": {},
    "file_path": "kaggle_datasets/197/problem_summary.md"
  },
  "358": {
    "problem_id": "358",
    "title": "Santa 2019 - Revenge of the Accountants - Workshop Scheduling Optimization",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Santa 2019 - Revenge of the Accountants - Workshop Scheduling Optimization\n\n## Problem Description\n* **Problem Type:** Optimization (Combinatorial Scheduling)\n* **Objective:** Minimize Santa's total penalty cost for suboptimal workshop scheduling by assigning families to specific days while adhering to occupancy constraints. The goal is to balance:\n  * Family preference satisfaction (minimizing consolation gift costs)\n  * Accounting penalties (complex empirical formula based on daily occupancy patterns)\n* **Key Points:**\n  * **Hard Constraints:**\n    * Daily occupancy must be between 125-300 people (else submission errors)\n    * Each family must be assigned exactly one day\n  * **Soft Constraints:**\n    * Preference costs scale with how far assigned days deviate from families' top 10 choices\n    * Accounting penalty depends on:\n      * Daily occupancy levels (𝑁𝑑)\n      * Differences between current day's occupancy and previous 5 days' occupancies (𝑁𝑑+𝑗)\n\n## Dataset Overview\n* **Data Type:** Tabular data (family preferences and sizes)\n* **Context:** North Pole workshop scheduling with 1,000 additional families compared to previous competition\n* **Data Files:**\n  * `family_data.csv`: Contains family IDs, number of members (`n_people`), and 10 preferred days (`choice_0` to `choice_9`)\n  * `sample_submission.csv`: Submission format template\n* **Key Features:**\n  * Family ID (unique identifier)\n  * `n_people` (integer, family size)\n  * 10 preference columns (`choice_0`=most preferred to `choice_9`=least preferred)\n\n## Evaluation Metrics\n* **Primary Metric:** Total Penalty Score = `preference_cost` + `accounting_penalty`\n* **Components:**\n  * **Preference Cost:**\n    * Hierarchical consolation gifts based on how far assigned day is from preferences:\n      * `choice_0`: $0\n      * `choice_1`: $50\n      * `choice_2`: $50 + ($9 × n_people)\n      * ... up to `otherwise`: $500 + ($36 × n_people) + ($398 × n_people)\n  * **Accounting Penalty:**\n    * Complex empirical formula: \n      ```\n      ∑(from d=100 to 1)∑(j=1 to 5)[(",
    "sections": {},
    "file_path": "kaggle_datasets/358/problem_summary.md"
  },
  "531": {
    "problem_id": "531",
    "title": "Binary Classification with a Tabular Credit Card Fraud Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Tabular Credit Card Fraud Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:**  \n  * Predict the probability of credit card fraud (target variable `Class`) for each transaction in the test set.  \n  * The dataset simulates real-world fraud detection, where the goal is to distinguish fraudulent transactions (Class=1) from legitimate ones (Class=0).  \n* **Key Points:**  \n  * Dataset is synthetically generated from the original [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) dataset using deep learning.  \n  * Participants are encouraged to explore differences between synthetic and original data and test whether incorporating the original data improves performance.  \n\n## Dataset Overview  \n* **Data Type & Context:**  \n  * Tabular data representing credit card transactions with anonymized numerical features.  \n  * Features likely include transaction amounts, timestamps, and other engineered attributes (exact names anonymized).  \n* **Data Files:**  \n  * `train.csv`: Contains features and the binary target `Class`.  \n  * `test.csv`: Contains features only; participants must predict `Class`.  \n  * `sample_submission.csv`: Example submission file with `id` and predicted probabilities.  \n\n## Evaluation Metrics  \n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC).  \n  * Measures the model's ability to rank fraudulent transactions higher than legitimate ones.  \n  * Ranges from 0 to 1, where 1 indicates perfect discrimination.  \n* **Submission Format:**  \n  * Requires predicted probabilities (not binary labels) for the `Class` column in the test set.",
    "sections": {},
    "file_path": "kaggle_datasets/531/problem_summary.md"
  },
  "367": {
    "problem_id": "367",
    "title": "COVID-19 Cumulative Case and Fatality Forecasting in California",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict the *cumulative* number of confirmed COVID-19 cases and fatalities in California for future dates (March 26 - April 23, 2020). The primary goal is not just accurate forecasting but identifying factors impacting COVID-19 transmission rates.",
    "evaluation_metric": null,
    "full_content": "# COVID-19 Cumulative Case and Fatality Forecasting in California\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict the *cumulative* number of confirmed COVID-19 cases and fatalities in California for future dates (March 26 - April 23, 2020). The primary goal is not just accurate forecasting but identifying factors impacting COVID-19 transmission rates.\n* **Key Points:**\n  * Focus on California-specific data (subset of a global forecasting task).\n  * Encourages incorporation of external datasets (e.g., policy interventions, environmental factors) to explain transmission variability.\n  * Public and private leaderboard periods use different evaluation windows (public: March 12-25; private: March 26-April 23).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data tracking COVID-19 spread in California.\n* **Data Files:**\n  * `ca_train.csv`: Training data with historical cumulative cases/fatalities up to March 18, 2020.\n  * `ca_test.csv`: Dates for prediction (includes overlap with training data for public leaderboard).\n  * `ca_submission.csv`: Sample submission file format.\n* **Features:** Likely includes date, location (California), and cumulative counts of confirmed cases and fatalities (exact features not detailed, but sourced from JHU CSSE time series data).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n  * **Calculation:**  \n    * For each column (ConfirmedCases, Fatalities):  \n      $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}$$  \n      where \\(p_i\\) = predicted value, \\(a_i\\) = actual value, \\(n\\) = number of observations.\n    * Final score averages RMSLE across both columns.\n  * **Purpose:** Penalizes proportional errors equally across large and small values (suitable for cumulative counts).",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict the *cumulative* number of confirmed COVID-19 cases and fatalities in California for future dates (March 26 - April 23, 2020). The primary goal is not just accurate forecasting but identifying factors impacting COVID-19 transmission rates.\n* **Key Points:**\n  * Focus on California-specific data (subset of a global forecasting task).\n  * Encourages incorporation of external datasets (e.g., policy interventions, environmental factors) to explain transmission variability.\n  * Public and private leaderboard periods use different evaluation windows (public: March 12-25; private: March 26-April 23).",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data tracking COVID-19 spread in California.\n* **Data Files:**\n  * `ca_train.csv`: Training data with historical cumulative cases/fatalities up to March 18, 2020.\n  * `ca_test.csv`: Dates for prediction (includes overlap with training data for public leaderboard).\n  * `ca_submission.csv`: Sample submission file format.\n* **Features:** Likely includes date, location (California), and cumulative counts of confirmed cases and fatalities (exact features not detailed, but sourced from JHU CSSE time series data).",
      "Evaluation Metrics": "* **Primary Metric:** Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n  * **Calculation:**  \n    * For each column (ConfirmedCases, Fatalities):  \n      $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}$$  \n      where \\(p_i\\) = predicted value, \\(a_i\\) = actual value, \\(n\\) = number of observations.\n    * Final score averages RMSLE across both columns.\n  * **Purpose:** Penalizes proportional errors equally across large and small values (suitable for cumulative counts)."
    },
    "file_path": "kaggle_datasets/367/problem_summary.md"
  },
  "155": {
    "problem_id": "155",
    "title": "Predicting West Nile Virus Presence in Mosquitoes",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting West Nile Virus Presence in Mosquitoes\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict the presence of West Nile virus (WNV) in mosquitoes across Chicago, given weather, location, testing, and spraying data. The goal is to help public health officials efficiently allocate resources for mosquito control and virus prevention.\n- **Key Points:**\n  - Predictions are needed for specific combinations of time, location, and mosquito species.\n  - The presence of WNV is influenced by environmental factors like weather conditions and spraying efforts.\n  - The dataset includes satellite traps and historical spray data, which may affect mosquito populations and virus presence.\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing mosquito trap records, weather data, and spray GIS data from Chicago (2007-2014).\n- **Data Files:**\n  - `train.csv`, `test.csv`: Main datasets with trap records, including species, location, and WNV presence.\n  - `spray.csv`: GIS data of pesticide spraying efforts in 2011 and 2013.\n  - `weather.csv`: Weather conditions from NOAA during testing months.\n  - `mapdata_copyright_openstreetmap_contributors.rds/txt`: Map files for visualization.\n  - `sampleSubmission.csv`: Example submission file.\n- **Features:**\n  - Trap details: Species, block, street, latitude, longitude.\n  - Weather variables: Temperature, precipitation, humidity, etc.\n  - Spray data: Date, time, and coordinates of pesticide applications.\n\n## Evaluation Metrics\n- **Evaluation Metric:** Area Under the ROC Curve (AUC)\n  - **Components:**\n    - Predictions are probabilities (ranging from 0 to 1) of WNV presence.\n    - The ROC curve plots the true positive rate against the false positive rate at various thresholds.\n    - AUC measures the model's ability to distinguish between positive (WNV present) and negative (WNV absent) cases.",
    "sections": {},
    "file_path": "kaggle_datasets/155/problem_summary.md"
  },
  "393": {
    "problem_id": "393",
    "title": "Photo Slideshow Optimization with Tag-Based Transitions",
    "problem_type": "Optimization (Combinatorial Optimization)",
    "objective": "Arrange a collection of photos into an ordered slideshow that maximizes the \"interest factor\" of transitions between consecutive slides. The challenge involves:",
    "evaluation_metric": null,
    "full_content": "# Photo Slideshow Optimization with Tag-Based Transitions\n\n**Problem Description:**\n* **Problem Type:** Optimization (Combinatorial Optimization)\n* **Objective:** Arrange a collection of photos into an ordered slideshow that maximizes the \"interest factor\" of transitions between consecutive slides. The challenge involves:\n    * Handling two photo orientations (horizontal/vertical)\n    * Combining vertical photos into slides\n    * Optimizing sequence transitions based on tag relationships\n* **Key Points:**\n    * Slides can contain either:\n        * 1 horizontal photo (inherits its tags)\n        * 2 vertical photos (union of their tags)\n    * Each photo can be used at most once\n    * Must produce at least one slide\n\n**Dataset Overview:**\n* **Data Type:** Textual metadata describing photos and their tags\n* **Data Files:**\n    * Input text file (e.g., `d_pet_pictures.txt`) containing:\n        * Photo count (N)\n        * For each photo: Orientation (H/V), tag count, and tag strings\n    * Sample submission file (`sample_submission.txt`)\n* **Features:**\n    * Photo ID (integer)\n    * Orientation (H/V)\n    * Tags (1-100 lowercase alphanumeric strings per photo)\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Custom \"Interest Factor\" score calculated as:\n    * For each consecutive slide pair (Si, Si+1):\n        1. Common tags: |tags(Si) ∩ tags(Si+1)|\n        2. Unique to Si: |tags(Si) - tags(Si+1)|\n        3. Unique to Si+1: |tags(Si+1) - tags(Si)|\n    * Transition score = minimum of these three values\n    * Total score = sum of all transition scores\n* **Key Aspects:**\n    * Balances continuity (shared tags) with novelty (unique tags)\n    * Vertical photo combinations affect tag unions\n    * Order sensitivity (sequence optimization)",
    "sections": {
      "Problem Description": "* **Problem Type:** Optimization (Combinatorial Optimization)\n* **Objective:** Arrange a collection of photos into an ordered slideshow that maximizes the \"interest factor\" of transitions between consecutive slides. The challenge involves:\n    * Handling two photo orientations (horizontal/vertical)\n    * Combining vertical photos into slides\n    * Optimizing sequence transitions based on tag relationships\n* **Key Points:**\n    * Slides can contain either:\n        * 1 horizontal photo (inherits its tags)\n        * 2 vertical photos (union of their tags)\n    * Each photo can be used at most once\n    * Must produce at least one slide",
      "Dataset Overview": "* **Data Type:** Textual metadata describing photos and their tags\n* **Data Files:**\n    * Input text file (e.g., `d_pet_pictures.txt`) containing:\n        * Photo count (N)\n        * For each photo: Orientation (H/V), tag count, and tag strings\n    * Sample submission file (`sample_submission.txt`)\n* **Features:**\n    * Photo ID (integer)\n    * Orientation (H/V)\n    * Tags (1-100 lowercase alphanumeric strings per photo)",
      "Evaluation Metrics": "* **Evaluation Metric:** Custom \"Interest Factor\" score calculated as:\n    * For each consecutive slide pair (Si, Si+1):\n        1. Common tags: |tags(Si) ∩ tags(Si+1)|\n        2. Unique to Si: |tags(Si) - tags(Si+1)|\n        3. Unique to Si+1: |tags(Si+1) - tags(Si)|\n    * Transition score = minimum of these three values\n    * Total score = sum of all transition scores\n* **Key Aspects:**\n    * Balances continuity (shared tags) with novelty (unique tags)\n    * Vertical photo combinations affect tag unions\n    * Order sensitivity (sequence optimization)"
    },
    "file_path": "kaggle_datasets/393/problem_summary.md"
  },
  "199": {
    "problem_id": "199",
    "title": "Shelter Animal Outcome Prediction",
    "problem_type": "Multi-class Classification",
    "objective": "Predict the outcome status of shelter animals as they leave the animal center. The goal is to classify each animal into one of several possible outcome categories to help shelters improve animal welfare and resource allocation.",
    "evaluation_metric": null,
    "full_content": "# Shelter Animal Outcome Prediction\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the outcome status of shelter animals as they leave the animal center. The goal is to classify each animal into one of several possible outcome categories to help shelters improve animal welfare and resource allocation.\n    * **Key Points:**\n        * Focus on improving shelter animal welfare through predictive modeling\n        * Dataset intended to help identify animals needing extra assistance in finding homes\n        * Encourages publication of insights for public benefit\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing intake records from Austin Animal Center (October 2013 - March 2016)\n* **Data Files:**\n    * train.csv - training set with features and outcome labels\n    * test.csv - test set for making predictions\n    * sample_submission.csv - example submission format\n* **Key Features:** Includes animal characteristics such as:\n    * Breed\n    * Color\n    * Sex\n    * Age\n    * Animal ID (unique identifier)\n    * Intake information\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * Calculated as: -1/N * Σ(Σ(y_ij * log(p_ij))) where:\n            * N = number of animals in test set\n            * M = number of outcome classes\n            * y_ij = 1 if observation i is in class j, 0 otherwise\n            * p_ij = predicted probability observation i belongs to class j\n        * Probabilities are rescaled (each row divided by row sum)\n        * Predicted probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the outcome status of shelter animals as they leave the animal center. The goal is to classify each animal into one of several possible outcome categories to help shelters improve animal welfare and resource allocation.\n    * **Key Points:**\n        * Focus on improving shelter animal welfare through predictive modeling\n        * Dataset intended to help identify animals needing extra assistance in finding homes\n        * Encourages publication of insights for public benefit",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing intake records from Austin Animal Center (October 2013 - March 2016)\n* **Data Files:**\n    * train.csv - training set with features and outcome labels\n    * test.csv - test set for making predictions\n    * sample_submission.csv - example submission format\n* **Key Features:** Includes animal characteristics such as:\n    * Breed\n    * Color\n    * Sex\n    * Age\n    * Animal ID (unique identifier)\n    * Intake information",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * Calculated as: -1/N * Σ(Σ(y_ij * log(p_ij))) where:\n            * N = number of animals in test set\n            * M = number of outcome classes\n            * y_ij = 1 if observation i is in class j, 0 otherwise\n            * p_ij = predicted probability observation i belongs to class j\n        * Probabilities are rescaled (each row divided by row sum)\n        * Predicted probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes"
    },
    "file_path": "kaggle_datasets/199/problem_summary.md"
  },
  "509": {
    "problem_id": "509",
    "title": "Book Sales Forecasting Across Countries and Stores",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict book sales for a full year (2021) across six countries, two competing stores, and four different book items. The challenge involves forecasting under non-standard conditions (tumultuous year 2021).",
    "evaluation_metric": null,
    "full_content": "# Book Sales Forecasting Across Countries and Stores\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict book sales for a full year (2021) across six countries, two competing stores, and four different book items. The challenge involves forecasting under non-standard conditions (tumultuous year 2021).\n* **Key Points:**\n  * Multi-dimensional forecasting: Must account for country, store, and item combinations.\n  * Real-world effects present in data: weekend/holiday effects, seasonality.\n  * Public leaderboard scored on Q1 test data; Private on remaining quarters.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data of fictional book sales across stores/countries.\n* **Data Files:**\n  * `train.csv`: Historical sales data by date-country-store-item combinations\n  * `test.csv`: Dates to forecast (full year 2021)\n  * `sample_submission.csv`: Submission format template\n* **Features:** \n  * Date-based features (implied by time series nature)\n  * Categorical features: country, store, book item\n  * Target: `num_sold` (units sold)\n\n**Evaluation Metrics:**\n* **Primary Metric:** SMAPE (Symmetric Mean Absolute Percentage Error)\n  * Special case: SMAPE = 0 when both actual and predicted values are 0\n  * Formula: `SMAPE = (100%/n) * Σ(|F_t - A_t| / (|A_t| + |F_t|))` \n    * Where F_t = forecast, A_t = actual, n = number of observations",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict book sales for a full year (2021) across six countries, two competing stores, and four different book items. The challenge involves forecasting under non-standard conditions (tumultuous year 2021).\n* **Key Points:**\n  * Multi-dimensional forecasting: Must account for country, store, and item combinations.\n  * Real-world effects present in data: weekend/holiday effects, seasonality.\n  * Public leaderboard scored on Q1 test data; Private on remaining quarters.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data of fictional book sales across stores/countries.\n* **Data Files:**\n  * `train.csv`: Historical sales data by date-country-store-item combinations\n  * `test.csv`: Dates to forecast (full year 2021)\n  * `sample_submission.csv`: Submission format template\n* **Features:** \n  * Date-based features (implied by time series nature)\n  * Categorical features: country, store, book item\n  * Target: `num_sold` (units sold)",
      "Evaluation Metrics": "* **Primary Metric:** SMAPE (Symmetric Mean Absolute Percentage Error)\n  * Special case: SMAPE = 0 when both actual and predicted values are 0\n  * Formula: `SMAPE = (100%/n) * Σ(|F_t - A_t| / (|A_t| + |F_t|))` \n    * Where F_t = forecast, A_t = actual, n = number of observations"
    },
    "file_path": "kaggle_datasets/509/problem_summary.md"
  },
  "39": {
    "problem_id": "39",
    "title": "Online Product Sales Prediction",
    "problem_type": "Time Series Forecasting (Multi-step Regression)",
    "objective": "Predict the first 12 months of online sales for consumer products based on product features and advertising campaign data.",
    "evaluation_metric": null,
    "full_content": "# Online Product Sales Prediction\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (Multi-step Regression)\n* **Objective:** Predict the first 12 months of online sales for consumer products based on product features and advertising campaign data.\n    * Key Points:\n        * Focuses on monthly sales trajectory after product launch.\n        * Uses anonymized product features and campaign timing data.\n        * Requires predicting sales for all 12 months simultaneously.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing different consumer products with:\n    * Time-based features (ad campaign dates)\n    * Product features (quantitative and categorical)\n    * Monthly sales outcomes for training\n* **Data Files:**\n    * TrainingDataset.csv (contains sales outcomes)\n    * TestDataset.csv\n    * sample_submission_using_training_column_means.csv\n* **Key Features:**\n    * Outcome_M1 through Outcome_M12 (monthly sales)\n    * Date_1 (major campaign launch day)\n    * Date_2 (pre-release campaign day)\n    * Quan_x (quantitative variables)\n    * Cat_x (categorical variables, binary encoded)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Logarithmic Error (RMSLE) across all 12 prediction columns\n    * Calculation Components:\n        * Computes log difference between predicted (p_i) and actual (a_i) sales\n        * Adds 1 to avoid log(0)\n        * Averages squared errors across all predictions\n        * Takes square root of the mean\n        * Formula: \n            ε = √[1/n Σ(log(p_i+1) - log(a_i+1))²]",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (Multi-step Regression)\n* **Objective:** Predict the first 12 months of online sales for consumer products based on product features and advertising campaign data.\n    * Key Points:\n        * Focuses on monthly sales trajectory after product launch.\n        * Uses anonymized product features and campaign timing data.\n        * Requires predicting sales for all 12 months simultaneously.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing different consumer products with:\n    * Time-based features (ad campaign dates)\n    * Product features (quantitative and categorical)\n    * Monthly sales outcomes for training\n* **Data Files:**\n    * TrainingDataset.csv (contains sales outcomes)\n    * TestDataset.csv\n    * sample_submission_using_training_column_means.csv\n* **Key Features:**\n    * Outcome_M1 through Outcome_M12 (monthly sales)\n    * Date_1 (major campaign launch day)\n    * Date_2 (pre-release campaign day)\n    * Quan_x (quantitative variables)\n    * Cat_x (categorical variables, binary encoded)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Logarithmic Error (RMSLE) across all 12 prediction columns\n    * Calculation Components:\n        * Computes log difference between predicted (p_i) and actual (a_i) sales\n        * Adds 1 to avoid log(0)\n        * Averages squared errors across all predictions\n        * Takes square root of the mean\n        * Formula: \n            ε = √[1/n Σ(log(p_i+1) - log(a_i+1))²]"
    },
    "file_path": "kaggle_datasets/39/problem_summary.md"
  },
  "394": {
    "problem_id": "394",
    "title": "Instance Segmentation on Open Images Dataset (RVC 2020)",
    "problem_type": "Computer Vision - Instance Segmentation",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Instance Segmentation on Open Images Dataset (RVC 2020)\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Instance Segmentation\n* **Objective:**  \n  Participants are tasked with generating precise segmentation masks for object instances in images, identifying both the object class and its pixel-level boundaries. The goal is to predict masks for 300 predefined object categories across diverse images.\n* **Key Points:**\n  * Part of the Robust Vision Challenge (RVC) 2020, emphasizing robustness across multiple datasets.\n  * Focus on high-quality mask generation, with validation/test sets manually annotated for accuracy.\n  * Submission requires encoded masks (RLE + compression + base64) with confidence scores.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  RGB images with instance-level segmentation masks for 300 object categories. The dataset is large-scale (train: 2.1M masks, validation: 23k masks) and covers diverse real-world scenes.\n* **Data Files:**  \n  * `test.zip`: 99,999 test images (independent from Open Images public dataset).  \n  * Sample submission files (`sample_empty_submission.csv`, `sample_truncated_submission.csv`).  \n* **Features:**  \n  * Images vary in resolution and content (e.g., CC-licensed photos).  \n  * Masks are provided in COCO-style RLE format for training/validation.  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision (mAP) over 300 classes.  \n* **Components:**  \n  * Mask-to-mask matching (IoU-based) for precision/recall calculation.  \n  * Average precision computed per class, then averaged.  \n  * Uses COCO evaluation protocols with adaptations for segmentation (details [here](https://storage.googleapis.com/openimages/web/evaluation.html#instance_segmentation_eval)).  \n* **Submission Format:**  \n  CSV with `ImageID`, `ImageWidth`, `ImageHeight`, and `PredictionString` (containing `Label`, `Confidence`, and encoded `EncodedMask` for each detection).",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Instance Segmentation\n* **Objective:**  \n  Participants are tasked with generating precise segmentation masks for object instances in images, identifying both the object class and its pixel-level boundaries. The goal is to predict masks for 300 predefined object categories across diverse images.\n* **Key Points:**\n  * Part of the Robust Vision Challenge (RVC) 2020, emphasizing robustness across multiple datasets.\n  * Focus on high-quality mask generation, with validation/test sets manually annotated for accuracy.\n  * Submission requires encoded masks (RLE + compression + base64) with confidence scores.",
      "Dataset Overview": "* **Data Type & Context:**  \n  RGB images with instance-level segmentation masks for 300 object categories. The dataset is large-scale (train: 2.1M masks, validation: 23k masks) and covers diverse real-world scenes.\n* **Data Files:**  \n  * `test.zip`: 99,999 test images (independent from Open Images public dataset).  \n  * Sample submission files (`sample_empty_submission.csv`, `sample_truncated_submission.csv`).  \n* **Features:**  \n  * Images vary in resolution and content (e.g., CC-licensed photos).  \n  * Masks are provided in COCO-style RLE format for training/validation.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision (mAP) over 300 classes.  \n* **Components:**  \n  * Mask-to-mask matching (IoU-based) for precision/recall calculation.  \n  * Average precision computed per class, then averaged.  \n  * Uses COCO evaluation protocols with adaptations for segmentation (details [here](https://storage.googleapis.com/openimages/web/evaluation.html#instance_segmentation_eval)).  \n* **Submission Format:**  \n  CSV with `ImageID`, `ImageWidth`, `ImageHeight`, and `PredictionString` (containing `Label`, `Confidence`, and encoded `EncodedMask` for each detection)."
    },
    "file_path": "kaggle_datasets/394/problem_summary.md"
  },
  "152": {
    "problem_id": "152",
    "title": "Poker Hand Classification via Rule Induction",
    "problem_type": "Multiclass Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Poker Hand Classification via Rule Induction\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:**  \n    * Predict the poker hand category (from 10 possible classes) based on five given playing cards.  \n    * **Key Challenge:** The competition emphasizes *automatic rules induction* - participants must learn poker hand classification rules purely from data without using pre-defined poker hand heuristics or human knowledge.  \n    * **Nuances:**  \n        * The order of cards matters (e.g., 480 possible Royal Flush combinations instead of 4).  \n        * Straight flush and Royal flush classes are artificially oversampled in the training data (14.43x and 129.82x more frequent, respectively).  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Tabular data representing poker hands (5 cards per hand) from a standard 52-card deck.  \n* **Data Files:**  \n    * `train.csv` (25,010 hands with labels)  \n    * `test.csv` (1,000,000 hands without labels)  \n    * `sampleSubmission.csv` (submission format example)  \n* **Features:**  \n    * `S1-S5`: Suit of each card (1-4, ordinal: Hearts, Spades, Diamonds, Clubs)  \n    * `C1-C5`: Rank of each card (1-13, numerical: Ace=1, King=13)  \n    * **Label (`hand`)**: Integer (0-9) representing the poker hand category (e.g., 0=\"Nothing\", 9=\"Royal flush\").  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy (percentage of correctly classified hands).  \n* **Calculation:**  \n    * Simple ratio: `(Correct Predictions) / (Total Predictions)`.  \n    * No weighting or stratification by class.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:**  \n    * Predict the poker hand category (from 10 possible classes) based on five given playing cards.  \n    * **Key Challenge:** The competition emphasizes *automatic rules induction* - participants must learn poker hand classification rules purely from data without using pre-defined poker hand heuristics or human knowledge.  \n    * **Nuances:**  \n        * The order of cards matters (e.g., 480 possible Royal Flush combinations instead of 4).  \n        * Straight flush and Royal flush classes are artificially oversampled in the training data (14.43x and 129.82x more frequent, respectively).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Tabular data representing poker hands (5 cards per hand) from a standard 52-card deck.  \n* **Data Files:**  \n    * `train.csv` (25,010 hands with labels)  \n    * `test.csv` (1,000,000 hands without labels)  \n    * `sampleSubmission.csv` (submission format example)  \n* **Features:**  \n    * `S1-S5`: Suit of each card (1-4, ordinal: Hearts, Spades, Diamonds, Clubs)  \n    * `C1-C5`: Rank of each card (1-13, numerical: Ace=1, King=13)  \n    * **Label (`hand`)**: Integer (0-9) representing the poker hand category (e.g., 0=\"Nothing\", 9=\"Royal flush\").",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy (percentage of correctly classified hands).  \n* **Calculation:**  \n    * Simple ratio: `(Correct Predictions) / (Total Predictions)`.  \n    * No weighting or stratification by class."
    },
    "file_path": "kaggle_datasets/152/problem_summary.md"
  },
  "360": {
    "problem_id": "360",
    "title": "Predicting Early Childhood Learning Outcomes from Gameplay Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Early Childhood Learning Outcomes from Gameplay Data\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Ordinal)\n* **Objective:** Predict how many attempts a child will take to pass an in-game assessment in the PBS KIDS Measure Up! app, based on their anonymous gameplay history. The goal is to uncover relationships between educational media engagement and learning outcomes to improve game design.\n* **Key Points:**\n  * Predict `accuracy_group` (0-3) representing assessment performance:\n    * 3: Solved on first attempt\n    * 2: Solved on second attempt\n    * 1: Solved after ≥3 attempts\n    * 0: Never solved\n  * Focuses on five specific assessments (e.g., Bird Measurer, Cart Balancer)\n  * Test set contains truncated gameplay history up to a randomly selected assessment start event\n\n## Dataset Overview\n* **Data Type:** Tabular gameplay event data with JSON-formatted event parameters\n* **Context:** Anonymous interaction logs from an educational app for children ages 3-5, focused on STEM concepts (length, width, capacity, weight)\n* **Data Files:**\n  * `train.csv`/`test.csv`: Main gameplay events with metadata\n  * `specs.csv`: Event type specifications\n  * `train_labels.csv`: Ground truth labels for training assessments\n  * `sample_submission.csv`: Submission format example\n* **Key Features:**\n  * Event types (games, assessments, activities, clips)\n  * Event codes/counts/timestamps\n  * Game titles and educational worlds (Treetop City, Magma Peak, etc.)\n  * JSON-formatted `event_data` containing game-specific parameters\n\n## Evaluation Metrics\n* **Primary Metric:** Quadratic Weighted Kappa (measures agreement between predicted and actual ordinal outcomes)\n* **Metric Components:**\n  * Calculates agreement while accounting for chance\n  * Weights disagreements based on squared difference between categories\n  * Formula: \n    ```\n    κ = 1 - (∑w_i,j * O_i,j) / (∑w_i,j * E_i,j)\n    ```\n    where:\n    * O = observed agreement matrix\n    * E = expected agreement matrix\n    * w_i,j = (i-j)²/(N-1)² (weight matrix)",
    "sections": {},
    "file_path": "kaggle_datasets/360/problem_summary.md"
  },
  "536": {
    "problem_id": "536",
    "title": "Binary Classification with a Tabular Reservation Cancellation Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Tabular Reservation Cancellation Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict whether a hotel reservation will be canceled (`booking_status` = 1) or not (`booking_status` = 0) based on tabular data.  \n* **Key Points:**  \n  * Synthetic dataset generated from a deep learning model trained on real-world reservation cancellation data.  \n  * Participants may incorporate the original dataset for potential performance improvements.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular data representing hotel reservation records with anonymized features.  \n* **Data Files:**  \n  * `train.csv`: Contains features and target (`booking_status`).  \n  * `test.csv`: Contains features for prediction.  \n  * `sample_submission.csv`: Example submission format.  \n* **Features:** Likely includes reservation details (e.g., lead time, party size, booking channel), though specific features are anonymized.  \n\n## Evaluation Metrics  \n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n* **Components:**  \n  * Measures the model's ability to distinguish between canceled and non-canceled reservations.  \n  * Higher AUC indicates better classification performance (1.0 = perfect, 0.5 = random).",
    "sections": {},
    "file_path": "kaggle_datasets/536/problem_summary.md"
  },
  "596": {
    "problem_id": "596",
    "title": "3D Blood Vessel Segmentation in Human Kidney Scans",
    "problem_type": "Computer Vision - 3D Image Segmentation",
    "objective": "Develop a model to segment blood vessels in 3D HiP-CT scans of human kidneys, creating precise segmentation masks that can help researchers understand vascular structures.",
    "evaluation_metric": null,
    "full_content": "# 3D Blood Vessel Segmentation in Human Kidney Scans\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - 3D Image Segmentation\n* **Objective:** Develop a model to segment blood vessels in 3D HiP-CT scans of human kidneys, creating precise segmentation masks that can help researchers understand vascular structures.\n* **Key Points:**\n  * Focus on automating a currently manual process that takes experts 6+ months per dataset\n  * Must generalize across varying image quality due to evolving HiP-CT technology\n  * Aims to contribute to the Vasculature Common Coordinate Framework (VCCF) for cellular mapping\n  * Potential applications include simulating blood flow and studying vascular changes related to demographics\n\n**Dataset Overview:**\n* **Data Type:** 3D medical imaging (Hierarchical Phase-Contrast Tomography/HiP-CT scans)\n* **Context:** High-resolution scans of human kidney vasculature at multiple scales (1.4μm-50μm resolution)\n* **Data Files:**\n  * `train/{dataset}/images/` - TIFF slices of 3D kidney volumes\n  * `train/{dataset}/labels/` - Corresponding segmentation masks\n  * `test/{dataset}/images/` - Test set TIFF slices\n  * `train_rles.csv` - Run-length encoded training masks\n  * `sample_submission.csv` - Submission format example\n* **Features:**\n  * Multiple kidney datasets with different segmentation densities (dense vs sparse)\n  * Varying resolutions (5.2μm to 50μm)\n  * Different beamline sources\n  * Test sets include kidneys scanned at 25.14μm and 15.77μm (binned to lower resolutions)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Surface Dice with tolerance of 0.0\n  * Measures overlap between predicted and ground truth segmentation surfaces\n  * Particularly suited for evaluating thin structures like blood vessels\n  * Implementation available via competition-provided notebook\n* **Submission Format:**\n  * Run-length encoded (RLE) segmentation masks\n  * Format: `id,rle` where id is `{dataset}_{slice}`\n  * Empty masks represented as `1 0`",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - 3D Image Segmentation\n* **Objective:** Develop a model to segment blood vessels in 3D HiP-CT scans of human kidneys, creating precise segmentation masks that can help researchers understand vascular structures.\n* **Key Points:**\n  * Focus on automating a currently manual process that takes experts 6+ months per dataset\n  * Must generalize across varying image quality due to evolving HiP-CT technology\n  * Aims to contribute to the Vasculature Common Coordinate Framework (VCCF) for cellular mapping\n  * Potential applications include simulating blood flow and studying vascular changes related to demographics",
      "Dataset Overview": "* **Data Type:** 3D medical imaging (Hierarchical Phase-Contrast Tomography/HiP-CT scans)\n* **Context:** High-resolution scans of human kidney vasculature at multiple scales (1.4μm-50μm resolution)\n* **Data Files:**\n  * `train/{dataset}/images/` - TIFF slices of 3D kidney volumes\n  * `train/{dataset}/labels/` - Corresponding segmentation masks\n  * `test/{dataset}/images/` - Test set TIFF slices\n  * `train_rles.csv` - Run-length encoded training masks\n  * `sample_submission.csv` - Submission format example\n* **Features:**\n  * Multiple kidney datasets with different segmentation densities (dense vs sparse)\n  * Varying resolutions (5.2μm to 50μm)\n  * Different beamline sources\n  * Test sets include kidneys scanned at 25.14μm and 15.77μm (binned to lower resolutions)",
      "Evaluation Metrics": "* **Primary Metric:** Surface Dice with tolerance of 0.0\n  * Measures overlap between predicted and ground truth segmentation surfaces\n  * Particularly suited for evaluating thin structures like blood vessels\n  * Implementation available via competition-provided notebook\n* **Submission Format:**\n  * Run-length encoded (RLE) segmentation masks\n  * Format: `id,rle` where id is `{dataset}_{slice}`\n  * Empty masks represented as `1 0`"
    },
    "file_path": "kaggle_datasets/596/problem_summary.md"
  },
  "334": {
    "problem_id": "334",
    "title": "Generative Dog Image Creation with GANs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Generative Dog Image Creation with GANs\n\n## Problem Description\n* **Problem Type**: Generative Modeling (Computer Vision - Image Generation)\n* **Objective**: \n    * Train generative models (specifically GANs) to create realistic dog images.\n    * Submissions must consist of 10,000 original 64x64 RGB PNG images generated by the model (not modified from existing dog images).\n* **Key Points**:\n    * Strict anti-cheating measures: Submissions must be generated images (no alterations of existing dog images).\n    * Kernels-only competition with strict runtime/resource constraints (CPU/GPU limits, no internet access, no external data).\n    * Focus on advancing generative methods for potential applications in data augmentation.\n\n## Dataset Overview\n* **Data Type**: Image data (dog photographs)\n* **Context**: \n    * Uses the **Stanford Dogs Dataset** (provided as `all-dogs.zip`), containing labeled dog images across 120 breeds.\n    * Includes bounding box annotations (`Annotations.zip`).\n* **Data Files**:\n    * `all-dogs.zip`: Training images\n    * `Annotations.zip`: Class labels and bounding boxes\n* **Features**:\n    * RGB images of various resolutions (pre-processing to 64x64 required for submission)\n    * Breed annotations available for training but not required for submission\n\n## Evaluation Metrics\n* **Primary Metric**: MiFID (Memorization-informed Fréchet Inception Distance)\n    * Modification of FID that penalizes memorization of training samples\n* **Metric Components**:\n    * **Fréchet Distance Calculation**:\n        * Uses Inception network features to compare generated vs. real image distributions\n        * Computes: FID = ||μ₁ - μ₂||² + Tr(Σ₁ + Σ₂ - 2(Σ₁Σ₂)^½\n    * **Memorization Term**:\n        * Measures minimum cosine distance between generated images and training set\n        * Thresholded penalty applied when distance is below empirical threshold\n    * **Final Score**: MiFID = FID × (1/d_thr) where d_thr is the thresholded memorization term\n* **Evaluation Process**:\n    * Public score uses Inception network and ImageNet Dogs\n    * Private score uses undisclosed model/dataset",
    "sections": {},
    "file_path": "kaggle_datasets/334/problem_summary.md"
  },
  "106": {
    "problem_id": "106",
    "title": "Flu Forecasting: Predicting Occurrence, Peak, and Severity",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Flu Forecasting: Predicting Occurrence, Peak, and Severity\n\n## Problem Description\n- **Problem Type**: Time Series Forecasting (with potential spatial and severity components)\n- **Objective**: Build an algorithm to predict three key aspects of influenza spread:\n  * **When** the flu will occur (temporal prediction)\n  * **Where** it will occur (spatial prediction)\n  * **What level of severity** it will reach (intensity prediction)\n- **Key Points**:\n  * Focuses on seasonal influenza epidemics in the United States\n  * Must account for rapid viral reproduction and short generation times\n  * Needs to handle multiple evolving influenza strains\n  * Challenge involves predicting both timing and magnitude of outbreaks\n\n## Dataset Overview\n- **Data Type**: Likely time series data with spatial components (specific files not listed)\n- **Context**: Public health data tracking influenza spread patterns\n- **Data Files**: (Not explicitly listed in provided context, but typical structure would include):\n  * Training data with historical flu patterns\n  * Test data for prediction\n  * Sample submission file\n- **Features**: (Not fully specified, but likely includes):\n  * Temporal features (week/month/season indicators)\n  * Spatial features (region/state identifiers)\n  * Severity indicators (hospitalization rates, case counts)\n\n## Evaluation Metrics\n- **Primary Metric**: Root Mean Square Logarithmic Error (RMSLE)\n- **Calculation**:\n  * 𝜀 = sqrt(1/N * Σ[log(p_i + 1) - log(a_i + 1)]^2)\n  * Where:\n    - p = predicted value\n    - a = actual value\n    - log = natural logarithm\n  * +1 terms prevent undefined log(0) cases\n  * Penalizes proportional errors equally across scales",
    "sections": {},
    "file_path": "kaggle_datasets/106/problem_summary.md"
  },
  "99": {
    "problem_id": "99",
    "title": "Multi-label Classification of Weather-related Tweets",
    "problem_type": "Multi-label Classification (with confidence scores)",
    "objective": "Predict confidence scores for 24 labels across three categories in weather-related tweets:",
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Weather-related Tweets\n\n**Problem Description:**\n* **Problem Type:** Multi-label Classification (with confidence scores)\n* **Objective:** Predict confidence scores for 24 labels across three categories in weather-related tweets:\n    * **Sentiment:** Positive, negative, neutral, unrelated, or unclear\n    * **When:** Past, present, future weather, or unclear\n    * **Kind:** Specific weather conditions (e.g., rain, snow, hot, humid) or unclear\n* **Key Points:**\n    * Requires predicting confidence scores (not just binary labels) for each possible label.\n    * Human raters could select multiple \"kind\" labels but only one \"sentiment\" and \"when\" label per tweet.\n    * The confidence scores account for rater disagreement and individual rater trust (though trust levels are not provided in the data).\n\n**Dataset Overview:**\n* **Data Type:** Text data (tweets) with associated metadata (locations) and multi-label annotations.\n* **Data Files:**\n    * `train.csv`: Contains tweets, locations, and confidence scores for all 24 labels.\n    * `test.csv`: Contains tweets for which predictions must be made.\n    * `sampleSubmission.csv`: Example submission file format.\n    * `variableNames.txt`: Describes the 24 label categories.\n* **Features:**\n    * Primary feature: Tweet text content.\n    * Additional feature: Location (if provided in the tweet).\n    * Labels: 24 confidence scores (3 sentiment, 4 \"when\", 15 \"kind\" labels).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Root Mean Squared Error (RMSE)\n    * **Components:**\n        * \\( n \\): 24 times the total number of tweets (i.e., total number of label predictions).\n        * \\( p_i \\): Predicted confidence score for a label.\n        * \\( a_i \\): Actual confidence score for a label.\n    * **Calculation:** \n        \\[\n        \\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^n (p_i - a_i)^2}{n}}\n        \\]\n    * **Note:** The RMSE is computed across all 24 label predictions for each tweet, treating each label prediction as an individual data point.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Classification (with confidence scores)\n* **Objective:** Predict confidence scores for 24 labels across three categories in weather-related tweets:\n    * **Sentiment:** Positive, negative, neutral, unrelated, or unclear\n    * **When:** Past, present, future weather, or unclear\n    * **Kind:** Specific weather conditions (e.g., rain, snow, hot, humid) or unclear\n* **Key Points:**\n    * Requires predicting confidence scores (not just binary labels) for each possible label.\n    * Human raters could select multiple \"kind\" labels but only one \"sentiment\" and \"when\" label per tweet.\n    * The confidence scores account for rater disagreement and individual rater trust (though trust levels are not provided in the data).",
      "Dataset Overview": "* **Data Type:** Text data (tweets) with associated metadata (locations) and multi-label annotations.\n* **Data Files:**\n    * `train.csv`: Contains tweets, locations, and confidence scores for all 24 labels.\n    * `test.csv`: Contains tweets for which predictions must be made.\n    * `sampleSubmission.csv`: Example submission file format.\n    * `variableNames.txt`: Describes the 24 label categories.\n* **Features:**\n    * Primary feature: Tweet text content.\n    * Additional feature: Location (if provided in the tweet).\n    * Labels: 24 confidence scores (3 sentiment, 4 \"when\", 15 \"kind\" labels).",
      "Evaluation Metrics": "* **Evaluation Metric:** Root Mean Squared Error (RMSE)\n    * **Components:**\n        * \\( n \\): 24 times the total number of tweets (i.e., total number of label predictions).\n        * \\( p_i \\): Predicted confidence score for a label.\n        * \\( a_i \\): Actual confidence score for a label.\n    * **Calculation:** \n        \\[\n        \\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^n (p_i - a_i)^2}{n}}\n        \\]\n    * **Note:** The RMSE is computed across all 24 label predictions for each tweet, treating each label prediction as an individual data point."
    },
    "file_path": "kaggle_datasets/99/problem_summary.md"
  },
  "562": {
    "problem_id": "562",
    "title": "Predicting Student Performance from Game Play Logs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Student Performance from Game Play Logs\n\n## Problem Description\n* **Problem Type:** Binary Classification (Time Series)\n* **Objective:** Predict whether students will answer questions correctly during game-based learning sessions using real-time gameplay logs. The goal is to advance knowledge-tracing methods for educational games.\n    * **Key Points:**\n        * Focus on real-time prediction during gameplay (sequential data processing)\n        * Three question checkpoints at different game levels (0-4, 5-12, 13-22)\n        * Applications in improving educational game design and learning analytics dashboards\n        * Efficiency constraints (CPU-only, limited RAM) to encourage lightweight models\n\n## Dataset Overview\n* **Data Type:** Time series event logs from educational game interactions\n* **Context:** Logs from \"Jo Wilder\" educational game capturing player actions, game states, and question responses\n* **Data Files:**\n    * `train.csv` - Gameplay event logs (session_id, timestamps, event types, game state)\n    * `train_labels.csv` - Binary correctness labels for 18 questions per session\n    * `test.csv` - Hidden test set with same structure as training data\n* **Key Features:**\n    * Temporal features: `elapsed_time`, `hover_duration`\n    * Game interaction features: `event_name`, `level`, `room_coor_x/y`\n    * Game state features: `fullscreen`, `music`, `hq`\n    * Text context features: `text`, `fqid`, `room_fqid`\n\n## Evaluation Metrics\n* **Primary Metric:** Macro F1-Score\n    * Calculated across all question predictions\n    * Balanced measure considering both precision and recall\n* **Efficiency Metric (Bonus Prize):**\n    * Combined score considering both F1 performance and runtime:\n        ```\n        Efficiency = 1/(Benchmark - maxF1)*F1 + (1/32400)*RuntimeSeconds\n        ```\n    * Where:\n        * Benchmark = F1 of sample submission\n        * maxF1 = Best F1 on private leaderboard\n        * RuntimeSeconds = Execution time in seconds",
    "sections": {},
    "file_path": "kaggle_datasets/562/problem_summary.md"
  },
  "52": {
    "problem_id": "52",
    "title": "Wind Power Forecasting with 48-Hour Horizon for 7 Wind Farms",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Wind Power Forecasting with 48-Hour Horizon for 7 Wind Farms\n\n## Problem Description\n- **Problem Type:** Time Series Forecasting (Multi-step, Multi-site)\n- **Objective:** Predict hourly normalized wind power generation for 7 wind farms up to 48 hours ahead, using historical power measurements and meteorological forecasts.\n- **Key Points:**\n  - Operational forecasting scenario mimicking real-world conditions\n  - Two distinct data periods:\n    * Training: 2009/7/1 - 2010/12/31 (model development)\n    * Evaluation: 2011/1/1 - 2012/6/28 (operational simulation)\n  - Specific 48-hour forecasting windows repeated every 7 days in evaluation period\n  - Requires using only meteorological forecasts that would be available in real-time\n\n## Dataset Overview\n- **Data Type:** Time series (tabular) with meteorological forecasts\n- **Context:** Wind energy production forecasting with normalized power outputs\n- **Data Files:**\n  - `train.csv`: Timestamped hourly power measurements (7 wind farms)\n  - `windforecasts_wf[1-7].csv`: Wind forecast data per farm (zonal/meridional components, speed, direction)\n  - `benchmark.csv`: Example submission format with persistence forecasts\n- **Key Features:**\n  - Timestamp (date + hour)\n  - 7 normalized power output columns (wp1-wp7)\n  - Forecast lead time (hors)\n  - Wind components (u, v), speed (ws), and direction (wd)\n\n## Evaluation Metrics\n- **Primary Metric:** Root Mean Square Error (RMSE)\n- **Calculation:**\n  - Square root of average squared forecast errors\n  - All forecasts equally weighted in final score\n  - Evaluated across all 7 wind farms and all forecast horizons (1-48 hours)",
    "sections": {},
    "file_path": "kaggle_datasets/52/problem_summary.md"
  },
  "139": {
    "problem_id": "139",
    "title": "Multi-class Plankton Image Classification for Ocean Health Assessment",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Plankton Image Classification for Ocean Health Assessment\n\n## Problem Description\n* **Problem Type:** Multi-class Image Classification\n* **Objective:** Develop an algorithm to automatically classify microscopic plankton images into 121 distinct biological classes to assess ocean ecosystem health. The goal is to replace manual classification which is infeasible due to the scale of data (50M+ images).\n* **Key Points:**\n  * Plankton populations serve as critical indicators of ocean health\n  * Images contain challenging variations including:\n    * Multiple orientations of organisms\n    * Partial/decomposed organisms (\"whale snot\")\n    * Noisy/ambiguous images even experts struggle with\n    * \"Unknown\" class categories\n  * Classes represent scientifically meaningful groupings including:\n    * Taxonomic relationships\n    * Behavioral states\n    * Morphological similarities\n\n## Dataset Overview\n* **Data Type:** Microscopic grayscale plankton images (2048×2048 pixels cropped to regions of interest)\n* **Context:** Collected via underwater imaging system (ISIIS) in Straits of Florida\n* **Data Files:**\n  * train.zip - 30k labeled images organized in 121 class folders\n  * test.zip - Unlabeled images for prediction (plus ignored images not scored)\n  * sampleSubmission.csv - Submission format template\n  * plankton_identification.pdf - Class relationship diagram\n* **Features:**\n  * Single organism/entity per image\n  * Shadowgraph imagery makes size distance-invariant\n  * Contains full spectrum of image quality from blurry to clear\n  * Some partial organisms due to segmentation artifacts\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class Logarithmic Loss (logloss)\n* **Calculation:**\n  * For each image, submit probability distribution across all classes\n  * Formula: \n    ```\n    logloss = -1/N Σ(i=1 to N) Σ(j=1 to M) y_ij log(p_ij)\n    ```\n    Where:\n    * N = number of images\n    * M = number of classes\n    * y_ij = 1 if image i is class j, else 0\n    * p_ij = predicted probability image i is class j\n  * Technical notes:\n    * Probabilities rescaled to sum to 1 per image\n    * Clipped to [10^-15, 1-10^-15] to avoid log(0)",
    "sections": {},
    "file_path": "kaggle_datasets/139/problem_summary.md"
  },
  "565": {
    "problem_id": "565",
    "title": "Instance Segmentation of Kidney Microvasculature in Histology Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Instance Segmentation of Kidney Microvasculature in Histology Images\n\n## Problem Description\n* **Problem Type:** Computer Vision - Instance Segmentation  \n* **Objective:**  \n    * Segment instances of microvascular structures (capillaries, arterioles, and venules) from 2D PAS-stained histology images of healthy human kidney tissue.  \n    * The goal is to automate the identification of blood vessel arrangements to aid in building a Vascular Common Coordinate Framework (VCCF) for human cell mapping.  \n* **Key Points:**  \n    * Focus on three kidney regions: renal cortex, medulla, and papilla, each with distinct vascular structures.  \n    * Must distinguish target vessels (`blood_vessel`) from confounding structures (`glomerulus` and `unsure` annotations).  \n    * Part of a broader effort by HuBMAP to create a Human Reference Atlas (HRA) of cellular relationships.  \n\n## Dataset Overview  \n* **Data Type & Context:**  \n    * 512x512 TIFF image tiles extracted from Whole Slide Images (WSI) of PAS-stained kidney tissue.  \n    * Includes three datasets:  \n        * **Dataset 1:** Expert-reviewed annotations (training/public test).  \n        * **Dataset 2:** Sparse, non-expert-reviewed annotations (additional training data).  \n        * **Dataset 3:** Unannotated tiles from 9 WSIs for semi-supervised learning.  \n* **Data Files:**  \n    * `train/`, `test/`: Folders with TIFF images.  \n    * `polygons.jsonl`: JSONL file with polygonal masks for `blood_vessel`, `glomerulus`, and `unsure` structures.  \n    * `tile_meta.csv`: Tile-level metadata (WSI source, coordinates, dataset type).  \n    * `wsi_meta.csv`: Donor demographics (age, sex, BMI) and WSI details.  \n* **Key Features:**  \n    * Annotations include vessel type labels and polygon coordinates.  \n    * Tiles are derived from multiple kidney regions (cortex, medulla, papilla), each with distinct vascular patterns.  \n\n## Evaluation Metrics  \n* **Primary Metric:** Average Precision (AP) with IoU threshold of 0.6.  \n    * Adapted from the OpenImages Instance Segmentation Challenge.  \n* **Components:**  \n    * Predictions require:  \n        * Instance segmentation masks (",
    "sections": {},
    "file_path": "kaggle_datasets/565/problem_summary.md"
  },
  "101": {
    "problem_id": "101",
    "title": "Predicting Insurance Customer Churn",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that current insurance customers will churn (leave the company) within the next 12 months. The goal is to enable early intervention strategies to reduce customer attrition.",
    "evaluation_metric": null,
    "full_content": "# Predicting Insurance Customer Churn\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that current insurance customers will churn (leave the company) within the next 12 months. The goal is to enable early intervention strategies to reduce customer attrition.\n    * **Key Points:**\n        * Focuses on predicting both *if* and *when* customers will churn.\n        * Aims to support proactive customer retention efforts.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing insurance customer information (likely including demographics, policy details, and interaction history).\n    * **Data Files:** (Specific files not listed in provided context, but typically would include):\n        * `train.csv` - Labeled training data with churn status\n        * `test.csv` - Unlabeled data for prediction\n        * `sample_submission.csv` - Example submission format\n    * **Features:** Customer attributes relevant to churn prediction (exact features not specified, but common examples would include tenure, claim history, premium amounts, etc.)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between churners and non-churners\n        * Evaluates true positive rate vs false positive rate across all classification thresholds\n        * Higher AUC indicates better predictive performance (1.0 = perfect, 0.5 = random)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that current insurance customers will churn (leave the company) within the next 12 months. The goal is to enable early intervention strategies to reduce customer attrition.\n    * **Key Points:**\n        * Focuses on predicting both *if* and *when* customers will churn.\n        * Aims to support proactive customer retention efforts.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing insurance customer information (likely including demographics, policy details, and interaction history).\n    * **Data Files:** (Specific files not listed in provided context, but typically would include):\n        * `train.csv` - Labeled training data with churn status\n        * `test.csv` - Unlabeled data for prediction\n        * `sample_submission.csv` - Example submission format\n    * **Features:** Customer attributes relevant to churn prediction (exact features not specified, but common examples would include tenure, claim history, premium amounts, etc.)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between churners and non-churners\n        * Evaluates true positive rate vs false positive rate across all classification thresholds\n        * Higher AUC indicates better predictive performance (1.0 = perfect, 0.5 = random)"
    },
    "file_path": "kaggle_datasets/101/problem_summary.md"
  },
  "333": {
    "problem_id": "333",
    "title": "Kinship Verification from Facial Images",
    "problem_type": "Binary Classification (Computer Vision - Facial Kinship Verification)",
    "objective": "Determine whether two individuals are blood-related based solely on facial images. The goal is to build a model that can accurately classify image pairs as \"related\" (kin) or \"unrelated\" (non-kin).",
    "evaluation_metric": null,
    "full_content": "# Kinship Verification from Facial Images\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Computer Vision - Facial Kinship Verification)\n* **Objective:** Determine whether two individuals are blood-related based solely on facial images. The goal is to build a model that can accurately classify image pairs as \"related\" (kin) or \"unrelated\" (non-kin).\n    * **Key Points:**\n        * Focus on bridging the gap between facial image classification and familial markers (e.g., DNA results).\n        * Address limitations of existing kinship recognition databases (size and diversity).\n        * Develop a discriminant model capable of handling hidden factors affecting familial facial relationships.\n\n**Dataset Overview:**\n* **Data Type & Context:** Image data (facial photographs) organized by families and individuals, sourced from the Families In the Wild (FIW) database (publicly available celebrity images).\n    * **Data Files:**\n        * `train-faces.zip`: Training images organized by family (`FXXXX`) and individual (`MIDx`).\n        * `train.csv`/`train_relationships.csv`: Training labels indicating kinship relationships (not all individuals in a family are necessarily related).\n        * `test-faces.zip`: Test images of unknown individuals.\n        * `sample_submission.csv`: Submission template with `img_pair` (format: `image1-image2`) and `is_related` (predicted probability).\n    * **Features:**\n        * Image pairs (e.g., `abcdef-ghijkl` corresponds to `abcdef.jpg` and `ghijkl.jpg`).\n        * Labels indicate whether pairs share a blood relationship (1: related, 0: unrelated).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Area Under the ROC Curve (AUC).\n    * **Components:**\n        * ROC curve plots True Positive Rate (TPR) vs. False Positive Rate (FPR) across probability thresholds.\n        * AUC measures the model's ability to distinguish between related and unrelated pairs (higher AUC = better performance).\n        * Only a subset of pairs may be scored (details not specified in the overview).",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Computer Vision - Facial Kinship Verification)\n* **Objective:** Determine whether two individuals are blood-related based solely on facial images. The goal is to build a model that can accurately classify image pairs as \"related\" (kin) or \"unrelated\" (non-kin).\n    * **Key Points:**\n        * Focus on bridging the gap between facial image classification and familial markers (e.g., DNA results).\n        * Address limitations of existing kinship recognition databases (size and diversity).\n        * Develop a discriminant model capable of handling hidden factors affecting familial facial relationships.",
      "Dataset Overview": "* **Data Type & Context:** Image data (facial photographs) organized by families and individuals, sourced from the Families In the Wild (FIW) database (publicly available celebrity images).\n    * **Data Files:**\n        * `train-faces.zip`: Training images organized by family (`FXXXX`) and individual (`MIDx`).\n        * `train.csv`/`train_relationships.csv`: Training labels indicating kinship relationships (not all individuals in a family are necessarily related).\n        * `test-faces.zip`: Test images of unknown individuals.\n        * `sample_submission.csv`: Submission template with `img_pair` (format: `image1-image2`) and `is_related` (predicted probability).\n    * **Features:**\n        * Image pairs (e.g., `abcdef-ghijkl` corresponds to `abcdef.jpg` and `ghijkl.jpg`).\n        * Labels indicate whether pairs share a blood relationship (1: related, 0: unrelated).",
      "Evaluation Metrics": "* **Evaluation Metric:** Area Under the ROC Curve (AUC).\n    * **Components:**\n        * ROC curve plots True Positive Rate (TPR) vs. False Positive Rate (FPR) across probability thresholds.\n        * AUC measures the model's ability to distinguish between related and unrelated pairs (higher AUC = better performance).\n        * Only a subset of pairs may be scored (details not specified in the overview)."
    },
    "file_path": "kaggle_datasets/333/problem_summary.md"
  },
  "591": {
    "problem_id": "591",
    "title": "NFL Tackling Strategy and Performance Analysis",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NFL Tackling Strategy and Performance Analysis\n\n## Problem Description\n- **Problem Type**: Sports Analytics / Multi-faceted Evaluation (Metric Development, Predictive Modeling, and Coaching Insights)\n- **Objective**: Develop actionable metrics and insights to evaluate tackling tactics and performance in NFL games using player tracking data. The goal is to create novel statistics that can be used by teams to assess defensive strategies, player performance, and tackle effectiveness.\n- **Key Points**:\n  - Focus on **tackling dynamics**, including predicting tackle probability, location, and time.\n  - Evaluate **player and team performance** (e.g., yards saved, tackle value, missed tackles).\n  - Consider **tackle context** (e.g., solo vs. gang tackles, open field vs. trenches).\n  - **Three submission tracks**:\n    - Undergraduate track (students only).\n    - Metric track (focused on performance/strategy metrics).\n    - Coaching presentation track (data storytelling for coaches).\n\n## Dataset Overview\n- **Data Type**: Tabular and spatiotemporal tracking data from NFL games.\n- **Context**: Player tracking data from Weeks 1-9 of the 2022 NFL season, including position, speed, acceleration, and event annotations.\n- **Data Files**:\n  - `games.csv`: Game metadata (teams, scores, dates).\n  - `plays.csv`: Play-level details (down, distance, possession team).\n  - `players.csv`: Player attributes (position, height, weight).\n  - `tackles.csv`: Tackle outcomes (tackles, assists, forced fumbles).\n  - `tracking_week_[1-9].csv`: Player and ball tracking data per week.\n- **Key Features**:\n  - **Tracking data**: Player coordinates (`x`, `y`), speed (`s`), acceleration (`a`), orientation (`o`), direction (`dir`).\n  - **Play context**: Expected points, win probability, defensive formations.\n  - **Tackle outcomes**: Binary indicators for tackles, assists, and missed tackles.\n\n## Evaluation Metrics\n- **Evaluation Method**: Multi-criteria scoring by NFL analysts (weighted average of four components):\n  - **Football Score (30%)**:\n    - Practical utility for NFL teams.\n    - Handling of football-specific complexities.\n    - Uniqueness of insights.\n  - **Data Science Score (30%)**:\n    - Methodological correctness.\n    -",
    "sections": {},
    "file_path": "kaggle_datasets/591/problem_summary.md"
  },
  "55": {
    "problem_id": "55",
    "title": "Predicting Census Mail Return Rates at Block Group Level",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Census Mail Return Rates at Block Group Level\n\n## Problem Description\n* **Problem Type**: Regression\n* **Objective**: Develop a statistical model to predict census mail return rates at the Census block group level of geography. The model will be used by the Census Bureau for planning purposes in future decennial censuses and demographic surveys.\n* **Key Points**:\n  * Focus on improving predictive analytics for geographic units\n  * Model-based estimates will be publicly released in the Census \"planning database\"\n  * Only US citizens and residents were eligible for prizes\n\n## Dataset Overview\n* **Data Type**: Tabular data containing demographic characteristics at Census block group level\n* **Data Files**:\n  * Training set (training_filev1)\n  * Test set (test_filev1)\n* **Key Features**:\n  * Target variable: `Mail_Return_Rate_CEN_2010` (census mail return rate for each block group)\n  * Census tracts randomly assigned to Training, Public Leaderboard, or Private Leaderboard sets\n  * Note: Original files had issues with \"GIDBG\" which were corrected in v1 files\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Mean Absolute Error\n  * Weighted by `Tot_Population_CEN_2010`\n  * Calculation: \n    * Compute absolute errors between predicted and actual return rates\n    * Weight each error by the corresponding block group's total population\n    * Take the mean of these weighted absolute errors",
    "sections": {},
    "file_path": "kaggle_datasets/55/problem_summary.md"
  },
  "137": {
    "problem_id": "137",
    "title": "Sentiment Analysis on Movie Reviews",
    "problem_type": "NLP - Multi-class Classification (Sentiment Analysis)",
    "objective": "Classify the sentiment of phrases from movie reviews into one of five categories:",
    "evaluation_metric": null,
    "full_content": "# Sentiment Analysis on Movie Reviews\n\n**Problem Description:**\n* **Problem Type:** NLP - Multi-class Classification (Sentiment Analysis)\n* **Objective:** Classify the sentiment of phrases from movie reviews into one of five categories:\n  * 0 - Negative  \n  * 1 - Somewhat Negative  \n  * 2 - Neutral  \n  * 3 - Somewhat Positive  \n  * 4 - Positive  \n* **Key Points:**\n  * Focuses on fine-grained sentiment analysis (beyond binary positive/negative).\n  * Challenges include handling sentence negation, sarcasm, ambiguity, and terseness.\n  * Phrases are parsed from sentences using the Stanford parser, with each phrase labeled individually.\n\n**Dataset Overview:**\n* **Data Type & Context:** Text data (phrases from Rotten Tomatoes movie reviews) with sentiment labels.\n* **Data Files:**\n  * `train.tsv`: Contains phrases with corresponding sentiment labels and SentenceIds.\n  * `test.tsv`: Contains phrases to be labeled (no sentiment labels provided).\n  * `sampleSubmission.csv`: Example submission file format.\n* **Features:**\n  * `PhraseId`: Unique identifier for each phrase.\n  * `SentenceId`: Links phrases originating from the same sentence.\n  * `Phrase`: The text content to be classified.\n  * `Sentiment`: Label (only in training data).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Classification Accuracy (percentage of correctly predicted labels).\n* **Components:**\n  * Predictions are compared against ground truth labels (0-4) for each phrase.\n  * Score is calculated as: `(Number of Correct Predictions) / (Total Predictions) * 100`.",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Multi-class Classification (Sentiment Analysis)\n* **Objective:** Classify the sentiment of phrases from movie reviews into one of five categories:\n  * 0 - Negative  \n  * 1 - Somewhat Negative  \n  * 2 - Neutral  \n  * 3 - Somewhat Positive  \n  * 4 - Positive  \n* **Key Points:**\n  * Focuses on fine-grained sentiment analysis (beyond binary positive/negative).\n  * Challenges include handling sentence negation, sarcasm, ambiguity, and terseness.\n  * Phrases are parsed from sentences using the Stanford parser, with each phrase labeled individually.",
      "Dataset Overview": "* **Data Type & Context:** Text data (phrases from Rotten Tomatoes movie reviews) with sentiment labels.\n* **Data Files:**\n  * `train.tsv`: Contains phrases with corresponding sentiment labels and SentenceIds.\n  * `test.tsv`: Contains phrases to be labeled (no sentiment labels provided).\n  * `sampleSubmission.csv`: Example submission file format.\n* **Features:**\n  * `PhraseId`: Unique identifier for each phrase.\n  * `SentenceId`: Links phrases originating from the same sentence.\n  * `Phrase`: The text content to be classified.\n  * `Sentiment`: Label (only in training data).",
      "Evaluation Metrics": "* **Primary Metric:** Classification Accuracy (percentage of correctly predicted labels).\n* **Components:**\n  * Predictions are compared against ground truth labels (0-4) for each phrase.\n  * Score is calculated as: `(Number of Correct Predictions) / (Total Predictions) * 100`."
    },
    "file_path": "kaggle_datasets/137/problem_summary.md"
  },
  "305": {
    "problem_id": "305",
    "title": "Predicting Customer Loyalty Scores for Merchant Recommendations",
    "problem_type": "Regression",
    "objective": "Predict a loyalty score for each customer (`card_id`) to help Elo recommend personalized merchant discounts and promotions. The goal is to improve customer experience and merchant repeat business by identifying the most relevant opportunities for individuals.",
    "evaluation_metric": null,
    "full_content": "# Predicting Customer Loyalty Scores for Merchant Recommendations\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict a loyalty score for each customer (`card_id`) to help Elo recommend personalized merchant discounts and promotions. The goal is to improve customer experience and merchant repeat business by identifying the most relevant opportunities for individuals.\n    * **Key Points:**\n        * Focus on uncovering signals in customer transaction data that indicate loyalty.\n        * Personalization is critical to reduce unwanted campaigns and create the right customer experience.\n        * The competition involves predicting a continuous target variable (loyalty score) rather than discrete categories.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing customer transaction records and merchant information from a Brazilian payment brand (Elo).\n* **Data Files:**\n    * `train.csv` - Contains `card_id`s and loyalty scores (target) for training.\n    * `test.csv` - Contains `card_id`s for which predictions are required.\n    * `historical_transactions.csv` - Up to 3 months of transaction history per `card_id`.\n    * `new_merchant_transactions.csv` - Transactions at new merchants (not visited historically) over 2 months.\n    * `merchants.csv` - Additional information about each merchant.\n    * `sample_submission.csv` - Example submission file format.\n* **Features:**\n    * Card-specific features (e.g., first active month).\n    * Transaction details (e.g., purchase amount, merchant ID, purchase date).\n    * Merchant-level aggregated information.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:** \n        * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n        * Where y_i is the actual loyalty score and ŷ_i is the predicted loyalty score for each `card_id`.\n    * **Interpretation:** Lower RMSE indicates better performance, with perfect predictions yielding RMSE = 0.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict a loyalty score for each customer (`card_id`) to help Elo recommend personalized merchant discounts and promotions. The goal is to improve customer experience and merchant repeat business by identifying the most relevant opportunities for individuals.\n    * **Key Points:**\n        * Focus on uncovering signals in customer transaction data that indicate loyalty.\n        * Personalization is critical to reduce unwanted campaigns and create the right customer experience.\n        * The competition involves predicting a continuous target variable (loyalty score) rather than discrete categories.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing customer transaction records and merchant information from a Brazilian payment brand (Elo).\n* **Data Files:**\n    * `train.csv` - Contains `card_id`s and loyalty scores (target) for training.\n    * `test.csv` - Contains `card_id`s for which predictions are required.\n    * `historical_transactions.csv` - Up to 3 months of transaction history per `card_id`.\n    * `new_merchant_transactions.csv` - Transactions at new merchants (not visited historically) over 2 months.\n    * `merchants.csv` - Additional information about each merchant.\n    * `sample_submission.csv` - Example submission file format.\n* **Features:**\n    * Card-specific features (e.g., first active month).\n    * Transaction details (e.g., purchase amount, merchant ID, purchase date).\n    * Merchant-level aggregated information.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n    * **Calculation:** \n        * RMSE = √(1/n Σ(y_i - ŷ_i)²)\n        * Where y_i is the actual loyalty score and ŷ_i is the predicted loyalty score for each `card_id`.\n    * **Interpretation:** Lower RMSE indicates better performance, with perfect predictions yielding RMSE = 0."
    },
    "file_path": "kaggle_datasets/305/problem_summary.md"
  },
  "553": {
    "problem_id": "553",
    "title": "Bird Species Identification from Audio Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Bird Species Identification from Audio Recordings\n\n## Problem Description\n* **Problem Type:** Multiclass Classification (Audio)\n* **Objective:** Develop a machine learning model to identify Eastern African bird species by their calls in continuous audio recordings. The goal is to process passive acoustic monitoring data to assess avian biodiversity, particularly for conservation efforts in Kenya.\n    * **Key Points:**\n        * Focus on limited training data scenarios (need for reliable classifiers with sparse data)\n        * Real-world application in biodiversity monitoring and restoration project evaluation\n        * Requires processing long audio recordings (10-minute soundscapes) into 5-second prediction windows\n        * Geographic considerations (latitude/longitude metadata may indicate call 'dialects')\n\n## Dataset Overview\n* **Data Type & Context:** Audio recordings (bird calls) with associated metadata\n    * **Data Files:**\n        * `train_audio/`: 16,945 short recordings of individual bird calls (OGG format, 32kHz)\n        * `test_soundscapes/`: ~200 long field recordings (10 minutes each, OGG format)\n        * `train_metadata.csv`: Contains species labels (`primary_label`), geographic coordinates, and recording details\n        * `eBird_Taxonomy_v2021.csv`: Species relationship data\n        * `sample_submission.csv`: Submission template with 264 bird species columns\n    * **Key Features:**\n        * Audio spectrograms/features (not raw data)\n        * Geographic coordinates (latitude/longitude)\n        * 264 target bird species (multilabel prediction)\n\n## Evaluation Metrics\n* **Primary Metric:** Padded cmAP (macro-averaged average precision with padding)\n    * **Implementation Details:**\n        * Derived from scikit-learn's `average_precision_score` with macro averaging\n        * Padding adds 5 true positive rows per species to:\n            * Support species with zero true positives in test set\n            * Reduce impact of species with very few positives\n        * Predictions required for 5-second windows across all 264 species simultaneously",
    "sections": {},
    "file_path": "kaggle_datasets/553/problem_summary.md"
  },
  "97": {
    "problem_id": "97",
    "title": "Multi-label Bird Species Classification from Audio Recordings",
    "problem_type": "Multi-label Classification (Audio Signal Processing)",
    "objective": "Identify which of 87 classes of birds, amphibians, and insects are present in continuous wild sound recordings. Each audio clip may contain multiple species simultaneously.",
    "evaluation_metric": null,
    "full_content": "# Multi-label Bird Species Classification from Audio Recordings\n\n**Problem Description:**\n* **Problem Type:** Multi-label Classification (Audio Signal Processing)\n* **Objective:** Identify which of 87 classes of birds, amphibians, and insects are present in continuous wild sound recordings. Each audio clip may contain multiple species simultaneously.\n    * **Key Points:**\n        * Focus on bioacoustics - identifying species from their vocalizations in natural environments\n        * Includes both birds (primary focus) and non-bird species (7 insects + 1 batracian)\n        * Must handle polyphonic audio (multiple species vocalizing simultaneously)\n        * Empty background noise class in training data (not to be predicted)\n        * More complex version of previous ICML4B challenge (expanded species set)\n\n**Dataset Overview:**\n* **Data Type & Context:** Audio recordings (WAV files) of wild environments containing bird/animal vocalizations\n    * **Data Files:**\n        * Training set: 687 WAV files (~10 per species)\n        * Test set: 1000 WAV files\n        * Provided MFCC features (17 coefficients x N frames) for both sets\n    * **Key Features:**\n        * 44.1 kHz sample rate recordings\n        * Variable duration audio clips (histogram provided)\n        * Some species have separate labels for song vs. call vocalizations\n        * Baseline MFCC features provided (17 coefficients per frame)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Implementation Notes:**\n        * Calculated per-class, then averaged\n        * Standard implementations provided for MATLAB, R, and Python\n        * Submission requires probability predictions for all 87 classes per test file",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Classification (Audio Signal Processing)\n* **Objective:** Identify which of 87 classes of birds, amphibians, and insects are present in continuous wild sound recordings. Each audio clip may contain multiple species simultaneously.\n    * **Key Points:**\n        * Focus on bioacoustics - identifying species from their vocalizations in natural environments\n        * Includes both birds (primary focus) and non-bird species (7 insects + 1 batracian)\n        * Must handle polyphonic audio (multiple species vocalizing simultaneously)\n        * Empty background noise class in training data (not to be predicted)\n        * More complex version of previous ICML4B challenge (expanded species set)",
      "Dataset Overview": "* **Data Type & Context:** Audio recordings (WAV files) of wild environments containing bird/animal vocalizations\n    * **Data Files:**\n        * Training set: 687 WAV files (~10 per species)\n        * Test set: 1000 WAV files\n        * Provided MFCC features (17 coefficients x N frames) for both sets\n    * **Key Features:**\n        * 44.1 kHz sample rate recordings\n        * Variable duration audio clips (histogram provided)\n        * Some species have separate labels for song vs. call vocalizations\n        * Baseline MFCC features provided (17 coefficients per frame)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Implementation Notes:**\n        * Calculated per-class, then averaged\n        * Standard implementations provided for MATLAB, R, and Python\n        * Submission requires probability predictions for all 87 classes per test file"
    },
    "file_path": "kaggle_datasets/97/problem_summary.md"
  },
  "108": {
    "problem_id": "108",
    "title": "Predicting Malfunctional Components in ASUS Notebooks",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Malfunctional Components in ASUS Notebooks\n\n## Problem Description\n- **Problem Type:** Time Series Forecasting (Regression)\n- **Objective:** Predict the monthly repair volume for specific notebook components (module-component pairs) from January 2010 to July 2011, using historical sales and repair data.\n- **Key Points:**\n  - Focuses on forecasting repair demand for maintenance planning\n  - Requires predicting 19 monthly values for each module-component combination\n  - Uses anonymized product data (models M1-M9, components P1-P31)\n  - Combines sales data (2005-2008) with repair logs (2005-2009)\n\n## Dataset Overview\n- **Data Type:** Tabular time series data (sales and repair logs)\n- **Context:** ASUS notebook component sales and repair history\n- **Data Files:**\n  - `SaleTrain.csv`: Monthly sales records (module, component, date, sales count)\n  - `RepairTrain.csv`: Repair logs (module, component, sale date, repair date, repair count)\n  - `Output_TargetID_Mapping.csv`: Defines prediction targets (module-component-month combinations)\n  - `SampleSubmission.csv`: Submission format template\n- **Key Features:**\n  - Module and component identifiers (categorical)\n  - Temporal features (year/month of sale and repair)\n  - Count features (number_sale, number_repair)\n\n## Evaluation Metrics\n- **Primary Metric:** Mean Absolute Error (MAE)\n- **Calculation:**\n  - MAE = (1/n) * Σ|y_i - ŷ_i|\n  - Where:\n    - n = number of predictions\n    - y_i = actual repair count\n    - ŷ_i = predicted repair count\n- **Implementation:**\n  - Evaluates accuracy of monthly repair volume predictions\n  - Penalizes absolute deviations equally across all predictions",
    "sections": {},
    "file_path": "kaggle_datasets/108/problem_summary.md"
  },
  "598": {
    "problem_id": "598",
    "title": "Predicting Stock Closing Price Movements Using Auction and Order Book Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Stock Closing Price Movements Using Auction and Order Book Data\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting (Financial Markets)\n* **Objective**: Develop a model to predict 60-second future price movements of Nasdaq-listed stocks relative to a synthetic index, using order book and closing auction data. The goal is to forecast the target variable representing the basis point difference between stock WAP (Weighted Average Price) movement and index WAP movement.\n* **Key Points**:\n  * Focuses on the critical last 10 minutes of trading (Nasdaq Closing Cross auction)\n  * Requires handling high-frequency financial data with temporal dependencies\n  * Must predict relative price movements (stock vs. index) rather than absolute prices\n  * Uses a custom synthetic index constructed by Optiver as benchmark\n  * Target is measured in basis points (0.01% increments)\n\n## Dataset Overview\n* **Data Type**: Time-series tabular data (financial market microstructure)\n* **Context**: Nasdaq stock exchange closing auction data with order book information\n* **Data Files**:\n  * train.csv - Contains historical auction data with target values\n  * test.csv - Delivered via API during competition\n  * revealed_targets.csv - Provides true target values for previous date\n  * public_timeseries_testing_util.py - Helper for offline API testing\n* **Key Features**:\n  * Auction dynamics: imbalance_size, imbalance_buy_sell_flag, reference_price, matched_size\n  * Price levels: far_price, near_price, bid/ask prices\n  * Size information: bid/ask sizes, matched_size\n  * Temporal marker: seconds_in_bucket (auction timeline)\n  * Market indicators: wap (weighted average price)\n  * Identifiers: stock_id, date_id\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE)\n  * Formula: MAE = (1/n) * Σ|y_i - x_i|\n    * n = total number of data points\n    * y_i = predicted value for data point i\n    * x_i = observed value for data point i\n* **Target Interpretation**:\n  * Target represents basis point difference between stock WAP movement and index WAP movement over 60 seconds\n  * Calculated as: (StockWAP_t+60/StockWAP_t - IndexWAP_t+60/IndexWAP_t)",
    "sections": {},
    "file_path": "kaggle_datasets/598/problem_summary.md"
  },
  "63": {
    "problem_id": "63",
    "title": "Event Recommendation Engine Challenge",
    "problem_type": "Recommendation System (Ranking Problem)",
    "objective": "Predict user interest in events by generating a ranked list of events for each user, ordered from most to least likely to be of interest. The goal is to improve event recommendations based on:",
    "evaluation_metric": null,
    "full_content": "# Event Recommendation Engine Challenge\n\n**Problem Description:**\n* **Problem Type:** Recommendation System (Ranking Problem)\n* **Objective:** Predict user interest in events by generating a ranked list of events for each user, ordered from most to least likely to be of interest. The goal is to improve event recommendations based on:\n    * User actions (clicks, responses)\n    * Event metadata\n    * Demographic information\n    * Social connections (friends)\n* **Key Points:**\n    * Requires ranking events by predicted user interest rather than binary classification.\n    * Leverages multiple data sources (user demographics, event content, social graph).\n    * Focuses on personalized recommendations at scale.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with mixed features (user-event interactions, user demographics, event metadata, social connections)\n* **Data Files:**\n    * `train.csv`: User-event interactions with labels (interested/not interested)\n    * `test.csv`: User-event interactions without labels\n    * `users.csv`: Demographic data (locale, birthyear, gender, location, timezone)\n    * `user_friends.csv`: Social connections between users\n    * `events.csv`: Event metadata (location, time, 100-dimensional word stem counts from descriptions)\n    * `event_attendees.csv`: Event attendance lists (yes/maybe/invited/no)\n* **Key Features:**\n    * User-event interaction features (timestamp, invitation status)\n    * User demographics (age, gender, location)\n    * Event content features (word stems from descriptions)\n    * Event location and time\n    * Social graph information\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision at 200 (MAP@200)\n    * Measures quality of ranked recommendations\n    * Evaluates precision at each position in the recommendation list\n    * Averages across all users\n    * Only considers top 200 recommendations (though maximum events per user is 116)\n* **Submission Format:**\n    * CSV with two columns: User ID and space-delimited list of recommended events\n    * Events must be ordered by predicted interest (most to least interesting)",
    "sections": {
      "Problem Description": "* **Problem Type:** Recommendation System (Ranking Problem)\n* **Objective:** Predict user interest in events by generating a ranked list of events for each user, ordered from most to least likely to be of interest. The goal is to improve event recommendations based on:\n    * User actions (clicks, responses)\n    * Event metadata\n    * Demographic information\n    * Social connections (friends)\n* **Key Points:**\n    * Requires ranking events by predicted user interest rather than binary classification.\n    * Leverages multiple data sources (user demographics, event content, social graph).\n    * Focuses on personalized recommendations at scale.",
      "Dataset Overview": "* **Data Type:** Tabular data with mixed features (user-event interactions, user demographics, event metadata, social connections)\n* **Data Files:**\n    * `train.csv`: User-event interactions with labels (interested/not interested)\n    * `test.csv`: User-event interactions without labels\n    * `users.csv`: Demographic data (locale, birthyear, gender, location, timezone)\n    * `user_friends.csv`: Social connections between users\n    * `events.csv`: Event metadata (location, time, 100-dimensional word stem counts from descriptions)\n    * `event_attendees.csv`: Event attendance lists (yes/maybe/invited/no)\n* **Key Features:**\n    * User-event interaction features (timestamp, invitation status)\n    * User demographics (age, gender, location)\n    * Event content features (word stems from descriptions)\n    * Event location and time\n    * Social graph information",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision at 200 (MAP@200)\n    * Measures quality of ranked recommendations\n    * Evaluates precision at each position in the recommendation list\n    * Averages across all users\n    * Only considers top 200 recommendations (though maximum events per user is 116)\n* **Submission Format:**\n    * CSV with two columns: User ID and space-delimited list of recommended events\n    * Events must be ordered by predicted interest (most to least interesting)"
    },
    "file_path": "kaggle_datasets/63/problem_summary.md"
  },
  "554": {
    "problem_id": "554",
    "title": "Feature Imputation with a Heat Flux Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Feature Imputation with a Heat Flux Dataset\n\n## Problem Description\n* **Problem Type**: Regression (Feature Imputation)\n* **Objective**: Predict missing values of the feature `x_e_out [-]` (equilibrium quality) in a heat flux dataset. The task involves imputing the missing values for this specific feature across the dataset.\n* **Key Points**:\n  * The dataset is synthetically generated from a deep learning model trained on the original \"Predicting Critical Heat Flux\" dataset.\n  * Participants are encouraged to explore differences between the synthetic and original datasets and investigate whether incorporating the original data improves model performance.\n\n## Dataset Overview\n* **Data Type**: Tabular data (heat flux measurements)\n* **Context**: Physics-related dataset involving heat flux and equilibrium quality measurements.\n* **Data Files**:\n  * `data.csv`: Contains the competition dataset with missing values in the `x_e_out [-]` feature.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features**: The dataset contains 12 columns (specific features not named in description), with `x_e_out [-]` being the target feature for imputation.\n\n## Evaluation Metrics\n* **Evaluation Metric**: Root Mean Squared Error (RMSE)\n* **Components**:\n  * RMSE is calculated as: √(1/N * Σ(y_i - ŷ_i)²)\n  * Where:\n    * y_i = original value\n    * ŷ_i = predicted value\n    * N = number of observations\n  * Submissions must predict missing values for `x_e_out [-]` and include corresponding row `id`s.",
    "sections": {},
    "file_path": "kaggle_datasets/554/problem_summary.md"
  },
  "302": {
    "problem_id": "302",
    "title": "Predicting Pilot Cognitive States from Physiological Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Pilot Cognitive States from Physiological Data\n\n## Problem Description\n* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the probability of a pilot being in one of four cognitive states (baseline or three distracted states) using physiological data, with the goal of real-time monitoring to prevent aviation fatalities.\n    * **Key Points:**\n        * Focus on detecting loss of \"airplane state awareness\" due to cognitive distractions.\n        * Models must process data in real-time for practical deployment.\n        * Three target distraction states:\n            * Channelized Attention (CA) - Over-focus on one task\n            * Diverted Attention (DA) - Attention split by decision-making\n            * Startle/Surprise (SS) - Reaction to unexpected events\n        * Baseline state (A) represents normal operation.\n\n## Dataset Overview\n* **Data Type & Context:** Time-series physiological sensor data from pilots in controlled experiments and flight simulations.\n    * **Data Files:**\n        * train.csv (training experiments)\n        * test.csv (LOFT flight simulation)\n        * sample_submission.csv\n    * **Key Features:**\n        * 25 EEG channels (eeg_* columns)\n        * ECG (electrocardiogram) readings\n        * Respiration (r) and Galvanic Skin Response (gsr) measurements\n        * Experiment type (CA/DA/SS for training, LOFT for test)\n        * Crew ID, seat position, and timestamp data\n        * Sample rate: 256Hz (high-frequency temporal data)\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-Class Log Loss\n    * **Components:**\n        * Submitted probabilities are rescaled (each row divided by row sum)\n        * Probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes\n        * Requires probability predictions for all 4 classes (A, B, C, D) for each test sample",
    "sections": {},
    "file_path": "kaggle_datasets/302/problem_summary.md"
  },
  "130": {
    "problem_id": "130",
    "title": "Predicting Social Circles in Networks",
    "problem_type": "Graph-Based Clustering / Social Network Analysis",
    "objective": "Automatically infer users' social circles (friend groupings) in social networks, where circles may be disjoint, overlapping, or hierarchically nested. Participants must predict the membership of friends in multiple circles for each user.",
    "evaluation_metric": null,
    "full_content": "# Predicting Social Circles in Networks\n\n**Problem Description:**\n* **Problem Type:** Graph-Based Clustering / Social Network Analysis\n* **Objective:** Automatically infer users' social circles (friend groupings) in social networks, where circles may be disjoint, overlapping, or hierarchically nested. Participants must predict the membership of friends in multiple circles for each user.\n    * **Key Points:**\n        * Uses anonymized Facebook data (users, friends, and ego networks).\n        * Requires handling of overlapping and hierarchical social circles.\n        * Leverages friend connections (ego networks) and anonymized profile features.\n\n**Dataset Overview:**\n* **Data Type & Context:** Graph-structured social network data with tabular features.\n    * **Data Files:**\n        * `egonets/`: Contains adjacency lists of connections between friends for each user (undirected edges).\n        * `features/`: Anonymized profile features for users and friends (e.g., `first_name;435`, `hometown;name;567`).\n        * `Training/`: Human-labeled circles for training (each line: `circleID: friend1 friend2 ...`).\n        * `testSet_users_friends.csv`: Lists test users and their friends.\n        * `sample_submission.csv`: Example submission file.\n    * **Features:** Anonymized user attributes (e.g., names, workplaces, hometowns) and friend connection graphs.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Edit Distance (minimum operations to transform predicted circles into ground truth).\n    * **Components of Edit Distance:**\n        * Cost of 1 per operation:\n            * Adding a user to an existing circle.\n            * Creating a new circle with one user.\n            * Removing a user from a circle.\n            * Deleting a circle with one user.\n    * Example: If predicted circles are `3 1 2; 2 3; 5 4 6` and ground truth is `1 3 2 4; 4 5`, the edit distance is 4 (add 4 to circle1, delete 2 and 3 from circle2, delete 6 from circle3).",
    "sections": {
      "Problem Description": "* **Problem Type:** Graph-Based Clustering / Social Network Analysis\n* **Objective:** Automatically infer users' social circles (friend groupings) in social networks, where circles may be disjoint, overlapping, or hierarchically nested. Participants must predict the membership of friends in multiple circles for each user.\n    * **Key Points:**\n        * Uses anonymized Facebook data (users, friends, and ego networks).\n        * Requires handling of overlapping and hierarchical social circles.\n        * Leverages friend connections (ego networks) and anonymized profile features.",
      "Dataset Overview": "* **Data Type & Context:** Graph-structured social network data with tabular features.\n    * **Data Files:**\n        * `egonets/`: Contains adjacency lists of connections between friends for each user (undirected edges).\n        * `features/`: Anonymized profile features for users and friends (e.g., `first_name;435`, `hometown;name;567`).\n        * `Training/`: Human-labeled circles for training (each line: `circleID: friend1 friend2 ...`).\n        * `testSet_users_friends.csv`: Lists test users and their friends.\n        * `sample_submission.csv`: Example submission file.\n    * **Features:** Anonymized user attributes (e.g., names, workplaces, hometowns) and friend connection graphs.",
      "Evaluation Metrics": "* **Evaluation Metric:** Edit Distance (minimum operations to transform predicted circles into ground truth).\n    * **Components of Edit Distance:**\n        * Cost of 1 per operation:\n            * Adding a user to an existing circle.\n            * Creating a new circle with one user.\n            * Removing a user from a circle.\n            * Deleting a circle with one user.\n    * Example: If predicted circles are `3 1 2; 2 3; 5 4 6` and ground truth is `1 3 2 4; 4 5`, the edit distance is 4 (add 4 to circle1, delete 2 and 3 from circle2, delete 6 from circle3)."
    },
    "file_path": "kaggle_datasets/130/problem_summary.md"
  },
  "64": {
    "problem_id": "64",
    "title": "Binary Classification with AUC Evaluation at Strata 2013",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with AUC Evaluation at Strata 2013\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:**  \n  * Participants are tasked with building a predictive model as part of an introductory tutorial at Strata 2013. The competition serves as a hands-on exercise to experience model creation and overfitting challenges in a time-constrained setting.  \n* **Key Points:**  \n  * Designed as a \"for-fun\" competition with no prizes or Kaggle points.  \n  * Extremely short duration (~1 hour) requiring rapid model development.  \n  * Unlimited submissions allowed (with server constraints).  \n  * Data was encrypted until competition start to prevent head starts.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular data (contents not fully specified, but typical of introductory binary classification problems).  \n* **Data Files:**  \n  * `train.csv` (training features)  \n  * `train_labels.csv` (training labels)  \n  * `test.csv` (test features)  \n  * `example_submission.csv` (submission format)  \n  * Auxiliary R scripts (`firstPrediction.r`, `secondPrediction.r`)  \n* **Features:** Not explicitly described, but likely anonymized numerical/categorical features given the tutorial nature of the competition.  \n\n## Evaluation Metrics  \n* **Primary Metric:** Area Under the ROC Curve (AUC)  \n* **Metric Details:**  \n  * Measures the model's ability to distinguish between positive and negative classes across all classification thresholds.  \n  * Implementations provided in:  \n    * MATLAB (`perfcurve`),  \n    * R (`ROCR` package),  \n    * Python (`scikit-learn` metrics).",
    "sections": {},
    "file_path": "kaggle_datasets/64/problem_summary.md"
  },
  "90": {
    "problem_id": "90",
    "title": "Predicting Short-Term Stock Price Movements with News and Sentiment Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Short-Term Stock Price Movements with News and Sentiment Data\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting (Financial Markets)\n* **Objective**: Predict percentage changes in stock prices 2 hours ahead using intraday trading data and news sentiment features.\n    * Participants must forecast movements for 198 different financial instruments.\n    * Predictions are based on 5-minute interval data throughout a trading day.\n* **Key Points**:\n    * Focus on short-term (2-hour) price movement prediction.\n    * Data is anonymized - no feature names or specific dates provided to prevent cheating.\n    * Competition serves as a qualifier for live trading strategy presentations.\n\n## Dataset Overview\n* **Data Type**: Tabular time series data (financial instrument features and price movements)\n* **Context**: Intraday trading data with news sentiment features from RavenPack\n* **Data Files**:\n    * `data.zip`: Contains features for 510 trading days (200 training, 310 testing)\n    * `trainLabels.csv`: Target values for the 200 training days\n    * `sampleSubmission.csv`: Example submission format\n* **Features**:\n    * Input features anonymized as I1, I2, I3, etc. (representing various financial features)\n    * Output variables anonymized as O1, O2, O3, etc. (representing percentage changes for 198 securities)\n    * Data structured with:\n        * Line 1: Previous day's close (4PM ET)\n        * Line 2: Current day's open (9:30AM ET)\n        * Subsequent lines: 5-minute intervals up to 1:55PM ET\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE)\n    * Formula: MAE = (1/n) * Σ|y_i - ŷ_i| \n    * Where:\n        * y_i = actual percentage change\n        * ŷ_i = predicted percentage change\n        * n = number of predictions\n* **Submission Format**:\n    * Requires predictions for all 198 outputs (O1-O198) for each test FileId\n    * Predictions must be percentage changes relative to previous day's close",
    "sections": {},
    "file_path": "kaggle_datasets/90/problem_summary.md"
  },
  "419": {
    "problem_id": "419",
    "title": "Multi-label Audio Classification of Bird and Frog Species in Rainforest Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Audio Classification of Bird and Frog Species in Rainforest Recordings\n\n## Problem Description\n* **Problem Type:** Multi-label Audio Classification\n* **Objective:** Automate detection of bird and frog species in tropical soundscape recordings, where:\n    * Some audio files contain single species while others contain multiple species\n    * Predictions are made at the audio file level (no timestamp localization required)\n* **Key Points:**\n    * Focus on detecting species in acoustically complex environments with background noise (insects, etc.)\n    * Training data is limited, particularly for rarer species\n    * Includes both true positive and false positive training examples to improve model robustness\n    * Real-world application for biodiversity monitoring and conservation efforts\n\n## Dataset Overview\n* **Data Type:** Audio recordings (FLAC format) with time-frequency annotations\n* **Context:** Tropical rainforest soundscapes collected by acoustic sensors\n* **Data Files:**\n    * `train_tp.csv` - True positive species labels with time localization\n    * `train_fp.csv` - False positive species labels with time localization\n    * `train/` - Training audio files\n    * `test/` - Test audio files\n    * TFRecord versions available for both train/test sets\n* **Key Features:**\n    * Audio features: 16-bit PCM encoded waveforms\n    * Annotations include species_id, songtype_id, time-frequency bounds (t_min, f_min, t_max, f_max)\n    * False positive indicators to help train against misclassifications\n\n## Evaluation Metrics\n* **Primary Metric:** Label-weighted Label-Ranking Average Precision (LRAP)\n    * Generalization of mean reciprocal rank for multi-label cases\n    * Each label in test set receives equal weight (unlike standard LRAP which weights by observations)\n    * Final score is average over all labels, weighted by number of ground truth labels per observation\n* **Submission Format:**\n    * Probability predictions for each species (columns s0-s23) per recording_id\n    * Example:\n        ```\n        recording_id,s0,...,s23\n        000316da7,0.1,....,0.3\n        ",
    "sections": {},
    "file_path": "kaggle_datasets/419/problem_summary.md"
  },
  "270": {
    "problem_id": "270",
    "title": "Fine-Grained Image Classification of Furniture and Home Goods",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Image Classification of Furniture and Home Goods\n\n## Problem Description\n- **Problem Type**: Multi-class Image Classification (Fine-Grained Visual Categorization)\n- **Objective**: Develop algorithms to accurately classify images of furniture and home goods into 128 fine-grained categories, despite challenges like varying lighting, angles, backgrounds, and occlusion.\n- **Key Points**:\n  - Focus on subtle visual differences between similar categories (e.g., ball chair vs egg chair)\n  - Real-world application for automatic product recognition in e-commerce\n  - Part of the FGVC5 workshop at CVPR 2018, pushing state-of-the-art in fine-grained recognition\n\n## Dataset Overview\n- **Data Type**: Image data (furniture and home goods photos)\n- **Context**: URLs to product images needing fine-grained classification\n- **Data Files**:\n  - train.json (194,828 images with labels)\n  - validation.json (6,400 images with labels)\n  - test.json (12,800 images without labels)\n  - sample_submission_randomlabel.csv (example submission format)\n- **Features**:\n  - Image URLs (participants must download images themselves)\n  - Integer labels (1-128) corresponding to product categories\n  - Note: Label names were intentionally omitted to prevent manual labeling of test data\n\n## Evaluation Metrics\n- **Evaluation Metric**: Classification Error Rate (1 - Accuracy)\n  - For each image: 0 if prediction matches ground truth, 1 otherwise\n  - Final score: Average error across all test images\n- **Submission Format**:\n  - CSV file with 'id' (image_id) and 'predicted' (label_id) columns\n  - Requires exactly 1 predicted label per test image",
    "sections": {},
    "file_path": "kaggle_datasets/270/problem_summary.md"
  },
  "426": {
    "problem_id": "426",
    "title": "Binary Classification with Synthetic Insurance Claim Data",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of a binary target variable based on anonymized feature columns. The dataset is synthetic but based on a real-world insurance claim prediction problem.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Synthetic Insurance Claim Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary target variable based on anonymized feature columns. The dataset is synthetic but based on a real-world insurance claim prediction problem.\n    * **Key Points:**\n        * Designed as an approachable competition for beginners, bridging the gap between simple starter competitions and complex featured competitions.\n        * Features are anonymized but mimic properties of real-world insurance claim data.\n        * Synthetic dataset generated using CTGAN (Conditional Tabular GAN).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with a mix of categorical and continuous features, simulating insurance claim data.\n* **Data Files:**\n    * `train.csv` - Contains both features and the binary target column.\n    * `test.csv` - Contains only features; participants must predict target probabilities.\n    * `sample_submission.csv` - Demonstrates submission format.\n* **Features:**\n    * 19 categorical features (`cat0`-`cat18`)\n    * 11 continuous features (`cont0`-`cont10`)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between the two classes.\n        * Plots True Positive Rate vs False Positive Rate at various threshold settings.\n        * Higher AUC values (closer to 1) indicate better model performance.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary target variable based on anonymized feature columns. The dataset is synthetic but based on a real-world insurance claim prediction problem.\n    * **Key Points:**\n        * Designed as an approachable competition for beginners, bridging the gap between simple starter competitions and complex featured competitions.\n        * Features are anonymized but mimic properties of real-world insurance claim data.\n        * Synthetic dataset generated using CTGAN (Conditional Tabular GAN).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with a mix of categorical and continuous features, simulating insurance claim data.\n* **Data Files:**\n    * `train.csv` - Contains both features and the binary target column.\n    * `test.csv` - Contains only features; participants must predict target probabilities.\n    * `sample_submission.csv` - Demonstrates submission format.\n* **Features:**\n    * 19 categorical features (`cat0`-`cat18`)\n    * 11 continuous features (`cont0`-`cont10`)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between the two classes.\n        * Plots True Positive Rate vs False Positive Rate at various threshold settings.\n        * Higher AUC values (closer to 1) indicate better model performance."
    },
    "file_path": "kaggle_datasets/426/problem_summary.md"
  },
  "614": {
    "problem_id": "614",
    "title": "Predicting Academic Risk of Students in Higher Education",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Academic Risk of Students in Higher Education\n\n## Problem Description\n* **Problem Type:** Multi-class Classification\n* **Objective:** Predict the academic risk level of students in higher education, categorized as a categorical target variable (`Target`). The goal is to assess which students are at risk of academic failure or dropout.\n    * **Key Points:**\n        * The dataset is synthetically generated but based on real-world data about student academic success.\n        * Participants are encouraged to explore the original dataset for feature explanations and potential performance improvements.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing student academic and demographic features in higher education.\n* **Data Files:**\n    * `train.csv` - Contains the training data with the `Target` column (categorical label).\n    * `test.csv` - Contains the test data for which predictions must be made.\n    * `sample_submission.csv` - Example submission file in the required format.\n* **Features:** The dataset includes 77 columns (features) related to student academic performance and demographics. The exact features are not listed, but they are derived from a real-world dataset about student dropout and academic success.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Accuracy Score\n    * **Components:** \n        * The metric calculates the proportion of correctly predicted labels (`Target`) over the total predictions.\n        * Submissions must predict the categorical class (`Target`) for each student in the test set.",
    "sections": {},
    "file_path": "kaggle_datasets/614/problem_summary.md"
  },
  "284": {
    "problem_id": "284",
    "title": "Movie Review Sentiment Analysis (Kernels Only)",
    "problem_type": "NLP - Multi-class Classification (Sentiment Analysis)",
    "objective": "Classify the sentiment of movie review phrases from the Rotten Tomatoes dataset into one of five ordered categories:",
    "evaluation_metric": null,
    "full_content": "# Movie Review Sentiment Analysis (Kernels Only)\n\n**Problem Description:**\n* **Problem Type:** NLP - Multi-class Classification (Sentiment Analysis)\n* **Objective:** Classify the sentiment of movie review phrases from the Rotten Tomatoes dataset into one of five ordered categories:\n  * 0 - negative\n  * 1 - somewhat negative\n  * 2 - neutral\n  * 3 - somewhat positive\n  * 4 - positive\n* **Key Points:**\n  * Focuses on fine-grained sentiment analysis at the phrase level (parsed by Stanford parser)\n  * Challenges include handling:\n    * Sentence negation\n    * Sarcasm\n    * Terseness\n    * Language ambiguity\n\n**Dataset Overview:**\n* **Data Type & Context:** Text data (movie review phrases) with sentiment labels\n* **Data Files:**\n  * train.tsv - Contains phrases with sentiment labels and SentenceIds\n  * test.tsv - Contains phrases requiring sentiment prediction\n  * sampleSubmission.csv - Example submission file\n* **Key Features:**\n  * PhraseId - Unique identifier for each phrase\n  * SentenceId - Groups phrases from the same original sentence\n  * Phrase - The text content to analyze\n  * Sentiment - Target label (only in training data)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Classification Accuracy (percentage of correctly predicted labels)\n* **Implementation Details:**\n  * Evaluated on every parsed phrase in test set\n  * Predictions must match exact sentiment labels (0-4)\n  * Submission format requires PhraseId and predicted Sentiment for each test phrase",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Multi-class Classification (Sentiment Analysis)\n* **Objective:** Classify the sentiment of movie review phrases from the Rotten Tomatoes dataset into one of five ordered categories:\n  * 0 - negative\n  * 1 - somewhat negative\n  * 2 - neutral\n  * 3 - somewhat positive\n  * 4 - positive\n* **Key Points:**\n  * Focuses on fine-grained sentiment analysis at the phrase level (parsed by Stanford parser)\n  * Challenges include handling:\n    * Sentence negation\n    * Sarcasm\n    * Terseness\n    * Language ambiguity",
      "Dataset Overview": "* **Data Type & Context:** Text data (movie review phrases) with sentiment labels\n* **Data Files:**\n  * train.tsv - Contains phrases with sentiment labels and SentenceIds\n  * test.tsv - Contains phrases requiring sentiment prediction\n  * sampleSubmission.csv - Example submission file\n* **Key Features:**\n  * PhraseId - Unique identifier for each phrase\n  * SentenceId - Groups phrases from the same original sentence\n  * Phrase - The text content to analyze\n  * Sentiment - Target label (only in training data)",
      "Evaluation Metrics": "* **Primary Metric:** Classification Accuracy (percentage of correctly predicted labels)\n* **Implementation Details:**\n  * Evaluated on every parsed phrase in test set\n  * Predictions must match exact sentiment labels (0-4)\n  * Submission format requires PhraseId and predicted Sentiment for each test phrase"
    },
    "file_path": "kaggle_datasets/284/problem_summary.md"
  },
  "248": {
    "problem_id": "248",
    "title": "Threat Detection in Passenger Body Scans",
    "problem_type": "Binary Classification (Multi-label across 17 body zones)",
    "objective": "Improve airport security screening by accurately predicting the probability of threat presence in 17 predefined body zones from millimeter wave scanner images. The goal is to reduce false alarms that cause unnecessary manual screenings while maintaining security.",
    "evaluation_metric": null,
    "full_content": "# Threat Detection in Passenger Body Scans\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Multi-label across 17 body zones)\n* **Objective:** Improve airport security screening by accurately predicting the probability of threat presence in 17 predefined body zones from millimeter wave scanner images. The goal is to reduce false alarms that cause unnecessary manual screenings while maintaining security.\n    * **Key Points:**\n        * Two-stage competition with different volunteers in each stage to test generalization\n        * Threats are simulated with inert objects of varying material properties\n        * Must handle diverse conditions: different clothing types, body types, and threat configurations\n        * Images may contain sensitive content (volunteer body scans)\n\n**Dataset Overview:**\n* **Data Type:** 3D millimeter wave body scan images (HD-AIT system) with multiple representations\n* **Context:** Airport security screening scans containing simulated threats under various conditions\n* **Data Files:**\n    * Four proprietary binary formats per scan: .aps, .a3d, .a3daps, .ahi (varying sizes from 10MB to 2.26GB per file)\n    * stage1_labels.csv (training labels)\n    * stage1_sample_submission.csv (submission format)\n* **Features:**\n    * Multiple representations of 3D scan data (projected angle sequences, combined 3D volumes, raw data)\n    * Each scan has 17 associated binary labels (one per body zone)\n    * Scans vary by clothing type, body type, and threat configuration\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss (binary cross-entropy)\n    * **Calculation:**\n        * For N = 17 * number of test scans\n        * −1/N * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]\n        * ŷ_i = predicted probability (clipped to [1e-15, 1-1e-15])\n        * y_i = actual label (1 if threat present, 0 otherwise)\n    * Evaluates prediction quality across all 17 zones simultaneously\n    * Smaller values indicate better performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Multi-label across 17 body zones)\n* **Objective:** Improve airport security screening by accurately predicting the probability of threat presence in 17 predefined body zones from millimeter wave scanner images. The goal is to reduce false alarms that cause unnecessary manual screenings while maintaining security.\n    * **Key Points:**\n        * Two-stage competition with different volunteers in each stage to test generalization\n        * Threats are simulated with inert objects of varying material properties\n        * Must handle diverse conditions: different clothing types, body types, and threat configurations\n        * Images may contain sensitive content (volunteer body scans)",
      "Dataset Overview": "* **Data Type:** 3D millimeter wave body scan images (HD-AIT system) with multiple representations\n* **Context:** Airport security screening scans containing simulated threats under various conditions\n* **Data Files:**\n    * Four proprietary binary formats per scan: .aps, .a3d, .a3daps, .ahi (varying sizes from 10MB to 2.26GB per file)\n    * stage1_labels.csv (training labels)\n    * stage1_sample_submission.csv (submission format)\n* **Features:**\n    * Multiple representations of 3D scan data (projected angle sequences, combined 3D volumes, raw data)\n    * Each scan has 17 associated binary labels (one per body zone)\n    * Scans vary by clothing type, body type, and threat configuration",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss (binary cross-entropy)\n    * **Calculation:**\n        * For N = 17 * number of test scans\n        * −1/N * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]\n        * ŷ_i = predicted probability (clipped to [1e-15, 1-1e-15])\n        * y_i = actual label (1 if threat present, 0 otherwise)\n    * Evaluates prediction quality across all 17 zones simultaneously\n    * Smaller values indicate better performance"
    },
    "file_path": "kaggle_datasets/248/problem_summary.md"
  },
  "283": {
    "problem_id": "283",
    "title": "Costa Rican Household Poverty Level Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Costa Rican Household Poverty Level Prediction\n\n## Problem Description\n- **Problem Type:** Multiclass Classification (Ordinal Target Variable)\n- **Objective:** Predict the poverty level of Costa Rican households to identify those most in need of social welfare assistance. The goal is to improve upon traditional Proxy Means Test (PMT) methods used by governments to assess household need.\n    - **Key Points:**\n        - Focuses on predicting ordinal poverty levels (1-4) based on household characteristics.\n        - Targets heads of households for scoring, though data includes all household members.\n        - Aims to address inaccuracies in existing PMT models used in Latin America.\n\n## Dataset Overview\n- **Data Type:** Tabular data (household and individual-level socio-economic features)\n- **Context:** Data represents observable attributes of Costa Rican households (e.g., housing materials, assets, demographics).\n- **Data Files:**\n    - `train.csv`: Contains household features and the Target variable\n    - `test.csv`: Contains household features without Target (for prediction)\n    - `sample_submission.csv`: Example submission format\n- **Key Features:**\n    - 142 total columns including:\n        - Household characteristics (wall/roof materials, utilities)\n        - Asset ownership (refrigerator, tablet, etc.)\n        - Demographic information (age, gender, education)\n        - Household composition variables\n    - Critical identifiers:\n        - `idhogar`: Unique household identifier\n        - `parentesco1`: Indicator for head of household\n        - `Target`: Ordinal poverty level (1=extreme poverty to 4=non-vulnerable)\n\n## Evaluation Metrics\n- **Primary Metric:** Macro F1-Score\n    - **Components:**\n        - Calculates F1-score for each class independently\n        - Averages the per-class F1-scores without weighting\n        - Suitable for imbalanced class distributions\n        - Emphasizes both precision and recall equally for all classes",
    "sections": {},
    "file_path": "kaggle_datasets/283/problem_summary.md"
  },
  "613": {
    "problem_id": "613",
    "title": "AI Mathematical Olympiad Problem Solving",
    "problem_type": "NLP - Mathematical Problem Solving (Integer Prediction)",
    "objective": "Develop AI models capable of solving intermediate-level high school math problems written in LaTeX format, predicting correct integer answers.",
    "evaluation_metric": null,
    "full_content": "# AI Mathematical Olympiad Problem Solving\n\n**Problem Description:**\n* **Problem Type:** NLP - Mathematical Problem Solving (Integer Prediction)\n* **Objective:** Develop AI models capable of solving intermediate-level high school math problems written in LaTeX format, predicting correct integer answers.\n    * **Key Points:**\n        * Problems are similar to AIME/AMC'12 level math competitions\n        * Answers are non-negative integers reported modulo 1000\n        * Focus on preventing train-test leakage with novel problems\n        * Problems cover arithmetic, algebra, and geometry (text-only, no diagrams)\n\n**Dataset Overview:**\n* **Data Type:** Text data (LaTeX-formatted math problems)\n* **Context:** 110 original math problems created for AI evaluation\n* **Data Files:**\n    * train.csv (10 problems)\n    * test.csv (50 placeholder problems - replaced during scoring)\n    * sample_submission.csv\n* **Features:**\n    * `id`: Unique problem identifier\n    * `problem`: LaTeX-formatted problem statement\n    * `answer`: Integer solution (0-999)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Accuracy\n    * Exact match between predicted integer (0-999) and ground truth\n    * Score = (Number of correct predictions) / (Total problems)\n    * Test sets:\n        * Public: 50 problems during competition\n        * Private: Different 50 problems for final evaluation",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Mathematical Problem Solving (Integer Prediction)\n* **Objective:** Develop AI models capable of solving intermediate-level high school math problems written in LaTeX format, predicting correct integer answers.\n    * **Key Points:**\n        * Problems are similar to AIME/AMC'12 level math competitions\n        * Answers are non-negative integers reported modulo 1000\n        * Focus on preventing train-test leakage with novel problems\n        * Problems cover arithmetic, algebra, and geometry (text-only, no diagrams)",
      "Dataset Overview": "* **Data Type:** Text data (LaTeX-formatted math problems)\n* **Context:** 110 original math problems created for AI evaluation\n* **Data Files:**\n    * train.csv (10 problems)\n    * test.csv (50 placeholder problems - replaced during scoring)\n    * sample_submission.csv\n* **Features:**\n    * `id`: Unique problem identifier\n    * `problem`: LaTeX-formatted problem statement\n    * `answer`: Integer solution (0-999)",
      "Evaluation Metrics": "* **Primary Metric:** Accuracy\n    * Exact match between predicted integer (0-999) and ground truth\n    * Score = (Number of correct predictions) / (Total problems)\n    * Test sets:\n        * Public: 50 problems during competition\n        * Private: Different 50 problems for final evaluation"
    },
    "file_path": "kaggle_datasets/613/problem_summary.md"
  },
  "421": {
    "problem_id": "421",
    "title": "Rock, Paper, Scissors AI Agent Competition",
    "problem_type": "Reinforcement Learning / Game Theory Simulation",
    "objective": "Develop an AI agent that can play the Rock, Paper, Scissors game strategically to outperform opponents in a \"Best-of-1000\" match format.",
    "evaluation_metric": null,
    "full_content": "# Rock, Paper, Scissors AI Agent Competition\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning / Game Theory Simulation\n* **Objective:** Develop an AI agent that can play the Rock, Paper, Scissors game strategically to outperform opponents in a \"Best-of-1000\" match format.\n    * Key Points:\n        * Agents must predict and counter opponents' moves based on observed patterns\n        * The challenge focuses on exploiting non-random behavior in opponents\n        * Fundamental applications in ML, AI, and human psychology studies\n\n**Dataset Overview:**\n* **Data Type:** Game state observations (sequential move history)\n    * Context: Simulated gameplay environment tracking opponent moves\n* **Key Data Structures:**\n    * Observation object containing:\n        - `step`: Current round number\n        - `lastOpponentAction`: ID of opponent's previous move (None on first step)\n* **Move Encoding:**\n    - 0 = Rock\n    - 1 = Paper\n    - 2 = Scissors\n\n**Evaluation Metrics:**\n* **Primary Metric:** Dynamic Skill Rating (Gaussian N(μ,σ²))\n    * Components:\n        - Initial μ₀ = 600 for new submissions\n        - Rating updates after each match:\n            * Winner's μ increases, loser's μ decreases\n            * Draws move both agents' μ toward their mean\n        - Uncertainty (σ) decreases with more matches\n    * Matchmaking:\n        - Agents face opponents with similar ratings\n        - Each agent plays ~8 matches/day\n    * Final ranking based on highest μ achieved",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning / Game Theory Simulation\n* **Objective:** Develop an AI agent that can play the Rock, Paper, Scissors game strategically to outperform opponents in a \"Best-of-1000\" match format.\n    * Key Points:\n        * Agents must predict and counter opponents' moves based on observed patterns\n        * The challenge focuses on exploiting non-random behavior in opponents\n        * Fundamental applications in ML, AI, and human psychology studies",
      "Dataset Overview": "* **Data Type:** Game state observations (sequential move history)\n    * Context: Simulated gameplay environment tracking opponent moves\n* **Key Data Structures:**\n    * Observation object containing:\n        - `step`: Current round number\n        - `lastOpponentAction`: ID of opponent's previous move (None on first step)\n* **Move Encoding:**\n    - 0 = Rock\n    - 1 = Paper\n    - 2 = Scissors",
      "Evaluation Metrics": "* **Primary Metric:** Dynamic Skill Rating (Gaussian N(μ,σ²))\n    * Components:\n        - Initial μ₀ = 600 for new submissions\n        - Rating updates after each match:\n            * Winner's μ increases, loser's μ decreases\n            * Draws move both agents' μ toward their mean\n        - Uncertainty (σ) decreases with more matches\n    * Matchmaking:\n        - Agents face opponents with similar ratings\n        - Each agent plays ~8 matches/day\n    * Final ranking based on highest μ achieved"
    },
    "file_path": "kaggle_datasets/421/problem_summary.md"
  },
  "277": {
    "problem_id": "277",
    "title": "Compact Video Classification with YouTube-8M Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Compact Video Classification with YouTube-8M Dataset\n\n## Problem Description\n* **Problem Type:** Multi-label Video Classification\n* **Objective:**  \n    Develop a compact model (≤1GB) to predict multiple labels for YouTube videos using pre-extracted features, focusing on efficient representation learning under strict size constraints.\n    * **Key Points:**\n        * Model size strictly limited to 1GB (enforced via upload verification)\n        * Focus on single-model efficiency rather than ensembles\n        * Labels drawn from 3,700+ visual entity vocabulary\n        * Test data contains anonymized video IDs to ensure fairness\n\n## Dataset Overview\n* **Data Type & Context:**  \n    Pre-processed video features from YouTube videos (frame-level and video-level)\n    * **Primary Files:**\n        * `train*.tfrecord`, `validate*.tfrecord`, `test*.tfrecord` (TFRecords format)\n        * `label_names_2018.csv` (label ID to name mapping)\n        * `vocabulary.csv` (full label dictionary with descriptions)\n    * **Key Features:**\n        * Video-level: `mean_rgb` (1024-dim), `mean_audio` (128-dim)\n        * Frame-level: Per-frame `rgb` (1024-dim) and `audio` (128-dim) features\n        * Anonymized video IDs in test/validation sets\n\n## Evaluation Metrics\n* **Primary Metric:** Global Average Precision (GAP) at k=20\n    * **Calculation:**\n        * For each video: Submit top 20 predicted labels with confidence scores\n        * All predictions pooled globally across videos\n        * Computed as:  \n            𝐺𝐴𝑃 = ∑(𝑝(𝑖) × Δ𝑟(𝑖)) across all predictions  \n            where:\n            * 𝑝(𝑖) = precision at prediction 𝑖\n            * Δ𝑟(𝑖) = recall change at prediction 𝑖\n        * Implementation: YouTube-8M's [average_precision_calculator.py](https://github.com/google/youtube-8m)",
    "sections": {},
    "file_path": "kaggle_datasets/277/problem_summary.md"
  },
  "428": {
    "problem_id": "428",
    "title": "NCAA Women's Basketball Tournament Outcome Prediction",
    "problem_type": "Binary Classification (Probability Prediction)",
    "objective": "Predict the probability of one team beating another in NCAA Women's Basketball Tournament games. The competition has two stages:",
    "evaluation_metric": null,
    "full_content": "# NCAA Women's Basketball Tournament Outcome Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Probability Prediction)\n* **Objective:** Predict the probability of one team beating another in NCAA Women's Basketball Tournament games. The competition has two stages:\n  * Stage 1: Predict outcomes for historical tournaments (2015-2019)\n  * Stage 2: Predict outcomes for the 2021 tournament\n* **Key Points:**\n  * Participants are encouraged to use external data sources (with proper disclosure)\n  * Predictions must cover all possible team matchups (n*(n-1)/2 predictions per tournament)\n  * The competition focuses on probabilistic predictions rather than binary outcomes\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing historical NCAA Women's Basketball game results, team statistics, and tournament information from 1998-2021\n* **Key Data Files:**\n  * `WTeams.csv` - Team IDs and names\n  * `WSeasons.csv` - Season information and tournament structure\n  * `WNCAATourneySeeds.csv` - Tournament seeds by year\n  * `WRegularSeasonCompactResults.csv` - Regular season game results\n  * `WNCAATourneyCompactResults.csv` - Tournament game results\n  * `WRegularSeasonDetailedResults.csv` - Team-level box scores (2010+)\n  * `WNCAATourneyDetailedResults.csv` - Tournament box scores (2010+)\n* **Important Features:**\n  * Game outcomes (win/loss, scores)\n  * Team performance statistics (field goals, rebounds, assists, etc.)\n  * Tournament seeds and bracket structure\n  * Game locations and dates\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n* **Calculation:**\n  * For each game prediction, compares predicted probability with actual outcome\n  * Formula: `LogLoss = -1/n * Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)]`\n    * `n` = number of games\n    * `y_i` = 1 if team1 wins, 0 if team2 wins\n    * `p_i` = predicted probability of team1 winning\n  * Key properties:\n    * Heavily penalizes confident incorrect predictions\n    * Predictions are bounded away from 0 and 1 to avoid infinite",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Probability Prediction)\n* **Objective:** Predict the probability of one team beating another in NCAA Women's Basketball Tournament games. The competition has two stages:\n  * Stage 1: Predict outcomes for historical tournaments (2015-2019)\n  * Stage 2: Predict outcomes for the 2021 tournament\n* **Key Points:**\n  * Participants are encouraged to use external data sources (with proper disclosure)\n  * Predictions must cover all possible team matchups (n*(n-1)/2 predictions per tournament)\n  * The competition focuses on probabilistic predictions rather than binary outcomes",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing historical NCAA Women's Basketball game results, team statistics, and tournament information from 1998-2021\n* **Key Data Files:**\n  * `WTeams.csv` - Team IDs and names\n  * `WSeasons.csv` - Season information and tournament structure\n  * `WNCAATourneySeeds.csv` - Tournament seeds by year\n  * `WRegularSeasonCompactResults.csv` - Regular season game results\n  * `WNCAATourneyCompactResults.csv` - Tournament game results\n  * `WRegularSeasonDetailedResults.csv` - Team-level box scores (2010+)\n  * `WNCAATourneyDetailedResults.csv` - Tournament box scores (2010+)\n* **Important Features:**\n  * Game outcomes (win/loss, scores)\n  * Team performance statistics (field goals, rebounds, assists, etc.)\n  * Tournament seeds and bracket structure\n  * Game locations and dates",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (LogLoss)\n* **Calculation:**\n  * For each game prediction, compares predicted probability with actual outcome\n  * Formula: `LogLoss = -1/n * Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)]`\n    * `n` = number of games\n    * `y_i` = 1 if team1 wins, 0 if team2 wins\n    * `p_i` = predicted probability of team1 winning\n  * Key properties:\n    * Heavily penalizes confident incorrect predictions\n    * Predictions are bounded away from 0 and 1 to avoid infinite"
    },
    "file_path": "kaggle_datasets/428/problem_summary.md"
  },
  "241": {
    "problem_id": "241",
    "title": "Genetic Mutation Classification for Personalized Cancer Treatment",
    "problem_type": "Multiclass Classification (Text + Tabular Data)",
    "objective": "Develop a machine learning algorithm to automatically classify genetic mutations into one of nine classes based on clinical evidence (text) and genetic variant information. The goal is to distinguish driver mutations (contributing to tumor growth) from passenger mutations (neutral).",
    "evaluation_metric": null,
    "full_content": "# Genetic Mutation Classification for Personalized Cancer Treatment\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification (Text + Tabular Data)\n    * **Objective:** Develop a machine learning algorithm to automatically classify genetic mutations into one of nine classes based on clinical evidence (text) and genetic variant information. The goal is to distinguish driver mutations (contributing to tumor growth) from passenger mutations (neutral).\n    * **Key Points:**\n        * Manual classification of genetic mutations is time-consuming and requires expert review of clinical literature.\n        * The competition aims to automate this process using an expert-annotated knowledge base.\n        * Some test data is machine-generated to prevent hand labeling, and these samples are ignored in evaluation.\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * **Tabular Data:** Genetic mutation details (Gene, Variation) linked to clinical text evidence via ID.\n    * **Text Data:** Clinical evidence (medical literature excerpts) used by experts to classify mutations.\n* **Data Files:**\n    * `training_variants.csv`: Contains ID, Gene, Variation, and Class (1-9) for training.\n    * `training_text.csv`: Contains ID and Text (clinical evidence) for training, linked to variants via ID.\n    * `test_variants.csv` and `test_text.csv`: Analogous files for testing (without Class labels).\n    * `submissionSample.csv`: Example submission file format.\n* **Key Features:**\n    * **Genetic Features:** Gene name and specific amino acid change (Variation).\n    * **Text Features:** Raw clinical evidence text used for manual classification.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-Class Log Loss (Cross-Entropy Loss)\n    * **Components:**\n        * Predictions must be probabilities for each of the 9 classes (summing to 1 per row).\n        * The metric penalizes confident incorrect predictions more heavily.\n        * Lower values indicate better performance (perfect prediction would have log loss of 0).",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification (Text + Tabular Data)\n    * **Objective:** Develop a machine learning algorithm to automatically classify genetic mutations into one of nine classes based on clinical evidence (text) and genetic variant information. The goal is to distinguish driver mutations (contributing to tumor growth) from passenger mutations (neutral).\n    * **Key Points:**\n        * Manual classification of genetic mutations is time-consuming and requires expert review of clinical literature.\n        * The competition aims to automate this process using an expert-annotated knowledge base.\n        * Some test data is machine-generated to prevent hand labeling, and these samples are ignored in evaluation.",
      "Dataset Overview": "* **Data Type & Context:** \n    * **Tabular Data:** Genetic mutation details (Gene, Variation) linked to clinical text evidence via ID.\n    * **Text Data:** Clinical evidence (medical literature excerpts) used by experts to classify mutations.\n* **Data Files:**\n    * `training_variants.csv`: Contains ID, Gene, Variation, and Class (1-9) for training.\n    * `training_text.csv`: Contains ID and Text (clinical evidence) for training, linked to variants via ID.\n    * `test_variants.csv` and `test_text.csv`: Analogous files for testing (without Class labels).\n    * `submissionSample.csv`: Example submission file format.\n* **Key Features:**\n    * **Genetic Features:** Gene name and specific amino acid change (Variation).\n    * **Text Features:** Raw clinical evidence text used for manual classification.",
      "Evaluation Metrics": "* **Primary Metric:** Multi-Class Log Loss (Cross-Entropy Loss)\n    * **Components:**\n        * Predictions must be probabilities for each of the 9 classes (summing to 1 per row).\n        * The metric penalizes confident incorrect predictions more heavily.\n        * Lower values indicate better performance (perfect prediction would have log loss of 0)."
    },
    "file_path": "kaggle_datasets/241/problem_summary.md"
  },
  "625": {
    "problem_id": "625",
    "title": "Multi-class Classification of Lumbar Spine Degenerative Conditions from MRI Scans",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Classification of Lumbar Spine Degenerative Conditions from MRI Scans\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Medical Image Analysis)\n* **Objective:** Develop models to classify the severity of five degenerative lumbar spine conditions from MRI scans:\n    * Left Neural Foraminal Narrowing\n    * Right Neural Foraminal Narrowing\n    * Left Subarticular Stenosis\n    * Right Subarticular Stenosis\n    * Spinal Canal Stenosis\n* **Key Points:**\n    * Severity is graded as Normal/Mild, Moderate, or Severe for each condition\n    * Conditions are evaluated at five intervertebral disc levels (L1/L2 to L5/S1)\n    * Models should simulate radiologist performance in diagnosis\n    * Focus on standardized classification to improve patient outcomes\n\n## Dataset Overview\n* **Data Type:** Medical Imaging (MRI scans) with tabular annotations\n* **Context:** Multi-institutional dataset curated by radiologists, collected from 8 sites across 5 continents\n* **Data Files:**\n    * `train.csv` - Contains study IDs and target labels for training data\n    * `train_label_coordinates.csv` - Provides image-level coordinates for labeled conditions\n    * `[train/test]_images/` - DICOM format MRI scans organized by study/series\n    * `[train/test]_series_descriptions.csv` - Scan orientation metadata\n* **Key Features:**\n    * MRI scans in DICOM format (3D stacks)\n    * Annotations include condition type, severity, and vertebral level\n    * Some cases have incomplete labels (missing annotations)\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Log Loss (average of sample-weighted log losses and an `any_severe_spinal` prediction)\n* **Weighting Scheme:**\n    * 1× weight for Normal/Mild predictions\n    * 2× weight for Moderate predictions\n    * 4× weight for Severe predictions\n* **Submission Format:**\n    * Requires probability predictions for all three severity classes per condition/level\n    * Format: `row_id, normal_mild, moderate, severe`\n    * Must predict for all levels, even when not visible in scan (though these won't be scored)",
    "sections": {},
    "file_path": "kaggle_datasets/625/problem_summary.md"
  },
  "417": {
    "problem_id": "417",
    "title": "Regression with Tabular Playground Series - Jan 2021",
    "problem_type": "Regression",
    "objective": "Predict a continuous target variable based on provided feature columns. The competition is designed as an approachable challenge for beginners to practice regression skills on tabular data.",
    "evaluation_metric": null,
    "full_content": "# Regression with Tabular Playground Series - Jan 2021\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict a continuous target variable based on provided feature columns. The competition is designed as an approachable challenge for beginners to practice regression skills on tabular data.\n* **Key Points:**\n  * Intended as a beginner-friendly competition, positioned between introductory challenges (like Titanic) and more complex Featured competitions.\n  * Focuses on foundational regression skills without additional constraints like fairness or adversarial aspects.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with continuous features and a continuous target variable.\n* **Data Files:**\n  * `train.csv`: Contains both feature columns (`cont1`-`cont14`) and the target variable.\n  * `test.csv`: Contains only feature columns; participants must predict the target.\n  * `sample_submission.csv`: Demonstrates the required submission format.\n* **Features:** 14 continuous features (`cont1` through `cont14`), all anonymized/numerical.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Components of RMSE:**\n  * Calculated as the square root of the average squared differences between predicted values and actual values.\n  * Formula: $$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n  * Penalizes larger errors more heavily due to the squaring operation.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict a continuous target variable based on provided feature columns. The competition is designed as an approachable challenge for beginners to practice regression skills on tabular data.\n* **Key Points:**\n  * Intended as a beginner-friendly competition, positioned between introductory challenges (like Titanic) and more complex Featured competitions.\n  * Focuses on foundational regression skills without additional constraints like fairness or adversarial aspects.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with continuous features and a continuous target variable.\n* **Data Files:**\n  * `train.csv`: Contains both feature columns (`cont1`-`cont14`) and the target variable.\n  * `test.csv`: Contains only feature columns; participants must predict the target.\n  * `sample_submission.csv`: Demonstrates the required submission format.\n* **Features:** 14 continuous features (`cont1` through `cont14`), all anonymized/numerical.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Components of RMSE:**\n  * Calculated as the square root of the average squared differences between predicted values and actual values.\n  * Formula: $$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n  * Penalizes larger errors more heavily due to the squaring operation."
    },
    "file_path": "kaggle_datasets/417/problem_summary.md"
  },
  "279": {
    "problem_id": "279",
    "title": "Santander Transaction Value Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Santander Transaction Value Prediction\n\n## Problem Description\n- **Problem Type:** Regression\n- **Objective:** Predict the monetary value of future transactions for potential banking customers. The goal is to enable Santander to personalize financial services by anticipating customer needs at scale.\n- **Key Points:**\n  - Focuses on predicting exact transaction amounts rather than just identifying service needs\n  - Part of Santander's digital transformation to provide proactive, personalized banking\n  - Uses anonymized customer data to protect privacy while solving the business problem\n\n## Dataset Overview\n- **Data Type:** Tabular data with anonymized numerical features\n- **Context:** Banking/financial services customer data with transaction values\n- **Data Files:**\n  - train.csv (contains target values for model training)\n  - test.csv (contains records for prediction)\n  - sample_submission.csv (demonstrates submission format)\n- **Features:**\n  - All features are anonymized numerical variables\n  - Contains an ID column (string) and target column (numeric transaction value)\n  - Dataset contains 9,987 columns (high-dimensional feature space)\n\n## Evaluation Metrics\n- **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n- **Metric Components:**\n  - Calculated as: √(1/n * Σ(log(p_i + 1) - log(a_i + 1))²\n  - Where:\n    - n = number of observations\n    - p_i = predicted value\n    - a_i = actual target value\n    - log = natural logarithm\n  - Penalizes underestimates more than overestimates due to logarithmic scaling\n  - Handles large value ranges effectively by compressing scale",
    "sections": {},
    "file_path": "kaggle_datasets/279/problem_summary.md"
  },
  "410": {
    "problem_id": "410",
    "title": "Drone Delivery Fleet Optimization",
    "problem_type": "Optimization (Scheduling & Resource Allocation)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Drone Delivery Fleet Optimization\n\n**Problem Description:**\n* **Problem Type:** Optimization (Scheduling & Resource Allocation)\n* **Objective:**  \n  Coordinate a fleet of drones to fulfill customer orders as quickly as possible, considering:\n  * Multiple drones with limited capacity\n  * Warehouses with distributed product inventories\n  * Customer orders with specific product requirements and delivery locations\n  * Time-based scoring system prioritizing faster deliveries\n* **Key Points:**\n  * **Constraints:** \n    * Drones have payload limits (product weight constraints)\n    * Warehouses have finite inventory\n    * Orders must be fully fulfilled (all items delivered) to count as completed\n  * **Dynamic Scoring:** Points per order depend on completion time (earlier = higher score)\n\n**Dataset Overview:**\n* **Data Type:** Synthetic simulation data (text-based input files defining the problem scenario)\n* **Data Files:**\n  * `busy_day.in`: Primary input file containing:\n    * Drone fleet specifications (count, capacity)\n    * Warehouse locations and inventory\n    * Customer order details (products requested, delivery locations)\n  * `hashcode_delivery_instructions.pdf`: Detailed problem specification with:\n    * Simulation rules\n    * Turn-based mechanics\n    * Additional constraints (e.g., drone movement between locations)\n* **Key Features:**\n  * Grid-based world with coordinates for all locations\n  * Product types with associated weights\n  * Time measured in discrete \"turns\"\n\n**Evaluation Metrics:**\n* **Primary Metric:** Time-weighted order completion score\n* **Scoring Components:**\n  * Each order earns `ceil((T - t)/T * 100)` points, where:\n    * `T` = Total simulation duration (turns)\n    * `t` = Turn when the order was completed (last item delivered)\n  * **Key Properties:**\n    * Maximum 100 points per order (completed on turn 0)\n    * Points decrease linearly with completion time\n    * Final score aggregates points across all orders",
    "sections": {
      "Problem Description": "* **Problem Type:** Optimization (Scheduling & Resource Allocation)\n* **Objective:**  \n  Coordinate a fleet of drones to fulfill customer orders as quickly as possible, considering:\n  * Multiple drones with limited capacity\n  * Warehouses with distributed product inventories\n  * Customer orders with specific product requirements and delivery locations\n  * Time-based scoring system prioritizing faster deliveries\n* **Key Points:**\n  * **Constraints:** \n    * Drones have payload limits (product weight constraints)\n    * Warehouses have finite inventory\n    * Orders must be fully fulfilled (all items delivered) to count as completed\n  * **Dynamic Scoring:** Points per order depend on completion time (earlier = higher score)",
      "Dataset Overview": "* **Data Type:** Synthetic simulation data (text-based input files defining the problem scenario)\n* **Data Files:**\n  * `busy_day.in`: Primary input file containing:\n    * Drone fleet specifications (count, capacity)\n    * Warehouse locations and inventory\n    * Customer order details (products requested, delivery locations)\n  * `hashcode_delivery_instructions.pdf`: Detailed problem specification with:\n    * Simulation rules\n    * Turn-based mechanics\n    * Additional constraints (e.g., drone movement between locations)\n* **Key Features:**\n  * Grid-based world with coordinates for all locations\n  * Product types with associated weights\n  * Time measured in discrete \"turns\"",
      "Evaluation Metrics": "* **Primary Metric:** Time-weighted order completion score\n* **Scoring Components:**\n  * Each order earns `ceil((T - t)/T * 100)` points, where:\n    * `T` = Total simulation duration (turns)\n    * `t` = Turn when the order was completed (last item delivered)\n  * **Key Properties:**\n    * Maximum 100 points per order (completed on turn 0)\n    * Points decrease linearly with completion time\n    * Final score aggregates points across all orders"
    },
    "file_path": "kaggle_datasets/410/problem_summary.md"
  },
  "622": {
    "problem_id": "622",
    "title": "Binary Prediction of Poisonous Mushrooms",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Prediction of Poisonous Mushrooms\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a mushroom is edible (`e`) or poisonous (`p`) based on its physical characteristics.\n* **Key Points:**\n  * Synthetic dataset derived from the UCI Mushroom dataset, with intentionally uncleaned categorical artifacts.\n  * Competitors may incorporate the original UCI dataset to explore differences or improve model performance.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing physical characteristics of mushrooms.\n* **Data Files:**\n  * `train.csv`: Training dataset with binary target column `class` (`e` or `p`).\n  * `test.csv`: Test dataset for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in required format.\n* **Features:** 45 columns of mushroom attributes (specific features not listed, but derived from UCI Mushroom dataset properties).\n\n## Evaluation Metrics\n* **Primary Metric:** Matthews Correlation Coefficient (MCC)\n* **Metric Details:**\n  * Measures quality of binary classifications, especially useful for imbalanced datasets.\n  * Ranges from -1 (total disagreement) to +1 (perfect prediction).\n  * Accounts for all four confusion matrix categories (TP, TN, FP, FN).",
    "sections": {},
    "file_path": "kaggle_datasets/622/problem_summary.md"
  },
  "246": {
    "problem_id": "246",
    "title": "Multi-class Image Classification for E-commerce Products",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Image Classification for E-commerce Products\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Computer Vision - Image Classification)\n* **Objective:**  \n    * Predict the category of an e-commerce product based on its image(s).  \n    * Each product may have 1-4 associated images, requiring models to handle multi-image inputs.  \n    * The task involves classifying products into one of over 5,000 possible categories.  \n* **Key Points:**  \n    * Extreme multi-class setting (5,000+ categories).  \n    * Products may have multiple images, requiring aggregation or fusion techniques.  \n    * Hierarchical category structure exists (level1, level2, level3 in French), though not required for submission.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Image data (product photos) from Cdiscount's e-commerce catalog.  \n    * Images are provided in JPEG format (180x180 resolution) embedded in BSON files.  \n* **Data Files:**  \n    * `train.bson` (58.2 GB): Contains 7M+ products with `_id`, `category_id`, and 1-4 images per product.  \n    * `test.bson` (14.5 GB): 1.7M products to classify (no `category_id` provided).  \n    * `category_names.csv`: Maps `category_id` to hierarchical French labels (level1-level3).  \n    * `sample_submission.csv`: Submission template with `_id` and predicted `category_id`.  \n* **Key Features:**  \n    * Binary image data stored as JPEG strings in BSON format.  \n    * Each product has a unique `_id` and may belong to any of 5,000+ `category_id` classes.  \n\n## Evaluation Metrics\n* **Primary Metric:** Categorization Accuracy  \n    * Percentage of correctly predicted `category_id` values for test products.  \n* **Submission Format:**  \n    * CSV file with `_id` and predicted `category_id` (one row per product).  \n    * Predictions must match exact `category_id` labels (no partial credit for hierarchical proximity).",
    "sections": {},
    "file_path": "kaggle_datasets/246/problem_summary.md"
  },
  "444": {
    "problem_id": "444",
    "title": "Multiclass Classification of eCommerce Product Categories",
    "problem_type": "Multiclass Classification",
    "objective": "Predict the probability that an eCommerce product belongs to each of multiple class categories based on anonymized product attributes.",
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of eCommerce Product Categories\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the probability that an eCommerce product belongs to each of multiple class categories based on anonymized product attributes.\n    * **Key Points:**\n        * Dataset is synthetic but based on real eCommerce product data\n        * Designed as an approachable competition for beginners\n        * Features are anonymized but maintain real-world properties\n        * Increased observations, features, and class labels compared to previous month's competition\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing eCommerce product listings with anonymized features\n* **Data Files:**\n    * `train.csv` - Contains product IDs, anonymized features (`feature_*` columns), and class labels (`target`)\n    * `test.csv` - Contains product IDs and anonymized features for prediction\n    * `sample_submission.csv` - Example submission file format\n* **Features:** \n    * All features are anonymized (named as `feature_*`)\n    * Features maintain properties of real-world product attributes\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * For each row, predicted probabilities must be provided for all classes\n        * Probabilities are rescaled (each row divided by row sum) before scoring\n        * Probabilities are clipped to [10^-15, 1-10^-15] to avoid extremes of log function\n        * Formula: \n            ```\n            log loss = -1/N * Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n            ```\n            Where N = number of rows, M = number of classes, y_ij = 1 if observation i is in class j (0 otherwise), p_ij = predicted probability for class j",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the probability that an eCommerce product belongs to each of multiple class categories based on anonymized product attributes.\n    * **Key Points:**\n        * Dataset is synthetic but based on real eCommerce product data\n        * Designed as an approachable competition for beginners\n        * Features are anonymized but maintain real-world properties\n        * Increased observations, features, and class labels compared to previous month's competition",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing eCommerce product listings with anonymized features\n* **Data Files:**\n    * `train.csv` - Contains product IDs, anonymized features (`feature_*` columns), and class labels (`target`)\n    * `test.csv` - Contains product IDs and anonymized features for prediction\n    * `sample_submission.csv` - Example submission file format\n* **Features:** \n    * All features are anonymized (named as `feature_*`)\n    * Features maintain properties of real-world product attributes",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * For each row, predicted probabilities must be provided for all classes\n        * Probabilities are rescaled (each row divided by row sum) before scoring\n        * Probabilities are clipped to [10^-15, 1-10^-15] to avoid extremes of log function\n        * Formula: \n            ```\n            log loss = -1/N * Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n            ```\n            Where N = number of rows, M = number of classes, y_ij = 1 if observation i is in class j (0 otherwise), p_ij = predicted probability for class j"
    },
    "file_path": "kaggle_datasets/444/problem_summary.md"
  },
  "212": {
    "problem_id": "212",
    "title": "Character Recognition in Google Street View Images with Julia",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Character Recognition in Google Street View Images with Julia\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (Image Recognition)\n* **Objective**: Identify characters (A-Z, a-z, 0-9) from Google Street View images using the Julia programming language. The task focuses on recognizing characters with varied fonts and backgrounds, differing from traditional OCR tasks.\n* **Key Points**:\n  * Designed as an introductory competition to promote Julia for data science\n  * Features non-uniform backgrounds and multiple font styles\n  * Includes tutorials on Julia implementation and parallel processing\n\n## Dataset Overview\n* **Data Type**: Image data (character crops from Google Street View)\n* **Context**: 20x20 pixel grayscale images of alphanumeric characters\n* **Data Files**:\n  * train.zip/test.zip: Original Bmp images\n  * trainResized.zip/testResized.zip: Preprocessed 20x20 images\n  * trainLabels.csv: Character labels for training set\n  * sampleSubmission.csv: Submission format template\n* **Features**:\n  * 400 features per image (20x20 pixel values)\n  * Characters include uppercase, lowercase letters and digits\n\n## Evaluation Metrics\n* **Primary Metric**: Classification Accuracy\n* **Calculation**: \n  * Accuracy = (Number of correct predictions) / (Total predictions)\n  * Evaluated on exact character matches (case-sensitive)\n* **Submission Format**:\n  * CSV with ImageId and predicted Class columns\n  * Requires case-sensitive predictions (e.g., 'A' vs 'a')",
    "sections": {},
    "file_path": "kaggle_datasets/212/problem_summary.md"
  },
  "215": {
    "problem_id": "215",
    "title": "Optimizing Gift Bag Packing with Uncertain Weights",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Optimizing Gift Bag Packing with Uncertain Weights\n\n## Problem Description\n* **Problem Type:** Combinatorial Optimization with Uncertainty\n* **Objective:** Help Santa optimally pack gifts into bags under weight uncertainty constraints:\n    * Maximize total weight of gifts packed across 1000 bags\n    * Each bag must contain ≥3 gifts\n    * No bag can exceed 50lbs (or entire bag is discarded)\n    * No gift can be used more than once\n* **Key Points:**\n    * Gift weights are unknown but follow known probability distributions\n    * Must handle stochastic weight constraints probabilistically\n    * Classic bin packing problem with stochastic elements\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing Christmas gifts with uncertain weights\n* **Data Files:**\n    * `gifts.csv`: Contains GiftIds (e.g., \"horse_0\", \"book_345\") indicating gift types\n    * `sample_submission.csv`: Example submission format\n* **Features:**\n    * Gift types (9 categories) with distinct weight distributions:\n        * horse, ball, bike, train, coal, book, doll, block, gloves\n    * Each type follows a specific statistical distribution (normal, beta, gamma, etc.)\n\n## Evaluation Metrics\n* **Primary Metric:** Total weight of successfully packed gifts across all valid bags\n* **Scoring Rules:**\n    * Any bag >50lbs contributes 0 weight\n    * Bags with <3 gifts contribute 0 weight\n    * Each gift can only appear in one bag\n    * Score = Σ(weight of all valid bags)",
    "sections": {},
    "file_path": "kaggle_datasets/215/problem_summary.md"
  },
  "443": {
    "problem_id": "443",
    "title": "Identifying Dataset Mentions in Scientific Publications",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Identifying Dataset Mentions in Scientific Publications\n\n## Problem Description\n* **Problem Type:** NLP - Text Extraction/Information Retrieval\n* **Objective:** Automate the discovery of how scientific datasets are referenced in publications by identifying short excerpts that mention datasets.\n* **Key Points:**\n  * Focus on finding dataset mentions \"hidden in plain sight\" within research articles\n  * Goal is to link words in articles to referenced datasets\n  * Must generalize to unseen datasets, not just match known examples\n  * Supports evidence-based policymaking by tracking usage of public data\n\n## Dataset Overview\n* **Data Type:** Scientific publication texts (JSON) with labeled dataset mentions\n* **Context:** Full texts of publications from various research areas, provided by CHORUS publisher members\n* **Data Files:**\n  * `train/` - Training publications in JSON format (sectioned text)\n  * `test/` - Test publications in JSON format\n  * `train.csv` - Labels and metadata for training set\n  * `sample_submission.csv` - Submission format example\n* **Key Features:**\n  * JSON files contain sectioned publication text with section titles\n  * Training labels include:\n    * Publication IDs and titles\n    * Dataset titles mentioned\n    * Text excerpts containing dataset labels\n    * Cleaned label versions (lowercase, alphanumeric only)\n\n## Evaluation Metrics\n* **Primary Metric:** Micro F0.5 score based on Jaccard similarity\n* **Scoring Components:**\n  * Jaccard score calculated between predicted and ground truth text snippets\n  * Predictions processed alphabetically, with best matches selected\n  * Threshold of 0.5 Jaccard score for true positives\n  * Final score considers:\n    * True Positives (TP) - Correct matches above threshold\n    * False Positives (FP) - Incorrect or below-threshold predictions\n    * False Negatives (FN) - Unmatched ground truths\n  * Text cleaning applied before comparison:\n    * Convert to lowercase\n    * Keep only alphanumeric characters\n    * Replace other characters with spaces",
    "sections": {},
    "file_path": "kaggle_datasets/443/problem_summary.md"
  },
  "488": {
    "problem_id": "488",
    "title": "Location-Based Species Presence Prediction with Multi-Modal Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Location-Based Species Presence Prediction with Multi-Modal Data\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (Top-K Species Prediction)\n* **Objective**: Predict a set of candidate species likely to be observed at given GPS locations using multi-modal environmental data. The goal is to return a ranked list of 30 species for each location that should contain the true observed species.\n* **Key Points**:\n  * Focuses on biodiversity prediction for conservation/management applications\n  * Combines visual (aerial imagery) and environmental features\n  * Covers 17K species (9K plants, 8K animals) across France and US\n  * Spatial block holdout used to prevent spatial bias in evaluation\n\n## Dataset Overview\n* **Data Type**: Multi-modal (Geospatial + Image + Tabular)\n* **Context**: 1.6M geo-localized species observations paired with environmental features\n* **Data Files**:\n  * Species observation files (CSV with GPS coordinates and species IDs)\n  * Aerial imagery patches (256x256m RGB-IR JPEGs)\n  * Environmental rasters (land cover, altitude, bioclimatic, pedologic data in TIFF/GeoTIFF)\n* **Key Features**:\n  * High-resolution remote sensing imagery (1m/pixel)\n  * 19 bioclimatic variables (~1km resolution)\n  * 8 pedologic variables (250m resolution)\n  * Land cover and altitude data (1m resolution)\n\n## Evaluation Metrics\n* **Primary Metric**: Top-30 Error Rate\n  * For each observation, model must predict 30 candidate species\n  * Error = 1 if true species not in top-30, 0 otherwise\n  * Final score = average error across all test observations\n* **Evaluation Protocol**:\n  * Spatial block holdout (5km×5km quadrats)\n  * 2.5% quadrats randomly sampled for test set\n  * Validation set created with same procedure",
    "sections": {},
    "file_path": "kaggle_datasets/488/problem_summary.md"
  },
  "481": {
    "problem_id": "481",
    "title": "Predicting NCAA Men's Basketball Tournament Outcomes",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Men's Basketball Tournament Outcomes\n\n## Problem Description\n* **Problem Type:** Binary Classification (Probability Prediction)\n* **Objective:** Predict the probability of one team beating another in the NCAA Men's Basketball Tournament (March Madness). Participants must forecast outcomes for all possible matchups in:\n  * Stage 1: Past tournaments (2016-2019, 2021) for model validation\n  * Stage 2: The 2022 tournament (primary competition focus)\n* **Key Points:**\n  * Two-phase competition with separate historical and future prediction stages\n  * Encourages use of external data sources beyond provided datasets\n  * Requires predictions for all possible team pairings (n*(n-1)/2 matchups per tournament)\n  * Special consideration for play-in games and bracket structure\n\n## Dataset Overview\n* **Data Type:** Tabular data with extensive basketball statistics and tournament records\n* **Context:** Comprehensive NCAA Division I men's basketball data from 1985-2022 season\n* **Key Data Files:**\n  * Team metadata (MTeams.csv)\n  * Tournament seeds (MNCAATourneySeeds.csv)\n  * Game results (compact and detailed versions):\n    * Regular season (MRegularSeasonCompactResults.csv)\n    * Tournament games (MNCAATourneyCompactResults.csv)\n  * Team box scores (2003 onward)\n  * Geographical data (game locations)\n  * Public ranking systems (Massey Ordinals)\n  * Supplemental data (coaches, conferences, alternative spellings)\n* **Important Features:**\n  * Team IDs and historical performance\n  * Game scores and outcomes\n  * Advanced basketball statistics (FG%, rebounds, assists etc.)\n  * Tournament seeds and bracket positions\n  * Geographical game locations\n  * Multiple ranking system outputs\n\n## Evaluation Metrics\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n* **Metric Components:**\n  * Formula: -1/n Σ[y_i*log(p_i) + (1-y_i)*log(1-p_i)]\n    * n = number of games\n    * y_i = 1 if team1 wins, 0 if team2 wins\n    * p_i = predicted probability of team1 winning\n  * Key Properties:\n    * Heavily penalizes confident incorrect predictions\n    * Predictions bounded away from 0 and 1 to avoid infinite penalties\n    * Natural logarithm used in calculation",
    "sections": {},
    "file_path": "kaggle_datasets/481/problem_summary.md"
  },
  "475": {
    "problem_id": "475",
    "title": "Ranking Toxic Comment Severity",
    "problem_type": "Ordinal Regression / Ranking (Relative Toxicity Scoring)",
    "objective": "Develop a model that assigns toxicity severity scores to comments such that the rankings of comment pairs match human annotators' judgments of relative toxicity. The goal is to capture the *degree* of toxicity rather than binary classification.",
    "evaluation_metric": null,
    "full_content": "# Ranking Toxic Comment Severity\n\n**Problem Description:**\n* **Problem Type:** Ordinal Regression / Ranking (Relative Toxicity Scoring)\n* **Objective:** Develop a model that assigns toxicity severity scores to comments such that the rankings of comment pairs match human annotators' judgments of relative toxicity. The goal is to capture the *degree* of toxicity rather than binary classification.\n    * **Key Points:**\n        * Focuses on ranking severity across a spectrum (from innocuous to highly toxic).\n        * Annotators compared pairs of comments to determine relative toxicity, avoiding absolute labeling challenges.\n        * No predefined numeric range for scores; only relative ordering matters.\n        * Ties in scores are penalized (evaluated as `0` agreement).\n\n**Dataset Overview:**\n* **Data Type:** Text data (English comments from online discussions, potentially containing offensive language).\n* **Data Files:**\n    * `comments_to_score.csv`: Contains ~14k comments to be scored (no labels provided).\n    * `validation_data.csv`: Paired rankings from annotators (worker IDs, comment pairs, and their relative toxicity judgments).\n    * `sample_submission.csv`: Example submission format (`comment_id`, `score`).\n* **Features:**\n    * Primary feature: Raw comment text (`text` column).\n    * No explicit training data; participants may leverage external datasets (e.g., prior Jigsaw competitions) or pretrained models.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Average Agreement with Annotators.\n    * **Components:**\n        1. For each comment pair in the test set, the model's scores are used to rank the pair.\n        2. A score of `1` is assigned if the model's ranking matches the annotator's ranking; `0` otherwise.\n        3. Final score is the average across all ~200,000 pairwise comparisons.\n    * **Note:** Scores need not be bounded (e.g., can range arbitrarily), but ties are discouraged.",
    "sections": {
      "Problem Description": "* **Problem Type:** Ordinal Regression / Ranking (Relative Toxicity Scoring)\n* **Objective:** Develop a model that assigns toxicity severity scores to comments such that the rankings of comment pairs match human annotators' judgments of relative toxicity. The goal is to capture the *degree* of toxicity rather than binary classification.\n    * **Key Points:**\n        * Focuses on ranking severity across a spectrum (from innocuous to highly toxic).\n        * Annotators compared pairs of comments to determine relative toxicity, avoiding absolute labeling challenges.\n        * No predefined numeric range for scores; only relative ordering matters.\n        * Ties in scores are penalized (evaluated as `0` agreement).",
      "Dataset Overview": "* **Data Type:** Text data (English comments from online discussions, potentially containing offensive language).\n* **Data Files:**\n    * `comments_to_score.csv`: Contains ~14k comments to be scored (no labels provided).\n    * `validation_data.csv`: Paired rankings from annotators (worker IDs, comment pairs, and their relative toxicity judgments).\n    * `sample_submission.csv`: Example submission format (`comment_id`, `score`).\n* **Features:**\n    * Primary feature: Raw comment text (`text` column).\n    * No explicit training data; participants may leverage external datasets (e.g., prior Jigsaw competitions) or pretrained models.",
      "Evaluation Metrics": "* **Evaluation Metric:** Average Agreement with Annotators.\n    * **Components:**\n        1. For each comment pair in the test set, the model's scores are used to rank the pair.\n        2. A score of `1` is assigned if the model's ranking matches the annotator's ranking; `0` otherwise.\n        3. Final score is the average across all ~200,000 pairwise comparisons.\n    * **Note:** Scores need not be bounded (e.g., can range arbitrarily), but ties are discouraged."
    },
    "file_path": "kaggle_datasets/475/problem_summary.md"
  },
  "223": {
    "problem_id": "223",
    "title": "Fish Species Classification from Boat Camera Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fish Species Classification from Boat Camera Images\n\n## Problem Description\n* **Problem Type:** Multi-class Image Classification\n* **Objective:** Develop algorithms to automatically detect and classify species of fish caught by fishing boats from camera images, accelerating the video review process for conservation monitoring.\n    * **Key Points:**\n        * Focus on identifying 8 distinct categories of marine life (tuna species, sharks, etc.) or absence of fish.\n        * Addresses conservation challenges by automating detection of illegal/unreported fishing.\n        * Each image contains only one primary fish category (though small bait fish may be present).\n\n## Dataset Overview\n* **Data Type:** Image data (photos captured from fishing boat cameras)\n* **Context:** Conservation monitoring of fishing activities in the Pacific Ocean\n* **Data Files:**\n    * `train.zip` - Labeled training images organized by species folders\n    * `test_stg1.zip` - Stage 1 test images\n    * `test_stg2.zip` - Stage 2 test images (released later)\n    * Sample submission files for both stages\n* **Features:**\n    * Images showing fishing boat decks with various angles/lighting\n    * 8 target classes: 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * For each image, submit predicted probabilities for all 8 classes\n        * Probabilities are rescaled to sum to 1 before scoring\n        * Formula: \n            ```\n            logloss = -1/N * Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n            ```\n            where N = number of images, M = number of classes\n        * Probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes",
    "sections": {},
    "file_path": "kaggle_datasets/223/problem_summary.md"
  },
  "224": {
    "problem_id": "224",
    "title": "Predicting Rental Listing Interest Levels",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Rental Listing Interest Levels\n\n## Problem Description\n* **Problem Type:** Multiclass Classification  \n* **Objective:** Predict the level of interest (high, medium, low) a new rental listing on RentHop will receive based on listing features. The goal is to help RentHop improve fraud control, identify listing quality issues, and understand renter preferences.  \n* **Key Points:**  \n  * Interest level is derived from the number of inquiries a listing receives while active.  \n  * Focus on leveraging listing content (text, photos, metadata) to predict popularity.  \n  * Real-world business applications include fraud detection and market trend analysis.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular and multimodal data (text, images, geospatial) from RentHop apartment listings in New York City.  \n* **Data Files:**  \n  * `train.json`/`test.json`: Contain listing details (features, descriptions, photos).  \n  * `sample_submission.csv`: Example submission format.  \n  * Optional image files (`images_sample.zip`, `Kaggle-renthop.7z`).  \n* **Key Features:**  \n  * Structured: `bathrooms`, `bedrooms`, `price`, `latitude/longitude`, `manager_id`.  \n  * Unstructured: `description`, `photos`, `features` (list of amenities).  \n  * Target: `interest_level` (3 classes: high/medium/low).  \n\n## Evaluation Metrics  \n* **Primary Metric:** Multi-class logarithmic loss (log loss).  \n* **Components:**  \n  * Formula:  \n    ```\n    logloss = −1/N ∑(i=1 to N) ∑(j=1 to M) y_ij log(p_ij)\n    ```  \n    Where:  \n    - `N` = number of listings, `M` = 3 classes.  \n    - `y_ij` = 1 if observation `i` is in class `j`, else 0.  \n    - `p_ij` = predicted probability of class `j` for observation `i`.  \n  * Probabilities are rescaled (row-wise normalization).  \n  * Clamping: Predictions bounded to `[10^-15, 1-10^-15]` to avoid log(0).",
    "sections": {},
    "file_path": "kaggle_datasets/224/problem_summary.md"
  },
  "640": {
    "problem_id": "640",
    "title": "Backpack Price Prediction Challenge",
    "problem_type": "Regression",
    "objective": "Predict the price of backpacks given various attributes. The goal is to build a model that accurately estimates backpack prices based on provided features.",
    "evaluation_metric": null,
    "full_content": "# Backpack Price Prediction Challenge\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the price of backpacks given various attributes. The goal is to build a model that accurately estimates backpack prices based on provided features.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world backpack price data.\n  * Participants can optionally use the original dataset to explore differences or improve model performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing various attributes of backpacks and their corresponding prices.\n* **Data Files:**\n  * train.csv - training dataset with Price as target\n  * train_extra.csv - additional training data\n  * test.csv - test dataset for predictions\n  * sample_submission.csv - example submission file\n* **Features:** The dataset contains 34 columns (specific features not named, but presumably include various backpack characteristics that influence price).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Metric Calculation:**\n  * RMSE = √(1/N * Σ(y_i - ŷ_i)²)\n  * Where:\n    * N = number of observations\n    * y_i = actual value\n    * ŷ_i = predicted value\n  * Lower RMSE values indicate better performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the price of backpacks given various attributes. The goal is to build a model that accurately estimates backpack prices based on provided features.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world backpack price data.\n  * Participants can optionally use the original dataset to explore differences or improve model performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing various attributes of backpacks and their corresponding prices.\n* **Data Files:**\n  * train.csv - training dataset with Price as target\n  * train_extra.csv - additional training data\n  * test.csv - test dataset for predictions\n  * sample_submission.csv - example submission file\n* **Features:** The dataset contains 34 columns (specific features not named, but presumably include various backpack characteristics that influence price).",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n* **Metric Calculation:**\n  * RMSE = √(1/N * Σ(y_i - ŷ_i)²)\n  * Where:\n    * N = number of observations\n    * y_i = actual value\n    * ŷ_i = predicted value\n  * Lower RMSE values indicate better performance"
    },
    "file_path": "kaggle_datasets/640/problem_summary.md"
  },
  "472": {
    "problem_id": "472",
    "title": "Optimizing Movie Permutation Schedules for SantaTV+",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Optimizing Movie Permutation Schedules for SantaTV+\n\n## Problem Description\n- **Problem Type**: Combinatorial Optimization (Permutation Coverage with Constraints)\n- **Objective**: \n  - Create three strings (schedules) that collectively contain all permutations of seven Christmas-themed symbols (🎅, 🤶, 🦌, 🧝, 🎄, 🎁, 🎀) as substrings.\n  - Key Constraints:\n    * Every permutation must appear in at least one string\n    * All permutations starting with 🎅🤶 must appear in all three strings\n    * Each string may contain up to two 🌟 wildcards (which match any symbol)\n    * No 7-length substring may contain >1 wildcard to count as a valid permutation\n- **Key Points**:\n  * This is a minimization problem - score is the length of the longest string\n  * Inspired by the shortest superpermutation problem with team constraints\n  * Wildcards act as optimization shortcuts to reduce total schedule length\n\n## Dataset Overview\n- **Data Type**: Symbolic permutation strings (no traditional training data)\n- **Context**: Theoretical problem with Christmas movie scheduling theme\n- **Data Files**:\n  * `permutations.csv`: All 5040 possible permutations of the 7 symbols\n  * `distance_matrix.csv`: Asymmetric distance matrix between permutations (for TSP formulation)\n  * `wildcards.csv`: Mapping of wildcard-containing strings to valid permutations\n  * `sample_submission.csv`: Example submission format\n- **Features**:\n  * Problem revolves around permutations of 7 distinct symbols\n  * Wildcards introduce partial matching capability\n  * Distance matrix enables graph-based approaches\n\n## Evaluation Metrics\n- **Primary Metric**: Length of the longest schedule string (minimization)\n- **Scoring Logic**:\n  1. All 5040 permutations must be covered across three strings\n  2. All 120 permutations starting with 🎅🤶 must appear in all three strings\n  3. Wildcards (🌟) can represent any symbol when matching permutations\n  4. Score = max(len(string1), len(string2), len(string3))\n- **Validation**:\n  * Solutions must pass permutation coverage verification\n  * Wildcard usage must comply with constraints (≤2 per string, ≤1 per 7-length window)",
    "sections": {},
    "file_path": "kaggle_datasets/472/problem_summary.md"
  },
  "486": {
    "problem_id": "486",
    "title": "Cryptocurrency Price Return Forecasting with High-Frequency Market Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Cryptocurrency Price Return Forecasting with High-Frequency Market Data\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting (Financial Returns Prediction)\n* **Objective**: Predict short-term (15-minute) residualized returns for 14 cryptocurrencies using high-frequency market data. The competition focuses on identifying persistent alpha signals in highly volatile, non-stationary crypto markets.\n* **Key Points**:\n  * Targets 15-minute residualized returns (see linked tutorial for calculation details)\n  * Data exhibits high volatility and non-stationary correlation structures\n  * Competition features a unique two-phase evaluation:\n    * Initial training phase with historical data\n    * Live forecasting phase using real market data post-submission\n  * Special emphasis on avoiding overfitting given transitory market signals\n\n## Dataset Overview\n* **Data Type**: High-frequency time series tabular data (minute-level crypto market metrics)\n* **Context**: Millions of rows of crypto trade data from 2018 onward covering 14 major cryptocurrencies\n* **Data Files**:\n  * `train.csv`: Primary training data with minute-level market metrics\n  * `supplemental_train.csv`: Additional training data covering part of submission period\n  * `asset_details.csv`: Mapping of Asset_IDs to crypto names and metric weights\n  * Example/test files for API integration\n* **Key Features**:\n  * Timestamp, Asset_ID\n  * OHLCV metrics (Open, High, Low, Close, Volume)\n  * Trade counts and VWAP (Volume Weighted Average Price)\n  * Target: 15-minute residualized returns\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Pearson Correlation Coefficient\n  * Weights assigned per cryptoasset (provided in asset_details.csv)\n  * Measures correlation between predicted and actual returns\n* **Implementation Notes**:\n  * Uses custom time-series API to prevent lookahead\n  * Submissions must handle live data streaming\n  * Null/infinite values prohibited\n  * Evaluation occurs over 3-month period post-submission using real market data",
    "sections": {},
    "file_path": "kaggle_datasets/486/problem_summary.md"
  },
  "278": {
    "problem_id": "278",
    "title": "Particle Tracking in CERN Detectors",
    "problem_type": "Clustering (Particle Tracking in High Energy Physics)",
    "objective": "Reconstruct particle tracks from 3D hit points in silicon detectors by grouping hits that belong to the same initial particle. The goal is to accurately associate each recorded hit with its originating particle trajectory.",
    "evaluation_metric": null,
    "full_content": "# Particle Tracking in CERN Detectors\n\n**Problem Description:**\n* **Problem Type:** Clustering (Particle Tracking in High Energy Physics)\n* **Objective:** Reconstruct particle tracks from 3D hit points in silicon detectors by grouping hits that belong to the same initial particle. The goal is to accurately associate each recorded hit with its originating particle trajectory.\n* **Key Points:**\n  * Each hit must be uniquely assigned to one reconstructed track\n  * The challenge consists of two phases: Accuracy (scoring) and Throughput (speed)\n  * The detector environment produces complex patterns from hundreds of millions of collisions per second\n  * Solution must handle independent collision events separately\n\n**Dataset Overview:**\n* **Data Type:** 3D spatial coordinates from particle detector hits with associated metadata\n* **Context:** Simulated proton collision data from CERN's Large Hadron Collider\n* **Data Files:**\n  * Train files (train_{1-5}.zip, train_sample.zip) - 8850 events total\n  * Test files (test.zip) - 125 events\n  * Detector geometry files (detectors.zip)\n  * Sample submission file\n* **Key Features:**\n  * Hits data: x,y,z coordinates, volume/layer/module IDs\n  * Truth data: particle IDs, true intersection points, momenta, scoring weights\n  * Particles data: initial positions, momenta, charges\n  * Cells data: detector cell information for each hit\n  * Detector geometry: module positions and orientations\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Custom weighted intersection metric\n* **Components:**\n  * Each hit has a pre-defined weight (higher for important hits)\n  * Tracks are matched to particles using double majority rule:\n    * >50% of track points must belong to the matched particle\n    >50% of particle's points must belong to the matched track\n  * Score per track = sum of weights of correctly matched hits\n  * Event score = sum of all track scores (normalized to 1 per event)\n  * Final score = average over all test events\n  * Perfect score = 1, random solution ≈ 0",
    "sections": {
      "Problem Description": "* **Problem Type:** Clustering (Particle Tracking in High Energy Physics)\n* **Objective:** Reconstruct particle tracks from 3D hit points in silicon detectors by grouping hits that belong to the same initial particle. The goal is to accurately associate each recorded hit with its originating particle trajectory.\n* **Key Points:**\n  * Each hit must be uniquely assigned to one reconstructed track\n  * The challenge consists of two phases: Accuracy (scoring) and Throughput (speed)\n  * The detector environment produces complex patterns from hundreds of millions of collisions per second\n  * Solution must handle independent collision events separately",
      "Dataset Overview": "* **Data Type:** 3D spatial coordinates from particle detector hits with associated metadata\n* **Context:** Simulated proton collision data from CERN's Large Hadron Collider\n* **Data Files:**\n  * Train files (train_{1-5}.zip, train_sample.zip) - 8850 events total\n  * Test files (test.zip) - 125 events\n  * Detector geometry files (detectors.zip)\n  * Sample submission file\n* **Key Features:**\n  * Hits data: x,y,z coordinates, volume/layer/module IDs\n  * Truth data: particle IDs, true intersection points, momenta, scoring weights\n  * Particles data: initial positions, momenta, charges\n  * Cells data: detector cell information for each hit\n  * Detector geometry: module positions and orientations",
      "Evaluation Metrics": "* **Evaluation Metric:** Custom weighted intersection metric\n* **Components:**\n  * Each hit has a pre-defined weight (higher for important hits)\n  * Tracks are matched to particles using double majority rule:\n    * >50% of track points must belong to the matched particle\n    >50% of particle's points must belong to the matched track\n  * Score per track = sum of weights of correctly matched hits\n  * Event score = sum of all track scores (normalized to 1 per event)\n  * Final score = average over all test events\n  * Perfect score = 1, random solution ≈ 0"
    },
    "file_path": "kaggle_datasets/278/problem_summary.md"
  },
  "623": {
    "problem_id": "623",
    "title": "Binary Classification of Skin Cancer Lesions from 3D Total Body Photos",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Skin Cancer Lesions from 3D Total Body Photos\n\n## Problem Description\n- **Problem Type:** Binary Classification (Medical Image Analysis)\n- **Objective:** Develop an AI algorithm to differentiate histologically confirmed malignant skin lesions from benign ones using single-lesion crops from 3D total body photographs (TBP). The goal is to improve early triage for skin cancer detection in non-specialized settings.\n- **Key Points:**\n  - Focus on clinical utility: Algorithms must maintain high sensitivity (≥80% TPR) for cancer detection\n  - Images resemble lower-quality smartphone photos (non-dermoscopic), unlike typical dermatology clinic images\n  - Novel dataset includes all lesions per patient, enabling \"ugly duckling\" outlier detection approaches\n  - Intended for use in telemedicine and primary care settings with limited dermatology access\n\n## Dataset Overview\n- **Data Type:** JPEG images (15x15mm lesion crops) + tabular metadata\n- **Context:** Standardized lesion images extracted from 3D whole-body photography systems across 9 institutions\n- **Data Files:**\n  - `train-image/` or `train-image.hdf5`: Training images (~400k)\n  - `train-metadata.csv`: Training labels + 101 clinical/demographic/lesion features\n  - `test-image.hdf5`: Hidden test images (~500k)\n  - `test-metadata.csv`: Test set metadata (without labels)\n- **Key Features:**\n  - Image data: Lesion crops with smartphone-like quality\n  - Tabular features: Patient demographics, lesion location, color metrics (L*A*B*), border irregularity scores, size measurements\n  - Clinical targets: Histopathology-confirmed malignancy labels (`target`)\n\n## Evaluation Metrics\n- **Primary Metric:** Partial AUC (pAUC) above 80% True Positive Rate\n  - Focuses on high-sensitivity region critical for cancer detection\n  - Scores range [0.0, 0.2], calculated as area under ROC curve where TPR ≥ 80%\n  - Implementation: Uses trapezoidal integration over specified TPR range\n- **Secondary Prizes:**\n  - **Top-15 Retrieval Sensitivity:** Weighted average of malignancies found in top-15 scored lesions per patient\n  - **Model Efficiency:** Combined score of pAUC performance and runtime (max 12hr limit)",
    "sections": {},
    "file_path": "kaggle_datasets/623/problem_summary.md"
  },
  "411": {
    "problem_id": "411",
    "title": "Reinforcement Learning for Halite Resource Management",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Reinforcement Learning for Halite Resource Management\n\n## Problem Description:\n* **Problem Type:** Reinforcement Learning (Multi-agent Strategy Game)\n* **Objective:** Develop an AI agent to control a fleet of ships in a 2-player competitive environment to maximize halite (energy resource) collection. The agent must dynamically manage ship movements, shipyard creation, resource mining, and strategic collisions while adapting to opponent actions.\n* **Key Points:**\n  * Competitive resource management with real-time decision making (3-second timeout per turn)\n  * Core mechanics include ship movement, halite mining (25% per turn), ship-to-shipyard conversion (500 halite cost), and ship spawning (500 halite cost)\n  * Strategic elements: collision mechanics (smallest ship survives), halite regeneration (2% per cell), and end-game halite deposition requirements\n  * Game ends after 400 turns or when one player is eliminated (no ships/shipyards and insufficient halite to spawn)\n\n## Dataset Overview:\n* **Data Type:** Game state observations (JSON format) with full environment visibility\n* **Data Files:** \n  * Game engine provided via Kaggle Environments (`halite.py`)\n  * Observations include: \n    * Board state (21x21 grid with halite values)\n    * Ship positions/cargo (per player)\n    * Shipyard positions (per player)\n    * Current player halite reserves\n    * Turn count\n* **Features:**\n  * Grid coordinates with wrap-around topology\n  * Halite amounts per cell (0-500 units)\n  * Ship status (position, cargo load, owner)\n  * Shipyard positions and ownership\n  * Player halite reserves\n\n## Evaluation Metrics:\n* **Primary Metric:** Win/Loss Ranking (Skill Rating System)\n  * Gaussian model N(μ,σ²) estimates player skill\n  * μ increases with wins, decreases with losses\n  * σ decreases with more games played\n* **Game Resolution:**\n  * Players ranked by total deposited halite at game end\n  * Undeposited ship cargo doesn't count\n  * Eliminated players score 0\n  * Ties possible with equal halite amounts\n* **Matchmaking:**\n  * Bots face opponents with similar skill ratings\n  * ~8 games/day per submission for rating stability",
    "sections": {},
    "file_path": "kaggle_datasets/411/problem_summary.md"
  },
  "247": {
    "problem_id": "247",
    "title": "Spooky Author Identification",
    "problem_type": "NLP - Multi-class Classification",
    "objective": "Predict the author of excerpts from horror stories written by Edgar Allan Poe (EAP), HP Lovecraft (HPL), and Mary Shelley (MWS) based on textual features.",
    "evaluation_metric": null,
    "full_content": "# Spooky Author Identification\n\n**Problem Description:**\n* **Problem Type:** NLP - Multi-class Classification\n* **Objective:** Predict the author of excerpts from horror stories written by Edgar Allan Poe (EAP), HP Lovecraft (HPL), and Mary Shelley (MWS) based on textual features.\n* **Key Points:**\n  * The task involves classifying sentences into one of three authors.\n  * The dataset consists of sentences extracted from larger texts using CoreNLP's MaxEnt sentence tokenizer, which may include some non-sentences.\n  * The competition emphasizes community learning, with prizes for valuable kernels and discussions.\n\n**Dataset Overview:**\n* **Data Type & Context:** Text data from public domain horror fiction works.\n* **Data Files:**\n  * `train.csv`: Contains labeled sentences (text and author).\n  * `test.csv`: Contains sentences for which the author needs to be predicted.\n  * `sample_submission.csv`: Example submission file in the correct format.\n* **Features:**\n  * `id`: Unique identifier for each sentence.\n  * `text`: The sentence or text excerpt.\n  * `author` (in train.csv): The ground truth author label (EAP, HPL, or MWS).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Multi-class logarithmic loss (log loss).\n* **Components:**\n  * The log loss formula is:  \n    \\( \\text{logloss} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{ij} \\log(p_{ij}) \\),  \n    where:\n    * \\( N \\) = number of observations in the test set.\n    * \\( M \\) = number of class labels (3).\n    * \\( y_{ij} \\) = 1 if observation \\( i \\) belongs to class \\( j \\), else 0.\n    * \\( p_{ij} \\) = predicted probability that observation \\( i \\) belongs to class \\( j \\).\n  * Predicted probabilities are clipped to \\([10^{-15}, 1-10^{-15}]\\) to avoid extremes of the log function.\n  * Submissions must include probabilities for all three classes, rescaled to sum to 1.",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Multi-class Classification\n* **Objective:** Predict the author of excerpts from horror stories written by Edgar Allan Poe (EAP), HP Lovecraft (HPL), and Mary Shelley (MWS) based on textual features.\n* **Key Points:**\n  * The task involves classifying sentences into one of three authors.\n  * The dataset consists of sentences extracted from larger texts using CoreNLP's MaxEnt sentence tokenizer, which may include some non-sentences.\n  * The competition emphasizes community learning, with prizes for valuable kernels and discussions.",
      "Dataset Overview": "* **Data Type & Context:** Text data from public domain horror fiction works.\n* **Data Files:**\n  * `train.csv`: Contains labeled sentences (text and author).\n  * `test.csv`: Contains sentences for which the author needs to be predicted.\n  * `sample_submission.csv`: Example submission file in the correct format.\n* **Features:**\n  * `id`: Unique identifier for each sentence.\n  * `text`: The sentence or text excerpt.\n  * `author` (in train.csv): The ground truth author label (EAP, HPL, or MWS).",
      "Evaluation Metrics": "* **Evaluation Metric:** Multi-class logarithmic loss (log loss).\n* **Components:**\n  * The log loss formula is:  \n    \\( \\text{logloss} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{ij} \\log(p_{ij}) \\),  \n    where:\n    * \\( N \\) = number of observations in the test set.\n    * \\( M \\) = number of class labels (3).\n    * \\( y_{ij} \\) = 1 if observation \\( i \\) belongs to class \\( j \\), else 0.\n    * \\( p_{ij} \\) = predicted probability that observation \\( i \\) belongs to class \\( j \\).\n  * Predicted probabilities are clipped to \\([10^{-15}, 1-10^{-15}]\\) to avoid extremes of the log function.\n  * Submissions must include probabilities for all three classes, rescaled to sum to 1."
    },
    "file_path": "kaggle_datasets/247/problem_summary.md"
  },
  "429": {
    "problem_id": "429",
    "title": "Predicting NCAA Men's Basketball Tournament Point Spreads",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Men's Basketball Tournament Point Spreads\n\n## Problem Description\n- **Problem Type**: Regression (Point Spread Prediction)\n- **Objective**: Predict the margin of victory (point spread) for all possible matchups in the NCAA Men's Basketball Tournament (March Madness). The competition has two stages:\n  - *Stage 1*: Predict historical spreads for past tournaments (2015-2019) to build/test models\n  - *Stage 2*: Predict spreads for the 2021 tournament (real competition)\n- **Key Points**:\n  - Focuses on predicting exact victory margins rather than just winners\n  - Uses a combinatorial approach - requires predictions for all possible team pairings\n  - Encourages use of external data sources beyond provided historical data\n\n## Dataset Overview\n- **Data Type**: Tabular data with basketball game statistics, team metadata, and tournament structures\n- **Context**: NCAA Division I men's basketball games (1985-2021 seasons)\n- **Key Data Files**:\n  - `Teams.csv`: Team IDs and metadata (357 active D-I teams)\n  - `MRegularSeasonCompactResults.csv`: Game scores (1985-2021)\n  - `MNCAATourneyCompactResults.csv`: Tournament game scores\n  - `MNCAATourneySeeds.csv`: Tournament seeding since 1985\n  - `MRegularSeasonDetailedResults.csv`: Team-level box scores (2003+)\n  - `MMasseyOrdinals.csv`: Weekly team rankings from various systems\n- **Important Features**:\n  - Game outcomes (scores, locations, overtime periods)\n  - Team performance statistics (FG%, rebounds, assists, etc.)\n  - Historical tournament seeds and bracket structures\n  - Multiple ranking systems (Pomeroy, Sagarin, RPI, etc.)\n\n## Evaluation Metrics\n- **Primary Metric**: Root Mean Squared Error (RMSE)\n  - Calculated between predicted and actual point spreads\n  - Lower values indicate better performance (smaller prediction errors)\n- **Submission Format**:\n  - Requires predictions for all possible team pairings\n  - Format: `Season_TeamID1_TeamID2,PredictedSpread`\n  - Spread is relative to lower-ID team (positive if predicted to win)",
    "sections": {},
    "file_path": "kaggle_datasets/429/problem_summary.md"
  },
  "240": {
    "problem_id": "240",
    "title": "Non-targeted Adversarial Attack on Image Classification Models",
    "problem_type": "Adversarial Attack (Computer Vision - Image Classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Non-targeted Adversarial Attack on Image Classification Models\n\n**Problem Description:**\n* **Problem Type:** Adversarial Attack (Computer Vision - Image Classification)\n* **Objective:**  \n  The goal is to create imperceptibly modified versions of input images (adversarial examples) that cause unknown machine learning classifiers to misclassify them. The modifications must be subtle enough that humans cannot detect them, but significant enough to fool the models.\n* **Key Points:**\n  * Non-targeted attack: The attack only needs to cause misclassification, not necessarily to a specific target class.\n  * The adversarial perturbations must be bounded (L∞ norm constraint between 4-16 pixel values).\n  * Attacks will be evaluated against multiple unknown defense models.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  ImageNet-compatible images (299x299 pixels) with 1001-class labels (including background). The dataset consists of:\n  * DEV set: 1000 images for development/testing\n  * TEST set: Secret evaluation set (released post-competition)\n* **Data Files:**\n  * dev_dataset.zip (development images)\n  * Pre-trained Inception-v3 model files\n  * Sample attack/defense implementations\n* **Key Features:**\n  * RGB image pixels (299x299x3)\n  * ImageNet-compatible labels (0-1000)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Attack Success Rate  \n  Score = ∑ (all defenses) ∑ (all images) [defense(attack(image)) ≠ true_label]\n* **Components:**\n  * Each successful misclassification against any defense earns 1 point\n  * Final score is the total number of successful misclassifications across all defense models and all test images\n  * Higher scores indicate more effective attacks\n  * Adversarial examples must satisfy the L∞ perturbation constraint",
    "sections": {
      "Problem Description": "* **Problem Type:** Adversarial Attack (Computer Vision - Image Classification)\n* **Objective:**  \n  The goal is to create imperceptibly modified versions of input images (adversarial examples) that cause unknown machine learning classifiers to misclassify them. The modifications must be subtle enough that humans cannot detect them, but significant enough to fool the models.\n* **Key Points:**\n  * Non-targeted attack: The attack only needs to cause misclassification, not necessarily to a specific target class.\n  * The adversarial perturbations must be bounded (L∞ norm constraint between 4-16 pixel values).\n  * Attacks will be evaluated against multiple unknown defense models.",
      "Dataset Overview": "* **Data Type & Context:**  \n  ImageNet-compatible images (299x299 pixels) with 1001-class labels (including background). The dataset consists of:\n  * DEV set: 1000 images for development/testing\n  * TEST set: Secret evaluation set (released post-competition)\n* **Data Files:**\n  * dev_dataset.zip (development images)\n  * Pre-trained Inception-v3 model files\n  * Sample attack/defense implementations\n* **Key Features:**\n  * RGB image pixels (299x299x3)\n  * ImageNet-compatible labels (0-1000)",
      "Evaluation Metrics": "* **Primary Metric:** Attack Success Rate  \n  Score = ∑ (all defenses) ∑ (all images) [defense(attack(image)) ≠ true_label]\n* **Components:**\n  * Each successful misclassification against any defense earns 1 point\n  * Final score is the total number of successful misclassifications across all defense models and all test images\n  * Higher scores indicate more effective attacks\n  * Adversarial examples must satisfy the L∞ perturbation constraint"
    },
    "file_path": "kaggle_datasets/240/problem_summary.md"
  },
  "416": {
    "problem_id": "416",
    "title": "Predicting Student Answer Correctness with Knowledge Tracing",
    "problem_type": "Binary Classification (Time Series)",
    "objective": "Predict whether a student will answer their next question correctly based on their historical interactions with an educational platform. The goal is to model student knowledge over time (\"Knowledge Tracing\") to enable personalized learning experiences.",
    "evaluation_metric": null,
    "full_content": "# Predicting Student Answer Correctness with Knowledge Tracing\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Time Series)\n* **Objective:** Predict whether a student will answer their next question correctly based on their historical interactions with an educational platform. The goal is to model student knowledge over time (\"Knowledge Tracing\") to enable personalized learning experiences.\n    * **Key Points:**\n        * Time-series nature: Predictions must account for sequential student interactions.\n        * Must handle both question-answering events (content_type_id=0) and lecture-watching events (content_type_id=1).\n        * Some test users will not appear in training data, requiring models to generalize to new students.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time-series data of student interactions with an educational platform (EdNet dataset), containing over 100 million interactions from 1M+ students.\n* **Data Files:**\n    * `train.csv`: Contains historical student interactions (questions answered and lectures watched).\n    * `questions.csv`: Metadata about questions (correct answers, TOEIC sections, tags).\n    * `lectures.csv`: Metadata about lectures (categories, tags, purposes).\n    * `example_test_rows.csv`: Sample of test set format with time-series API structure.\n* **Key Features:**\n    * Student interaction features: timestamp, user_id, content_type_id, user_answer, answered_correctly.\n    * Contextual features: prior_question_elapsed_time, prior_question_had_explanation.\n    * Question/lecture metadata: part, tags, bundle_id, type_of.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures how well the model distinguishes between correct and incorrect answers.\n    * Evaluates predicted probabilities against binary correctness labels.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Time Series)\n* **Objective:** Predict whether a student will answer their next question correctly based on their historical interactions with an educational platform. The goal is to model student knowledge over time (\"Knowledge Tracing\") to enable personalized learning experiences.\n    * **Key Points:**\n        * Time-series nature: Predictions must account for sequential student interactions.\n        * Must handle both question-answering events (content_type_id=0) and lecture-watching events (content_type_id=1).\n        * Some test users will not appear in training data, requiring models to generalize to new students.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time-series data of student interactions with an educational platform (EdNet dataset), containing over 100 million interactions from 1M+ students.\n* **Data Files:**\n    * `train.csv`: Contains historical student interactions (questions answered and lectures watched).\n    * `questions.csv`: Metadata about questions (correct answers, TOEIC sections, tags).\n    * `lectures.csv`: Metadata about lectures (categories, tags, purposes).\n    * `example_test_rows.csv`: Sample of test set format with time-series API structure.\n* **Key Features:**\n    * Student interaction features: timestamp, user_id, content_type_id, user_answer, answered_correctly.\n    * Contextual features: prior_question_elapsed_time, prior_question_had_explanation.\n    * Question/lecture metadata: part, tags, bundle_id, type_of.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures how well the model distinguishes between correct and incorrect answers.\n    * Evaluates predicted probabilities against binary correctness labels."
    },
    "file_path": "kaggle_datasets/416/problem_summary.md"
  },
  "624": {
    "problem_id": "624",
    "title": "Regression of Used Car Prices",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Regression of Used Car Prices  \n\n## Problem Description  \n- **Problem Type:** Regression  \n- **Objective:** Predict the price of used cars based on various attributes. The goal is to build a model that accurately estimates the continuous target variable (`price`) using the provided features.  \n- **Key Points:**  \n  - The dataset is synthetically generated but closely mirrors real-world used car price data.  \n  - Participants may optionally incorporate the original dataset for training or exploration.  \n\n## Dataset Overview  \n- **Data Type & Context:** Tabular data containing attributes of used cars (e.g., make, model, year, mileage, etc.).  \n- **Data Files:**  \n  - `train.csv`: Training dataset with the target column `price`.  \n  - `test.csv`: Test dataset without the target column.  \n  - `sample_submission.csv`: Example submission file in the required format.  \n- **Features:** Likely include numerical (e.g., mileage, year) and categorical (e.g., make, model) features, though specific columns are not listed.  \n\n## Evaluation Metrics  \n- **Primary Metric:** Root Mean Squared Error (RMSE).  \n- **Components of RMSE:**  \n  - Calculated as the square root of the average squared differences between predicted (`ŷ`) and actual (`y`) values:  \n    $$  \n    RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2}  \n    $$  \n  - Lower RMSE indicates better model performance.",
    "sections": {},
    "file_path": "kaggle_datasets/624/problem_summary.md"
  },
  "249": {
    "problem_id": "249",
    "title": "Predicting Subscription Churn for a Music Streaming Service",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a subscriber will churn (not renew their subscription within 30 days after expiration) for KKBOX, Asia's leading music streaming service.",
    "evaluation_metric": null,
    "full_content": "# Predicting Subscription Churn for a Music Streaming Service\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a subscriber will churn (not renew their subscription within 30 days after expiration) for KKBOX, Asia's leading music streaming service.\n    * **Key Points:**\n        * Churn is specifically defined as no new valid subscription transaction within 30 days after membership expiration.\n        * Active cancellation (`is_cancel`) does not necessarily indicate churn (users may cancel for plan changes).\n        * Focus on predicting churn behavior for users whose subscriptions expire in specific months (February 2017 for train, March 2017 for test).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing user subscription transactions, demographic information, and music listening behavior logs.\n    * **Data Files:**\n        * `train.csv`/`train_v2.csv`: User IDs and churn labels (1=churn, 0=renewal)\n        * `transactions.csv`/`transactions_v2.csv`: Subscription transactions (payment details, dates, cancellation status)\n        * `user_logs.csv`/`user_logs_v2.csv`: Daily listening behavior (song plays, unique songs, total seconds)\n        * `members.csv`/`members_v3.csv`: User demographics (age, gender, city, registration method)\n        * Sample submission files\n    * **Key Features:**\n        * Transaction dates and membership expiration dates\n        * Payment details (method, plan days, amounts)\n        * Listening behavior metrics (song completion percentages, unique songs)\n        * User demographics (with age outliers noted)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss\n    * **Calculation:** \n        * \\( logloss = -\\frac{1}{N}\\sum_{i=1}^{N}(y_i\\log(p_i) + (1-y_i)\\log(1-p_i)) \\)\n        * Where:\n            * N = number of observations\n            * \\( y_i \\) = binary target (1=churn, 0=renewal)\n            * \\( p_i \\) = predicted probability of churn\n        * Note: Predicted probabilities are clipped to [10^-15, 1-10^-15] for stability",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a subscriber will churn (not renew their subscription within 30 days after expiration) for KKBOX, Asia's leading music streaming service.\n    * **Key Points:**\n        * Churn is specifically defined as no new valid subscription transaction within 30 days after membership expiration.\n        * Active cancellation (`is_cancel`) does not necessarily indicate churn (users may cancel for plan changes).\n        * Focus on predicting churn behavior for users whose subscriptions expire in specific months (February 2017 for train, March 2017 for test).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing user subscription transactions, demographic information, and music listening behavior logs.\n    * **Data Files:**\n        * `train.csv`/`train_v2.csv`: User IDs and churn labels (1=churn, 0=renewal)\n        * `transactions.csv`/`transactions_v2.csv`: Subscription transactions (payment details, dates, cancellation status)\n        * `user_logs.csv`/`user_logs_v2.csv`: Daily listening behavior (song plays, unique songs, total seconds)\n        * `members.csv`/`members_v3.csv`: User demographics (age, gender, city, registration method)\n        * Sample submission files\n    * **Key Features:**\n        * Transaction dates and membership expiration dates\n        * Payment details (method, plan days, amounts)\n        * Listening behavior metrics (song completion percentages, unique songs)\n        * User demographics (with age outliers noted)",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss\n    * **Calculation:** \n        * \\( logloss = -\\frac{1}{N}\\sum_{i=1}^{N}(y_i\\log(p_i) + (1-y_i)\\log(1-p_i)) \\)\n        * Where:\n            * N = number of observations\n            * \\( y_i \\) = binary target (1=churn, 0=renewal)\n            * \\( p_i \\) = predicted probability of churn\n        * Note: Predicted probabilities are clipped to [10^-15, 1-10^-15] for stability"
    },
    "file_path": "kaggle_datasets/249/problem_summary.md"
  },
  "420": {
    "problem_id": "420",
    "title": "Cassava Leaf Disease Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Cassava Leaf Disease Classification\n\n## Problem Description\n* **Problem Type:** Multiclass Classification (Computer Vision - Image Classification)\n* **Objective:** Classify images of cassava leaves into one of five categories: four disease types or a healthy leaf. The goal is to enable farmers to quickly identify diseased plants using mobile-quality images, addressing labor-intensive and costly manual diagnosis methods.\n* **Key Points:**\n  * Focus on real-world applicability: Models must perform well under constraints like low-bandwidth and mobile-quality images.\n  * Dataset reflects realistic conditions, with images crowdsourced from farmers and annotated by experts.\n  * Diseases impact food security in Sub-Saharan Africa, making accurate detection critical.\n\n## Dataset Overview\n* **Data Type & Context:** Image data of cassava leaves, collected via farmer surveys in Uganda. Images represent real-world conditions (e.g., varying lighting, angles, and camera quality).\n* **Data Files:**\n  * `train_images/`: Folder containing 21,367 labeled training images.\n  * `test_images/`: Folder containing ~15,000 test images (disclosed only during scoring).\n  * `train.csv`: Maps `image_id` (filename) to `label` (disease code).\n  * `sample_submission.csv`: Submission template with `image_id` and predicted `label`.\n  * `label_num_to_disease_map.json`: Maps disease codes to names (e.g., \"0\": \"Cassava Bacterial Blight\").\n  * TFRecord versions of the datasets are also provided (`train_tfrecords/`, `test_tfrecords/`).\n* **Features:**\n  * Images are the primary input, with labels indicating disease type or healthy status.\n  * No explicit feature engineering is described; the focus is on raw image pixels.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Categorization Accuracy (simple accuracy, i.e., the proportion of correctly classified images).\n* **Components:**\n  * Submissions are scored based on the percentage of correct predictions across all test images.\n  * The metric is straightforward, emphasizing correct classification over nuanced trade-offs (e.g., precision/recall).",
    "sections": {},
    "file_path": "kaggle_datasets/420/problem_summary.md"
  },
  "612": {
    "problem_id": "612",
    "title": "Bird Species Identification from Audio in the Western Ghats",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Bird Species Identification from Audio in the Western Ghats\n\n## Problem Description\n* **Problem Type:** Multiclass Classification (Audio)\n* **Objective:**  \n  * Develop computational solutions to identify under-studied Indian bird species by their calls in continuous audio recordings.  \n  * Focus on species endemic to the Western Ghats, endangered species, and nocturnal species with limited training data.  \n* **Key Points:**  \n  * Address challenges of training reliable classifiers with limited labeled data.  \n  * Support biodiversity monitoring in a conservation context (passive acoustic monitoring).  \n  * Focus on three specific sub-goals:  \n    * Identifying endemic species of the Western Ghats \"sky-islands\".  \n    * Detecting endangered species with sparse training examples.  \n    * Classifying poorly understood nocturnal species.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n  * Audio recordings (bird calls) from the Western Ghats biodiversity hotspot, supplemented by metadata.  \n  * Includes both isolated calls (training) and continuous soundscapes (testing).  \n* **Data Files:**  \n  * `train_audio/`: Short, labeled bird call recordings (OGG format, 32 kHz).  \n  * `test_soundscapes/`: 4-minute continuous recordings for evaluation (hidden during competition).  \n  * `unlabeled_soundscapes/`: Additional unlabeled audio from test locations.  \n  * `train_metadata.csv`: Contains species labels (`primary_label`), geographic coordinates, and recording details.  \n  * `eBird_Taxonomy_v2021.csv`: Species relationship data.  \n* **Key Features:**  \n  * Audio features (spectrograms, MFCCs, etc.) must be derived from raw OGG files.  \n  * Metadata includes species codes, latitude/longitude (for potential dialect analysis), and recording authors.  \n\n## Evaluation Metrics\n* **Primary Metric:** Macro-averaged ROC-AUC (skipping classes with no true positive labels).  \n* **Components:**  \n  * For each 5-second audio window (`row_id`), predict probabilities for 182 bird species.  \n  * AUC is calculated per species, then averaged (unweighted) across species.  \n  * Species with no true positives in the test set are excluded from the average.  \n* **Submission Format:**  \n  * CSV with `row_id` (e.g., `soundscape_[ID]_[",
    "sections": {},
    "file_path": "kaggle_datasets/612/problem_summary.md"
  },
  "276": {
    "problem_id": "276",
    "title": "Audio Tagging in Diverse Real-World Environments",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Audio Tagging in Diverse Real-World Environments\n\n## Problem Description\n- **Problem Type**: Multi-class Audio Classification\n- **Objective**: Develop a general-purpose automatic audio tagging system capable of recognizing sounds from 41 diverse categories in real-world environments. The system should handle:\n  * Variable recording quality (due to diverse sources)\n  * Single-label classification (despite potential background noise)\n  * Mixed reliability of training annotations (~39% manually verified, rest estimated 65-70% accurate)\n- **Key Points**:\n  * Part of DCASE 2018 Challenge (Task 2)\n  * Focus on real-world audio with ambient noise\n  * Dataset combines verified and unverified annotations\n  * Must predict up to 3 possible labels per audio clip\n\n## Dataset Overview\n- **Data Type**: Uncompressed PCM 16-bit, 44.1 kHz mono audio files (.wav)\n- **Context**: Curated from Freesound.org with Creative Commons licenses, covering 41 sound categories\n- **Data Files**:\n  * `train.csv` - File names, labels, verification flags\n  * `audio_train.zip` - ~9.5k training samples\n  * `audio_test.zip` - ~1.6k test samples + 7.8k padding files\n  * Post-competition files with additional metadata\n- **Key Features**:\n  * Duration: 300ms to 30s clips\n  * Categories include musical instruments, human sounds, animals, domestic sounds\n  * `manually_verified` flag indicates annotation reliability\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Average Precision @ 3 (MAP@3)\n- **Metric Components**:\n  * For each audio file (U total):\n    * Calculate precision at cutoff positions k (1 through 3)\n    * Sum the precision values\n  * Average across all test files\n  * Formula: \n    ```\n    MAP@3 = (1/U) * Σ(u=1→U) Σ(k=1→min(n,3)) P(k)\n    ```\n  * Rewards systems that place correct labels higher in prediction ranking",
    "sections": {},
    "file_path": "kaggle_datasets/276/problem_summary.md"
  },
  "282": {
    "problem_id": "282",
    "title": "Visual Relationship Detection in Images",
    "problem_type": "Computer Vision - Object Relationship Detection",
    "objective": "Build an algorithm to detect pairs of objects in specific visual relationships (e.g., \"woman playing guitar\", \"beer on table\"). The task involves:",
    "evaluation_metric": null,
    "full_content": "# Visual Relationship Detection in Images\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Relationship Detection\n* **Objective:** Build an algorithm to detect pairs of objects in specific visual relationships (e.g., \"woman playing guitar\", \"beer on table\"). The task involves:\n    * Identifying bounding boxes for two related objects\n    * Classifying the relationship between them\n* **Key Points:**\n    * Focuses on relationship triplets (subject-predicate-object)\n    * Training set contains 329 distinct relationship types occurring 374,768 times\n    * Part of Google's effort to advance computer vision beyond simple object detection\n\n**Dataset Overview:**\n* **Data Type:** Image data with annotated visual relationships\n* **Context:** Large-scale dataset with complex object interactions in real-world scenes\n* **Data Files:**\n    * test.zip - 99,999 test images\n    * VRD_sample_submission.csv - submission format example\n* **Features:**\n    * Images containing multiple objects\n    * Annotations include:\n        * Bounding boxes for objects\n        * Relationship labels between object pairs\n\n**Evaluation Metrics:**\n* **Primary Metric:** Weighted combination of three scores:\n    * 40% - Mean Average Precision (AP) on relationships detection\n    * 20% - Recall@50 (retrieval of correct relationships in top 50 predictions)\n    * 40% - Mean AP on phrase detection (relationship triplet as a whole)\n* **Scoring Details:**\n    * Final score = 0.4*AP_relationships + 0.2*Recall@50 + 0.4*AP_phrases\n    * Evaluation code available in TensorFlow Object Detection API",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Relationship Detection\n* **Objective:** Build an algorithm to detect pairs of objects in specific visual relationships (e.g., \"woman playing guitar\", \"beer on table\"). The task involves:\n    * Identifying bounding boxes for two related objects\n    * Classifying the relationship between them\n* **Key Points:**\n    * Focuses on relationship triplets (subject-predicate-object)\n    * Training set contains 329 distinct relationship types occurring 374,768 times\n    * Part of Google's effort to advance computer vision beyond simple object detection",
      "Dataset Overview": "* **Data Type:** Image data with annotated visual relationships\n* **Context:** Large-scale dataset with complex object interactions in real-world scenes\n* **Data Files:**\n    * test.zip - 99,999 test images\n    * VRD_sample_submission.csv - submission format example\n* **Features:**\n    * Images containing multiple objects\n    * Annotations include:\n        * Bounding boxes for objects\n        * Relationship labels between object pairs",
      "Evaluation Metrics": "* **Primary Metric:** Weighted combination of three scores:\n    * 40% - Mean Average Precision (AP) on relationships detection\n    * 20% - Recall@50 (retrieval of correct relationships in top 50 predictions)\n    * 40% - Mean AP on phrase detection (relationship triplet as a whole)\n* **Scoring Details:**\n    * Final score = 0.4*AP_relationships + 0.2*Recall@50 + 0.4*AP_phrases\n    * Evaluation code available in TensorFlow Object Detection API"
    },
    "file_path": "kaggle_datasets/282/problem_summary.md"
  },
  "418": {
    "problem_id": "418",
    "title": "Time Series Forecasting for Water Availability in Diverse Waterbodies",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Time Series Forecasting for Water Availability in Diverse Waterbodies\n\n## Problem Description\n- **Problem Type**: Time Series Forecasting (with potential regression components)\n- **Objective**: Predict water availability (level/flow) in four distinct types of waterbodies (aquifers, water springs, rivers, lakes) across nine independent datasets. The goal is to help manage water resources by forecasting daily/monthly volumes based on environmental factors.\n- **Key Points**:\n  - Must develop separate models for each waterbody type (4 categories)\n  - Models should account for delayed effects of rainfall/temperature on water features\n  - Solutions must be applicable to new, unseen waterbodies of the same categories\n  - Time intervals vary (daily/monthly predictions) depending on waterbody characteristics\n\n## Dataset Overview\n- **Data Type**: Tabular time series data with mixed numerical features\n- **Context**: Independent datasets from Italian water utility monitoring 9 waterbodies (4 aquifers, 3 water springs, 1 river, 1 lake)\n- **Data Files**:\n  - 9 waterbody-specific CSV files (e.g., Aquifer_Auser.csv, Water_Spring_Amiata.csv)\n  - Metadata files describing each waterbody (ODS/XLSX)\n- **Features**:\n  - Common temporal features: Date timestamps\n  - Target variables: Water level, flow rate, or depth (varies by waterbody)\n  - Predictive features: Rainfall, temperature, humidity, drainage volumes, hydrometry (with delayed effects)\n  - Waterbody-specific measurements (well levels, spring outputs, river/lake metrics)\n\n## Evaluation Metrics\n- **Primary Metrics**:\n  - Mean Absolute Error (MAE)\n  - Root Mean Square Error (RMSE)\n- **Judging Criteria**:\n  - **Methodology/Completeness (0-5 pts)**:\n    - Model appropriateness for each waterbody type\n    - Performance metrics (MAE/RMSE)\n    - Validation approach\n  - **Presentation (0-5 pts)**:\n    - Narrative quality and visualization\n    - Feature-prediction relationship analysis\n    - Code documentation\n  - **Application (0-5 pts)**:\n    - Practical forecasting utility\n    - Generalizability to new waterbodies\n    - Automated insight generation",
    "sections": {},
    "file_path": "kaggle_datasets/418/problem_summary.md"
  },
  "285": {
    "problem_id": "285",
    "title": "Detecting Rare Particle Decay τ → μμμ in Physics Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Detecting Rare Particle Decay τ → μμμ in Physics Data\n\n## Problem Description\n- **Problem Type**: Binary Classification (Signal vs. Background)\n- **Objective**: Identify rare τ → 3μ decay events in particle collision data, which would indicate lepton flavor violation and \"new physics\" beyond the Standard Model.\n- **Key Points**:\n  - Must distinguish between simulated signal events and real background events.\n  - Model must pass two physics-inspired validation tests:\n    * **Agreement Test**: Classifier outputs must show similar distributions for real vs. simulated control channel data (Ds → φπ decays) via Kolmogorov-Smirnov test (KS < 0.09).\n    * **Correlation Test**: Predictions must be uncorrelated with τ mass (Cramer-von Mises test value < 0.002).\n\n## Dataset Overview\n- **Data Type**: Tabular data from high-energy physics experiments (LHCb detector at CERN)\n- **Data Files**:\n  - `training.csv`: Labeled dataset (signal=1, background=0) with simulated signal and real background events\n  - `test.csv`: Unlabeled events including signal simulations, real background, and control channel data\n  - `check_agreement.csv`: Control channel data for agreement testing (contains `weight` column)\n  - `check_correlation.csv`: Background events with `mass` column for correlation testing\n- **Key Features**:\n  - Particle kinematics: FlightDistance, LifeTime, Impact Parameters (IP), transverse momentum (pt)\n  - Track isolation variables (isolationa-f, ISO_SumBDT)\n  - Muon track properties (p0_p, p1_eta, p2_IP, etc.)\n  - Vertex quality measures (VertexChi2, DOCA values)\n  - **Excluded in test**: mass, production, min_ANNmuon (used for evaluation only)\n\n## Evaluation Metrics\n- **Primary Metric**: Weighted Area Under ROC Curve (AUC)\n  - **Weighting Scheme**:\n    * TPR [0.0, 0.2]: ×2.0 weight\n    * TPR [0.2, 0.4]: ×1.5 weight  \n    * TPR [0.4, 0.6]: ×1.0 weight\n    * TPR [0.6, 0.8]:",
    "sections": {},
    "file_path": "kaggle_datasets/285/problem_summary.md"
  },
  "271": {
    "problem_id": "271",
    "title": "Multi-label Fashion Image Classification Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Fashion Image Classification Challenge\n\n## Problem Description\n- **Problem Type:** Multi-label Image Classification (Computer Vision)\n- **Objective:** Develop algorithms to automatically assign multiple attribute labels to fashion images, accounting for challenges like varying lighting, angles, backgrounds, and occlusion. The goal is to recognize subtle visual differences between fine-grained fashion categories (e.g., royal blue vs. turquoise).\n- **Key Points:**\n  - Focus on fine-grained visual categorization (FGVC) of fashion attributes\n  - Each image can have multiple ground truth labels\n  - Must handle real-world variations in product photography\n  - Part of a CVPR workshop pushing state-of-the-art in automatic product detection\n\n## Dataset Overview\n- **Data Type & Context:** Fashion product images with multiple attribute labels (228 classes)\n- **Data Files:**\n  - train.json: 1,014,544 training images with URLs and labels\n  - validation.json: 10,586 validation images\n  - test.json: 42,590 test images (URLs only)\n  - sample_submission_randomlabel.csv: Example submission file\n- **Features:**\n  - Image URLs (participants must download images themselves)\n  - Multiple integer label IDs per image in training/validation sets\n  - Label names intentionally omitted to prevent hand-labeling of test set\n\n## Evaluation Metrics\n- **Primary Metric:** Mean F1 score (micro-averaged)\n- **Metric Details:**\n  - Example-based F-score for multi-label learning\n  - Equally weights precision and recall\n  - Favors balanced performance over excelling in one metric at the expense of the other\n  - Predictions submitted as space-delimited lists of label IDs per image",
    "sections": {},
    "file_path": "kaggle_datasets/271/problem_summary.md"
  },
  "615": {
    "problem_id": "615",
    "title": "Automated Essay Scoring for Student Learning Outcomes",
    "problem_type": "NLP - Text Regression (Ordinal Prediction)",
    "objective": "Develop an automated system to score student-written argumentative essays on a holistic scale of 1-6, replicating human grading standards to reduce educator workload and provide timely feedback.",
    "evaluation_metric": null,
    "full_content": "# Automated Essay Scoring for Student Learning Outcomes\n\n**Problem Description:**\n* **Problem Type:** NLP - Text Regression (Ordinal Prediction)\n* **Objective:** Develop an automated system to score student-written argumentative essays on a holistic scale of 1-6, replicating human grading standards to reduce educator workload and provide timely feedback.\n    * **Key Points:**\n        * Focus on improving open-source Automated Writing Evaluation (AWE) systems for broader accessibility\n        * Address algorithmic bias by using a nationally diverse dataset spanning economic/location populations\n        * Build upon the 2012 ASAP competition with updated data/methods\n        * Winning solutions will be released as open-source for educational equity\n\n**Dataset Overview:**\n* **Data Type & Context:** 24,000+ student essays (text data) with human-assigned holistic scores, representing realistic classroom writing samples aligned to educational standards.\n* **Data Files:**\n    * `train.csv`: Contains `essay_id`, `full_text`, and `score` (1-6)\n    * `test.csv`: Contains `essay_id` and `full_text` (no scores)\n    * `sample_submission.csv`: Template for predictions\n* **Key Features:** \n    * Primary feature: Raw essay text (`full_text`)\n    * Target: Ordinal score (1-6) based on [Holistic Scoring Rubric](https://storage.googleapis.com/kaggle-forum-message-attachments/2733927/20538/Rubric_%20Holistic%20Essay%20Scoring.pdf)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Quadratic Weighted Kappa (measures agreement between predicted and actual scores)\n    * **Calculation:**\n        1. Construct N×N histogram matrix O of actual vs predicted scores\n        2. Compute weight matrix w where wᵢⱼ = (i-j)²/(N-1)²\n        3. Calculate expected agreement matrix E as outer product of score histograms\n        4. Kappa = 1 - (∑wᵢⱼOᵢⱼ)/(∑wᵢⱼEᵢⱼ)\n    * **Interpretation:** Ranges from <0 (worse than chance) to 1 (perfect agreement)",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Text Regression (Ordinal Prediction)\n* **Objective:** Develop an automated system to score student-written argumentative essays on a holistic scale of 1-6, replicating human grading standards to reduce educator workload and provide timely feedback.\n    * **Key Points:**\n        * Focus on improving open-source Automated Writing Evaluation (AWE) systems for broader accessibility\n        * Address algorithmic bias by using a nationally diverse dataset spanning economic/location populations\n        * Build upon the 2012 ASAP competition with updated data/methods\n        * Winning solutions will be released as open-source for educational equity",
      "Dataset Overview": "* **Data Type & Context:** 24,000+ student essays (text data) with human-assigned holistic scores, representing realistic classroom writing samples aligned to educational standards.\n* **Data Files:**\n    * `train.csv`: Contains `essay_id`, `full_text`, and `score` (1-6)\n    * `test.csv`: Contains `essay_id` and `full_text` (no scores)\n    * `sample_submission.csv`: Template for predictions\n* **Key Features:** \n    * Primary feature: Raw essay text (`full_text`)\n    * Target: Ordinal score (1-6) based on [Holistic Scoring Rubric](https://storage.googleapis.com/kaggle-forum-message-attachments/2733927/20538/Rubric_%20Holistic%20Essay%20Scoring.pdf)",
      "Evaluation Metrics": "* **Primary Metric:** Quadratic Weighted Kappa (measures agreement between predicted and actual scores)\n    * **Calculation:**\n        1. Construct N×N histogram matrix O of actual vs predicted scores\n        2. Compute weight matrix w where wᵢⱼ = (i-j)²/(N-1)²\n        3. Calculate expected agreement matrix E as outer product of score histograms\n        4. Kappa = 1 - (∑wᵢⱼOᵢⱼ)/(∑wᵢⱼEᵢⱼ)\n    * **Interpretation:** Ranges from <0 (worse than chance) to 1 (perfect agreement)"
    },
    "file_path": "kaggle_datasets/615/problem_summary.md"
  },
  "427": {
    "problem_id": "427",
    "title": "Predicting NCAA Women's Basketball Tournament Point Spreads",
    "problem_type": "Regression (Point Spread Prediction)",
    "objective": "Predict the margin of victory (point spread) for all possible matchups in the NCAA Women's Basketball Tournament. The competition has two stages:",
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Women's Basketball Tournament Point Spreads\n\n**Problem Description:**\n* **Problem Type:** Regression (Point Spread Prediction)\n* **Objective:** Predict the margin of victory (point spread) for all possible matchups in the NCAA Women's Basketball Tournament. The competition has two stages:\n    * Stage 1: Predict spreads for historical tournaments (2015-2019) to build/test models\n    * Stage 2: Predict spreads for the 2021 tournament (real competition)\n* **Key Points:**\n    * Requires predicting spreads for all possible team pairings (64 teams → 2,016 matchups per year)\n    * Spreads must be expressed relative to the team with lower ID (e.g., +7 if lower-ID team wins by 7, -3 if they lose by 3)\n    * Participants are encouraged to use external data sources (with proper attribution)\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with basketball game statistics and team information\n* **Context:** Historical NCAA Women's Division I basketball games (regular season and tournament)\n* **Key Data Files:**\n    * `WTeams.csv`: Team IDs and names (369 teams total)\n    * `WSeasons.csv`: Season metadata and tournament region names\n    * `WNCAATourneySeeds.csv`: Tournament seeds for all teams (1998-2021)\n    * `WRegularSeasonCompactResults.csv`: Game results (1998-2021)\n    * `WNCAATourneyCompactResults.csv`: Tournament game results (1998-2021)\n    * `WRegularSeasonDetailedResults.csv`: Team-level box scores (2010-2021)\n    * `WNCAATourneyDetailedResults.csv`: Tournament box scores (2010-2021)\n    * Additional files for cities, conferences, and team spellings\n* **Important Features:**\n    * Game outcomes (scores, locations, overtime periods)\n    * Team statistics (field goals, rebounds, assists, turnovers, etc.)\n    * Tournament seeds and bracket structure\n    * Team conference affiliations\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root-Mean-Squared-Error (RMSE) between predicted and actual point spreads\n* **Calculation:**\n    * For each game, compute squared error between predicted and actual margin\n    * Average these squared errors across all predictions\n    * Take the square root of this average",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Point Spread Prediction)\n* **Objective:** Predict the margin of victory (point spread) for all possible matchups in the NCAA Women's Basketball Tournament. The competition has two stages:\n    * Stage 1: Predict spreads for historical tournaments (2015-2019) to build/test models\n    * Stage 2: Predict spreads for the 2021 tournament (real competition)\n* **Key Points:**\n    * Requires predicting spreads for all possible team pairings (64 teams → 2,016 matchups per year)\n    * Spreads must be expressed relative to the team with lower ID (e.g., +7 if lower-ID team wins by 7, -3 if they lose by 3)\n    * Participants are encouraged to use external data sources (with proper attribution)",
      "Dataset Overview": "* **Data Type:** Tabular data with basketball game statistics and team information\n* **Context:** Historical NCAA Women's Division I basketball games (regular season and tournament)\n* **Key Data Files:**\n    * `WTeams.csv`: Team IDs and names (369 teams total)\n    * `WSeasons.csv`: Season metadata and tournament region names\n    * `WNCAATourneySeeds.csv`: Tournament seeds for all teams (1998-2021)\n    * `WRegularSeasonCompactResults.csv`: Game results (1998-2021)\n    * `WNCAATourneyCompactResults.csv`: Tournament game results (1998-2021)\n    * `WRegularSeasonDetailedResults.csv`: Team-level box scores (2010-2021)\n    * `WNCAATourneyDetailedResults.csv`: Tournament box scores (2010-2021)\n    * Additional files for cities, conferences, and team spellings\n* **Important Features:**\n    * Game outcomes (scores, locations, overtime periods)\n    * Team statistics (field goals, rebounds, assists, turnovers, etc.)\n    * Tournament seeds and bracket structure\n    * Team conference affiliations",
      "Evaluation Metrics": "* **Primary Metric:** Root-Mean-Squared-Error (RMSE) between predicted and actual point spreads\n* **Calculation:**\n    * For each game, compute squared error between predicted and actual margin\n    * Average these squared errors across all predictions\n    * Take the square root of this average"
    },
    "file_path": "kaggle_datasets/427/problem_summary.md"
  },
  "487": {
    "problem_id": "487",
    "title": "Personalized Fashion Recommendations for H&M Customers",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Personalized Fashion Recommendations for H&M Customers\n\n## Problem Description\n* **Problem Type:** Recommender Systems (Multi-label Ranking)\n* **Objective:** Develop personalized product recommendations for H&M customers based on their purchase history and product metadata. The goal is to predict which fashion articles (up to 12) each customer will purchase in the 7-day period immediately following the training data timeframe.\n* **Key Points:**\n  * Must predict for all customer_ids in submission file, including those without purchase history in training data\n  * Recommendations should enhance shopping experience and reduce returns (sustainability focus)\n  * Multiple data modalities available: transactional data, customer metadata, product metadata, text descriptions, and product images\n  * No restrictions on approach - can use categorical algorithms, NLP, or computer vision techniques\n\n## Dataset Overview\n* **Data Type & Context:** Multi-modal retail data including:\n  * Tabular transaction records\n  * Customer demographic data\n  * Product metadata\n  * Product images\n  * Text descriptions\n* **Data Files:**\n  * `transactions_train.csv` - Historical purchase records (customer_id, article_id, timestamp)\n  * `customers.csv` - Customer metadata (age, club_member_status, etc.)\n  * `articles.csv` - Product metadata (garment type, color, description, etc.)\n  * `images/` - Folder containing product images (organized by article_id)\n  * `sample_submission.csv` - Submission format template\n* **Key Features:**\n  * Transactional: customer_id, article_id, timestamp, price\n  * Customer: age, fashion_news_frequency, postal_code\n  * Product: product_type, color, department, textile\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Average Precision @ 12 (MAP@12)\n* **Metric Components:**\n  * Calculated as average precision across all customers\n  * For each customer:\n    * Precision is calculated at each cutoff point k (up to 12)\n    * Only relevant (correct) predictions contribute to the score\n    * No penalty for predicting full 12 items when true purchases < 12\n  * Customers with no purchases during test period are excluded\n  * Formula: \n    ```\n    MAP@12 = (1/U) * Σ[u=1 to U] (1/min(m,12)) * Σ[k=1 to min(n,12)] P(k) ×",
    "sections": {},
    "file_path": "kaggle_datasets/487/problem_summary.md"
  },
  "225": {
    "problem_id": "225",
    "title": "Video Tag Prediction with YouTube-8M Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Video Tag Prediction with YouTube-8M Dataset\n\n## Problem Description\n* **Problem Type**: Multi-label Classification (Video Understanding)\n* **Objective**: Develop classification algorithms to accurately assign video-level labels using the YouTube-8M V2 dataset. The goal is to predict multiple tags (from 4716 possible classes) for each video based on pre-extracted audio/visual features.\n* **Key Points**:\n  * Focus on large-scale video understanding (7M+ videos, 450K hours)\n  * Uses pre-extracted features rather than raw video data\n  * Average of 3.4 labels per video\n  * Test set contains anonymized video IDs to ensure fairness\n\n## Dataset Overview\n* **Data Type**: Pre-processed video features (frame-level and video-level)\n* **Context**: YouTube videos with diverse content categories\n* **Data Files**:\n  * Video-level data (31GB): Contains video_id, labels, mean_rgb (1024-dim), mean_audio (128-dim)\n  * Frame-level data (1.71TB): Contains per-frame rgb (1024-dim) and audio (128-dim) features\n  * label_names.csv: Mapping between label IDs and names\n  * sample_submission.csv: Example submission format\n* **Key Features**:\n  * Pre-extracted visual (RGB) and audio features\n  * Features provided at both video-level (aggregated) and frame-level (per-second)\n  * 4716 possible class labels\n\n## Evaluation Metrics\n* **Primary Metric**: Global Average Precision (GAP) at k=20\n* **Metric Components**:\n  * For each video, submit top 20 predicted labels with confidence scores\n  * All predictions across all videos are treated as one global set\n  * GAP is calculated as: ∑(p(i) * Δr(i)) where:\n    * p(i) = precision at prediction i\n    * Δr(i) = change in recall at prediction i\n  * Implementation available in competition's GitHub repository",
    "sections": {},
    "file_path": "kaggle_datasets/225/problem_summary.md"
  },
  "473": {
    "problem_id": "473",
    "title": "Predicting Pet Photo Popularity with Image and Metadata Analysis",
    "problem_type": "Regression (with potential multi-modal data fusion)",
    "objective": "Predict the \"Pawpularity\" score of shelter pet photos based on image content and optional metadata. The goal is to help animal welfare organizations improve pet profile appeal and adoption rates.",
    "evaluation_metric": null,
    "full_content": "# Predicting Pet Photo Popularity with Image and Metadata Analysis\n\n**Problem Description:**\n* **Problem Type:** Regression (with potential multi-modal data fusion)\n* **Objective:** Predict the \"Pawpularity\" score of shelter pet photos based on image content and optional metadata. The goal is to help animal welfare organizations improve pet profile appeal and adoption rates.\n* **Key Points:**\n  * Combines computer vision (image analysis) and tabular data (metadata labels)\n  * Focus on interpretable predictions to generate actionable recommendations (e.g., photo composition improvements)\n  * Real-world application: Solution may be integrated into PetFinder.my's AI tools\n\n**Dataset Overview:**\n* **Data Type & Context:** \n  * Image data: Pet photos in JPG format (~1.04GB total)\n  * Tabular data: Manually-labeled metadata for each photo\n* **Data Files:**\n  * `train/`: Folder with training images (named by unique ID)\n  * `train.csv`: Metadata + Pawpularity scores for training set\n  * `test/`: Folder with test images (format similar to train)\n  * `test.csv`: Metadata for test set\n  * `sample_submission.csv`: Submission format example\n* **Key Features:**\n  * **Image Features:** Raw pet photos (no pre-processing specified)\n  * **Metadata Features (binary labels):** Focus, Eyes, Face, Near, Action, Accessory, Group, Collage, Human, Occlusion, Info, Blur\n  * **Target Variable:** Pawpularity score (derived from normalized page view statistics)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Error (RMSE)\n  * Formula: √(1/n Σ(y_i - ŷ_i)²)\n  * Where:\n    * y_i = true Pawpularity score\n    * ŷ_i = predicted score\n    * n = number of predictions\n* **Interpretation:** Lower RMSE indicates better prediction accuracy for the continuous popularity scores",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (with potential multi-modal data fusion)\n* **Objective:** Predict the \"Pawpularity\" score of shelter pet photos based on image content and optional metadata. The goal is to help animal welfare organizations improve pet profile appeal and adoption rates.\n* **Key Points:**\n  * Combines computer vision (image analysis) and tabular data (metadata labels)\n  * Focus on interpretable predictions to generate actionable recommendations (e.g., photo composition improvements)\n  * Real-world application: Solution may be integrated into PetFinder.my's AI tools",
      "Dataset Overview": "* **Data Type & Context:** \n  * Image data: Pet photos in JPG format (~1.04GB total)\n  * Tabular data: Manually-labeled metadata for each photo\n* **Data Files:**\n  * `train/`: Folder with training images (named by unique ID)\n  * `train.csv`: Metadata + Pawpularity scores for training set\n  * `test/`: Folder with test images (format similar to train)\n  * `test.csv`: Metadata for test set\n  * `sample_submission.csv`: Submission format example\n* **Key Features:**\n  * **Image Features:** Raw pet photos (no pre-processing specified)\n  * **Metadata Features (binary labels):** Focus, Eyes, Face, Near, Action, Accessory, Group, Collage, Human, Occlusion, Info, Blur\n  * **Target Variable:** Pawpularity score (derived from normalized page view statistics)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Error (RMSE)\n  * Formula: √(1/n Σ(y_i - ŷ_i)²)\n  * Where:\n    * y_i = true Pawpularity score\n    * ŷ_i = predicted score\n    * n = number of predictions\n* **Interpretation:** Lower RMSE indicates better prediction accuracy for the continuous popularity scores"
    },
    "file_path": "kaggle_datasets/473/problem_summary.md"
  },
  "641": {
    "problem_id": "641",
    "title": "Adversarial Essay Generation for LLM Judge Disagreement",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Adversarial Essay Generation for LLM Judge Disagreement\n\n## Problem Description\n- **Problem Type**: Adversarial Text Generation with NLP Evaluation\n- **Objective**: Generate essays that maximize disagreement between multiple LLM judges evaluating essay quality, while adhering to English language constraints and avoiding repetition.\n- **Key Points**:\n  * Target vulnerabilities in LLM-as-a-judge systems (self-bias, position-bias, length-bias, style-bias)\n  * Test robustness of multi-model LLM judging committees\n  * Essays must be ~100 words per topic\n  * Avoid non-English content and repetitive approaches\n\n## Dataset Overview\n- **Data Type**: Text data (essay topics and submissions)\n- **Context**: Adversarial generation challenge against LLM evaluation systems\n- **Data Files**:\n  * `test.csv`: Contains essay topics with unique IDs\n  * `sample_submission.csv`: Example submission format\n- **Features**:\n  * `topic`: Prompt for essay generation\n  * `essay`: Generated adversarial text (~100 words)\n\n## Evaluation Metrics\n- **Primary Metric**: Custom composite score combining multiple factors:\n  * `avg_q`: Average quality score from 3 judges (range [0,9])\n  * `avg_h`: Horizontal variance (between-judge disagreement per essay)\n  * `min_v`: Minimum vertical variance (within-judge consistency across essays)\n  * `avg_e`: English language confidence score (range [0,1])\n  * `avg_s`: Sequence similarity score (anti-repetition, range [0,1], floored at 0.2)\n- **Score Calculation**:\n  ```\n  final_score = (avg_h × min_v × avg_e) / (avg_s × (9 - avg_q))\n  ```\n  * Higher scores indicate better adversarial performance\n  * Penalizes non-English content and repetitive approaches\n  * Rewards both between-judge disagreement and within-judge inconsistency",
    "sections": {},
    "file_path": "kaggle_datasets/641/problem_summary.md"
  },
  "646": {
    "problem_id": "646",
    "title": "Binary Prediction with a Rainfall Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Prediction with a Rainfall Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of rainfall for each day of the year based on given features. The target variable is binary (`rainfall`), indicating whether it rained or not.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world rainfall data.\n  * Participants are encouraged to explore differences between the synthetic and original datasets and potentially incorporate the original data to improve model performance.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data related to weather and climate, specifically rainfall prediction.\n* **Data Files:**\n  * `train.csv`: Contains the training data with the binary target `rainfall`.\n  * `test.csv`: Contains the test data for which predictions need to be made.\n  * `sample_submission.csv`: Provides a submission template with the required format (`id, rainfall`).\n* **Features:** The dataset includes 27 columns (features) related to rainfall prediction, though specific features are not detailed in the provided context. The data is synthetically generated but designed to mimic real-world distributions.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Area Under the ROC Curve (AUC-ROC)\n  * **Components:** \n    * Submissions are evaluated based on the predicted probabilities of rainfall (`rainfall`) for each test sample.\n    * The ROC curve measures the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) across different probability thresholds.\n    * AUC-ROC summarizes this curve into a single value between 0 and 1, where higher values indicate better model performance.",
    "sections": {},
    "file_path": "kaggle_datasets/646/problem_summary.md"
  },
  "474": {
    "problem_id": "474",
    "title": "Time Series Forecasting for Store Sales",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict future sales of Kaggle merchandise for two fictional store chains (KaggleMart and KaggleRama) across three countries to determine which chain should become the official Kaggle outlet.",
    "evaluation_metric": null,
    "full_content": "# Time Series Forecasting for Store Sales\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict future sales of Kaggle merchandise for two fictional store chains (KaggleMart and KaggleRama) across three countries to determine which chain should become the official Kaggle outlet.\n    * **Key Points:**\n        * Forecast a full year of sales for three items at two stores in three countries.\n        * Dataset includes real-world effects like seasonality, weekends, and holidays.\n        * Designed as an approachable competition for beginners to practice time series forecasting.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data containing historical sales records of merchandise items across stores and countries.\n* **Data Files:**\n    * `train.csv`: Training data with date-country-store-item sales combinations.\n    * `test.csv`: Test data requiring sales predictions for date-country-store-item combinations.\n    * `sample_submission.csv`: Example submission file format.\n* **Features:** Includes time-based features (date), categorical features (country, store, item), and target variable (num_sold).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE)\n    * **Components:**\n        * SMAPE = 0 when both actual and predicted values are 0.\n        * Formula: \\( \\text{SMAPE} = \\frac{100\\%}{n} \\sum_{t=1}^n \\frac{|F_t - A_t|}{(|A_t| + |F_t|)/2} \\)\n        * Evaluates accuracy of percentage errors symmetrically, treating over- and under-predictions equally.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict future sales of Kaggle merchandise for two fictional store chains (KaggleMart and KaggleRama) across three countries to determine which chain should become the official Kaggle outlet.\n    * **Key Points:**\n        * Forecast a full year of sales for three items at two stores in three countries.\n        * Dataset includes real-world effects like seasonality, weekends, and holidays.\n        * Designed as an approachable competition for beginners to practice time series forecasting.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data containing historical sales records of merchandise items across stores and countries.\n* **Data Files:**\n    * `train.csv`: Training data with date-country-store-item sales combinations.\n    * `test.csv`: Test data requiring sales predictions for date-country-store-item combinations.\n    * `sample_submission.csv`: Example submission file format.\n* **Features:** Includes time-based features (date), categorical features (country, store, item), and target variable (num_sold).",
      "Evaluation Metrics": "* **Primary Metric:** Symmetric Mean Absolute Percentage Error (SMAPE)\n    * **Components:**\n        * SMAPE = 0 when both actual and predicted values are 0.\n        * Formula: \\( \\text{SMAPE} = \\frac{100\\%}{n} \\sum_{t=1}^n \\frac{|F_t - A_t|}{(|A_t| + |F_t|)/2} \\)\n        * Evaluates accuracy of percentage errors symmetrically, treating over- and under-predictions equally."
    },
    "file_path": "kaggle_datasets/474/problem_summary.md"
  },
  "222": {
    "problem_id": "222",
    "title": "Lung Cancer Detection from CT Scans",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Lung Cancer Detection from CT Scans\n\n## Problem Description\n- **Problem Type**: Binary Classification (Medical Image Analysis)\n- **Objective**: Develop an algorithm to predict whether a patient will be diagnosed with lung cancer within one year based on low-dose CT scan images. The goal is to reduce false positives in current detection methods and enable earlier interventions.\n- **Key Points**:\n  - Focus on analyzing high-resolution lung CT scans in DICOM format\n  - Must handle variable number of axial slices per scan (typically 100-400 slices)\n  - Images come from diverse sources with varying quality and slice thickness\n  - External data use permitted if publicly available and disclosed\n\n## Dataset Overview\n- **Data Type**: 3D Medical Imaging (DICOM format CT scans)\n- **Context**: Low-dose helical CT scans from high-risk patients, with pathology-confirmed labels\n- **Data Files**:\n  - stage1.7z: Compressed training/test set images (DICOM series)\n  - stage1_labels.csv: Ground truth labels for training set\n  - stage1_sample_submission.csv: Submission format template\n- **Key Features**:\n  - Each patient has a directory of DICOM slices forming a 3D volume\n  - DICOM headers contain metadata (patient ID, slice thickness, etc.)\n  - Image quality varies based on scanning equipment and era\n\n## Evaluation Metrics\n- **Primary Metric**: Logarithmic Loss (Log Loss)\n- **Calculation**:\n  - LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]\n  - Where:\n    - n = number of patients in test set\n    - ŷ_i = predicted probability of cancer (clipped to [10^-15, 1-10^-15])\n    - y_i = 1 if cancer diagnosis, 0 otherwise\n  - Predictions are patient-level probabilities (not per-slice)",
    "sections": {},
    "file_path": "kaggle_datasets/222/problem_summary.md"
  },
  "480": {
    "problem_id": "480",
    "title": "Predicting NCAA Women's Basketball Tournament Outcomes",
    "problem_type": "Binary Classification (with probabilistic outputs)",
    "objective": "Predict the outcome probabilities of all possible matchups in the NCAA Women's Basketball Tournament (March Madness). The competition has two stages:",
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Women's Basketball Tournament Outcomes\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the outcome probabilities of all possible matchups in the NCAA Women's Basketball Tournament (March Madness). The competition has two stages:\n  * Stage 1: Predict historical tournament outcomes (2016-2019, 2021) for model validation\n  * Stage 2: Predict the 2022 tournament outcomes before games begin\n* **Key Points:**\n  * Participants must predict probabilities for every possible matchup between the 64 tournament teams (2,016 predictions per year)\n  * Predictions must be made from the perspective of the team with the lower ID in each matchup\n  * External data sources are allowed but must be disclosed before the tournament begins\n\n**Dataset Overview:**\n* **Data Type:** Tabular data containing historical basketball game results, team statistics, and tournament information\n* **Data Context:** NCAA Division I Women's Basketball games from 1998-2022, including regular season and tournament games\n* **Key Data Files:**\n  * `WTeams.csv`: Team IDs and names\n  * `WSeasons.csv`: Season information and tournament structure\n  * `WNCAATourneySeeds.csv`: Tournament seeds by year\n  * `WRegularSeasonCompactResults.csv`: Game results (1998-2022)\n  * `WNCAATourneyCompactResults.csv`: Tournament results (1998-2022)\n  * `WRegularSeasonDetailedResults.csv`: Team-level box scores (2010-2022)\n  * `WNCAATourneyDetailedResults.csv`: Tournament box scores (2010-2022)\n* **Important Features:**\n  * Game outcomes (win/loss, scores)\n  * Team performance statistics (field goals, rebounds, assists, etc.)\n  * Tournament seeds and bracket structure\n  * Game locations and dates\n\n**Evaluation Metrics:**\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n* **Metric Components:**\n  * Calculated as: `-1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n  * Where:\n    * `n` = number of games\n    * `y_i` = 1 if team 1 wins, 0 if team 2 wins\n    * `ŷ_i`",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the outcome probabilities of all possible matchups in the NCAA Women's Basketball Tournament (March Madness). The competition has two stages:\n  * Stage 1: Predict historical tournament outcomes (2016-2019, 2021) for model validation\n  * Stage 2: Predict the 2022 tournament outcomes before games begin\n* **Key Points:**\n  * Participants must predict probabilities for every possible matchup between the 64 tournament teams (2,016 predictions per year)\n  * Predictions must be made from the perspective of the team with the lower ID in each matchup\n  * External data sources are allowed but must be disclosed before the tournament begins",
      "Dataset Overview": "* **Data Type:** Tabular data containing historical basketball game results, team statistics, and tournament information\n* **Data Context:** NCAA Division I Women's Basketball games from 1998-2022, including regular season and tournament games\n* **Key Data Files:**\n  * `WTeams.csv`: Team IDs and names\n  * `WSeasons.csv`: Season information and tournament structure\n  * `WNCAATourneySeeds.csv`: Tournament seeds by year\n  * `WRegularSeasonCompactResults.csv`: Game results (1998-2022)\n  * `WNCAATourneyCompactResults.csv`: Tournament results (1998-2022)\n  * `WRegularSeasonDetailedResults.csv`: Team-level box scores (2010-2022)\n  * `WNCAATourneyDetailedResults.csv`: Tournament box scores (2010-2022)\n* **Important Features:**\n  * Game outcomes (win/loss, scores)\n  * Team performance statistics (field goals, rebounds, assists, etc.)\n  * Tournament seeds and bracket structure\n  * Game locations and dates",
      "Evaluation Metrics": "* **Primary Metric:** Logarithmic Loss (LogLoss)\n* **Metric Components:**\n  * Calculated as: `-1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n  * Where:\n    * `n` = number of games\n    * `y_i` = 1 if team 1 wins, 0 if team 2 wins\n    * `ŷ_i`"
    },
    "file_path": "kaggle_datasets/480/problem_summary.md"
  },
  "214": {
    "problem_id": "214",
    "title": "Predicting Click-through Rates for Content Recommendations",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Click-through Rates for Content Recommendations\n\n## Problem Description\n- **Problem Type**: Binary Classification (Click Prediction) with Ranking\n- **Objective**: Predict the likelihood of a user clicking on a specific content recommendation (ad) within a set of displayed recommendations. The goal is to rank recommendations by their predicted click probability for each display context.\n- **Key Points**:\n  - Focus on personalized content recommendations at scale (250 billion monthly recommendations).\n  - Must handle large-scale relational data with user interactions, document metadata, and ad details.\n  - Predictions must be made for variable-sized sets of recommendations (different number of ads per display_id).\n  - Evaluation requires ranking predictions within each display context.\n\n## Dataset Overview\n- **Data Type**: Relational tabular data with user interaction logs, document metadata, and ad information\n- **Context**: Web content recommendation system with anonymized user behavior data\n- **Data Files**:\n  - `clicks_train.csv`: Training set with clicked ads (display_id, ad_id, clicked)\n  - `clicks_test.csv`: Test set without clicked labels (same format as train)\n  - `page_views.csv`: User browsing history (100GB+, 2B+ rows)\n  - `events.csv`: Context for each recommendation display\n  - `promoted_content.csv`: Ad metadata\n  - `documents_*.csv`: Various document metadata files\n  - `sample_submission.csv`: Submission format example\n- **Key Features**:\n  - User behavior: page views, timestamps, platforms, geo-locations\n  - Document attributes: topics, entities, categories, publishers\n  - Ad attributes: campaigns, advertisers\n  - Contextual features: display settings, traffic sources\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Average Precision @12 (MAP@12)\n- **Metric Components**:\n  - Calculated per display_id (recommendation set)\n  - Precision at cutoff k (P(k)) for each position up to min(12, n) where n is number of predictions\n  - Averaged across all display_ids\n  - Requires ranking predictions in order of decreasing click probability\n  - Submission format: space-delimited list of ad_ids per display_id, ordered by predicted likelihood",
    "sections": {},
    "file_path": "kaggle_datasets/214/problem_summary.md"
  },
  "442": {
    "problem_id": "442",
    "title": "Chemical Image to InChI Text Translation",
    "problem_type": "Computer Vision - Image-to-Text Translation (Multimodal Learning)",
    "objective": "Convert images of hand-drawn or scanned chemical structures (Skeletal formulas) into their corresponding machine-readable International Chemical Identifier (InChI) text strings. The goal is to automate the extraction of chemical structures from historical documents where manual conversion is currently required.",
    "evaluation_metric": null,
    "full_content": "# Chemical Image to InChI Text Translation\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Image-to-Text Translation (Multimodal Learning)\n* **Objective:** Convert images of hand-drawn or scanned chemical structures (Skeletal formulas) into their corresponding machine-readable International Chemical Identifier (InChI) text strings. The goal is to automate the extraction of chemical structures from historical documents where manual conversion is currently required.\n    * **Key Points:**\n        * Focus on handling real-world challenges like image rotation, varying resolutions, and noise corruption.\n        * Address the limitation of small public datasets by leveraging a large synthetic dataset provided by Bristol-Myers Squibb.\n        * Improve upon existing tools that fail under suboptimal conditions (e.g., noisy or rotated images).\n\n**Dataset Overview:**\n* **Data Type & Context:** PNG images of chemical structures paired with InChI text labels. The dataset simulates real-world challenges with variations in rotation, resolution, and noise.\n    * **Data Files:**\n        * `train/`: Training images (organized in a 3-level folder structure by `image_id`).\n        * `test/`: Test images (same structure as `train/`).\n        * `train_labels.csv`: Ground truth InChI labels for training images.\n        * `sample_submission.csv`: Example submission file.\n    * **Features:**\n        * Images: Chemical structure depictions (Skeletal formulas) with synthetic variations.\n        * Labels: InChI strings representing the chemical structures.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Levenshtein distance between predicted and ground truth InChI strings.\n    * **Components of Levenshtein Distance:**\n        * Measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change the predicted string into the true string.\n        * Lower scores indicate better performance (0 = perfect match).",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Image-to-Text Translation (Multimodal Learning)\n* **Objective:** Convert images of hand-drawn or scanned chemical structures (Skeletal formulas) into their corresponding machine-readable International Chemical Identifier (InChI) text strings. The goal is to automate the extraction of chemical structures from historical documents where manual conversion is currently required.\n    * **Key Points:**\n        * Focus on handling real-world challenges like image rotation, varying resolutions, and noise corruption.\n        * Address the limitation of small public datasets by leveraging a large synthetic dataset provided by Bristol-Myers Squibb.\n        * Improve upon existing tools that fail under suboptimal conditions (e.g., noisy or rotated images).",
      "Dataset Overview": "* **Data Type & Context:** PNG images of chemical structures paired with InChI text labels. The dataset simulates real-world challenges with variations in rotation, resolution, and noise.\n    * **Data Files:**\n        * `train/`: Training images (organized in a 3-level folder structure by `image_id`).\n        * `test/`: Test images (same structure as `train/`).\n        * `train_labels.csv`: Ground truth InChI labels for training images.\n        * `sample_submission.csv`: Example submission file.\n    * **Features:**\n        * Images: Chemical structure depictions (Skeletal formulas) with synthetic variations.\n        * Labels: InChI strings representing the chemical structures.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Levenshtein distance between predicted and ground truth InChI strings.\n    * **Components of Levenshtein Distance:**\n        * Measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change the predicted string into the true string.\n        * Lower scores indicate better performance (0 = perfect match)."
    },
    "file_path": "kaggle_datasets/442/problem_summary.md"
  },
  "489": {
    "problem_id": "489",
    "title": "Bird Species Identification in Audio Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Bird Species Identification in Audio Recordings\n\n## Problem Description\n* **Problem Type**: Audio Classification (Multi-label Binary Classification)\n* **Objective**: Develop a model to identify bird species by sound in continuous audio recordings, focusing on rare and endangered Hawaiian birds. The goal is to automate bioacoustic monitoring for conservation research.\n    * **Key Points**:\n        * Focus on limited training data scenarios (rare species)\n        * Process continuous audio data (1-minute soundscapes)\n        * Identify species presence in 5-second windows\n        * Address conservation challenges for endangered Hawaiian birds\n\n## Dataset Overview\n* **Data Type**: Audio recordings (soundscapes) with metadata\n* **Context**: Bioacoustic monitoring data from Hawaiian ecosystems and global bird recordings\n* **Data Files**:\n    * `train_metadata.csv` (species codes, audio filenames, quality ratings)\n    * `train_audio/` (short bird call recordings in .ogg format)\n    * `test_soundscapes/` (1-minute continuous recordings)\n    * `test.csv` (metadata for test set)\n    * `scored_birds.json` (subset of scored species)\n    * `eBird_Taxonomy_v2021.csv` (species relationship data)\n* **Key Features**:\n    * Audio files downsampled to 32kHz\n    * Species codes linked to eBird taxonomy\n    * Quality ratings (0.0-5.0) for training recordings\n    * 5-second window annotations in test set\n\n## Evaluation Metrics\n* **Primary Metric**: Custom weighted classification accuracy (similar to macro F1 score)\n    * **Components**:\n        * Only a subset of species are scored per audio file\n        * Species are weighted equally in final score\n        * True negatives and true positives weighted equally per species\n        * Designed to handle imbalanced species distribution\n    * **Closest Standard Metric**: Macro F1 score (recommended for cross-validation)",
    "sections": {},
    "file_path": "kaggle_datasets/489/problem_summary.md"
  },
  "445": {
    "problem_id": "445",
    "title": "Multi-Target Time Series Regression for Air Pollution Prediction",
    "problem_type": "Time Series Regression (Multi-Target)",
    "objective": "Predict three air pollution measurements (carbon monoxide, benzene, and nitrogen oxides) over time using weather data and sensor inputs.",
    "evaluation_metric": null,
    "full_content": "# Multi-Target Time Series Regression for Air Pollution Prediction\n\n**Problem Description:**\n* **Problem Type:** Time Series Regression (Multi-Target)\n* **Objective:** Predict three air pollution measurements (carbon monoxide, benzene, and nitrogen oxides) over time using weather data and sensor inputs.\n    * **Key Points:**\n        * Based on synthetic-generated data derived from real air pollution sensor data.\n        * Features include basic weather information (temperature, humidity) and 5 sensor inputs.\n        * Designed as an approachable competition for beginners between Titanic and Featured competitions.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data containing weather measurements and sensor readings for air pollution prediction.\n* **Data Files:**\n    * `train.csv` - Contains weather data, sensor data, and the 3 target values.\n    * `test.csv` - Same format as train.csv but without target values.\n    * `sample_submission.csv` - Example submission file with required format.\n* **Features:**\n    * Time-based features (date_time)\n    * Weather measurements (temperature, humidity)\n    * 5 sensor input values\n    * 3 target pollution measurements (carbon monoxide, benzene, nitrogen oxides)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Column-wise Root Mean Squared Logarithmic Error (MCRMSLE)\n    * **Components:**\n        1. RMSLE is calculated for each target column separately:\n            * sqrt(mean((log(p_i + 1) - log(a_i + 1))^2))\n            * Where p_i = prediction, a_i = actual value\n        2. Final score is the mean of RMSLE across all 3 target columns.\n    * **Special Considerations:**\n        * Uses logarithmic error to handle potentially large value ranges.\n        * Penalizes underestimates more than overestimates (due to log properties).",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Regression (Multi-Target)\n* **Objective:** Predict three air pollution measurements (carbon monoxide, benzene, and nitrogen oxides) over time using weather data and sensor inputs.\n    * **Key Points:**\n        * Based on synthetic-generated data derived from real air pollution sensor data.\n        * Features include basic weather information (temperature, humidity) and 5 sensor inputs.\n        * Designed as an approachable competition for beginners between Titanic and Featured competitions.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data containing weather measurements and sensor readings for air pollution prediction.\n* **Data Files:**\n    * `train.csv` - Contains weather data, sensor data, and the 3 target values.\n    * `test.csv` - Same format as train.csv but without target values.\n    * `sample_submission.csv` - Example submission file with required format.\n* **Features:**\n    * Time-based features (date_time)\n    * Weather measurements (temperature, humidity)\n    * 5 sensor input values\n    * 3 target pollution measurements (carbon monoxide, benzene, nitrogen oxides)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Column-wise Root Mean Squared Logarithmic Error (MCRMSLE)\n    * **Components:**\n        1. RMSLE is calculated for each target column separately:\n            * sqrt(mean((log(p_i + 1) - log(a_i + 1))^2))\n            * Where p_i = prediction, a_i = actual value\n        2. Final score is the mean of RMSLE across all 3 target columns.\n    * **Special Considerations:**\n        * Uses logarithmic error to handle potentially large value ranges.\n        * Penalizes underestimates more than overestimates (due to log properties)."
    },
    "file_path": "kaggle_datasets/445/problem_summary.md"
  },
  "213": {
    "problem_id": "213",
    "title": "Facial Keypoints Detection in Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Facial Keypoints Detection in Images\n\n## Problem Description\n- **Problem Type**: Computer Vision - Keypoint Detection (Regression)\n- **Objective**: Predict (x,y) coordinates of 15 facial keypoints (e.g., eye centers, nose tip, mouth corners) from 96×96 grayscale face images. The task involves precisely locating anatomical facial features.\n- **Key Points**:\n  - Faces exhibit large variations in pose, size, illumination, and individual appearance\n  - Some target keypoints may be missing in training data\n  - Applications include face tracking, expression analysis, medical diagnosis, and biometrics\n\n## Dataset Overview\n- **Data Type**: Tabular data containing image pixels + Computer Vision (grayscale facial images)\n- **Context**: 7,049 training and 1,783 test images of faces with potential keypoint annotations\n- **Data Files**:\n  - `training.csv`: Contains 30 columns (15 keypoint x/y coordinates) + image pixel values\n  - `test.csv`: Contains image IDs and pixel values only\n  - `submissionFileFormat.csv`: Specifies which keypoints to predict for each test image\n- **Features**:\n  - Images represented as space-separated pixel values (0-255) in row-major order (96×96=9216 pixels)\n  - 15 keypoints with x/y coordinates (e.g., left_eye_center_x, nose_tip_y)\n  - Some keypoint coordinates may be missing (NA values)\n\n## Evaluation Metrics\n- **Evaluation Metric**: Root Mean Squared Error (RMSE)\n- **Metric Components**:\n  - Computed as: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}$\n  - Measures deviation between predicted and actual keypoint coordinates\n  - Penalizes large errors more severely than small errors\n  - Applied only to specified keypoints in submission format file",
    "sections": {},
    "file_path": "kaggle_datasets/213/problem_summary.md"
  },
  "458": {
    "problem_id": "458",
    "title": "Landmark Image Retrieval Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Landmark Image Retrieval Challenge\n\n## Problem Description\n* **Problem Type**: Computer Vision - Image Retrieval  \n* **Objective**:  \n    * Develop models to retrieve relevant database images (depicting the same landmark) for a given query image from a large-scale dataset.  \n    * The task involves ranking index images by relevance to the query, prioritizing correct matches.  \n* **Key Points**:  \n    * Focus on global landmark diversity (test/index sets sampled from many countries).  \n    * Synchronous rerun code competition format (submissions via Kaggle Notebooks).  \n    * Part of the ICCV 2021 Instance-Level Recognition Workshop.  \n\n## Dataset Overview  \n* **Data Type & Context**:  \n    * Image data (landmark photos) organized into query (`test/`), index (`index/`), and training (`train/`) sets.  \n    * Training labels provided in `train.csv`.  \n* **Data Files**:  \n    * `train/` (images), `train.csv` (labels), `index/` (retrieval database), `test/` (query images), `sample_submission.csv`.  \n* **Features**:  \n    * Images stored in subfolders based on the first 3 characters of their unique `id`.  \n    * Dataset derived from Google Landmarks Dataset v2 (GLDv2), emphasizing geographic diversity.  \n\n## Evaluation Metrics  \n* **Primary Metric**: Mean Average Precision @ 100 (mAP@100)  \n* **Components**:  \n    * For each query image:  \n        * Precision at rank *k* (`Pq(k)`) is calculated for the top 100 predictions.  \n        * Relevance (`relq(k)`) is 1 if the *k*-th prediction matches the query landmark, else 0.  \n        * Average precision is computed over all relevant images (up to 100).  \n    * Final score: Mean of average precisions across all queries (`Q`).  \n* **Submission Format**:  \n    * Space-delimited list of index image IDs per query, ranked by relevance.  \n    * File format: `id,images` (e.g., `000088da12d664db,0370c4c856f096e8 766677ab964f4311...`).",
    "sections": {},
    "file_path": "kaggle_datasets/458/problem_summary.md"
  },
  "493": {
    "problem_id": "493",
    "title": "Fine-Grained Classification of North American Vascular Plants from Herbarium Specimens",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Classification of North American Vascular Plants from Herbarium Specimens\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective**: Identify plant species from herbarium specimen images, covering 15,501 vascular plant taxa native to North America. The goal is to replicate traditional botanical identification tools using AI.\n* **Key Points**:\n  * Focus on **vascular land plants** (lycophytes, ferns, gymnosperms, flowering plants).\n  * Dataset has a **long-tail distribution**, with 7-100 images per species.\n  * Includes **hierarchical taxonomic structure** (family-genus-species) and **phylogenetic distances** between genera.\n  * Test set distribution differs slightly from training set (species capped at 80 images in training).\n\n## Dataset Overview\n* **Data Type**: Image data (herbarium specimens) with hierarchical taxonomic metadata.\n* **Context**: 1.05M JPEG images from 60 botanical institutions, representing >90% of North American vascular plant taxa.\n* **Data Files**:\n  * `train_metadata.json` (COCO format with annotations, categories, phylogenetic distances)\n  * `test_metadata.json`\n  * `train_images/` (organized by subfolders)\n  * `test_images/`\n  * `sample_submission.csv`\n* **Features**:\n  * Images with blurred text labels (max 1000px in larger dimension)\n  * Hierarchical labels: `family`, `genus`, `species`\n  * Phylogenetic distance matrix between genera\n  * Institution and license metadata\n\n## Evaluation Metrics\n* **Primary Metric**: Macro F1-Score\n  * Calculated per species and averaged\n  * Formula:  \n    𝐹1 = 2 × (precision × recall) / (precision + recall)  \n    where:  \n    - precision = TP / (TP + FP)  \n    - recall = TP / (TP + FN)  \n* **Submission Format**: CSV with `Id` (image ID) and `Predicted` (category_id) columns.",
    "sections": {},
    "file_path": "kaggle_datasets/493/problem_summary.md"
  },
  "231": {
    "problem_id": "231",
    "title": "Fine-Grained Image Classification of 5,000 Species",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Image Classification of 5,000 Species\n\n## Problem Description\n* **Problem Type:** Multi-class Image Classification (Fine-Grained)\n* **Objective:**  \n  * Develop a model to accurately classify images of plants and animals into 5,089 distinct species.  \n  * Address challenges of fine-grained visual similarity between species, class imbalances, and large class count.  \n* **Key Points:**  \n  * Many species are visually similar (e.g., alpaca vs. llama, roses vs. kale).  \n  * Real-world data includes varied lighting, backgrounds, and capture conditions.  \n  * Participants must predict **5 possible labels** per image to account for potential ambiguities or multiple valid categories.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n  * **675,000 images** (training + validation) of plants/animals from inaturalist.org.  \n  * Test set images without labels.  \n  * Images resized to max 800px (JPEG format).  \n* **Data Files:**  \n  * `train_val_images.tar.gz`: Training/validation images (directory-structured by species).  \n  * `train_val2017.zip`: JSON annotations (COCO format) for training/validation.  \n  * `test2017.tar.gz`: Test images.  \n  * `test2017.zip`: Test image metadata.  \n* **Key Features:**  \n  * Images annotated with species IDs, supercategories, and metadata (e.g., license, rights holder).  \n  * JSON annotations include image dimensions, file paths, and taxonomic hierarchy.  \n\n## Evaluation Metrics\n* **Primary Metric:** **Top-5 Error Rate**  \n  * For each image, if the ground truth label is among the model’s top 5 predictions → error = 0; else error = 1.  \n  * Final score = average error across all test images (lower is better).  \n* **Rationale:**  \n  * Allows for disambiguation (e.g., using metadata like location/date).  \n  * Accounts for potential multi-label scenarios (e.g., bee on a flower).",
    "sections": {},
    "file_path": "kaggle_datasets/231/problem_summary.md"
  },
  "467": {
    "problem_id": "467",
    "title": "Lux AI Challenge: Resource Gathering and Survival Simulation",
    "problem_type": "Reinforcement Learning / Multi-Agent Strategy Game",
    "objective": "Design an AI agent to compete in a 1v1 resource gathering and survival game where teams must:",
    "evaluation_metric": null,
    "full_content": "# Lux AI Challenge: Resource Gathering and Survival Simulation\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Design an AI agent to compete in a 1v1 resource gathering and survival game where teams must:\n  * Collect resources (Wood, Coal, Uranium) to fuel cities\n  * Expand city territories while surviving periodic night cycles\n  * Outmaneuver opponents through strategic resource allocation and unit management\n* **Key Points:**\n  * Real-time strategy elements with turn-based execution (3-second decision window)\n  * Complete information game state\n  * Multi-variable optimization problem involving:\n    * Resource collection prioritization\n    * City expansion timing\n    * Opponent analysis and counter-strategies\n  * Day/night cycle mechanics (30 day turns + 10 night turns per cycle)\n  * Unit cooldown and movement systems affected by road networks\n\n**Dataset Overview:**\n* **Data Type:** Game state observations through API (no static dataset files)\n* **Context:** Simulated 2D grid world with:\n  * Resource tiles (Wood, Coal, Uranium)\n  * Player units (Workers, Carts)\n  * City tiles and urban development\n* **Key Game Elements:**\n  * Dynamic map (12x12 to 32x32 tiles)\n  * Resource regeneration mechanics (wood regrowth)\n  * Unit capabilities with different roles\n  * Research progression system\n  * Day/night survival requirements\n\n**Evaluation Metrics:**\n* **Primary Metric:** Skill Rating System (Gaussian N(μ,σ²))\n  * μ represents estimated skill level\n  * σ represents uncertainty in rating\n* **Rating Update Mechanics:**\n  * Agents play repeated matches against similarly-rated opponents\n  * Ratings updated based on match outcomes:\n    * Winner's μ increases, loser's μ decreases\n    * Draws move both agents' μ toward their mean\n  * Update magnitude depends on:\n    * Deviation from expected outcome\n    * Current uncertainty (σ) of both agents\n  * σ decreases with more matches played\n* **Final Ranking:** Based on number of CityTiles controlled after 360 turns\n  * Tiebreakers: Unit count, then declared draw",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Design an AI agent to compete in a 1v1 resource gathering and survival game where teams must:\n  * Collect resources (Wood, Coal, Uranium) to fuel cities\n  * Expand city territories while surviving periodic night cycles\n  * Outmaneuver opponents through strategic resource allocation and unit management\n* **Key Points:**\n  * Real-time strategy elements with turn-based execution (3-second decision window)\n  * Complete information game state\n  * Multi-variable optimization problem involving:\n    * Resource collection prioritization\n    * City expansion timing\n    * Opponent analysis and counter-strategies\n  * Day/night cycle mechanics (30 day turns + 10 night turns per cycle)\n  * Unit cooldown and movement systems affected by road networks",
      "Dataset Overview": "* **Data Type:** Game state observations through API (no static dataset files)\n* **Context:** Simulated 2D grid world with:\n  * Resource tiles (Wood, Coal, Uranium)\n  * Player units (Workers, Carts)\n  * City tiles and urban development\n* **Key Game Elements:**\n  * Dynamic map (12x12 to 32x32 tiles)\n  * Resource regeneration mechanics (wood regrowth)\n  * Unit capabilities with different roles\n  * Research progression system\n  * Day/night survival requirements",
      "Evaluation Metrics": "* **Primary Metric:** Skill Rating System (Gaussian N(μ,σ²))\n  * μ represents estimated skill level\n  * σ represents uncertainty in rating\n* **Rating Update Mechanics:**\n  * Agents play repeated matches against similarly-rated opponents\n  * Ratings updated based on match outcomes:\n    * Winner's μ increases, loser's μ decreases\n    * Draws move both agents' μ toward their mean\n  * Update magnitude depends on:\n    * Deviation from expected outcome\n    * Current uncertainty (σ) of both agents\n  * σ decreases with more matches played\n* **Final Ranking:** Based on number of CityTiles controlled after 360 turns\n  * Tiebreakers: Unit count, then declared draw"
    },
    "file_path": "kaggle_datasets/467/problem_summary.md"
  },
  "209": {
    "problem_id": "209",
    "title": "Seizure Prediction from Human Intracranial EEG Data",
    "problem_type": "Binary Classification (Preictal vs. Interictal EEG states)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Seizure Prediction from Human Intracranial EEG Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Preictal vs. Interictal EEG states)\n* **Objective:**  \n    * Predict the probability of an impending seizure using 10-minute intracranial EEG (iEEG) clips.  \n    * Distinguish between:\n        * **Preictal** (1-hour window before seizure onset, with 5-minute horizon)\n        * **Interictal** (baseline activity, ≥4 hours away from any seizure)\n* **Key Points:**  \n    * Focuses on *lead seizures* (occurring ≥4 hours after another seizure) to avoid clustering effects.  \n    * Data contains artifacts (e.g., signal dropouts with zero values) that must be handled.  \n    * Clinical motivation: Enable seizure warning systems to improve patient safety and targeted medication.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Time-series intracranial EEG (iEEG) recordings from 3 human patients with epilepsy.  \n    * Long-duration recordings (months to years) sampled at 400 Hz from 16 electrodes.  \n* **Data Files:**  \n    * Training: `I_J_K.mat` (patient `I`, segment `J`, class `K=0/1` for interictal/preictal).  \n    * Testing: `I_J.mat` (unlabeled segments).  \n* **Key Features:**  \n    * Each `.mat` file contains:  \n        * `data`: Time × electrode matrix (rows = samples, columns = channels).  \n        * `iEEGsamplingRate`: 400 Hz.  \n        * `sequence` (training only): Position in the 1-hour preictal/interictal sequence.  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n    * Measures the model’s ability to rank preictal clips higher than interictal clips.  \n* **Submission Format:**  \n    * CSV with `File` (e.g., `1_1.mat`) and predicted probability (`Class`) for each test clip.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Preictal vs. Interictal EEG states)\n* **Objective:**  \n    * Predict the probability of an impending seizure using 10-minute intracranial EEG (iEEG) clips.  \n    * Distinguish between:\n        * **Preictal** (1-hour window before seizure onset, with 5-minute horizon)\n        * **Interictal** (baseline activity, ≥4 hours away from any seizure)\n* **Key Points:**  \n    * Focuses on *lead seizures* (occurring ≥4 hours after another seizure) to avoid clustering effects.  \n    * Data contains artifacts (e.g., signal dropouts with zero values) that must be handled.  \n    * Clinical motivation: Enable seizure warning systems to improve patient safety and targeted medication.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Time-series intracranial EEG (iEEG) recordings from 3 human patients with epilepsy.  \n    * Long-duration recordings (months to years) sampled at 400 Hz from 16 electrodes.  \n* **Data Files:**  \n    * Training: `I_J_K.mat` (patient `I`, segment `J`, class `K=0/1` for interictal/preictal).  \n    * Testing: `I_J.mat` (unlabeled segments).  \n* **Key Features:**  \n    * Each `.mat` file contains:  \n        * `data`: Time × electrode matrix (rows = samples, columns = channels).  \n        * `iEEGsamplingRate`: 400 Hz.  \n        * `sequence` (training only): Position in the 1-hour preictal/interictal sequence.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC).  \n    * Measures the model’s ability to rank preictal clips higher than interictal clips.  \n* **Submission Format:**  \n    * CSV with `File` (e.g., `1_1.mat`) and predicted probability (`Class`) for each test clip."
    },
    "file_path": "kaggle_datasets/209/problem_summary.md"
  },
  "460": {
    "problem_id": "460",
    "title": "Binary Classification with Synthetic Molecular Response Data",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of a binary `target` variable based on anonymized features derived from molecular response data. The goal is to model the biological response of molecules using synthetic chemical property features.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Synthetic Molecular Response Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary `target` variable based on anonymized features derived from molecular response data. The goal is to model the biological response of molecules using synthetic chemical property features.\n    * **Key Points:**\n        * Dataset is synthetically generated using CTGAN but based on real-world molecular response data.\n        * Designed as an approachable competition for beginners between Titanic and Featured competition difficulty levels.\n        * Features include a mix of scaled continuous and binary variables.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing anonymized chemical properties (synthetic generation via GAN) with biological response targets.\n* **Data Files:**\n    * `train.csv` (contains `target` column)\n    * `test.csv` (requires target probability predictions)\n    * `sample_submission.csv` (format template)\n* **Features:** \n    * 574 anonymized features (mix of scaled continuous and binary)\n    * Target column represents binary biological response\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between classes\n        * Evaluates predicted probabilities against true binary labels\n        * Higher values indicate better classification performance (1.0 = perfect)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of a binary `target` variable based on anonymized features derived from molecular response data. The goal is to model the biological response of molecules using synthetic chemical property features.\n    * **Key Points:**\n        * Dataset is synthetically generated using CTGAN but based on real-world molecular response data.\n        * Designed as an approachable competition for beginners between Titanic and Featured competition difficulty levels.\n        * Features include a mix of scaled continuous and binary variables.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing anonymized chemical properties (synthetic generation via GAN) with biological response targets.\n* **Data Files:**\n    * `train.csv` (contains `target` column)\n    * `test.csv` (requires target probability predictions)\n    * `sample_submission.csv` (format template)\n* **Features:** \n    * 574 anonymized features (mix of scaled continuous and binary)\n    * Target column represents binary biological response",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between classes\n        * Evaluates predicted probabilities against true binary labels\n        * Higher values indicate better classification performance (1.0 = perfect)"
    },
    "file_path": "kaggle_datasets/460/problem_summary.md"
  },
  "236": {
    "problem_id": "236",
    "title": "New York City Taxi Trip Duration Prediction",
    "problem_type": "Regression (Time Series Forecasting)",
    "objective": "Predict the total ride duration (in seconds) of taxi trips in New York City based on trip attributes.",
    "evaluation_metric": null,
    "full_content": "# New York City Taxi Trip Duration Prediction\n\n**Problem Description:**\n* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the total ride duration (in seconds) of taxi trips in New York City based on trip attributes.\n    * **Key Points:**\n        * Focus on collaborative learning and data sharing (playground competition with incentives for publishing additional training data).\n        * Uses NYC Taxi and Limousine Commission data with geo-coordinates, timestamps, and passenger counts.\n        * Similar to previous ECML/PKDD trip time challenge but with a community-focused twist.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing historical NYC Yellow Cab trip records (sampled and cleaned for the competition).\n    * **Data Files:**\n        * `train.csv` (1,458,644 trips)\n        * `test.csv` (625,134 trips)\n        * `sample_submission.csv` (format reference)\n    * **Key Features:**\n        * Pickup/dropoff timestamps (`pickup_datetime`, `dropoff_datetime`)\n        * Geo-coordinates (`pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, `dropoff_latitude`)\n        * Passenger count (`passenger_count`)\n        * Vendor ID (`vendor_id`)\n        * Store-and-forward flag (`store_and_fwd_flag`)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * **Calculation:**\n        * \\(\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}\\)\n        * Where:\n            * \\(n\\) = number of observations\n            * \\(p_i\\) = predicted trip duration\n            * \\(a_i\\) = actual trip duration\n            * \\(\\log\\) = natural logarithm\n    * **Purpose:** Penalizes underestimates more heavily than overestimates due to the logarithmic scale.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Time Series Forecasting)\n* **Objective:** Predict the total ride duration (in seconds) of taxi trips in New York City based on trip attributes.\n    * **Key Points:**\n        * Focus on collaborative learning and data sharing (playground competition with incentives for publishing additional training data).\n        * Uses NYC Taxi and Limousine Commission data with geo-coordinates, timestamps, and passenger counts.\n        * Similar to previous ECML/PKDD trip time challenge but with a community-focused twist.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing historical NYC Yellow Cab trip records (sampled and cleaned for the competition).\n    * **Data Files:**\n        * `train.csv` (1,458,644 trips)\n        * `test.csv` (625,134 trips)\n        * `sample_submission.csv` (format reference)\n    * **Key Features:**\n        * Pickup/dropoff timestamps (`pickup_datetime`, `dropoff_datetime`)\n        * Geo-coordinates (`pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, `dropoff_latitude`)\n        * Passenger count (`passenger_count`)\n        * Vendor ID (`vendor_id`)\n        * Store-and-forward flag (`store_and_fwd_flag`)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * **Calculation:**\n        * \\(\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}\\)\n        * Where:\n            * \\(n\\) = number of observations\n            * \\(p_i\\) = predicted trip duration\n            * \\(a_i\\) = actual trip duration\n            * \\(\\log\\) = natural logarithm\n    * **Purpose:** Penalizes underestimates more heavily than overestimates due to the logarithmic scale."
    },
    "file_path": "kaggle_datasets/236/problem_summary.md"
  },
  "494": {
    "problem_id": "494",
    "title": "Binary Classification with Manufacturing Control Data",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a manufacturing machine is in state `0` or state `1` based on simulated control data. The competition focuses on identifying and exploiting feature interactions in the data.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with Manufacturing Control Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a manufacturing machine is in state `0` or state `1` based on simulated control data. The competition focuses on identifying and exploiting feature interactions in the data.\n    * **Key Points:**\n        * Designed as an approachable competition for beginners to practice ML skills.\n        * Emphasizes exploration of methods to detect and utilize feature interactions.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing simulated manufacturing control data, containing both normalized continuous and categorical features.\n* **Data Files:**\n    * `train.csv`: Training data with features and binary target variable.\n    * `test.csv`: Test set without target variable.\n    * `sample_submission.csv`: Example submission file format.\n* **Features:** Dataset contains 67 columns (specific features not detailed, but includes various feature interactions important for prediction).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between the two classes (state 0 vs state 1).\n        * Higher AUC indicates better model performance, with 1.0 representing perfect classification.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a manufacturing machine is in state `0` or state `1` based on simulated control data. The competition focuses on identifying and exploiting feature interactions in the data.\n    * **Key Points:**\n        * Designed as an approachable competition for beginners to practice ML skills.\n        * Emphasizes exploration of methods to detect and utilize feature interactions.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing simulated manufacturing control data, containing both normalized continuous and categorical features.\n* **Data Files:**\n    * `train.csv`: Training data with features and binary target variable.\n    * `test.csv`: Test set without target variable.\n    * `sample_submission.csv`: Example submission file format.\n* **Features:** Dataset contains 67 columns (specific features not detailed, but includes various feature interactions important for prediction).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the ability of the model to distinguish between the two classes (state 0 vs state 1).\n        * Higher AUC indicates better model performance, with 1.0 representing perfect classification."
    },
    "file_path": "kaggle_datasets/494/problem_summary.md"
  },
  "469": {
    "problem_id": "469",
    "title": "Multiclass Classification of Forest Cover Types",
    "problem_type": "Multiclass Classification",
    "objective": "Predict the forest cover type (a categorical target variable) based on various feature columns in the dataset. The competition is designed as an approachable challenge for beginners to practice machine learning skills on tabular data.",
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of Forest Cover Types\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the forest cover type (a categorical target variable) based on various feature columns in the dataset. The competition is designed as an approachable challenge for beginners to practice machine learning skills on tabular data.\n* **Key Points:**\n  * Dataset is synthetically generated using CTGAN, based on the original Forest Cover Type Prediction competition data.\n  * The synthetic data may or may not maintain the same relationships between features and target as the original data.\n  * Competition is part of a monthly series aimed at providing beginner-friendly challenges.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing forest characteristics and cover types (synthetic data based on real forest cover measurements).\n* **Data Files:**\n  * train.csv (contains target `Cover_Type` column)\n  * test.csv\n  * sample_submission.csv\n* **Features:** 113 columns total (exact features not specified, but based on original forest cover dataset which includes wilderness area designations, soil types, elevation, and other geographical features).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class classification accuracy\n  * Simple percentage of correctly predicted cover types\n  * No complex weighting or stratification mentioned",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the forest cover type (a categorical target variable) based on various feature columns in the dataset. The competition is designed as an approachable challenge for beginners to practice machine learning skills on tabular data.\n* **Key Points:**\n  * Dataset is synthetically generated using CTGAN, based on the original Forest Cover Type Prediction competition data.\n  * The synthetic data may or may not maintain the same relationships between features and target as the original data.\n  * Competition is part of a monthly series aimed at providing beginner-friendly challenges.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing forest characteristics and cover types (synthetic data based on real forest cover measurements).\n* **Data Files:**\n  * train.csv (contains target `Cover_Type` column)\n  * test.csv\n  * sample_submission.csv\n* **Features:** 113 columns total (exact features not specified, but based on original forest cover dataset which includes wilderness area designations, soil types, elevation, and other geographical features).",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class classification accuracy\n  * Simple percentage of correctly predicted cover types\n  * No complex weighting or stratification mentioned"
    },
    "file_path": "kaggle_datasets/469/problem_summary.md"
  },
  "200": {
    "problem_id": "200",
    "title": "Multi-class Classification of Distracted Driver Behaviors from Dashboard Camera Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Classification of Distracted Driver Behaviors from Dashboard Camera Images\n\n## Problem Description\n* **Problem Type:** Multi-class Classification (Computer Vision - Image Classification)\n* **Objective:** Classify dashboard camera images of drivers into 10 distinct categories of distracted behaviors or safe driving. The goal is to automatically detect dangerous activities (e.g., texting, phone calls) to improve road safety.\n* **Key Points:**\n  * Focus on real-world applicability for insurance safety programs\n  * Test set contains resized images to discourage manual labeling\n  * Data split ensures no driver appears in both train and test sets (subject-wise split)\n  * Classes include nuanced distinctions (e.g., phone usage on left vs right side)\n\n## Dataset Overview\n* **Data Type & Context:** 2D RGB dashboard camera images capturing drivers in various states of distraction within vehicle cabins\n* **Data Files:**\n  * `imgs.zip` - Contains all training and test images\n  * `driver_imgs_list.csv` - Maps training images to driver IDs and class labels\n  * `sample_submission.csv` - Example submission file with probability format\n* **Features:**\n  * Images with removed metadata (no temporal or EXIF data)\n  * 10 predefined behavior classes (from safe driving to specific distraction types)\n  * Driver IDs for proper train-test separation\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class Logarithmic Loss (Log Loss)\n* **Metric Components:**\n  * Formula: \n    ```\n    logloss = -1/N ∑(i=1 to N) ∑(j=1 to M) y_ij log(p_ij)\n    ```\n    Where:\n    * N = number of images\n    * M = number of classes (10)\n    * y_ij = 1 if image i belongs to class j, else 0\n    * p_ij = predicted probability for image i and class j\n  * Probability normalization: Each row of predictions is rescaled to sum to 1\n  * Numerical stability: Probabilities clipped to [10^-15, 1-10^-15]",
    "sections": {},
    "file_path": "kaggle_datasets/200/problem_summary.md"
  },
  "456": {
    "problem_id": "456",
    "title": "Analyzing COVID-19 Impact on Digital Learning Engagement",
    "problem_type": "Data Analytics / Exploratory Data Analysis (EDA)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Analyzing COVID-19 Impact on Digital Learning Engagement\n\n**Problem Description:**\n* **Problem Type:** Data Analytics / Exploratory Data Analysis (EDA)\n* **Objective:**  \n    * Analyze the impact of COVID-19 on digital learning engagement across U.S. school districts in 2020.  \n    * Investigate how engagement correlates with factors like district demographics, broadband access, and state/national policies.  \n    * Propose data-driven insights to address educational inequities exacerbated by the pandemic.  \n* **Key Points:**  \n    * Focus on uncovering trends in digital learning engagement patterns.  \n    * Explore disparities in access and usage across different demographic and socioeconomic groups.  \n    * Assess the effectiveness of state/national interventions on digital learning.  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * **Primary Data:** Tabular data from 200+ U.S. school districts, including daily edtech engagement metrics, product details, and district demographics.  \n    * **Additional Data:** Public datasets (e.g., COVID-19 policy databases, KIDS Count) are encouraged for supplementary analysis.  \n* **Data Files:**  \n    * `engagement_data/`: Folder with district-level daily engagement logs (page load events for 10K+ edtech products).  \n    * `products_info.csv`: Metadata for top 372 edtech products (e.g., product type, vendor).  \n    * `districts_info.csv`: District characteristics (e.g., demographics, broadband access from NCES/FCC).  \n* **Key Features:**  \n    * Engagement metrics (e.g., usage frequency, product adoption).  \n    * District attributes (e.g., socioeconomic status, race/ethnicity, locale).  \n    * Product categories (e.g., apps, websites, ebooks).  \n\n**Evaluation Metrics:**  \n* **Evaluation Criteria:** Submissions are judged holistically based on:  \n    * **Clarity (5 pts):**  \n        * Clear research questions and reproducible methodology.  \n        * Effective visualizations for technical and non-technical audiences.  \n    * **Accuracy (5 pts):**  \n        * Data processing and methodological rigor.  \n        * Reasonable and well-supported conclusions.  \n    * **Creativity (5 pts):**  \n        * Novel insights or innovative methods.  \n        * Use of external data sources to enrich analysis.  \n* **Note:** No single metric (e.g., RMSE) is used",
    "sections": {
      "Problem Description": "* **Problem Type:** Data Analytics / Exploratory Data Analysis (EDA)\n* **Objective:**  \n    * Analyze the impact of COVID-19 on digital learning engagement across U.S. school districts in 2020.  \n    * Investigate how engagement correlates with factors like district demographics, broadband access, and state/national policies.  \n    * Propose data-driven insights to address educational inequities exacerbated by the pandemic.  \n* **Key Points:**  \n    * Focus on uncovering trends in digital learning engagement patterns.  \n    * Explore disparities in access and usage across different demographic and socioeconomic groups.  \n    * Assess the effectiveness of state/national interventions on digital learning.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * **Primary Data:** Tabular data from 200+ U.S. school districts, including daily edtech engagement metrics, product details, and district demographics.  \n    * **Additional Data:** Public datasets (e.g., COVID-19 policy databases, KIDS Count) are encouraged for supplementary analysis.  \n* **Data Files:**  \n    * `engagement_data/`: Folder with district-level daily engagement logs (page load events for 10K+ edtech products).  \n    * `products_info.csv`: Metadata for top 372 edtech products (e.g., product type, vendor).  \n    * `districts_info.csv`: District characteristics (e.g., demographics, broadband access from NCES/FCC).  \n* **Key Features:**  \n    * Engagement metrics (e.g., usage frequency, product adoption).  \n    * District attributes (e.g., socioeconomic status, race/ethnicity, locale).  \n    * Product categories (e.g., apps, websites, ebooks).  \n\n**Evaluation Metrics:**  \n* **Evaluation Criteria:** Submissions are judged holistically based on:  \n    * **Clarity (5 pts):**  \n        * Clear research questions and reproducible methodology.  \n        * Effective visualizations for technical and non-technical audiences.  \n    * **Accuracy (5 pts):**  \n        * Data processing and methodological rigor.  \n        * Reasonable and well-supported conclusions.  \n    * **Creativity (5 pts):**  \n        * Novel insights or innovative methods.  \n        * Use of external data sources to enrich analysis.  \n* **Note:** No single metric (e.g., RMSE) is used"
    },
    "file_path": "kaggle_datasets/456/problem_summary.md"
  },
  "238": {
    "problem_id": "238",
    "title": "Defense Against Adversarial Attacks on Image Classification",
    "problem_type": "Computer Vision - Adversarial Robustness (Image Classification)",
    "objective": "Build a robust image classifier that can correctly classify images even when they have been subtly modified (\"adversarially perturbed\") to deceive machine learning models. The goal is to defend against both:",
    "evaluation_metric": null,
    "full_content": "# Defense Against Adversarial Attacks on Image Classification\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Adversarial Robustness (Image Classification)\n* **Objective:** Build a robust image classifier that can correctly classify images even when they have been subtly modified (\"adversarially perturbed\") to deceive machine learning models. The goal is to defend against both:\n    * **Non-targeted attacks:** Modifications causing misclassification to any incorrect class.\n    * **Targeted attacks:** Modifications forcing misclassification to a specific target class.\n* **Key Points:**\n    * Adversarial examples are designed to be imperceptible to humans but fool models.\n    * The competition evaluates defenses against a suite of unknown attacks (submitted by other participants).\n    * Solutions must be computationally efficient (500 sec limit for 100 images) and run within Docker constraints.\n\n**Dataset Overview:**\n* **Data Type:** Image data (299x299 pixels) compatible with ImageNet classifiers.\n* **Context:** Two datasets:\n    * **DEV set (public):** 1000 images with 1001 labels (including background class).\n    * **TEST set (secret):** Used for final evaluation, released post-competition.\n* **Data Files:**\n    * Provided in development toolkit (`dev_toolkit.zip`):\n        * Sample attack/defense implementations.\n        * Scripts to download data and evaluate defenses.\n    * Pre-trained models (e.g., Inception-v3) available separately.\n* **Features:**\n    * RGB image pixels (preprocessed for TensorFlow-Slim models).\n    * Labels follow ImageNet class hierarchy (1-1000) + background (0).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Defense Score  \n  Calculated as:  \n  ```\n  score_defense = Σ (over all attacks) Σ (over all images) [defense(attack(Image_k)) == TrueLabel_k]\n  ```\n  * Each correct classification of an adversarial image counts as +1.\n  * Higher scores indicate better robustness against diverse attacks.\n* **Components:**\n    * Attacks include both targeted and non-targeted variants.\n    * Evaluated on a held-out test set to prevent overfitting.\n    * Final ranking based on performance against all submitted attacks.",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Adversarial Robustness (Image Classification)\n* **Objective:** Build a robust image classifier that can correctly classify images even when they have been subtly modified (\"adversarially perturbed\") to deceive machine learning models. The goal is to defend against both:\n    * **Non-targeted attacks:** Modifications causing misclassification to any incorrect class.\n    * **Targeted attacks:** Modifications forcing misclassification to a specific target class.\n* **Key Points:**\n    * Adversarial examples are designed to be imperceptible to humans but fool models.\n    * The competition evaluates defenses against a suite of unknown attacks (submitted by other participants).\n    * Solutions must be computationally efficient (500 sec limit for 100 images) and run within Docker constraints.",
      "Dataset Overview": "* **Data Type:** Image data (299x299 pixels) compatible with ImageNet classifiers.\n* **Context:** Two datasets:\n    * **DEV set (public):** 1000 images with 1001 labels (including background class).\n    * **TEST set (secret):** Used for final evaluation, released post-competition.\n* **Data Files:**\n    * Provided in development toolkit (`dev_toolkit.zip`):\n        * Sample attack/defense implementations.\n        * Scripts to download data and evaluate defenses.\n    * Pre-trained models (e.g., Inception-v3) available separately.\n* **Features:**\n    * RGB image pixels (preprocessed for TensorFlow-Slim models).\n    * Labels follow ImageNet class hierarchy (1-1000) + background (0).",
      "Evaluation Metrics": "* **Primary Metric:** Defense Score  \n  Calculated as:  \n  ```\n  score_defense = Σ (over all attacks) Σ (over all images) [defense(attack(Image_k)) == TrueLabel_k]\n  ```\n  * Each correct classification of an adversarial image counts as +1.\n  * Higher scores indicate better robustness against diverse attacks.\n* **Components:**\n    * Attacks include both targeted and non-targeted variants.\n    * Evaluated on a held-out test set to prevent overfitting.\n    * Final ranking based on performance against all submitted attacks."
    },
    "file_path": "kaggle_datasets/238/problem_summary.md"
  },
  "451": {
    "problem_id": "451",
    "title": "Quantitative Trading Strategy Prediction with Market Data",
    "problem_type": "Binary Classification (with financial utility optimization)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Quantitative Trading Strategy Prediction with Market Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with financial utility optimization)\n* **Objective:**  \n    * Develop a model to predict whether to execute (action=1) or reject (action=0) anonymized trading opportunities in real-world financial markets.  \n    * Maximize a utility score based on weighted returns of accepted trades, accounting for market efficiency and risk-adjusted performance.  \n* **Key Points:**  \n    * Extremely low signal-to-noise ratio in financial data.  \n    * Must handle strong feature correlation and potential redundancy in anonymized features.  \n    * Model operates in a time-series context without peeking forward (enforced via API).  \n    * Non-trivial evaluation metric combines raw returns with risk-adjusted scaling.  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Tabular time-series data representing anonymized stock market trading opportunities.  \n    * Each row corresponds to a potential trade with anonymized features and metadata.  \n* **Data Files:**  \n    * `train.csv` (historical data with returns)  \n    * `features.csv` (metadata about anonymized features)  \n    * Example test/submission files provided for API structure only.  \n* **Key Features:**  \n    * `feature_0` to `feature_129`: Anonymized numerical features.  \n    * `weight` and `resp`: Trade importance and return used for scoring.  \n    * `date` and `ts_id`: Temporal identifiers.  \n    * `resp_1` to `resp_4` (train only): Alternative return horizons.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom utility score (`u`) combining:  \n    1. **Raw Return Component:** Sum of weighted returns for executed trades:  \n       * \\( p_i = \\sum_j (weight_{ij} \\times resp_{ij} \\times action_{ij}) \\)  \n    2. **Risk-Adjusted Scaling:**  \n       * \\( t = \\frac{\\sum p_i}{\\sqrt{\\sum p_i^2}} \\times \\sqrt{\\frac{250}{|i|}} \\) (annualized Sharpe-like ratio).  \n    3. **Final Utility:** \\( u = \\min(\\max(t, 0), 6) \\times \\sum p_i \\).  \n* **Key Notes:**  \n    * Trades with `weight=0` do not affect",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with financial utility optimization)\n* **Objective:**  \n    * Develop a model to predict whether to execute (action=1) or reject (action=0) anonymized trading opportunities in real-world financial markets.  \n    * Maximize a utility score based on weighted returns of accepted trades, accounting for market efficiency and risk-adjusted performance.  \n* **Key Points:**  \n    * Extremely low signal-to-noise ratio in financial data.  \n    * Must handle strong feature correlation and potential redundancy in anonymized features.  \n    * Model operates in a time-series context without peeking forward (enforced via API).  \n    * Non-trivial evaluation metric combines raw returns with risk-adjusted scaling.  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Tabular time-series data representing anonymized stock market trading opportunities.  \n    * Each row corresponds to a potential trade with anonymized features and metadata.  \n* **Data Files:**  \n    * `train.csv` (historical data with returns)  \n    * `features.csv` (metadata about anonymized features)  \n    * Example test/submission files provided for API structure only.  \n* **Key Features:**  \n    * `feature_0` to `feature_129`: Anonymized numerical features.  \n    * `weight` and `resp`: Trade importance and return used for scoring.  \n    * `date` and `ts_id`: Temporal identifiers.  \n    * `resp_1` to `resp_4` (train only): Alternative return horizons.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom utility score (`u`) combining:  \n    1. **Raw Return Component:** Sum of weighted returns for executed trades:  \n       * \\( p_i = \\sum_j (weight_{ij} \\times resp_{ij} \\times action_{ij}) \\)  \n    2. **Risk-Adjusted Scaling:**  \n       * \\( t = \\frac{\\sum p_i}{\\sqrt{\\sum p_i^2}} \\times \\sqrt{\\frac{250}{|i|}} \\) (annualized Sharpe-like ratio).  \n    3. **Final Utility:** \\( u = \\min(\\max(t, 0), 6) \\times \\sum p_i \\).  \n* **Key Notes:**  \n    * Trades with `weight=0` do not affect"
    },
    "file_path": "kaggle_datasets/451/problem_summary.md"
  },
  "207": {
    "problem_id": "207",
    "title": "Predicting Manufacturing Failures in Bosch Production Lines",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Manufacturing Failures in Bosch Production Lines\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict which parts will fail quality control (Response = 1) in Bosch's manufacturing process using measurements taken at various stages of production.\n    * **Key Points:**\n        * Focus on identifying internal failures to improve manufacturing quality and reduce costs.\n        * Dataset is highly imbalanced (few failures compared to passing parts).\n        * Features are anonymized and represent measurements across different production lines and stations.\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing production line measurements (numerical, categorical, and timestamp features).\n    * **Data Files:**\n        * `train_numeric.csv` (contains target 'Response' variable)\n        * `test_numeric.csv`\n        * `train_categorical.csv`\n        * `test_categorical.csv`\n        * `train_date.csv` (timestamps for measurements)\n        * `test_date.csv`\n        * `sample_submission.csv`\n    * **Features:**\n        * Anonymized numerical and categorical measurements.\n        * Features follow naming convention: `L{line}_S{station}_F{feature_number}` (e.g., `L3_S36_F3939`).\n        * Date features indicate measurement timestamps (e.g., `L0_S0_D1` corresponds to `L0_S0_F0`).\n\n## Evaluation Metrics\n- **Evaluation Metric:** Matthews Correlation Coefficient (MCC)\n    * **Components:**\n        * MCC = (TP×TN - FP×FN) / √[(TP+FP)(TP+FN)(TN+FP)(TN+FN)]\n        * Where:\n            * TP = True Positives\n            * TN = True Negatives\n            * FP = False Positives\n            * FN = False Negatives\n    * Suitable for imbalanced binary classification as it considers all confusion matrix categories.",
    "sections": {},
    "file_path": "kaggle_datasets/207/problem_summary.md"
  },
  "405": {
    "problem_id": "405",
    "title": "Motion Prediction for Autonomous Vehicles",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Motion Prediction for Autonomous Vehicles\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting (Trajectory Prediction)\n* **Objective**: Predict future trajectories of traffic agents (cars, cyclists, pedestrians) around autonomous vehicles. \n    * Key Points:\n        * Predict 50 future (X,Y) positions for each agent\n        * Supports multi-modal predictions (up to 3 hypotheses per agent)\n        * Must account for uncertainty in agent movements\n        * Focuses on real-world autonomous vehicle scenarios\n\n## Dataset Overview\n* **Data Type**: Multi-modal sensor data (tabular trajectory data + HD maps)\n    * Primary Context: Autonomous vehicle perception data\n* **Key Data Files**:\n    * `train.zarr`, `test.zarr` (main dataset files in Zarr format)\n    * `aerial_map/`, `semantic_map/` (HD map data)\n    * `mask.npz` (test set prediction mask)\n* **Important Features**:\n    * Agent trajectories (historical positions)\n    * Scene context (road layout, traffic lights)\n    * Agent types (cars, cyclists, pedestrians)\n    * Timestamp and track_id for each agent\n\n## Evaluation Metrics\n* **Primary Metric**: Negative Log-Likelihood (NLL) of ground truth given predictions\n    * Components:\n        * Evaluates multi-modal predictions using mixture of Gaussians\n        * For each predicted hypothesis:\n            * Compares predicted (X,Y) positions to ground truth\n            * Weighs by predicted confidence score\n            * Uses fixed covariance (σ=1)\n        * Final score: -log(sum of weighted Gaussian likelihoods)\n    * Submission requires:\n        * Up to 3 predicted trajectories per agent\n        * Confidence scores summing to 1\n        * 50 future (X,Y) positions per trajectory",
    "sections": {},
    "file_path": "kaggle_datasets/405/problem_summary.md"
  },
  "637": {
    "problem_id": "637",
    "title": "Forecasting Sticker Sales",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict future sales of Kaggle-branded stickers across different countries and stores. The goal is to forecast multiple years of sales data, capturing real-world effects like seasonality, weekends, and holidays.",
    "evaluation_metric": null,
    "full_content": "# Forecasting Sticker Sales\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict future sales of Kaggle-branded stickers across different countries and stores. The goal is to forecast multiple years of sales data, capturing real-world effects like seasonality, weekends, and holidays.\n    * **Key Points:**\n        * The dataset is synthetic but designed to mimic real-world sales patterns.\n        * Predictions must be made for each date-country-store-item combination in the test set.\n        * The Public Leaderboard is scored on the first year of test data, while the Private Leaderboard uses the remaining data.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data representing sales transactions of Kaggle stickers across fictitious stores in real countries.\n* **Data Files:**\n    * `train.csv`: Contains historical sales data for training (date-country-store-item combinations and corresponding sales).\n    * `test.csv`: Requires predictions for `num_sold` (sales volume) for future date-country-store-item combinations.\n    * `sample_submission.csv`: Demonstrates the submission format.\n* **Features:** Likely includes:\n    * Temporal features (date, day of week, holidays)\n    * Location features (country, store)\n    * Product features (item type)\n    * Target variable: `num_sold` (sales volume)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Percentage Error (MAPE)\n    * **Calculation:** \n        * Measures the average absolute percentage difference between predicted and actual sales.\n        * Formula: \\( \\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^n \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\), where \\(y_i\\) is actual and \\(\\hat{y}_i\\) is predicted.\n    * **Properties:**\n        * Scale-independent (expressed as percentage).\n        * Penalizes underestimates and overestimates symmetrically.\n        * Sensitive to zero or near-zero actual values (common limitation of MAPE).",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict future sales of Kaggle-branded stickers across different countries and stores. The goal is to forecast multiple years of sales data, capturing real-world effects like seasonality, weekends, and holidays.\n    * **Key Points:**\n        * The dataset is synthetic but designed to mimic real-world sales patterns.\n        * Predictions must be made for each date-country-store-item combination in the test set.\n        * The Public Leaderboard is scored on the first year of test data, while the Private Leaderboard uses the remaining data.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data representing sales transactions of Kaggle stickers across fictitious stores in real countries.\n* **Data Files:**\n    * `train.csv`: Contains historical sales data for training (date-country-store-item combinations and corresponding sales).\n    * `test.csv`: Requires predictions for `num_sold` (sales volume) for future date-country-store-item combinations.\n    * `sample_submission.csv`: Demonstrates the submission format.\n* **Features:** Likely includes:\n    * Temporal features (date, day of week, holidays)\n    * Location features (country, store)\n    * Product features (item type)\n    * Target variable: `num_sold` (sales volume)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Percentage Error (MAPE)\n    * **Calculation:** \n        * Measures the average absolute percentage difference between predicted and actual sales.\n        * Formula: \\( \\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^n \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\), where \\(y_i\\) is actual and \\(\\hat{y}_i\\) is predicted.\n    * **Properties:**\n        * Scale-independent (expressed as percentage).\n        * Penalizes underestimates and overestimates symmetrically.\n        * Sensitive to zero or near-zero actual values (common limitation of MAPE)."
    },
    "file_path": "kaggle_datasets/637/problem_summary.md"
  },
  "253": {
    "problem_id": "253",
    "title": "Grocery Sales Forecasting for a Large Retail Chain",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Grocery Sales Forecasting for a Large Retail Chain\n\n## Problem Description\n- **Problem Type**: Time Series Forecasting (Regression)\n- **Objective**: Predict unit sales for thousands of grocery items across multiple stores in Ecuador to optimize inventory management and reduce waste/stockouts.\n- **Key Points**:\n  - Must handle perishable vs. non-perishable goods differently (weighted evaluation)\n  - Requires modeling of promotions, holidays, and external factors like oil prices\n  - Must address data gaps (items with zero sales not recorded in training data)\n  - Needs to generalize to new store-item combinations not seen in training\n\n## Dataset Overview\n- **Data Type**: Tabular time series data with store/item metadata\n- **Context**: Sales data from Corporación Favorita's Ecuadorian supermarket chain (200,000+ products across hundreds of stores)\n- **Data Files**:\n  - `train.csv`: Date-store-item sales records with promotion flags\n  - `test.csv`: Date-store-item combinations requiring predictions\n  - `items.csv`: Product metadata including perishable flag\n  - `stores.csv`: Store location and type information\n  - `transactions.csv`: Daily transaction counts per store\n  - `oil.csv`: Daily oil prices (economically significant in Ecuador)\n  - `holidays_events.csv`: Complex holiday/event calendar with transfers\n- **Key Features**:\n  - Temporal features (date, day-of-week, holidays)\n  - Product hierarchy (family, class)\n  - Store characteristics (location, type, cluster)\n  - Promotion status\n  - External economic indicators (oil prices)\n\n## Evaluation Metrics\n- **Primary Metric**: Normalized Weighted Root Mean Squared Logarithmic Error (NWRMSLE)\n- **Metric Components**:\n  - Logarithmic transformation of both predicted and actual sales\n  - Weighted by item type (1.25 for perishable, 1.0 otherwise)\n  - Formula: \n    $$ NWRMSLE = \\sqrt{ \\frac{\\sum_{i=1}^n w_i \\left( \\ln(\\hat{y}_i + 1) - \\ln(y_i +1) \\right)^2 }{\\sum_{i=1}^n w_i}} $$\n  - Designed to:\n    - Handle large sales value ranges\n    - Penalize relative errors rather than absolute\n    - Emphasize accuracy for perishable goods",
    "sections": {},
    "file_path": "kaggle_datasets/253/problem_summary.md"
  },
  "298": {
    "problem_id": "298",
    "title": "Traveling Santa Route Optimization with Prime City Constraints",
    "problem_type": "Combinatorial Optimization (Traveling Salesman Problem variant)",
    "objective": "Find the shortest possible route for Santa to visit all cities, starting and ending at the North Pole (CityId = 0), with special constraints related to prime-numbered cities.",
    "evaluation_metric": null,
    "full_content": "# Traveling Santa Route Optimization with Prime City Constraints\n\n**Problem Description:**\n* **Problem Type:** Combinatorial Optimization (Traveling Salesman Problem variant)\n* **Objective:** Find the shortest possible route for Santa to visit all cities, starting and ending at the North Pole (CityId = 0), with special constraints related to prime-numbered cities.\n* **Key Points:**\n  * Must visit every city exactly once\n  * Path must start and end at CityId = 0 (North Pole)\n  * Special penalty condition:\n    * Every 10th move incurs 10% longer distance unless coming from a prime-numbered CityId\n  * Combines traditional TSP with number theory constraints\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with geographic coordinates\n* **Context:** Contains coordinates for all cities Santa must visit\n* **Data Files:**\n  * cities.csv (contains CityId, X, Y coordinates)\n  * sample_submission.csv (example submission format)\n* **Features:**\n  * CityId (unique identifier, with 0 = North Pole)\n  * X, Y coordinates (for Euclidean distance calculation)\n  * Prime status (must be calculated based on CityId)\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Custom Penalized Euclidean Distance\n* **Components:**\n  * Base distance: Sum of Euclidean distances between consecutive cities in path\n  * Penalty condition: \n    * For every 10th move in path (stepNumber % 10 == 0):\n      * If previous CityId is NOT prime: add 10% penalty to that segment's distance\n  * Final score: Sum of all segment distances (with penalties applied where applicable)",
    "sections": {
      "Problem Description": "* **Problem Type:** Combinatorial Optimization (Traveling Salesman Problem variant)\n* **Objective:** Find the shortest possible route for Santa to visit all cities, starting and ending at the North Pole (CityId = 0), with special constraints related to prime-numbered cities.\n* **Key Points:**\n  * Must visit every city exactly once\n  * Path must start and end at CityId = 0 (North Pole)\n  * Special penalty condition:\n    * Every 10th move incurs 10% longer distance unless coming from a prime-numbered CityId\n  * Combines traditional TSP with number theory constraints",
      "Dataset Overview": "* **Data Type:** Tabular data with geographic coordinates\n* **Context:** Contains coordinates for all cities Santa must visit\n* **Data Files:**\n  * cities.csv (contains CityId, X, Y coordinates)\n  * sample_submission.csv (example submission format)\n* **Features:**\n  * CityId (unique identifier, with 0 = North Pole)\n  * X, Y coordinates (for Euclidean distance calculation)\n  * Prime status (must be calculated based on CityId)",
      "Evaluation Metrics": "* **Evaluation Metric:** Custom Penalized Euclidean Distance\n* **Components:**\n  * Base distance: Sum of Euclidean distances between consecutive cities in path\n  * Penalty condition: \n    * For every 10th move in path (stepNumber % 10 == 0):\n      * If previous CityId is NOT prime: add 10% penalty to that segment's distance\n  * Final score: Sum of all segment distances (with penalties applied where applicable)"
    },
    "file_path": "kaggle_datasets/298/problem_summary.md"
  },
  "608": {
    "problem_id": "608",
    "title": "Credit Risk Prediction with Stability Focus",
    "problem_type": "Binary Classification (with stability constraints)",
    "objective": "Predict the probability of loan default for clients, with emphasis on model stability over time. The solution must maintain predictive performance across different time periods (weeks) in the test set.",
    "evaluation_metric": null,
    "full_content": "# Credit Risk Prediction with Stability Focus\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with stability constraints)\n* **Objective:** Predict the probability of loan default for clients, with emphasis on model stability over time. The solution must maintain predictive performance across different time periods (weeks) in the test set.\n    * **Key Points:**\n        * Focus on clients with limited credit history (financial inclusion challenge)\n        * Models must balance predictive power (AUC) with temporal stability\n        * Addresses real-world challenge of scorecard degradation over time\n        * Special Stability Prize rewards innovative approaches to incorporating stability directly into model training\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing loan application information from multiple sources (internal + external credit bureaus)\n    * **Data Files:**\n        * Base tables: `train_base.csv`, `test_base.csv`\n        * 15+ feature tables with depth indicators (static/depth=0, historical/depth=1-2)\n        * External data from multiple credit bureaus and tax registries\n    * **Key Features:**\n        * `case_id` as primary key\n        * Temporal indicators (`WEEK_NUM`, `date_decision`)\n        * Transformed financial features (payment history, credit amounts, etc.)\n        * Multi-source features with depth indicators for historical records\n\n**Evaluation Metrics:**\n* **Primary Metric:** Custom Stability Metric combining:\n    * Mean Gini coefficient (Gini = 2*AUC - 1) across weeks\n    * Penalty for performance degradation: `falling_rate = min(0, slope)` of weekly Gini scores\n    * Penalty for prediction variability: `0.5 * std(residuals)` from Gini trend line\n* **Formula:**  \n    `stability_metric = mean(gini) + 88.0 * falling_rate - 0.5 * std(residuals)`",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with stability constraints)\n* **Objective:** Predict the probability of loan default for clients, with emphasis on model stability over time. The solution must maintain predictive performance across different time periods (weeks) in the test set.\n    * **Key Points:**\n        * Focus on clients with limited credit history (financial inclusion challenge)\n        * Models must balance predictive power (AUC) with temporal stability\n        * Addresses real-world challenge of scorecard degradation over time\n        * Special Stability Prize rewards innovative approaches to incorporating stability directly into model training",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing loan application information from multiple sources (internal + external credit bureaus)\n    * **Data Files:**\n        * Base tables: `train_base.csv`, `test_base.csv`\n        * 15+ feature tables with depth indicators (static/depth=0, historical/depth=1-2)\n        * External data from multiple credit bureaus and tax registries\n    * **Key Features:**\n        * `case_id` as primary key\n        * Temporal indicators (`WEEK_NUM`, `date_decision`)\n        * Transformed financial features (payment history, credit amounts, etc.)\n        * Multi-source features with depth indicators for historical records",
      "Evaluation Metrics": "* **Primary Metric:** Custom Stability Metric combining:\n    * Mean Gini coefficient (Gini = 2*AUC - 1) across weeks\n    * Penalty for performance degradation: `falling_rate = min(0, slope)` of weekly Gini scores\n    * Penalty for prediction variability: `0.5 * std(residuals)` from Gini trend line\n* **Formula:**  \n    `stability_metric = mean(gini) + 88.0 * falling_rate - 0.5 * std(residuals)`"
    },
    "file_path": "kaggle_datasets/608/problem_summary.md"
  },
  "254": {
    "problem_id": "254",
    "title": "Speech Command Recognition with TensorFlow",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Speech Command Recognition with TensorFlow\n\n## Problem Description\n* **Problem Type:** Audio Classification (Multi-class Classification)\n* **Objective:** Build an algorithm that accurately classifies 1-second audio clips into one of 12 spoken command categories:\n  * Core commands: `yes`, `no`, `up`, `down`, `left`, `right`, `on`, `off`, `stop`, `go`\n  * Special categories: `silence` (background noise) and `unknown` (any other word)\n* **Key Points:**\n  * Focus on improving accuracy of open-source voice interface tools\n  * Test set contains commands from speakers not seen in training data\n  * Special TensorFlow Prize required models to be:\n    * Runnable on Raspberry Pi 3 with <200ms latency\n    * Frozen TensorFlow GraphDef files under 5MB\n    * Compatible with specific input/output tensor formats\n\n## Dataset Overview\n* **Data Type:** Audio files (1-second PCM-encoded WAV files at 16kHz sample rate)\n* **Context:** Crowdsourced recordings of spoken commands in uncontrolled environments\n* **Data Files:**\n  * `train.7z`: \n    * 65,000+ audio clips organized in folders by label (30 words total)\n    * Includes `_background_noise_` folder with longer noise clips\n  * `test.7z`: 150,000+ unlabeled audio clips for prediction\n  * `sample_submission.csv`: Submission format example\n* **Features:**\n  * Audio waveforms (16000 samples per clip)\n  * Speaker ID encoded in filenames (not provided for test set)\n  * Varied recording conditions to simulate real-world usage\n\n## Evaluation Metrics\n* **Primary Metric:** Multiclass Accuracy (average correct classification rate)\n* **Label Constraints:**\n  * Only 12 possible output labels for test set (10 commands + `silence` + `unknown`)\n  * All other words must be predicted as `unknown`\n* **Special Prize Evaluation:**\n  * Model latency measured on Raspberry Pi 3 (<200ms requirement)\n  * Verified via TensorFlow benchmark tool\n  * Size constraint: <5MB frozen model",
    "sections": {},
    "file_path": "kaggle_datasets/254/problem_summary.md"
  },
  "630": {
    "problem_id": "630",
    "title": "Creative Use Cases for Gemini's Long Context Window",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Creative Use Cases for Gemini's Long Context Window\n\n## Problem Description\n* **Problem Type:** Open-ended NLP Demonstration (Text/Video Generation)\n* **Objective:** \n    * Demonstrate innovative applications leveraging Gemini 1.5's long context window capability (up to 2M tokens)\n    * Stress-test the model's ability to process/extract insights from massive inputs (e.g., 100k+ lines of code, years of text messages, multiple novels)\n    * Explore alternatives to traditional methods like RAG through direct in-context retrieval or many-shot prompting\n* **Key Points:**\n    * Focus on creative use cases where long context provides unique value (e.g., long-document QA, video analysis, codebase documentation)\n    * Must process inputs >100k tokens and explain why long context was essential\n    * Submissions require both a public Kaggle notebook and a YouTube video\n\n## Dataset Overview\n* **Data Type:** No official dataset provided (participants source their own data)\n* **Context:** \n    * Participants must use Gemini 1.5 APIs (Pro/Flash variants) with self-sourced long-context data\n    * Example domains: Code repositories, book collections, lengthy transcripts, extended video content\n* **Data Files:** N/A (competition provides only submission instructions file)\n\n## Evaluation Metrics\n* **Composite Evaluation Rubric (90pts total):**\n    * **Notebook (50pts):**\n        * Usefulness (0-10pts): Output quality/helpfulness\n        * Informativeness (0-10pts): Explanation of long-context benefits\n        * Interest (0-10pts): Engagement value of use case\n        * Documentation (0-10pts): Clarity/best practices\n        * Efficiency (0-5pts): Context-caching utilization\n        * Novelty (0-5pts): Originality of approach\n    * **Video (40pts):**\n        * Accuracy (0-10pts): Technical correctness\n        * Informativeness (0-10pts): Discussion of key concepts\n        * Instructional Value (0-10pts): Educational quality\n        * Entertainment (0-10pts): Production quality/engagement",
    "sections": {},
    "file_path": "kaggle_datasets/630/problem_summary.md"
  },
  "402": {
    "problem_id": "402",
    "title": "Predicting mRNA Vaccine Degradation Rates for COVID-19",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting mRNA Vaccine Degradation Rates for COVID-19\n\n## Problem Description\n* **Problem Type**: Multi-target Regression (Time Series Prediction for Biological Sequences)\n* **Objective**: Predict RNA degradation rates at each base position in mRNA vaccine sequences to improve stability and enable wider distribution without intense refrigeration. The goal is to model degradation under different experimental conditions (reactivity, magnesium/pH10, magnesium/50°C).\n    * **Key Points**:\n        * Focus on predicting degradation for COVID-19 mRNA vaccine sequences designed by Eterna players.\n        * Must predict for all sequence positions despite only partial experimental data being available (first 68 bases in training, 91 in private test).\n        * Biological constraints: RNA sequences are prone to spontaneous degradation which renders vaccines ineffective.\n\n## Dataset Overview\n* **Data Type**: Biological sequence data (RNA) with structural annotations and experimental measurements.\n* **Context**: Over 3,000 RNA molecules with degradation measurements from Stanford's Eterna project, including newly designed COVID-19 vaccine candidates.\n* **Data Files**:\n    * `train.json`: 2,400 sequences with full experimental data (107-base sequences)\n    * `test.json`: 629 public test sequences (107-base) + 3,005 private test sequences (130-base)\n    * `sample_submission.csv`: Submission format template\n* **Key Features**:\n    * `sequence`: RNA base sequence (A/G/U/C)\n    * `structure`: Parentheses notation of base pairing\n    * `reactivity`, `deg_Mg_pH10`, `deg_Mg_50C`: Primary target variables (degradation measurements)\n    * `predicted_loop_type`: Structural context annotations\n    * Experimental error measurements for each target\n\n## Evaluation Metrics\n* **Primary Metric**: MCRMSE (Mean Columnwise Root Mean Squared Error)\n    * **Components**:\n        * Calculated separately for each target column (`reactivity`, `deg_Mg_pH10`, `deg_Mg_50C`)\n        * Averages RMSE across these three scored targets\n        * Formula:  \n          `MCRMSE = 1/Nt * ∑(j=1 to Nt) √(1/n * ∑(i=1 to n)(y_ij - ŷ_ij)²)`  \n          Where Nt=3 (scored targets) and n=sequence positions scored",
    "sections": {},
    "file_path": "kaggle_datasets/402/problem_summary.md"
  },
  "606": {
    "problem_id": "606",
    "title": "Predicting Abalone Age from Physical Measurements",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Abalone Age from Physical Measurements\n\n## Problem Description\n* **Problem Type:** Regression\n* **Objective:** Predict the age of abalone (a type of marine snail) based on various physical measurements. The target variable is the number of rings in the abalone shell, which is directly related to its age.\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on the original Abalone dataset.\n  * Participants are encouraged to explore differences between the synthetic and original datasets.\n  * Incorporating the original dataset in training may potentially improve model performance.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing physical measurements of abalone specimens.\n* **Data Files:**\n  * `train.csv` - Contains both features and target variable (`Rings`)\n  * `test.csv` - Contains only features (target to be predicted)\n  * `sample_submission.csv` - Example submission file format\n* **Features:** Physical measurements of abalone (specific features not listed but typically include dimensions, weights, and sex in original dataset).\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n* **Metric Components:**\n  * Calculated as: √[1/n Σ(log(1 + ŷ_i) - log(1 + y_i))²]\n  * Where:\n    * n = number of observations\n    * ŷ_i = predicted value\n    * y_i = actual value\n    * log = natural logarithm\n  * This metric penalizes underestimates more than overestimates and is commonly used for positive, right-skewed targets.",
    "sections": {},
    "file_path": "kaggle_datasets/606/problem_summary.md"
  },
  "434": {
    "problem_id": "434",
    "title": "Human Protein Atlas - Single Cell Image Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Human Protein Atlas - Single Cell Image Classification\n\n## Problem Description\n* **Problem Type**:  \n  * Weakly Supervised Multi-label Classification (Instance Segmentation)\n* **Objective**:  \n  * Develop a model to segment and classify individual cells in microscope images by their protein localization patterns, despite only having image-level labels (not cell-level labels) for training.\n  * Key challenges:  \n    * Weak supervision (labels apply to entire images, not individual cells).  \n    * Multi-label classification (cells may exhibit multiple protein localization patterns).  \n    * Instance segmentation (precise cell boundary detection required).  \n* **Key Points**:  \n  * Focus on single-cell heterogeneity to advance biological understanding of protein distribution.  \n  * Uses 4-channel microscopy images (red, blue, yellow, green filters) for protein and cellular landmark detection.  \n\n## Dataset Overview\n* **Data Type**:  \n  * Microscopy images (PNG/TIFF) with 4 channels per sample:  \n    * Green: Protein of interest (primary signal for classification).  \n    * Blue: Nuclei.  \n    * Red: Microtubules.  \n    * Yellow: Endoplasmic Reticulum.  \n* **Data Files**:  \n  * `train.zip`: Training images (4 files per sample, one per channel).  \n  * `test.zip`: Test images (format identical to training).  \n  * `train.csv`: Image IDs and multi-label annotations (19 possible classes).  \n  * `sample_submission.csv`: Submission format template.  \n* **Features**:  \n  * Image dimensions vary (1728x1728 to 3072x3072).  \n  * Labels include 18 organelle locations + 1 \"negative\" class (e.g., Nucleoplasm, Mitochondria, Vesicles).  \n\n## Evaluation Metrics\n* **Primary Metric**:  \n  * **Mean Average Precision (mAP)** at IoU threshold of 0.6, averaged over 19 classes.  \n* **Metric Components**:  \n  * **Segmentation**: Mask IoU (Intersection over Union) determines true/false positives.  \n  * **Classification**: Precision-recall curves for each class, weighted by confidence scores.  \n  * **Submission Format**:  \n    * CSV with `ImageID`, dimensions, and RLE-encoded masks + confidence scores per cell/class.  \n    * Example row: `ID,Width,Height,Label1",
    "sections": {},
    "file_path": "kaggle_datasets/434/problem_summary.md"
  },
  "262": {
    "problem_id": "262",
    "title": "Multi-label Classification of Toxic Online Comments",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Toxic Online Comments\n\n## Problem Description\n- **Problem Type:** Multi-label Text Classification (NLP)\n- **Objective:** Build a model to detect and classify toxic online comments into six distinct categories of toxicity. The goal is to improve upon existing toxicity detection models by enabling granular classification of different toxicity types.\n- **Key Points:**\n  - Focus on identifying multiple, non-exclusive toxicity types in a single comment (a comment can belong to multiple toxicity classes simultaneously).\n  - Target toxicity categories: toxic, severe_toxic, obscene, threat, insult, and identity_hate.\n  - The competition aims to help platforms better moderate discussions by allowing customizable toxicity filtering.\n  - Dataset contains potentially offensive content (profanity, vulgar language).\n\n## Dataset Overview\n- **Data Type:** Text data (Wikipedia talk page comments) with multi-label annotations.\n- **Context:** Human-labeled comments from Wikipedia's talk page edits, curated for toxicity analysis.\n- **Data Files:**\n  - `train.csv`: Contains comments with binary labels for all six toxicity types.\n  - `test.csv`: Comments for which predictions must be made (some excluded from scoring).\n  - `sample_submission.csv`: Example submission file format.\n  - `test_labels.csv`: Ground truth labels for test set (released post-competition).\n- **Features:**\n  - Primary feature: Raw comment text.\n  - Labels: Six binary columns indicating presence of each toxicity type.\n\n## Evaluation Metrics\n- **Evaluation Metric:** Mean column-wise ROC AUC (Average of individual AUC scores for each toxicity class).\n- **Components:**\n  - Calculate ROC AUC separately for each of the six toxicity types.\n  - Final score is the arithmetic mean of these six AUC values.\n  - Predictions must be probabilities (values between 0 and 1) for each class.",
    "sections": {},
    "file_path": "kaggle_datasets/262/problem_summary.md"
  },
  "296": {
    "problem_id": "296",
    "title": "PLAsTiCC Astronomical Time-Series Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# PLAsTiCC Astronomical Time-Series Classification\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (Time Series)\n* **Objective**: Classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set. The goal is to prepare for classifying data from the Large Synoptic Survey Telescope (LSST), which will discover 10-100x more variable astronomical sources than previously known.\n* **Key Points**:\n  * Includes an \"unknown\" class (`class_99`) in the test set not present in training, requiring models to handle novel classes.\n  * Focus on scalability to handle LSST's expected data volume.\n  * Time-series nature requires modeling temporal patterns in brightness measurements across different passbands.\n\n## Dataset Overview\n* **Data Type**: Time-series tabular data with astronomical metadata\n* **Context**: Photometric measurements of variable astronomical objects from simulated LSST observations\n* **Data Files**:\n  * `training_set.csv`, `test_set.csv`: Time-series flux measurements\n  * `training_set_metadata.csv`, `test_set_metadata.csv`: Static object attributes\n  * `sample_submission.csv`: Submission format example\n* **Key Features**:\n  * Time-series measurements: `mjd` (time), `passband` (wavelength), `flux` (brightness), `flux_err`\n  * Metadata: Celestial coordinates (`ra`, `decl`), redshift estimates (`hostgal_specz`, `hostgal_photoz`), dust extinction (`mwebv`)\n  * 14 known classes in training + 1 unknown class (`class_99`) in test\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Multi-class Logarithmic Loss\n* **Metric Details**:\n  * Formula: \n    ```\n    Log Loss = -(∑(w_i * ∑(y_ij/N_i * ln(p_ij)))/∑w_i)\n    ```\n    Where:\n    * N = number of objects\n    * M = number of classes\n    * w_i = class weights (making all classes equally important)\n    * y_ij = 1 if object i belongs to class j, else 0\n    * p_ij = predicted probability for class j\n  * Probabilities are rescaled to sum to 1 per object\n  * Clipped to [10^-15, 1-10^-",
    "sections": {},
    "file_path": "kaggle_datasets/296/problem_summary.md"
  },
  "639": {
    "problem_id": "639",
    "title": "3D Protein Complex Detection in CryoET Tomograms",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# 3D Protein Complex Detection in CryoET Tomograms\n\n## Problem Description\n* **Problem Type**: Computer Vision - 3D Object Detection\n* **Objective**: Develop ML algorithms to automatically annotate the 3D locations of five classes of protein complexes in cryo-electron tomography (cryoET) images. The goal is to identify biological particles in large 3D cellular volumes to accelerate biomedical discoveries.\n    * **Key Points**:\n        * Focus on detecting protein complexes in their natural, crowded cellular environments\n        * Five particle types with varying detection difficulty (3 \"easy\", 2 \"hard\")\n        * Beta-amylase particles are included but not scored\n        * Solution must generalize across diverse protein structures\n\n## Dataset Overview\n* **Data Type**: 3D volumetric imaging data (tomograms) in Zarr format, with JSON annotations\n* **Context**: Cryo-electron tomography images showing protein complexes in near-atomic detail\n* **Data Files**:\n    * `train/`: Contains denoised tomograms (.zarr) and ground truth particle locations (.json)\n    * `test/`: Contains only denoised tomograms for prediction\n    * `sample_submission.csv`: Example submission format\n* **Features**:\n    * 3D arrays representing cellular structures\n    * Particle types with different detection difficulties\n    * Multiple resolution levels in each tomogram\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted F-beta score (β=4)\n    * **Components**:\n        * Beta value of 4 prioritizes recall over precision\n        * Particle considered \"true\" if within 0.5× radius of ground truth\n        * Weighting: Easy particles (1x), Hard particles (2x)\n        * Micro-averaged across all tomograms\n        * Beta-amylase predictions ignored (weight=0)\n    * **Rationale**: Emphasizes finding all relevant particles, especially challenging ones, while tolerating some false positives",
    "sections": {},
    "file_path": "kaggle_datasets/639/problem_summary.md"
  },
  "291": {
    "problem_id": "291",
    "title": "Pneumonia Detection in Chest Radiographs with Bounding Box Localization",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Pneumonia Detection in Chest Radiographs with Bounding Box Localization\n\n## Problem Description\n* **Problem Type**: Computer Vision - Object Detection (Medical Imaging)\n* **Objective**: \n    * Develop an algorithm to automatically detect visual signals of pneumonia in chest radiographs (CXRs) by localizing lung opacities with bounding boxes.\n    * The solution must distinguish pneumonia-related opacities from other conditions (e.g., pulmonary edema, atelectasis, or pleural effusion) that may appear similar on CXRs.\n* **Key Points**:\n    * Pneumonia diagnosis is challenging due to overlapping visual features with other thoracic conditions.\n    * The algorithm aims to prioritize cases for radiologist review, improving diagnostic efficiency.\n    * Bounding boxes must precisely localize abnormalities, not just classify images.\n\n## Dataset Overview\n* **Data Type**: Medical Imaging (DICOM format chest X-rays) with annotated bounding boxes.\n* **Context**: \n    * Images sourced from the NIH Clinical Center Chest X-Ray dataset, annotated by radiologists.\n    * Includes both pneumonia-positive and negative cases, with some containing multiple bounding boxes per image.\n* **Data Files**:\n    * `stage_2_train_images.zip` / `stage_2_test_images.zip`: DICOM images.\n    * `stage_2_train_labels.csv`: Bounding box coordinates (x, y, width, height) and binary targets.\n    * `stage_2_detailed_class_info.csv`: Additional class information for training samples.\n* **Features**:\n    * `patientId`: Unique identifier for each CXR.\n    * Bounding box coordinates (x-min, y-min, width, height).\n    * `Target`: Binary label indicating pneumonia presence (1) or absence (0).\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Average Precision (mAP) across multiple Intersection over Union (IoU) thresholds.\n* **Components**:\n    * IoU thresholds range from 0.4 to 0.75 (step size 0.05).\n    * For each threshold (t):\n        * True Positive (TP): Predicted box with IoU > t matching a ground truth box.\n        * False Positive (FP): Predicted box with no matching ground truth.\n        * False Negative (FN): Ground truth box with no matching prediction.\n    * Precision at threshold t: TP(t) / [TP(t) + FP(t) + FN(t",
    "sections": {},
    "file_path": "kaggle_datasets/291/problem_summary.md"
  },
  "265": {
    "problem_id": "265",
    "title": "Nuclei Segmentation in Diverse Microscopy Images",
    "problem_type": "Computer Vision - Semantic Segmentation",
    "objective": "Develop an algorithm to automatically detect and segment nuclei in microscopy images across varying experimental conditions (cell types, magnifications, imaging modalities). The goal is to advance medical research by enabling more efficient analysis of cellular responses to treatments.",
    "evaluation_metric": null,
    "full_content": "# Nuclei Segmentation in Diverse Microscopy Images\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Semantic Segmentation\n* **Objective:** Develop an algorithm to automatically detect and segment nuclei in microscopy images across varying experimental conditions (cell types, magnifications, imaging modalities). The goal is to advance medical research by enabling more efficient analysis of cellular responses to treatments.\n* **Key Points:**\n  * Must handle diverse imaging conditions (brightfield vs. fluorescence microscopy)\n  * Nuclei masks must not overlap in predictions\n  * Algorithm needs to generalize to unseen experimental conditions in stage 2 test set\n\n**Dataset Overview:**\n* **Data Type:** Microscopy images (biological imaging) with pixel-level annotations\n* **Context:** Medical research imagery from varied experimental setups\n* **Data Files:**\n  * `stage1_train/`: Images + annotated masks (one mask per nucleus)\n  * `stage1_test/`: Test images without masks\n  * `stage2_test/`: Final test images (released later)\n  * CSV files for submission formatting and training labels\n* **Features:**\n  * Images vary in cell type, magnification, and modality\n  * Masks use run-length encoding for efficient storage\n  * Each nucleus is individually segmented\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision at different IoU thresholds\n* **Components:**\n  * IoU (Intersection over Union) thresholds from 0.5 to 0.95 (step 0.05)\n  * For each threshold t:\n    * Calculate precision using TP(t), FP(t), FN(t)\n    * TP = predicted object matches ground truth with IoU > t\n    * FP = predicted object with no matching ground truth\n    * FN = ground truth with no matching prediction\n  * Final score = mean of average precisions across all test images",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Semantic Segmentation\n* **Objective:** Develop an algorithm to automatically detect and segment nuclei in microscopy images across varying experimental conditions (cell types, magnifications, imaging modalities). The goal is to advance medical research by enabling more efficient analysis of cellular responses to treatments.\n* **Key Points:**\n  * Must handle diverse imaging conditions (brightfield vs. fluorescence microscopy)\n  * Nuclei masks must not overlap in predictions\n  * Algorithm needs to generalize to unseen experimental conditions in stage 2 test set",
      "Dataset Overview": "* **Data Type:** Microscopy images (biological imaging) with pixel-level annotations\n* **Context:** Medical research imagery from varied experimental setups\n* **Data Files:**\n  * `stage1_train/`: Images + annotated masks (one mask per nucleus)\n  * `stage1_test/`: Test images without masks\n  * `stage2_test/`: Final test images (released later)\n  * CSV files for submission formatting and training labels\n* **Features:**\n  * Images vary in cell type, magnification, and modality\n  * Masks use run-length encoding for efficient storage\n  * Each nucleus is individually segmented",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision at different IoU thresholds\n* **Components:**\n  * IoU (Intersection over Union) thresholds from 0.5 to 0.95 (step 0.05)\n  * For each threshold t:\n    * Calculate precision using TP(t), FP(t), FN(t)\n    * TP = predicted object matches ground truth with IoU > t\n    * FP = predicted object with no matching ground truth\n    * FN = ground truth with no matching prediction\n  * Final score = mean of average precisions across all test images"
    },
    "file_path": "kaggle_datasets/265/problem_summary.md"
  },
  "433": {
    "problem_id": "433",
    "title": "Glomeruli Segmentation in Kidney Tissue Images",
    "problem_type": "Computer Vision - Semantic Segmentation",
    "objective": "Detect and segment glomeruli (functional tissue units) in high-resolution human kidney tissue images. The goal is to create a robust algorithm that can identify these biological structures across different tissue preparation methods.",
    "evaluation_metric": null,
    "full_content": "# Glomeruli Segmentation in Kidney Tissue Images\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Semantic Segmentation\n* **Objective:** Detect and segment glomeruli (functional tissue units) in high-resolution human kidney tissue images. The goal is to create a robust algorithm that can identify these biological structures across different tissue preparation methods.\n* **Key Points:**\n  * Focus on identifying 3D functional tissue units (FTUs) centered around capillaries\n  * Must work with both fresh frozen and Formalin Fixed Paraffin Embedded (FFPE) tissue samples\n  * Glomeruli typically range from 100-350μm in diameter with spherical shapes\n  * Part of larger Human BioMolecular Atlas Program (HuBMAP) initiative to map human body at cellular level\n\n**Dataset Overview:**\n* **Data Type:** High-resolution microscopy images (TIFF format) of kidney tissue with annotations\n* **Context:** Medical imaging for biological research and human cell mapping\n* **Data Files:**\n  * Large TIFF image files (>500MB-5GB each)\n  * JSON files with polygon coordinates for annotations\n  * train.csv with RLE-encoded masks\n  * HuBMAP-20-dataset_information.csv with patient metadata\n* **Features:**\n  * Whole-slide kidney tissue images\n  * Glomeruli segmentations (both RLE-encoded and JSON formats)\n  * Additional anatomical structure segmentations\n  * Anonymized patient data (age, sex, etc.)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Dice coefficient (Sørensen-Dice coefficient)\n  * Formula: 2∗|𝑋∩𝑌|/(|𝑋|+|𝑌|)\n  * Measures pixel-wise agreement between predicted segmentation and ground truth\n  * Defined as 1 when both prediction and ground truth are empty\n* **Submission Format:**\n  * Run-length encoded (RLE) binary masks\n  * Space-delimited pairs indicating start position and run length\n  * Pixels numbered top-to-bottom, then left-to-right",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Semantic Segmentation\n* **Objective:** Detect and segment glomeruli (functional tissue units) in high-resolution human kidney tissue images. The goal is to create a robust algorithm that can identify these biological structures across different tissue preparation methods.\n* **Key Points:**\n  * Focus on identifying 3D functional tissue units (FTUs) centered around capillaries\n  * Must work with both fresh frozen and Formalin Fixed Paraffin Embedded (FFPE) tissue samples\n  * Glomeruli typically range from 100-350μm in diameter with spherical shapes\n  * Part of larger Human BioMolecular Atlas Program (HuBMAP) initiative to map human body at cellular level",
      "Dataset Overview": "* **Data Type:** High-resolution microscopy images (TIFF format) of kidney tissue with annotations\n* **Context:** Medical imaging for biological research and human cell mapping\n* **Data Files:**\n  * Large TIFF image files (>500MB-5GB each)\n  * JSON files with polygon coordinates for annotations\n  * train.csv with RLE-encoded masks\n  * HuBMAP-20-dataset_information.csv with patient metadata\n* **Features:**\n  * Whole-slide kidney tissue images\n  * Glomeruli segmentations (both RLE-encoded and JSON formats)\n  * Additional anatomical structure segmentations\n  * Anonymized patient data (age, sex, etc.)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Dice coefficient (Sørensen-Dice coefficient)\n  * Formula: 2∗|𝑋∩𝑌|/(|𝑋|+|𝑌|)\n  * Measures pixel-wise agreement between predicted segmentation and ground truth\n  * Defined as 1 when both prediction and ground truth are empty\n* **Submission Format:**\n  * Run-length encoded (RLE) binary masks\n  * Space-delimited pairs indicating start position and run length\n  * Pixels numbered top-to-bottom, then left-to-right"
    },
    "file_path": "kaggle_datasets/433/problem_summary.md"
  },
  "601": {
    "problem_id": "601",
    "title": "Predicting NCAA Basketball Tournament Outcomes with Bracket Portfolios",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Basketball Tournament Outcomes with Bracket Portfolios\n\n## Problem Description\n- **Problem Type:** Probability Forecasting / Multi-class Classification (Tournament Bracket Prediction)\n- **Objective:** Predict the outcomes of both the men's and women's 2024 NCAA Division I basketball tournaments by submitting a portfolio of bracket predictions. Participants must forecast winners for each game slot across all tournament rounds using historical game data.\n    * **Key Points:**\n        * Requires predicting both men's and women's tournaments simultaneously\n        * Participants submit a portfolio of 1-100,000 brackets per tournament\n        * Predictions must follow valid tournament paths (winners must advance correctly through rounds)\n        * Play-in games are not scored (only their winners are considered)\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing historical NCAA basketball game results, team statistics, and tournament structures from 1985-present (men) and 1998-present (women)\n- **Key Data Files:**\n    * Team information: `MTeams.csv`, `WTeams.csv`\n    * Tournament seeds: `MNCAATourneySeeds.csv`, `WNCAATourneySeeds.csv`\n    * Game results: \n        * Compact format: `MRegularSeasonCompactResults.csv`, `WNCAATourneyCompactResults.csv`, etc.\n        * Detailed stats: `MRegularSeasonDetailedResults.csv`, `WNCAATourneyDetailedResults.csv`, etc.\n    * Tournament structure: `MNCAATourneySlots.csv`, `WNCAATourneySlots.csv`\n    * Sample submission: `sample_submission.csv`\n- **Important Features:**\n    * Team IDs and names\n    * Game scores and outcomes\n    * Tournament seeds and regions\n    * Team-level box scores (for recent seasons)\n    * Game locations and dates\n    * Conference affiliations\n\n## Evaluation Metrics\n- **Primary Metric:** Average Brier Bracket Score\n    * **Components:**\n        * For each tournament (men's and women's), computes implied probabilities from the bracket portfolio\n        * Evaluates probabilities against actual outcomes using Brier score for each round\n        * Takes mean of six Brier scores (one per round) for final score\n        * Lower scores indicate better performance (perfect prediction = 0)\n    * **Key Requirements:**\n        * Brackets must contain predictions for every tournament slot\n        * Team predictions must follow",
    "sections": {},
    "file_path": "kaggle_datasets/601/problem_summary.md"
  },
  "239": {
    "problem_id": "239",
    "title": "Targeted Adversarial Attack on Image Classifiers",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Targeted Adversarial Attack on Image Classifiers\n\n## Problem Description\n* **Problem Type**: Adversarial Machine Learning (Targeted Attack)\n* **Objective**: Develop an adversarial attack that modifies input images in a way that causes unknown machine learning classifiers to misclassify them as a specified target class. The modifications must be subtle enough to be imperceptible to human observers.\n    * **Key Points**:\n        * Focus on targeted misclassification (forcing a specific wrong prediction)\n        * Perturbations must be minimal (bounded by max L∞ norm constraint)\n        * Attacks will be evaluated against multiple unknown defense classifiers\n        * Part of a larger competition on adversarial attacks/defenses at NIPS 2017\n\n## Dataset Overview\n* **Data Type**: Image data (ImageNet-compatible)\n    * **Context**: Computer vision classification task with adversarial perturbations\n* **Data Files**:\n    * DEV dataset (1000 images, 299×299 pixels)\n    * TEST dataset (secret until final evaluation)\n    * Pre-trained models (Inception v3, Inception ResNet v2)\n* **Features**:\n    * RGB image pixels (299×299×3)\n    * 1001-class labels (including background class)\n    * Target class specifications for each image\n\n## Evaluation Metrics\n* **Evaluation Metric**: Attack Success Rate against Defense Classifiers\n    * **Scoring Formula**:\n        ```\n        score = Σ_defenses Σ_images [defense(attack(image)) == target_label]\n        ```\n    * **Components**:\n        * Each defense's classification of each adversarial image is checked\n        * 1 point awarded for each successful target misclassification\n        * Final score is sum across all defenses and all images\n        * Higher scores indicate more effective attacks\n    * **Constraints**:\n        * Perturbation magnitude bounded by MAX_PERTURBATION (4-16 in L∞ norm)\n        * Runtime limit: 500 seconds per 100-image batch",
    "sections": {},
    "file_path": "kaggle_datasets/239/problem_summary.md"
  },
  "450": {
    "problem_id": "450",
    "title": "E.T. Signal Detection in Radio Telescope Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# E.T. Signal Detection in Radio Telescope Data\n\n## Problem Description\n* **Problem Type**: Binary Classification (Anomaly Detection)\n* **Objective**: Identify potential extraterrestrial technosignatures (\"needles\") hidden in radio telescope data scans while minimizing false positives from human-generated radio frequency interference (RFI).\n    * **Key Points**:\n        * Signals of interest appear only in on-target observations (specific star scans) but not in off-target scans.\n        * Signals may exhibit complex time-frequency patterns (e.g., Doppler drift like the Voyager 1 example).\n        * Training data contains simulated signals since no confirmed extraterrestrial signals exist.\n        * The challenge involves distinguishing subtle patterns from noisy background data.\n\n## Dataset Overview\n* **Data Type**: 3D spectrogram arrays (time-frequency-intensity) from radio telescope observations\n* **Context**: Green Bank Telescope observations of target stars interspersed with control scans\n* **Data Files**:\n    * `train/`: numpy files (shape: 6x273x256) containing cadence snippets (6 positions per observation)\n    * `train_labels.csv`: Target labels for training data\n    * `test/`: Similar numpy files for prediction\n    * `sample_submission.csv`: Submission format example\n* **Key Features**:\n    * Each sample contains 6 spectrogram panels (3 on-target, 3 off-target observations)\n    * Data represents signal intensity across 256 frequency channels over 273 time steps\n    * Simulated signals (\"needles\") exhibit various patterns only in on-target observations\n\n## Evaluation Metrics\n* **Primary Metric**: Area Under the ROC Curve (AUC)\n    * Measures model's ability to rank true signals higher than noise\n    * Suitable for imbalanced classification where false positives must be minimized",
    "sections": {},
    "file_path": "kaggle_datasets/450/problem_summary.md"
  },
  "206": {
    "problem_id": "206",
    "title": "Painter by Numbers: Identifying Artists from Painting Pairs",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Painter by Numbers: Identifying Artists from Painting Pairs\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Image Similarity)\n* **Objective:**  \n  Predict whether two given paintings were created by the same artist by analyzing visual features like brushstrokes, lighting, and composition. The task involves pairwise comparison of images to determine artistic authorship.\n  * **Key Points:**\n    * Focuses on stylistic fingerprints rather than subject matter.\n    * Requires algorithms to capture nuanced aspects of an artist's unique style.\n    * Uses a curated dataset of paintings with varying genres, styles, and time periods.\n\n## Dataset Overview\n* **Data Type & Context:**  \n  Image data (paintings in .jpg format) with metadata (artist, title, style, genre, date).\n* **Data Files:**\n  * `train.zip` / `test.zip`: Image sets for training and testing.\n  * `train_info.csv`: Metadata for training images (filename, artist hash, title, style, genre, date).\n  * `submission_info.csv`: Test image pairs to compare (index, image1, image2).\n  * `all_data_info.csv` (post-competition): Expanded metadata including artist names, image dimensions, and group assignments.\n* **Key Features:**\n  * Images sourced from WikiArt and other artists.\n  * Metadata includes artistic attributes (style, genre) but artist names are hashed for anonymity.\n  * Test set organized into 14 groups for efficient pairwise comparison.\n\n## Evaluation Metrics\n* **Primary Metric:** AUC (Area Under the ROC Curve)  \n  * Measures the model's ability to rank pairs correctly (higher probability for true same-artist pairs).\n  * **Submission Format:**  \n    Probabilistic predictions for each image pair in `submission_info.csv`, with columns:  \n    ```index, sameArtist```  \n    where `sameArtist` is the predicted probability (0 to 1) of shared authorship.",
    "sections": {},
    "file_path": "kaggle_datasets/206/problem_summary.md"
  },
  "468": {
    "problem_id": "468",
    "title": "Neuronal Cell Instance Segmentation in Microscopy Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Neuronal Cell Instance Segmentation in Microscopy Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - Instance Segmentation\n* **Objective**: Detect and delineate individual neuronal cells in phase contrast microscopy images to aid in neurological disorder research and drug discovery.\n    * **Key Points**:\n        * Focus on segmenting neuronal cells with irregular, concave morphologies (specifically SH-SY5Y neuroblastoma cell line)\n        * Address limitations of current segmentation methods which perform poorly on neuronal cells\n        * Goal is to enable robust quantitative analysis of cell responses to treatments\n        * Predictions must not overlap (unlike training labels which include overlaps)\n\n## Dataset Overview\n* **Data Type**: Microscopy image data (PNG format) with RLE-encoded mask annotations\n* **Context**: Phase contrast microscopy images of neuronal cells used in neurological disorder studies\n* **Data Files**:\n    * `train.csv` - Contains RLE masks and metadata for training objects\n    * `train/` - Training images (PNG)\n    * `test/` - Test images (PNG)\n    * `train_semi_supervised/` - Additional unlabeled images\n    * `LIVECell_dataset_2021/` - External dataset for transfer learning\n* **Key Features**:\n    * Image metadata includes cell type, plate/sample information, and temporal data\n    * Approximately 240 test images (most hidden)\n    * High number of annotated objects despite small image count\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Average Precision at IoU thresholds (mAP@[0.5:0.95])\n    * **Components**:\n        * Calculates precision at 10 IoU thresholds (0.5 to 0.95 in 0.05 steps)\n        * For each threshold:\n            * True Positive = Predicted mask with IoU > threshold to ground truth\n            * False Positive = Predicted mask with no matching ground truth\n            * False Negative = Ground truth with no matching prediction\n        * Final score = Mean of average precisions across all test images\n    * **Submission Format**:\n        * Space-delimited RLE-encoded masks\n        * Strict requirements: sorted pairs, no duplicates, no overlaps between predictions",
    "sections": {},
    "file_path": "kaggle_datasets/468/problem_summary.md"
  },
  "201": {
    "problem_id": "201",
    "title": "Ultrasound Nerve Segmentation in Ultrasound Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Ultrasound Nerve Segmentation in Ultrasound Images\n\n## Problem Description\n* **Problem Type:** Computer Vision - Image Segmentation\n* **Objective:**  \n    * Build a model to accurately segment nerve structures (Brachial Plexus) in ultrasound images of the neck to improve pain management catheter placement.\n    * Key challenges:\n        * Some images contain no nerve structures (model should predict empty masks in these cases).\n        * Human-annotated ground truth may contain noise, artifacts, or occasional errors.\n        * Dataset contains duplicate/similar images due to acquisition method.\n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Ultrasound images (medical imaging) of the neck region in TIFF format.\n    * Focus on segmenting the Brachial Plexus nerve structure.\n* **Key Files:**\n    * `/train/`: Subject-specific ultrasound images with corresponding mask files\n    * `/test/`: Test set images (no masks provided)\n    * `train_masks.csv`: Run-length encoded training masks\n    * `sample_submission.csv`: Submission format example\n* **Important Features:**\n    * Images named as `subject_imageNum.tif` (train) or `imageNum.tif` (test)\n    * Mask files are binary segmentation maps\n    * No subject overlap between train/test sets\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Dice Coefficient (Sørensen-Dice Index)\n    * Formula: `2 * |X ∩ Y| / (|X| + |Y|)`\n        * X = predicted pixels\n        * Y = ground truth pixels\n        * Defined as 1 when both X and Y are empty\n    * Implementation Notes:\n        * Uses run-length encoding (RLE) for submission format\n        * Pixels numbered top-to-bottom, then left-to-right\n        * Requires space-delimited pairs of (start_position, run_length)",
    "sections": {},
    "file_path": "kaggle_datasets/201/problem_summary.md"
  },
  "457": {
    "problem_id": "457",
    "title": "Landmark Recognition in Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Landmark Recognition in Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - Multi-class Classification (with potential null class for non-landmark images)\n* **Objective**: Build models that recognize the correct landmark (if any) depicted in test images, where landmarks may range from famous to obscure locations worldwide.\n* **Key Points**:\n  * Contains over 81,000 possible landmark classes (high cardinality)\n  * Many classes have limited training examples (long-tail distribution)\n  * Test set emphasizes global diversity with improved country representation\n  * Some query images may contain no landmarks at all (requires handling null predictions)\n  * Synchronous rerun format where models process private test set upon submission\n\n## Dataset Overview\n* **Data Type**: Image data (JPEG) of global landmarks with associated class labels\n* **Context**: Cleaned subset of Google Landmarks Dataset v2 (GLDv2), designed for instance-level recognition\n* **Data Files**:\n  * `train/` - Folder containing training images (organized in 3-level subdirectory structure by image ID)\n  * `train.csv` - Mapping of training image IDs to landmark labels\n  * `test/` - Folder containing representative test images (private test set used for scoring)\n  * `sample_submission.csv` - Submission format example\n* **Features**:\n  * Image pixels as primary input\n  * Landmark ID as target label (integer)\n  * Confidence scores required for predictions\n\n## Evaluation Metrics\n* **Primary Metric**: Global Average Precision (GAP) at k=1 (micro Average Precision)\n* **Calculation**:\n  * Predictions consist of (landmark ID, confidence score) pairs sorted by confidence\n  * GAP = (1/M) * Σ[P(i)*rel(i)] where:\n    * N = total predictions across all queries\n    * M = queries with at least one known landmark\n    * P(i) = precision at rank i\n    * rel(i) = 1 if prediction i is correct, else 0\n  * Handles both correct landmark identification and abstention (empty predictions)\n  * Emphasizes ranking confident correct predictions higher",
    "sections": {},
    "file_path": "kaggle_datasets/457/problem_summary.md"
  },
  "208": {
    "problem_id": "208",
    "title": "Multiclass Classification of Monster Types",
    "problem_type": "Multiclass Classification",
    "objective": "Classify creatures into one of three monster types (Ghost, Goblin, or Ghoul) based on physical and supernatural characteristics. The goal is to accurately identify and categorize the haunting creatures to help \"vanquish\" them from Kaggle's halls.",
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of Monster Types\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification\n* **Objective:** Classify creatures into one of three monster types (Ghost, Goblin, or Ghoul) based on physical and supernatural characteristics. The goal is to accurately identify and categorize the haunting creatures to help \"vanquish\" them from Kaggle's halls.\n    * **Key Points:**\n        * Uses biological and supernatural measurements for classification\n        * Focuses on distinguishing between three distinct monster classes\n        * Real-world inspired (though fictional) classification challenge\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing biological and supernatural measurements of mythical creatures\n* **Data Files:**\n    * train.csv (training set with labels)\n    * test.csv (test set for predictions)\n    * sample_submission.csv (example submission format)\n* **Key Features:**\n    * Physical characteristics: bone_length, rotting_flesh, hair_length\n    * Supernatural characteristics: has_soul (percentage)\n    * Categorical feature: color (with 6 possible values)\n    * Target variable: type (Ghost, Goblin, Ghoul)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy (percentage of correctly classified creatures)\n    * Simple classification accuracy calculated as: (Correct Predictions / Total Predictions) × 100\n    * Requires submission file with 'id' and predicted 'type' for each test sample",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification\n* **Objective:** Classify creatures into one of three monster types (Ghost, Goblin, or Ghoul) based on physical and supernatural characteristics. The goal is to accurately identify and categorize the haunting creatures to help \"vanquish\" them from Kaggle's halls.\n    * **Key Points:**\n        * Uses biological and supernatural measurements for classification\n        * Focuses on distinguishing between three distinct monster classes\n        * Real-world inspired (though fictional) classification challenge",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing biological and supernatural measurements of mythical creatures\n* **Data Files:**\n    * train.csv (training set with labels)\n    * test.csv (test set for predictions)\n    * sample_submission.csv (example submission format)\n* **Key Features:**\n    * Physical characteristics: bone_length, rotting_flesh, hair_length\n    * Supernatural characteristics: has_soul (percentage)\n    * Categorical feature: color (with 6 possible values)\n    * Target variable: type (Ghost, Goblin, Ghoul)",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy (percentage of correctly classified creatures)\n    * Simple classification accuracy calculated as: (Correct Predictions / Total Predictions) × 100\n    * Requires submission file with 'id' and predicted 'type' for each test sample"
    },
    "file_path": "kaggle_datasets/208/problem_summary.md"
  },
  "495": {
    "problem_id": "495",
    "title": "Image Matching Challenge 2022",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Image Matching Challenge 2022\n\n## Problem Description\n* **Problem Type**: Computer Vision - Image Registration / Epipolar Geometry Estimation\n* **Objective**:  \n    * Develop a machine learning algorithm to estimate the **fundamental matrix** between two images of the same scene captured from different viewpoints.  \n    * The fundamental matrix encapsulates the projective geometry (relative pose) between the two views, enabling 3D reconstruction via Structure-from-Motion (SfM).  \n* **Key Points**:  \n    * Focuses on **wide-baseline image matching** (images with significant viewpoint/lighting/time gaps).  \n    * Must handle **domain gaps** (e.g., urban scenes photographed months/years apart).  \n    * Applications in 3D mapping, cultural heritage preservation, and Google Maps.  \n\n## Dataset Overview\n* **Data Type**:  \n    * **Image pairs** (JPEG/PNG) of urban scenes with varying overlap, resized to ~800px longest edge.  \n    * **Tabular metadata** (CSV) with camera intrinsics (`calibration.csv`), extrinsics (`rotation_matrix`, `translation_vector`), and pair-wise covisibility estimates.  \n* **Data Files**:  \n    * `train/*/images/`: Scene-specific image batches.  \n    * `train/*/calibration.csv`: Camera parameters (intrinsics + extrinsics).  \n    * `train/*/pair_covisibility.csv`: Precomputed fundamental matrices and overlap scores.  \n    * `test.csv`: Hidden test pairs (image IDs).  \n* **Key Features**:  \n    * **Fundamental matrix (F)**: Target variable (3×3 matrix flattened to 9 values).  \n    * **Covisibility score**: Indicates overlap likelihood between image pairs.  \n    * **Scaling factors**: Converts SfM reconstructions to metric units.  \n\n## Evaluation Metrics\n* **Primary Metric**: **Mean Average Accuracy (mAA)**  \n    * Evaluates pose accuracy (rotation + translation) at multiple thresholds:  \n        ```python\n        thresholds_r = np.linspace(1, 10, 10)  # Rotation error (degrees).  \n        thresholds_t = np.geomspace(0.2, 5, 10)  # Translation error (meters).  \n        ```  \n    * **Calculation**:  \n        1. For each threshold pair,",
    "sections": {},
    "file_path": "kaggle_datasets/495/problem_summary.md"
  },
  "461": {
    "problem_id": "461",
    "title": "Computer Vision - Helmet Assignment in NFL Video Footage",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Computer Vision - Helmet Assignment in NFL Video Footage\n\n## Problem Description\n* **Problem Type**: Computer Vision - Object Detection & Tracking\n* **Objective**: \n    * Assign detected football helmets in NFL game footage to specific players by predicting bounding boxes and player labels.\n    * The goal is to accurately track player exposures to helmet impacts for injury research.\n* **Key Points**:\n    * Must work with synchronized multi-angle video (sideline + endzone views)\n    * Requires distinguishing between active players and sideline players (excluded from scoring)\n    * Definitive helmet impacts are weighted 1000x more in evaluation\n    * Solution should eliminate need for manual field mapping currently used by NFL\n\n## Dataset Overview\n* **Data Type**: Video frames with associated tracking data and helmet annotations\n* **Context**: NFL game footage for player safety research\n* **Data Files**:\n    * `train/` and `test/` folders with MP4 videos (sideline + endzone views)\n    * `train_labels.csv`: Ground truth helmet boxes and player assignments\n    * `[train/test]_baseline_helmets.csv`: Pre-computed helmet detection boxes\n    * `[train/test]_player_tracking.csv`: 10Hz player position/sensor data\n    * `images/` + `image_labels.csv`: Supplemental helmet images for training detectors\n* **Key Features**:\n    * Video frames (1280×720 resolution)\n    * Bounding box coordinates (left, width, top, height)\n    * Player tracking data (position, speed, acceleration, orientation)\n    * Impact type and definitiveness labels\n    * Player identification labels (e.g., \"H1\" for home team player 1)\n\n## Evaluation Metrics\n* **Primary Metric**: Weighted Accuracy\n    * Formula:  \n      `WeightedAccuracy = (TotalCorrect_nonimp + (TotalCorrect_imp * 1000)) / (TotalHelmets_nonimp + (TotalHelmets_imp * 1000))`\n* **Components**:\n    * **Intersection over Union (IoU)** threshold of 0.35 for box matching\n    * Two accuracy components:\n        * Regular helmet assignments (weight = 1)\n        * Definitive impact assignments (weight = 1000)\n    * **Constraints**:\n        * Max 22 helmet predictions per frame\n        * No duplicate labels per frame\n        *",
    "sections": {},
    "file_path": "kaggle_datasets/461/problem_summary.md"
  },
  "237": {
    "problem_id": "237",
    "title": "Car Image Segmentation Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Car Image Segmentation Challenge\n\n## Problem Description\n* **Problem Type:** Computer Vision - Image Segmentation  \n* **Objective:**  \n  * Develop an algorithm to automatically remove the photo studio background from car images by accurately segmenting the car boundaries.  \n  * The solution will enable Carvana to superimpose cars on various backgrounds for their online platform.  \n* **Key Points:**  \n  * Must handle challenges like bright reflections and cars with colors similar to the background.  \n  * Each car has 16 images taken from different angles, requiring consistent segmentation across views.  \n\n## Dataset Overview\n* **Data Type & Context:**  \n  * High-resolution car images (.jpg) taken in a custom rotating photo studio.  \n  * Includes metadata (make, model, year, trim) for each vehicle.  \n* **Data Files:**  \n  * `/train/` - Training set images  \n  * `/test/` - Test set images (predict masks for these)  \n  * `/train_masks/` - Ground truth masks in .gif format  \n  * `train_masks.csv` - Run-length encoded (RLE) training masks  \n  * `metadata.csv` - Car attributes (some missing values)  \n* **Key Features:**  \n  * Images named as `{car_id}_{angle_number}.jpg` (e.g., `0004d4463b50_01.jpg`).  \n  * Masks provided as RLE-encoded pixel values in submissions.  \n\n## Evaluation Metrics\n* **Primary Metric:** Mean Dice Coefficient (F1 Score for pixel-wise segmentation)  \n  * Formula: \\( \\frac{2 \\times |X \\cap Y|}{|X| + |Y|} \\), where:  \n    * \\( X \\) = Predicted pixel set  \n    * \\( Y \\) = Ground truth pixel set  \n  * Scores range from 0 (no overlap) to 1 (perfect match).  \n* **Submission Format:**  \n  * Space-delimited RLE pairs (e.g., `1 3 10 5` = pixels 1-3 and 10-14).  \n  * File must include header: `img,rle_mask`.",
    "sections": {},
    "file_path": "kaggle_datasets/237/problem_summary.md"
  },
  "459": {
    "problem_id": "459",
    "title": "Brain Tumor Radiogenomic Classification from MRI Scans",
    "problem_type": "Binary Classification (Medical Imaging)",
    "objective": "Predict the presence of MGMT promoter methylation (a genetic biomarker) in glioblastoma tumors using MRI scans. This biomarker is crucial for determining chemotherapy responsiveness in brain cancer patients.",
    "evaluation_metric": null,
    "full_content": "# Brain Tumor Radiogenomic Classification from MRI Scans\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Medical Imaging)\n* **Objective:** Predict the presence of MGMT promoter methylation (a genetic biomarker) in glioblastoma tumors using MRI scans. This biomarker is crucial for determining chemotherapy responsiveness in brain cancer patients.\n    * **Key Points:**\n        * Aims to replace invasive surgical biopsies with non-invasive imaging-based diagnosis\n        * Focuses on improving treatment planning and patient outcomes\n        * Part of a broader effort to advance AI in brain tumor diagnosis/treatment\n\n**Dataset Overview:**\n* **Data Type:** 3D Medical Imaging (DICOM MRI scans) with tabular labels\n* **Context:** Multi-institutional brain tumor MRI dataset from the BraTS challenge\n* **Data Files:**\n    * `train/` - Folder with patient-specific subfolders containing 4 MRI sequences per case\n    * `train_labels.csv` - Contains binary labels for methylation status (MGMT_value)\n    * `test/` - Hidden test set with same structure as training data\n    * `sample_submission.csv` - Submission template\n* **Features:**\n    * Four MRI sequences per patient: FLAIR, T1w, T1wCE (contrast-enhanced), T2w\n    * Each sequence contains multiple DICOM slices forming 3D volumes\n    * Cases identified by five-digit IDs (BraTS21ID)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model's ability to distinguish between positive (methylated) and negative cases\n    * Threshold-independent evaluation of binary classifier performance\n    * Ranges from 0.5 (random) to 1.0 (perfect discrimination)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Medical Imaging)\n* **Objective:** Predict the presence of MGMT promoter methylation (a genetic biomarker) in glioblastoma tumors using MRI scans. This biomarker is crucial for determining chemotherapy responsiveness in brain cancer patients.\n    * **Key Points:**\n        * Aims to replace invasive surgical biopsies with non-invasive imaging-based diagnosis\n        * Focuses on improving treatment planning and patient outcomes\n        * Part of a broader effort to advance AI in brain tumor diagnosis/treatment",
      "Dataset Overview": "* **Data Type:** 3D Medical Imaging (DICOM MRI scans) with tabular labels\n* **Context:** Multi-institutional brain tumor MRI dataset from the BraTS challenge\n* **Data Files:**\n    * `train/` - Folder with patient-specific subfolders containing 4 MRI sequences per case\n    * `train_labels.csv` - Contains binary labels for methylation status (MGMT_value)\n    * `test/` - Hidden test set with same structure as training data\n    * `sample_submission.csv` - Submission template\n* **Features:**\n    * Four MRI sequences per patient: FLAIR, T1w, T1wCE (contrast-enhanced), T2w\n    * Each sequence contains multiple DICOM slices forming 3D volumes\n    * Cases identified by five-digit IDs (BraTS21ID)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * Measures model's ability to distinguish between positive (methylated) and negative cases\n    * Threshold-independent evaluation of binary classifier performance\n    * Ranges from 0.5 (random) to 1.0 (perfect discrimination)"
    },
    "file_path": "kaggle_datasets/459/problem_summary.md"
  },
  "230": {
    "problem_id": "230",
    "title": "Multi-label Image Classification for Apparel Attributes",
    "problem_type": "Computer Vision - Multi-label Classification (Fine-Grained Visual Categorization)",
    "objective": "Develop algorithms to accurately assign multiple attribute labels to images of apparel products (outerwear, dresses, pants, shoes) despite challenges like varying lighting, angles, backgrounds, and occlusion.",
    "evaluation_metric": null,
    "full_content": "# Multi-label Image Classification for Apparel Attributes\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Multi-label Classification (Fine-Grained Visual Categorization)\n* **Objective:** Develop algorithms to accurately assign multiple attribute labels to images of apparel products (outerwear, dresses, pants, shoes) despite challenges like varying lighting, angles, backgrounds, and occlusion.\n    * **Key Points:**\n        * Focus on fine-grained attribute differences (e.g., distinguishing similar colors like royal blue vs. turquoise).\n        * Each image may have multiple labels across different tasks (e.g., color, material, style).\n        * Participants must predict labels for all specified tasks per image in the test set.\n\n**Dataset Overview:**\n* **Data Type:** Image data (apparel products) with JSON-structured metadata.\n* **Context:** Product images from four apparel classes with multiple attribute labels per image.\n* **Data Files:**\n    * `fgvc4_iMat.train.data.json` - Training images and labels.\n    * `fgvc4_iMat.validation.data.json` - Validation images and labels.\n    * `fgvc4_iMat.test.image.json` - Test images (URLs only).\n    * `fgvc4_iMat.task_map.json` - Task dictionary (e.g., \"dress_color\").\n    * `fgvc4_iMat.label_map.json` - Label dictionary (specific attribute IDs and names).\n    * Sample submission file in CSV format.\n* **Features:**\n    * Images provided via URLs (participants must download them).\n    * Each image has multiple annotations (image_id, task_id, label_id).\n    * Tasks are structured as \"class_attribute\" (e.g., \"shoe_material\").\n\n**Evaluation Metrics:**\n* **Primary Metric:** Top-1 Error Rate (average error over all test images and tasks).\n    * **Components:**\n        * For each image-task pair, prediction is correct if it matches any ground truth label for that task.\n        * Error per image-task pair: \n            ```\n            eit = min_k d(lit, git,k) \n            ```\n            where `d(x,y) = 0` if `x=y` else `1`.\n        * Final score: \n            ```\n            score = (1/N) * sum(eit) \n            ```\n            where `N` is the total number of valid ground truth labels.",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Multi-label Classification (Fine-Grained Visual Categorization)\n* **Objective:** Develop algorithms to accurately assign multiple attribute labels to images of apparel products (outerwear, dresses, pants, shoes) despite challenges like varying lighting, angles, backgrounds, and occlusion.\n    * **Key Points:**\n        * Focus on fine-grained attribute differences (e.g., distinguishing similar colors like royal blue vs. turquoise).\n        * Each image may have multiple labels across different tasks (e.g., color, material, style).\n        * Participants must predict labels for all specified tasks per image in the test set.",
      "Dataset Overview": "* **Data Type:** Image data (apparel products) with JSON-structured metadata.\n* **Context:** Product images from four apparel classes with multiple attribute labels per image.\n* **Data Files:**\n    * `fgvc4_iMat.train.data.json` - Training images and labels.\n    * `fgvc4_iMat.validation.data.json` - Validation images and labels.\n    * `fgvc4_iMat.test.image.json` - Test images (URLs only).\n    * `fgvc4_iMat.task_map.json` - Task dictionary (e.g., \"dress_color\").\n    * `fgvc4_iMat.label_map.json` - Label dictionary (specific attribute IDs and names).\n    * Sample submission file in CSV format.\n* **Features:**\n    * Images provided via URLs (participants must download them).\n    * Each image has multiple annotations (image_id, task_id, label_id).\n    * Tasks are structured as \"class_attribute\" (e.g., \"shoe_material\").",
      "Evaluation Metrics": "* **Primary Metric:** Top-1 Error Rate (average error over all test images and tasks).\n    * **Components:**\n        * For each image-task pair, prediction is correct if it matches any ground truth label for that task.\n        * Error per image-task pair: \n            ```\n            eit = min_k d(lit, git,k) \n            ```\n            where `d(x,y) = 0` if `x=y` else `1`.\n        * Final score: \n            ```\n            score = (1/N) * sum(eit) \n            ```\n            where `N` is the total number of valid ground truth labels."
    },
    "file_path": "kaggle_datasets/230/problem_summary.md"
  },
  "466": {
    "problem_id": "466",
    "title": "Wikipedia Image-to-Caption Matching",
    "problem_type": "Information Retrieval / Cross-Modal Matching (Image-to-Text)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Wikipedia Image-to-Caption Matching\n\n**Problem Description:**\n* **Problem Type:** Information Retrieval / Cross-Modal Matching (Image-to-Text)\n* **Objective:**  \n  * Build a model that automatically retrieves the most relevant text captions for given Wikipedia images. \n  * The model must associate images with either article titles or complex captions in multiple languages.\n  * Key challenge: Handle the semantic granularity of Wikipedia images (complex subjects requiring precise caption matching).\n* **Key Points:**\n  * Focus on improving accessibility by generating accurate alt-text/captions for Wikipedia's image-heavy content.\n  * Must handle multilingual caption retrieval.\n  * Unlike generic image captioning, requires retrieval from existing captions/titles rather than generative text output.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  * Multimodal dataset: Images paired with textual captions/titles from Wikipedia articles.\n  * Data derived from the WIT (Wikipedia-based Image Text) dataset by Google Research.\n* **Key Files:**  \n  * `train-{0000x}-of-00005.tsv`: Tab-separated training data (image-text pairs).\n  * `test.tsv`: Test data (image IDs for which captions must be retrieved).\n  * `test_captions_list.csv`: Candidate captions to rank for test images.\n  * `image_data_test/`: Contains:\n    * `image_pixels/`: Base64-encoded image files (300px resolution).\n    * `resnet_embeddings/`: Precomputed ResNet image embeddings (2048-dim vectors).\n  * Training image data hosted separately (~275GB, not all images have corresponding data).\n* **Notable Features:**  \n  * `caption_title_and_reference_description`: Target text to retrieve (multilingual).\n  * `b64_bytes`/`embedding`: Image representations (raw pixels or pre-extracted features).\n\n**Evaluation Metrics:**\n* **Primary Metric:** NDCG@5 (Normalized Discounted Cumulative Gain at rank 5).\n  * Measures ranking quality of retrieved captions, emphasizing higher relevance for top-ranked results.\n  * **Key Components:**\n    * Discounted Cumulative Gain (DCG): Sums relevance scores of top 5 predictions, with logarithmic discounting for lower ranks.\n    * Normalization: DCG is divided by the Ideal DCG (best possible ranking) to scale scores",
    "sections": {
      "Problem Description": "* **Problem Type:** Information Retrieval / Cross-Modal Matching (Image-to-Text)\n* **Objective:**  \n  * Build a model that automatically retrieves the most relevant text captions for given Wikipedia images. \n  * The model must associate images with either article titles or complex captions in multiple languages.\n  * Key challenge: Handle the semantic granularity of Wikipedia images (complex subjects requiring precise caption matching).\n* **Key Points:**\n  * Focus on improving accessibility by generating accurate alt-text/captions for Wikipedia's image-heavy content.\n  * Must handle multilingual caption retrieval.\n  * Unlike generic image captioning, requires retrieval from existing captions/titles rather than generative text output.",
      "Dataset Overview": "* **Data Type & Context:**  \n  * Multimodal dataset: Images paired with textual captions/titles from Wikipedia articles.\n  * Data derived from the WIT (Wikipedia-based Image Text) dataset by Google Research.\n* **Key Files:**  \n  * `train-{0000x}-of-00005.tsv`: Tab-separated training data (image-text pairs).\n  * `test.tsv`: Test data (image IDs for which captions must be retrieved).\n  * `test_captions_list.csv`: Candidate captions to rank for test images.\n  * `image_data_test/`: Contains:\n    * `image_pixels/`: Base64-encoded image files (300px resolution).\n    * `resnet_embeddings/`: Precomputed ResNet image embeddings (2048-dim vectors).\n  * Training image data hosted separately (~275GB, not all images have corresponding data).\n* **Notable Features:**  \n  * `caption_title_and_reference_description`: Target text to retrieve (multilingual).\n  * `b64_bytes`/`embedding`: Image representations (raw pixels or pre-extracted features).",
      "Evaluation Metrics": "* **Primary Metric:** NDCG@5 (Normalized Discounted Cumulative Gain at rank 5).\n  * Measures ranking quality of retrieved captions, emphasizing higher relevance for top-ranked results.\n  * **Key Components:**\n    * Discounted Cumulative Gain (DCG): Sums relevance scores of top 5 predictions, with logarithmic discounting for lower ranks.\n    * Normalization: DCG is divided by the Ideal DCG (best possible ranking) to scale scores"
    },
    "file_path": "kaggle_datasets/466/problem_summary.md"
  },
  "492": {
    "problem_id": "492",
    "title": "Animal Counting in Camera Trap Image Sequences",
    "problem_type": "Computer Vision - Object Counting (Regression)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Animal Counting in Camera Trap Image Sequences\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Counting (Regression)\n* **Objective:**  \n    * Predict the total count of individual animals across sequences of motion-triggered camera trap images, where sequences may contain 1-10 images.\n    * Key challenge: Avoid over/under-counting due to temporal bursts (e.g., same animal appearing in multiple frames) and varying species distributions across global camera locations.\n* **Key Points:**\n    * Test cameras have overlapping but non-identical species distributions compared to training data.\n    * Requires reasoning across temporal sequences rather than single-image detection.\n    * Multimodal solutions encouraged (e.g., combining image data with Landsat-8 multispectral imagery).\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Image data from motion-triggered camera traps across 414 global locations (323 train, 91 test).\n    * Supplemental data: iNaturalist species images and Landsat-8 multispectral patches (30m resolution, 9 spectral bands) for each location.\n* **Data Files:**  \n    * `train/`, `test/` directories with 201,399 training and 60,029 test images.\n    * Metadata files: `train_sequence_counts.csv` (count annotations for 1,780 sequences), `gps_locations.json` (obfuscated coordinates), and detection/segmentation outputs.\n    * Preprocessed outputs: MegaDetector bounding boxes (`iwildcam2022_mdv4_detections.json`) and DeepMAC instance masks (`instance_masks/`).\n* **Key Features:**  \n    * Sequence IDs (`seq_id`) to group burst images.\n    * Location/sub-location identifiers for geographic context.\n    * Species categories (`category_id`) for training images only.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)  \n    * Calculated as:  \n      ![MAE Formula](https://raw.githubusercontent.com/visipedia/iwildcam_comp/master/assets/MAE.png)  \n      where:\n      * `x_i` = predicted count for sequence `i`\n      * `y_i` = ground truth count for sequence `i`\n      * `n` = total test sequences\n    * Chosen for interpretability and reduced penalty on large-count errors compared to RMSE.",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Counting (Regression)\n* **Objective:**  \n    * Predict the total count of individual animals across sequences of motion-triggered camera trap images, where sequences may contain 1-10 images.\n    * Key challenge: Avoid over/under-counting due to temporal bursts (e.g., same animal appearing in multiple frames) and varying species distributions across global camera locations.\n* **Key Points:**\n    * Test cameras have overlapping but non-identical species distributions compared to training data.\n    * Requires reasoning across temporal sequences rather than single-image detection.\n    * Multimodal solutions encouraged (e.g., combining image data with Landsat-8 multispectral imagery).",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Image data from motion-triggered camera traps across 414 global locations (323 train, 91 test).\n    * Supplemental data: iNaturalist species images and Landsat-8 multispectral patches (30m resolution, 9 spectral bands) for each location.\n* **Data Files:**  \n    * `train/`, `test/` directories with 201,399 training and 60,029 test images.\n    * Metadata files: `train_sequence_counts.csv` (count annotations for 1,780 sequences), `gps_locations.json` (obfuscated coordinates), and detection/segmentation outputs.\n    * Preprocessed outputs: MegaDetector bounding boxes (`iwildcam2022_mdv4_detections.json`) and DeepMAC instance masks (`instance_masks/`).\n* **Key Features:**  \n    * Sequence IDs (`seq_id`) to group burst images.\n    * Location/sub-location identifiers for geographic context.\n    * Species categories (`category_id`) for training images only.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)  \n    * Calculated as:  \n      ![MAE Formula](https://raw.githubusercontent.com/visipedia/iwildcam_comp/master/assets/MAE.png)  \n      where:\n      * `x_i` = predicted count for sequence `i`\n      * `y_i` = ground truth count for sequence `i`\n      * `n` = total test sequences\n    * Chosen for interpretability and reduced penalty on large-count errors compared to RMSE."
    },
    "file_path": "kaggle_datasets/492/problem_summary.md"
  },
  "264": {
    "problem_id": "264",
    "title": "Predicting NCAA March Madness Tournament Outcomes",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA March Madness Tournament Outcomes\n\n## Problem Description\n- **Problem Type**: Binary Classification (Probability Prediction)\n- **Objective**: Predict the probability of one team beating another in NCAA Division I Men's Basketball Tournament (\"March Madness\") games. Participants must forecast outcomes for all possible matchups in:\n  * Stage 1: Past tournaments (2014-2017)\n  * Stage 2: The 2018 tournament before it begins\n- **Key Points**:\n  * Requires predicting every possible matchup between tournament teams (68*67/2=2,278 predictions per year)\n  * Play-in games are excluded from evaluation\n  * Predictions must be made before tournament begins (no updates allowed)\n  * Uses historical tournament data (1985-2017) for model development\n\n## Dataset Overview\n- **Data Type**: Tabular data with game results, team statistics, and rankings\n- **Context**: NCAA Division I Men's Basketball games (regular season and tournament)\n- **Key Data Files**:\n  * `Teams.csv` - Team IDs and metadata\n  * `Seasons.csv` - Tournament year information\n  * `NCAATourneySeeds.csv` - Tournament seeds by year\n  * `RegularSeasonCompactResults.csv` - Game results (1985-2017)\n  * `NCAATourneyCompactResults.csv` - Tournament game results\n  * `RegularSeasonDetailedResults.csv` - Team-level box scores (2003-2017)\n  * `NCAATourneyDetailedResults.csv` - Tournament box scores\n  * `MasseyOrdinals.csv` - Weekly team rankings from multiple systems\n- **Important Features**:\n  * Game outcomes (win/loss, scores)\n  * Team performance statistics (field goals, rebounds, assists, etc.)\n  * Tournament seeds and regions\n  * Multiple ranking systems (Pomeroy, Sagarin, RPI, etc.)\n  * Play-by-play data available for recent seasons\n\n## Evaluation Metrics\n- **Primary Metric**: Log Loss (Binary Cross-Entropy)\n- **Metric Components**:\n  * Calculated as: \n    ```\n    LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]\n    ```\n  * Where:\n    * n = number of games played\n    * ŷ_i = predicted probability of team",
    "sections": {},
    "file_path": "kaggle_datasets/264/problem_summary.md"
  },
  "600": {
    "problem_id": "600",
    "title": "Multi-class Classification of Harmful Brain Activity from EEG Signals",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Classification of Harmful Brain Activity from EEG Signals\n\n## Problem Description\n- **Problem Type**: Multi-class Classification (with probabilistic outputs)\n- **Objective**: Develop a model to classify electroencephalography (EEG) signals into six categories of harmful brain activity patterns in critically ill patients:\n  * Seizure (SZ)\n  * Generalized periodic discharges (GPD)\n  * Lateralized periodic discharges (LPD)\n  * Lateralized rhythmic delta activity (LRDA)\n  * Generalized rhythmic delta activity (GRDA)\n  * \"Other\" (non-harmful activity)\n- **Key Points**:\n  * Focuses on expert-annotated EEG segments with varying agreement levels:\n    * \"Idealized\" patterns (high expert agreement)\n    * \"Proto patterns\" (~50% experts label as \"other\")\n    * \"Edge cases\" (experts split between two named patterns)\n  * Aims to automate labor-intensive manual EEG analysis process prone to fatigue and inter-rater variability\n  * Potential applications in neurocritical care, epilepsy treatment, and drug development\n\n## Dataset Overview\n- **Data Type**: Time-series EEG signals and derived spectrograms\n- **Context**: EEG recordings from critically ill hospital patients, annotated by neurology experts\n- **Data Files**:\n  * `train.csv` - Metadata for training set (EEG IDs, subsample IDs, time offsets, patient IDs, expert consensus labels, vote counts per class)\n  * `test.csv` - Simplified metadata for test set\n  * `sample_submission.csv` - Submission format template\n  * `train_eegs/` - Raw EEG training data (200Hz sampling rate)\n  * `test_eegs/` - Raw EEG test data (50-second samples)\n  * `train_spectrograms/` - Derived spectrogram training data\n  * `test_spectrograms/` - Derived spectrogram test data (10-minute windows)\n- **Key Features**:\n  * EEG electrode measurements (standard 10-20 system locations)\n  * EKG lead data\n  * Spectrogram frequency bands (LL/RL/LP/RP electrode regions)\n  * Expert vote distributions across six pattern classes\n\n## Evaluation Metrics\n- **Primary Metric**: Kullback-Leibler (KL) Divergence\n  * Measures difference between predicted probability distribution and actual expert vote distribution\n  * Formula: KL(P||Q",
    "sections": {},
    "file_path": "kaggle_datasets/600/problem_summary.md"
  },
  "432": {
    "problem_id": "432",
    "title": "Product Matching via Image and Text Data",
    "problem_type": "Image and Text Similarity (Multi-modal Matching)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Product Matching via Image and Text Data\n\n**Problem Description:**\n* **Problem Type:** Image and Text Similarity (Multi-modal Matching)\n* **Objective:**  \n    * Build a model to predict whether two e-commerce product listings represent the same physical product, using both images and text metadata.\n    * Key applications include price comparison engines, spam detection, and product catalog deduplication.\n* **Key Points:**\n    * Must handle cases where identical products have vastly different images (e.g., different angles/lighting) and similar-looking images represent different products.\n    * Self-matches are required (each posting must match itself in predictions).\n    * Group predictions are capped at 50 matches per posting.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Multi-modal dataset containing product images paired with text metadata from Shopee's e-commerce platform.\n    * Real-world challenge: User-uploaded images and descriptions lead to high variability in representations of identical products.\n* **Data Files:**  \n    * `train.csv`/`test.csv`: Metadata including posting IDs, image hashes, titles, and label groups (train only).\n    * `train_images`/`test_images`: Folder containing product images (JPG format).\n    * `sample_submission.csv`: Example of space-delimited match format.\n* **Key Features:**  \n    * `image_phash`: Perceptual hash for approximate image similarity.\n    * `title`: Product description text.\n    * `label_group`: Ground truth product clusters (training only).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Sample-wise mean F1 score.\n    * Calculated by:  \n        1. Computing F1 score for each predicted row (precision/recall of matches).  \n        2. Averaging F1 scores across all predictions.\n* **Submission Format:**  \n    * Space-delimited lists of matching `posting_id`s in a CSV, with mandatory self-matches.\n    * Example: `posting_id,matches\\nA,A B C\\nB,B A C\\nC,C A B`",
    "sections": {
      "Problem Description": "* **Problem Type:** Image and Text Similarity (Multi-modal Matching)\n* **Objective:**  \n    * Build a model to predict whether two e-commerce product listings represent the same physical product, using both images and text metadata.\n    * Key applications include price comparison engines, spam detection, and product catalog deduplication.\n* **Key Points:**\n    * Must handle cases where identical products have vastly different images (e.g., different angles/lighting) and similar-looking images represent different products.\n    * Self-matches are required (each posting must match itself in predictions).\n    * Group predictions are capped at 50 matches per posting.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Multi-modal dataset containing product images paired with text metadata from Shopee's e-commerce platform.\n    * Real-world challenge: User-uploaded images and descriptions lead to high variability in representations of identical products.\n* **Data Files:**  \n    * `train.csv`/`test.csv`: Metadata including posting IDs, image hashes, titles, and label groups (train only).\n    * `train_images`/`test_images`: Folder containing product images (JPG format).\n    * `sample_submission.csv`: Example of space-delimited match format.\n* **Key Features:**  \n    * `image_phash`: Perceptual hash for approximate image similarity.\n    * `title`: Product description text.\n    * `label_group`: Ground truth product clusters (training only).",
      "Evaluation Metrics": "* **Primary Metric:** Sample-wise mean F1 score.\n    * Calculated by:  \n        1. Computing F1 score for each predicted row (precision/recall of matches).  \n        2. Averaging F1 scores across all predictions.\n* **Submission Format:**  \n    * Space-delimited lists of matching `posting_id`s in a CSV, with mandatory self-matches.\n    * Example: `posting_id,matches\\nA,A B C\\nB,B A C\\nC,C A B`"
    },
    "file_path": "kaggle_datasets/432/problem_summary.md"
  },
  "290": {
    "problem_id": "290",
    "title": "TGS Salt Identification Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# TGS Salt Identification Challenge  \n\n## Problem Description  \n- **Problem Type:** Computer Vision - Semantic Segmentation  \n- **Objective:**  \n  - Automatically segment salt deposits in seismic images to improve subsurface imaging accuracy.  \n  - The task involves classifying each pixel in 101x101 seismic images as either \"salt\" or \"sediment.\"  \n- **Key Points:**  \n  - Salt identification is critical for oil/gas drilling safety but traditionally relies on error-prone human interpretation.  \n  - The challenge focuses on pixel-level binary segmentation (salt vs. non-salt regions).  \n\n## Dataset Overview  \n- **Data Type & Context:**  \n  - Seismic images (101x101 pixels) with depth metadata, representing subsurface rock boundaries.  \n  - Each pixel is labeled as salt (1) or sediment (0).  \n- **Data Files:**  \n  - `train.csv`: Image IDs and corresponding run-length-encoded (RLE) masks.  \n  - `depths.csv`: Depth values for each image.  \n  - `train.zip`/`test.zip`: Folders containing seismic images (PNG format).  \n  - `sample_submission.csv`: Submission template with RLE format requirements.  \n- **Features:**  \n  - **Images:** Grayscale seismic reflection data.  \n  - **Depth:** Single numerical feature indicating subsurface depth (in meters or feet).  \n\n## Evaluation Metrics  \n- **Primary Metric:** Mean Average Precision at IoU Thresholds (mAP@[0.5:0.95]).  \n- **Components:**  \n  - **IoU Calculation:** Intersection-over-Union between predicted and ground truth masks.  \n  - **Thresholds:** Evaluated at 10 IoU thresholds (0.5 to 0.95, step 0.05).  \n  - **Precision at Each Threshold:**  \n    - True Positives (TP): Predicted mask overlaps GT with IoU > threshold.  \n    - False Positives (FP): Predicted mask has no GT match.  \n    - False Negatives (FN): GT mask has no predicted match.  \n  - **Final Score:** Mean of average precision values across all thresholds and images.  \n- **Submission Format:** Run-length encoding (RLE) of pixel masks to reduce file size.",
    "sections": {},
    "file_path": "kaggle_datasets/290/problem_summary.md"
  },
  "297": {
    "problem_id": "297",
    "title": "NFL Punt Player Safety Analytics Competition",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NFL Punt Player Safety Analytics Competition\n\n## Problem Description\n* **Problem Type**: Sports Analytics / Rule Proposal (Non-traditional ML competition)\n* **Objective**:  \n    * Analyze NFL punt play data to propose rule modifications that reduce concussion risks\n    * Identify high-risk play features through data analysis (player positions, velocities, collision types)\n    * Create actionable recommendations maintaining game integrity while improving safety\n* **Key Points**:\n    * Focus on punt plays (identified as higher concussion risk relative to play frequency)\n    * Must provide data-supported evidence for proposed rule changes\n    * Considerations include:\n        * Initial formations\n        * Tackling techniques\n        * Blocking rules\n        * Potential unintended consequences of changes\n\n## Dataset Overview\n* **Data Type**: Multi-modal sports analytics data\n    * Tabular game/play metadata\n    * Player tracking time-series data\n    * Video review annotations for injury plays\n* **Data Files**:\n    * Core datasets:\n        * `game_data.csv` (game metadata)\n        * `play_information.csv` (play context)\n        * `player_punt_data.csv` (player positions)\n        * `play_player_role_data.csv` (punt-specific roles)\n        * `video_review.csv` (injury annotations)\n    * NGS tracking data (time-series per season split):\n        * `NGS-2016-pre.csv`, `NGS-2017-reg-wk1-6.csv`, etc.\n* **Key Features**:\n    * Player movement metrics (position, speed, direction)\n    * Play context (score, quarter, game situation)\n    * Injury metadata (impact type, player activities)\n    * Team formations and player roles during punts\n\n## Evaluation Metrics\n* **Evaluation Criteria**:\n    * **Solution Efficacy** (50% weight):\n        * Demonstrated understanding of concussion risk factors\n        * Reproducible data analysis supporting proposals\n        * Clear causal link between proposed rules and risk reduction\n    * **Game Integrity** (50% weight):\n        * Practical implementability by NFL\n        * Preservation of game dynamics\n        * Consideration of potential new risks introduced\n* **Submission Format**:\n    * Required components:\n        1. Summary slides (PDF/PPT)\n        2. Public Kaggle kernels with analysis\n    * Judged by NFL experts rather than quantitative metric",
    "sections": {},
    "file_path": "kaggle_datasets/297/problem_summary.md"
  },
  "435": {
    "problem_id": "435",
    "title": "Indoor Smartphone Positioning with Sensor Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Indoor Smartphone Positioning with Sensor Data\n\n## Problem Description\n- **Problem Type:** Regression (with floor classification component)  \n- **Objective:** Predict the precise indoor position (floor, x, y coordinates) of smartphones in multi-level buildings using real-time sensor data.  \n- **Key Points:**  \n  - Combines continuous coordinate prediction (regression) with discrete floor-level classification  \n  - Focuses on overcoming poor accuracy of existing indoor positioning systems  \n  - Uses \"active\" localization data requiring user permission (not passive methods like radar)  \n  - Applications in retail, manufacturing, and autonomous devices  \n\n## Dataset Overview\n- **Data Type & Context:**  \n  - Time-series sensor data from smartphones (WiFi, geomagnetic field, iBeacons, IMU)  \n  - Collected from 200+ buildings with nearly 30,000 path traces  \n- **Data Files:**  \n  - `train/`: Path trace files (`.txt`) with sensor readings and ground truth waypoints  \n  - `test/`: Path trace files without waypoint data  \n  - `metadata/`: Floor plans (`floor_image.png`), spatial metadata (`geojson_map.json`)  \n  - `sample_submission.csv`: Submission template with site-path-timestamp IDs  \n- **Key Features:**  \n  - Raw sensor readings (accelerometer, gyroscope, magnetometer)  \n  - WiFi/Bluetooth signatures  \n  - Building/floor identifiers  \n  - Timestamped waypoint coordinates (in training data)  \n\n## Evaluation Metrics\n- **Primary Metric:** Mean Position Error (custom metric)  \n- **Calculation:**  \n  ```math\n  \\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sqrt{(\\hat{x}_i-x_i)^2 + (\\hat{y}_i-y_i)^2} + p\\cdot|\\hat{f}_i-f_i|\\right)\n  ```\n  - **Components:**  \n    - Euclidean distance error in (x,y) coordinates  \n    - Floor prediction error (absolute difference) with penalty multiplier p=15  \n    - Floor label mapping: F1/1F→0, F2/2F→1, B1/1B→-1, etc.  \n  - **Note:** Non-standard floor labels (e.g., LG2) appear in training but not in test set",
    "sections": {},
    "file_path": "kaggle_datasets/435/problem_summary.md"
  },
  "607": {
    "problem_id": "607",
    "title": "Multi-Label Plant Species Prediction from Geospatial and Environmental Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-Label Plant Species Prediction from Geospatial and Environmental Data\n\n## Problem Description\n* **Problem Type**: Multi-Label Classification (Species Distribution Modeling)\n* **Objective**: Predict which plant species are present at a given geographic location and time using multi-modal environmental data. The task involves:\n  * Handling presence-only (PO) and presence-absence (PA) data formats\n  * Addressing strong class imbalance (10,000+ European flora species)\n  * Integrating satellite imagery, time series data, and environmental rasters\n* **Key Points**:\n  * Multi-modal learning challenge combining image, time series, and tabular data\n  * Learning from single positive labels (PO data) while evaluated on multi-label PA data\n  * Large-scale prediction with ~5M training occurrences across Europe\n\n## Dataset Overview\n* **Data Type**: Multi-modal geospatial/environmental data with species occurrence labels\n* **Primary Data Files**:\n  * `GLC24_PO_metadata_train.csv` (5M presence-only occurrences)\n  * `GLC24_PA_metadata_train.csv` (90K presence-absence surveys)\n  * Satellite image patches (RGB/NIR JPEGs)\n  * Satellite time series CSVs (6 bands over 20 years)\n  * Environmental raster data (climate, soil, elevation, etc.)\n* **Key Features**:\n  * **Spatial**: Geographic coordinates, satellite image patches (128x128px RGB/NIR)\n  * **Temporal**: DayOfYear, 20-year satellite time series (quarterly values)\n  * **Environmental**: 19 bioclimatic variables, soil properties, elevation, land cover\n  * **Labels**: Species IDs (spId) with surveyId as observation identifier\n\n## Evaluation Metrics\n* **Primary Metric**: Micro F1-Score (samples-averaged)\n* **Calculation**:\n  * For each test sample (combination of patchID and dayOfYear):\n    * TP = Number of correctly predicted present species\n    * FP = Number of incorrectly predicted present species\n    * FN = Number of missed present species\n  * F1 = (1/N) * Σ [TP_i / (TP_i + (FP_i + FN_i)/2)] across all N test samples\n* **Submission Format**:\n  * CSV with ordered space-delimited species IDs for each surveyId\n  * Predictions evaluated against multi-label ground truth",
    "sections": {},
    "file_path": "kaggle_datasets/607/problem_summary.md"
  },
  "263": {
    "problem_id": "263",
    "title": "Predicting NCAA Women's Basketball Tournament Outcomes",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA Women's Basketball Tournament Outcomes\n\n## Problem Description\n* **Problem Type:** Binary Classification (Probability Prediction)\n* **Objective:** Predict the probability of one team beating another in NCAA Division I Women's Basketball Tournament games. Participants must:\n    * Forecast outcomes for all possible matchups in historical tournaments (Stage 1)\n    * Predict outcomes for the 2018 tournament before it begins (Stage 2)\n* **Key Points:**\n    * Uses historical tournament data (1998-2017) for model building\n    * Requires predictions for all possible team pairings (n*(n-1)/2 matchups)\n    * Predictions must be bounded away from 0 and 1 to avoid infinite log loss\n    * Team1 vs Team2 is equivalent to Team2 vs Team1 (only lower TeamID is listed first)\n\n## Dataset Overview\n* **Data Type:** Tabular data of basketball game results and team information\n* **Context:** NCAA Division I Women's Basketball games (regular season and tournament)\n* **Key Data Files:**\n    * `WTeams.csv` - Team IDs and names (3000-3999 range for women's teams)\n    * `WSeasons.csv` - Season information and region mappings\n    * `WNCAATourneySeeds.csv` - Tournament seeds by year and region\n    * `WRegularSeasonCompactResults.csv` - Regular season game results (1998-2017)\n    * `WNCAATourneyCompactResults.csv` - Tournament game results (1998-2017)\n    * `WCities.csv` and `WGameCities.csv` - Geographic game locations (2015+)\n* **Important Features:**\n    * Team IDs, scores, and game outcomes\n    * Tournament seeds and regions\n    * Game dates (as day numbers relative to season start)\n    * Location indicators (home/away/neutral)\n\n## Evaluation Metrics\n* **Primary Metric:** Logarithmic Loss (LogLoss)\n    * Formula: `-1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n    * Where:\n        * n = number of games played\n        * ŷ_i = predicted probability of team 1 winning\n        * y_i = 1 if team 1 wins, 0 otherwise\n    * **Key Properties:**\n        * Heavily penalizes confident incorrect predictions",
    "sections": {},
    "file_path": "kaggle_datasets/263/problem_summary.md"
  },
  "638": {
    "problem_id": "638",
    "title": "Text Descrambling with Perplexity Optimization",
    "problem_type": "NLP - Text Permutation Optimization",
    "objective": "Rearrange scrambled words in holiday-themed text passages to minimize perplexity scores when evaluated by a language model. The core challenge is to find the most coherent word ordering for each given sequence.",
    "evaluation_metric": null,
    "full_content": "# Text Descrambling with Perplexity Optimization\n\n**Problem Description:**\n* **Problem Type:** NLP - Text Permutation Optimization\n* **Objective:** Rearrange scrambled words in holiday-themed text passages to minimize perplexity scores when evaluated by a language model. The core challenge is to find the most coherent word ordering for each given sequence.\n* **Key Points:**\n  * Participants must work with fixed sets of words that can only be rearranged (no additions/deletions)\n  * Solutions must produce valid permutations of the original word sequences\n  * The task simulates descrambling Christmas stories for optimal readability\n\n**Dataset Overview:**\n* **Data Type:** Text data (holiday-themed word sequences)\n* **Context:** Scrambled versions of classic Christmas stories/tales\n* **Data Files:**\n  * `sample_submission.csv` (contains base word sequences to permute)\n* **Features:**\n  * `id` - Unique identifier for each text passage\n  * `text` - Space-separated scrambled words to be rearranged\n\n**Evaluation Metrics:**\n* **Primary Metric:** Average Perplexity (lower is better)\n* **Components:**\n  * Perplexity calculated using Gemma 2 9B language model\n  * Each submission's word sequences are evaluated for model perplexity\n  * Final score is the average perplexity across all test sequences\n  * Only valid permutations of original word sequences are scored",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Text Permutation Optimization\n* **Objective:** Rearrange scrambled words in holiday-themed text passages to minimize perplexity scores when evaluated by a language model. The core challenge is to find the most coherent word ordering for each given sequence.\n* **Key Points:**\n  * Participants must work with fixed sets of words that can only be rearranged (no additions/deletions)\n  * Solutions must produce valid permutations of the original word sequences\n  * The task simulates descrambling Christmas stories for optimal readability",
      "Dataset Overview": "* **Data Type:** Text data (holiday-themed word sequences)\n* **Context:** Scrambled versions of classic Christmas stories/tales\n* **Data Files:**\n  * `sample_submission.csv` (contains base word sequences to permute)\n* **Features:**\n  * `id` - Unique identifier for each text passage\n  * `text` - Space-separated scrambled words to be rearranged",
      "Evaluation Metrics": "* **Primary Metric:** Average Perplexity (lower is better)\n* **Components:**\n  * Perplexity calculated using Gemma 2 9B language model\n  * Each submission's word sequences are evaluated for model perplexity\n  * Final score is the average perplexity across all test sequences\n  * Only valid permutations of original word sequences are scored"
    },
    "file_path": "kaggle_datasets/638/problem_summary.md"
  },
  "255": {
    "problem_id": "255",
    "title": "Binary Classification of Satellite Images for Iceberg Detection",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Satellite Images for Iceberg Detection\n\n## Problem Description\n- **Problem Type:** Binary Classification (Image Classification)\n- **Objective:** Build an algorithm to automatically identify whether a remotely sensed target in a satellite image is a ship or an iceberg. The goal is to improve accuracy in detecting threatening icebergs to maintain safe offshore working conditions.\n- **Key Points:**\n  - Focus on discriminating between ships and icebergs in satellite radar images\n  - Applications in maritime safety and offshore operations monitoring\n  - Uses dual-polarization radar data (HH and HV channels)\n  - Must handle float-valued pixel data representing radar backscatter in decibels\n\n## Dataset Overview\n- **Data Type:** Satellite radar images with dual-polarization channels\n- **Context:** Images captured by Sentinel-1 satellite constellation monitoring ocean areas off Canada's East Coast\n- **Data Files:**\n  - train.json (contains band_1, band_2, inc_angle, and is_iceberg labels)\n  - test.json (contains band_1, band_2, and inc_angle)\n  - sample_submission.csv (submission format template)\n- **Features:**\n  - 75x75 pixel images with two bands (HH and HV polarization)\n  - Pixel values are float numbers representing radar backscatter in dB\n  - Incidence angle of image capture (with some missing values)\n  - Binary target variable (1 for iceberg, 0 for ship) in training data\n\n## Evaluation Metrics\n- **Evaluation Metric:** Log Loss (Binary Cross-Entropy)\n- **Components:**\n  - Measures the uncertainty of predicted probabilities compared to true labels\n  - Penalizes confident incorrect predictions more heavily\n  - Formula: -(y*log(p) + (1-y)*log(1-p)) where y is true label and p is predicted probability\n  - Lower values indicate better performance (perfect prediction would score 0)",
    "sections": {},
    "file_path": "kaggle_datasets/255/problem_summary.md"
  },
  "403": {
    "problem_id": "403",
    "title": "Predicting Pulmonary Fibrosis Progression from CT Scans",
    "problem_type": "Regression with Uncertainty Estimation (Medical Time Series Forecasting)",
    "objective": "Predict the decline in lung function (Forced Vital Capacity - FVC) for patients with pulmonary fibrosis based on baseline CT scans and clinical data. The goal is to forecast:",
    "evaluation_metric": null,
    "full_content": "# Predicting Pulmonary Fibrosis Progression from CT Scans\n\n**Problem Description:**\n* **Problem Type:** Regression with Uncertainty Estimation (Medical Time Series Forecasting)\n* **Objective:** Predict the decline in lung function (Forced Vital Capacity - FVC) for patients with pulmonary fibrosis based on baseline CT scans and clinical data. The goal is to forecast:\n    * Future FVC measurements (in ml) for three specific time points per patient\n    * A confidence measure (standard deviation) for each prediction\n* **Key Points:**\n    * Focus on both prediction accuracy and model confidence estimation\n    * Must handle irregular time intervals between patient visits\n    * Predictions required for all possible weeks, with only final three visits scored\n    * Real-world medical application with direct patient impact\n\n**Dataset Overview:**\n* **Data Type:** Multimodal (Medical Imaging + Tabular Clinical Data)\n    * DICOM format chest CT scans (baseline images)\n    * Tabular clinical time series data\n* **Data Files:**\n    * `train.csv` - Full history of FVC measurements and clinical data\n    * `test.csv` - Only baseline measurements\n    * `train/` - Training set DICOM images\n    * `test/` - Test set DICOM images\n* **Key Features:**\n    * Clinical: FVC, Percent (of normal), Age, Sex, SmokingStatus\n    * Imaging: Baseline chest CT scans (3D volumetric data)\n    * Temporal: Weeks since baseline measurement (can be negative)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Modified Laplace Log Likelihood\n    * Evaluates both prediction accuracy and confidence estimation\n* **Metric Components:**\n    * σ_clipped = max(σ, 70) [Confidence thresholded at 70ml]\n    * Δ = min(|FVC_true - FVC_predicted|, 1000) [Error capped at 1000ml]\n    * Metric = -(√2 * Δ/σ_clipped) - ln(√2 * σ_clipped)\n* **Scoring Notes:**\n    * Higher values are better (metric yields negative numbers)\n    * Final score is average across all test set Patient_Weeks\n    * Only the final three visits per patient contribute to score",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression with Uncertainty Estimation (Medical Time Series Forecasting)\n* **Objective:** Predict the decline in lung function (Forced Vital Capacity - FVC) for patients with pulmonary fibrosis based on baseline CT scans and clinical data. The goal is to forecast:\n    * Future FVC measurements (in ml) for three specific time points per patient\n    * A confidence measure (standard deviation) for each prediction\n* **Key Points:**\n    * Focus on both prediction accuracy and model confidence estimation\n    * Must handle irregular time intervals between patient visits\n    * Predictions required for all possible weeks, with only final three visits scored\n    * Real-world medical application with direct patient impact",
      "Dataset Overview": "* **Data Type:** Multimodal (Medical Imaging + Tabular Clinical Data)\n    * DICOM format chest CT scans (baseline images)\n    * Tabular clinical time series data\n* **Data Files:**\n    * `train.csv` - Full history of FVC measurements and clinical data\n    * `test.csv` - Only baseline measurements\n    * `train/` - Training set DICOM images\n    * `test/` - Test set DICOM images\n* **Key Features:**\n    * Clinical: FVC, Percent (of normal), Age, Sex, SmokingStatus\n    * Imaging: Baseline chest CT scans (3D volumetric data)\n    * Temporal: Weeks since baseline measurement (can be negative)",
      "Evaluation Metrics": "* **Primary Metric:** Modified Laplace Log Likelihood\n    * Evaluates both prediction accuracy and confidence estimation\n* **Metric Components:**\n    * σ_clipped = max(σ, 70) [Confidence thresholded at 70ml]\n    * Δ = min(|FVC_true - FVC_predicted|, 1000) [Error capped at 1000ml]\n    * Metric = -(√2 * Δ/σ_clipped) - ln(√2 * σ_clipped)\n* **Scoring Notes:**\n    * Higher values are better (metric yields negative numbers)\n    * Final score is average across all test set Patient_Weeks\n    * Only the final three visits per patient contribute to score"
    },
    "file_path": "kaggle_datasets/403/problem_summary.md"
  },
  "631": {
    "problem_id": "631",
    "title": "Predicting MCTS Variant Performance in Board Games",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting MCTS Variant Performance in Board Games\n\n## Problem Description\n* **Problem Type**: Regression\n* **Objective**: Predict the performance advantage of one Monte-Carlo Tree Search (MCTS) variant over another in a given board game, based on game features and agent configurations.\n    * **Key Points**:\n        * Focuses on two-player, sequential, zero-sum board games with perfect information\n        * Requires modeling interactions between game features and MCTS variant components\n        * Aims to generalize across hundreds of distinct games and MCTS configurations\n\n## Dataset Overview\n* **Data Type**: Tabular data with game features and MCTS agent configurations\n* **Context**: Outcomes from different MCTS variants playing over 1,000 distinct board games\n* **Data Files**:\n    * `train.csv`: Contains game features, agent configurations, and outcome statistics\n    * `test.csv`: Same structure as train.csv without outcome columns\n    * `concepts.csv`: Describes game concept features and their taxonomy\n* **Key Features**:\n    * Game properties (deterministic/stochastic, board shape, rule complexity)\n    * Agent configurations (selection strategy, exploration constant, play-out strategy)\n    * Game rules in both natural language and formal Ludii description language\n    * 1,635 concept-based features describing game characteristics\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Square Error (RMSE)\n    * **Calculation**: Square root of the average squared differences between predicted and actual utility values\n    * **Target Range**: Predictions must be between -1.0 (agent1 always loses) and 1.0 (agent1 always wins)",
    "sections": {},
    "file_path": "kaggle_datasets/631/problem_summary.md"
  },
  "636": {
    "problem_id": "636",
    "title": "Fine-Tuning Gemma for Multilingual and Cultural Adaptation",
    "problem_type": "NLP - Language Model Fine-Tuning for Text Generation",
    "objective": "Adapt Google's Gemma 2 model for specific languages or cultural contexts by creating clear, reproducible notebooks that demonstrate:",
    "evaluation_metric": null,
    "full_content": "# Fine-Tuning Gemma for Multilingual and Cultural Adaptation\n\n**Problem Description:**\n* **Problem Type:** NLP - Language Model Fine-Tuning for Text Generation\n* **Objective:** Adapt Google's Gemma 2 model for specific languages or cultural contexts by creating clear, reproducible notebooks that demonstrate:\n    * Dataset creation/curation for target languages/cultures\n    * Effective fine-tuning techniques (e.g., few-shot prompting, retrieval-augmented generation)\n    * Inference and evaluation methods\n* **Key Points:**\n    * Focus on 73 eligible languages representing diverse linguistic/cultural groups\n    * Applications include:\n        * Language fluency (translation, dialogue generation)\n        * Literary tradition adaptation (poetry, folklore)\n        * Historical text processing\n    * Requires publishing trained models on Kaggle Models\n    * Emphasis on documentation and reproducibility for community learning\n\n**Dataset Overview:**\n* **Data Type:** Text data for fine-tuning (participant-provided)\n* **Context:** Participants must create/curate their own datasets for target languages/cultural contexts\n* **Key Requirements:**\n    * Detailed documentation of data sources and preprocessing\n    * Considerations for data quality and cultural sensitivity\n    * No specific dataset files provided - participants source their own data\n\n**Evaluation Metrics:**\n* **Evaluation Rubric (0-40 points total):**\n    * **Technical (0-10pts):** Effective use of fine-tuning strategies\n    * **Descriptive (0-10pts):** Thorough documentation of dataset creation and methodology\n    * **Useful (0-10pts):** Practical utility of model outputs\n    * **Robust (0-10pts):** Performance consistency with additional inputs\n* **Eligibility Criteria (Pass/Fail):**\n    * Compliant with guidelines\n    * Topical relevance\n    * Open access (public notebook + published model)\n    * Use of eligible language",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Language Model Fine-Tuning for Text Generation\n* **Objective:** Adapt Google's Gemma 2 model for specific languages or cultural contexts by creating clear, reproducible notebooks that demonstrate:\n    * Dataset creation/curation for target languages/cultures\n    * Effective fine-tuning techniques (e.g., few-shot prompting, retrieval-augmented generation)\n    * Inference and evaluation methods\n* **Key Points:**\n    * Focus on 73 eligible languages representing diverse linguistic/cultural groups\n    * Applications include:\n        * Language fluency (translation, dialogue generation)\n        * Literary tradition adaptation (poetry, folklore)\n        * Historical text processing\n    * Requires publishing trained models on Kaggle Models\n    * Emphasis on documentation and reproducibility for community learning",
      "Dataset Overview": "* **Data Type:** Text data for fine-tuning (participant-provided)\n* **Context:** Participants must create/curate their own datasets for target languages/cultural contexts\n* **Key Requirements:**\n    * Detailed documentation of data sources and preprocessing\n    * Considerations for data quality and cultural sensitivity\n    * No specific dataset files provided - participants source their own data",
      "Evaluation Metrics": "* **Evaluation Rubric (0-40 points total):**\n    * **Technical (0-10pts):** Effective use of fine-tuning strategies\n    * **Descriptive (0-10pts):** Thorough documentation of dataset creation and methodology\n    * **Useful (0-10pts):** Practical utility of model outputs\n    * **Robust (0-10pts):** Performance consistency with additional inputs\n* **Eligibility Criteria (Pass/Fail):**\n    * Compliant with guidelines\n    * Topical relevance\n    * Open access (public notebook + published model)\n    * Use of eligible language"
    },
    "file_path": "kaggle_datasets/636/problem_summary.md"
  },
  "404": {
    "problem_id": "404",
    "title": "Pulmonary Embolism Detection in Chest CT Scans",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Pulmonary Embolism Detection in Chest CT Scans\n\n## Problem Description\n* **Problem Type:**  \n  * Binary Classification (Image-level and Exam-level)\n  * Multi-label Classification (Exam-level)\n\n* **Objective:**  \n  Detect and classify pulmonary embolism (PE) cases in chest CT pulmonary angiography (CTPA) scans. Participants must:\n  * Predict presence of PE at image-level (`pe_present_on_image`)\n  * Predict 9 exam-level labels related to PE characteristics and severity\n  * Adhere to strict label hierarchy constraints (e.g., mutually exclusive labels)\n\n* **Key Points:**\n  * Focus on reducing human delays/errors in PE diagnosis\n  * Must handle hundreds of images per study (CT scan series)\n  * Predictions must be logically consistent across hierarchy:\n    * Exam-level predictions must align with image-level predictions\n    * Mutually exclusive labels cannot be simultaneously predicted\n\n## Dataset Overview\n* **Data Type & Context:**  \n  * Medical imaging (DICOM format CT scans)\n  * 7,279 training studies (~230GB), 650 public test studies, 1,517 private test studies\n  * Each study contains multiple series/images with metadata\n\n* **Data Files:**\n  * `train.csv` - Contains image UIDs and all labels\n  * `test.csv` - Contains test image UIDs only\n  * `sample_submission.csv` - Submission template format\n  * DICOM image folders organized by Study/Series/SOPInstanceUID\n\n* **Key Features:**\n  * Image-level label: `pe_present_on_image`\n  * 9 exam-level labels including:\n    * PE presence (`negative_exam_for_pe`)\n    * PE type (chronic, acute+chronic)\n    * PE location (left, right, central)\n    * RV/LV ratio indicators\n  * 4 informational labels (not predicted) about scan quality\n\n## Evaluation Metrics\n* **Primary Metric:**  \n  Weighted Log Loss (separate components for image-level and exam-level predictions)\n\n* **Metric Components:**\n  * **Exam-level weighted log loss:**\n    * 9 labels with different weights (range: 0.062-0.235)\n    * Formula:  \n      `L_ij = -w_j * [y_ij*log(p_ij) + (1-y_ij)*log(1-p_ij)]`",
    "sections": {},
    "file_path": "kaggle_datasets/404/problem_summary.md"
  },
  "252": {
    "problem_id": "252",
    "title": "Santa Gift Matching Optimization Challenge",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Santa Gift Matching Optimization Challenge\n\n## Problem Description\n* **Problem Type**: Combinatorial Optimization with Matching Constraints\n* **Objective**: \n    * Match 1 million children with 1000 unique gifts (1000 units each) to maximize happiness for both children and Santa.\n    * Children have ranked wishlists of 100 preferred gifts.\n    * Santa has ranked lists of 1000 preferred children for each gift.\n    * Must satisfy special constraints for twins (4% of children) and triplets (0.5% of children) requiring identical gifts.\n* **Key Points**:\n    * Hard constraints:\n        * Twins (consecutive ChildId pairs) must receive same gift\n        * Triplets (consecutive ChildId triplets) must receive same gift\n        * Cannot exceed 1000 units per GiftId\n    * Soft optimization goals:\n        * Maximize child happiness (higher when receiving more preferred gifts)\n        * Maximize Santa's happiness (higher when giving to more preferred children)\n\n## Dataset Overview\n* **Data Type**: Tabular matching data with ranked preferences\n* **Key Files**:\n    * `child_wishlist_v2.csv`: Each row contains ChildId + 100 ranked GiftIds (most to least preferred)\n    * `gift_goodkids_v2.csv`: Each row contains GiftId + 1000 ranked ChildIds (most to least preferred)\n    * Sample submission file showing required format\n* **Important Features**:\n    * Child preferences: 100-item ranked lists per child\n    * Gift preferences: 1000-item ranked lists per gift\n    * Special markers for twins/triplets in ChildId sequencing\n\n## Evaluation Metrics\n* **Primary Metric**: Average Normalized Happiness (ANH)\n    * ANH = (AverageNormalizedChildHappiness³ + AverageNormalizedSantaHappiness³)\n* **Component Breakdown**:\n    * **Child Happiness**:\n        * 2*(list_length - rank_position) if gift is in wishlist\n        * -1 if gift is not in wishlist\n        * Normalized by maximum possible happiness (2*list_length)\n    * **Santa Happiness**:\n        * 2*(list_length - rank_position) if child is in gift's preferred list\n        * -1 if child is not in list\n        * Normalized by maximum possible happiness (2*list_length)\n    * Final score combines cubed averages",
    "sections": {},
    "file_path": "kaggle_datasets/252/problem_summary.md"
  },
  "609": {
    "problem_id": "609",
    "title": "Flood Probability Prediction",
    "problem_type": "Regression",
    "objective": "Predict the probability of a region flooding based on various environmental and geographical factors. The goal is to model the continuous target variable `FloodProbability` using the provided features.",
    "evaluation_metric": null,
    "full_content": "# Flood Probability Prediction\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the probability of a region flooding based on various environmental and geographical factors. The goal is to model the continuous target variable `FloodProbability` using the provided features.\n* **Key Points:**\n  * Synthetic dataset generated from real-world flood prediction data\n  * Encourages EDA and visualization due to feature distributions close to original data\n  * Participants may incorporate the original dataset for potential performance improvements\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing flood prediction factors (likely including hydrological, meteorological, and geographical features)\n* **Data Files:**\n  * `train.csv` - Training data with target variable `FloodProbability`\n  * `test.csv` - Test data for making predictions\n  * `sample_submission.csv` - Example submission file format\n* **Features:** Dataset contains 45 columns (specific features not named, but derived from real-world flood prediction factors)\n\n**Evaluation Metrics:**\n* **Primary Metric:** R2 Score (Coefficient of Determination)\n  * Measures proportion of variance in the dependent variable predictable from independent variables\n  * Ranges from -∞ to 1 (1 indicates perfect prediction)\n  * Formula: R² = 1 - (SS_res / SS_tot), where SS_res is sum of squared residuals and SS_tot is total sum of squares",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the probability of a region flooding based on various environmental and geographical factors. The goal is to model the continuous target variable `FloodProbability` using the provided features.\n* **Key Points:**\n  * Synthetic dataset generated from real-world flood prediction data\n  * Encourages EDA and visualization due to feature distributions close to original data\n  * Participants may incorporate the original dataset for potential performance improvements",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing flood prediction factors (likely including hydrological, meteorological, and geographical features)\n* **Data Files:**\n  * `train.csv` - Training data with target variable `FloodProbability`\n  * `test.csv` - Test data for making predictions\n  * `sample_submission.csv` - Example submission file format\n* **Features:** Dataset contains 45 columns (specific features not named, but derived from real-world flood prediction factors)",
      "Evaluation Metrics": "* **Primary Metric:** R2 Score (Coefficient of Determination)\n  * Measures proportion of variance in the dependent variable predictable from independent variables\n  * Ranges from -∞ to 1 (1 indicates perfect prediction)\n  * Formula: R² = 1 - (SS_res / SS_tot), where SS_res is sum of squared residuals and SS_tot is total sum of squares"
    },
    "file_path": "kaggle_datasets/609/problem_summary.md"
  },
  "299": {
    "problem_id": "299",
    "title": "Multi-label Classification of Subcellular Protein Patterns in Microscope Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Classification of Subcellular Protein Patterns in Microscope Images\n\n## Problem Description\n- **Problem Type**: Multi-label Image Classification (with 28 possible classes)\n- **Objective**: Develop models to classify mixed patterns of proteins in microscope images, identifying their subcellular locations. The goal is to automate biomedical image analysis to accelerate understanding of human cells and disease.\n- **Key Points**:\n  - Each image sample can have multiple labels (multi-label classification).\n  - Models will be integrated into a smart-microscopy system for high-throughput protein localization.\n  - Must handle patterns across different human cell types (27 cell types in dataset).\n  - Special prize consideration for models that maintain accuracy while being computationally efficient.\n\n## Dataset Overview\n- **Data Type**: Microscopy images with 4 channels (filters) per sample:\n  - Protein of interest (green)\n  - Cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow)\n- **Data Files**:\n  - `train.csv` (filenames and labels)\n  - `sample_submission.csv`\n  - `train.zip`/`test.zip` (512x512 PNG images)\n  - Optional full-size TIFF images (~250GB total)\n- **Features**:\n  - Each sample consists of 4 image files (one per filter/channel)\n  - Labels represent 28 possible subcellular locations (e.g., Nucleoplasm, Mitochondria, Plasma membrane)\n\n## Evaluation Metrics\n- **Evaluation Metric**: Macro F1-Score\n  - Calculated for each class independently and then averaged\n  - Suitable for multi-label classification as it balances precision and recall\n  - Handles class imbalance by giving equal weight to each class\n- **Submission Format**:\n  - Predictions must include all relevant labels for each sample (space-separated)\n  - Example: `00008af0-bad0-11e8-b2b8-ac1f6b6435d0,0 1` indicates classes 0 and 1 predicted",
    "sections": {},
    "file_path": "kaggle_datasets/299/problem_summary.md"
  },
  "46": {
    "problem_id": "46",
    "title": "Donor Prospect Ranking for Non-Profit Fundraising",
    "problem_type": "Regression (with ranking objective)",
    "objective": "Predict donor potential to help non-profits optimize direct mail campaigns by targeting the most receptive prospects. The goal is to rank prospects within each mailing to maximize donations from the top 75% of recipients.",
    "evaluation_metric": null,
    "full_content": "# Donor Prospect Ranking for Non-Profit Fundraising\n\n**Problem Description:**\n* **Problem Type:** Regression (with ranking objective)\n* **Objective:** Predict donor potential to help non-profits optimize direct mail campaigns by targeting the most receptive prospects. The goal is to rank prospects within each mailing to maximize donations from the top 75% of recipients.\n* **Key Points:**\n  * Focus on rank-ordering prospects within individual mailings rather than selecting entire mailings\n  * Target variable is transformed donation amount (\"Amount2\" = donation^1.15)\n  * Must avoid simply identifying \"good\" vs \"bad\" entire mailings (evaluation focuses on within-mailing rankings)\n  * Training data covers Nov 2011 and earlier, while test data covers Dec 2011 onward\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing direct mail campaign history and donor response data from multiple non-profit agencies\n* **Key Data Files:**\n  * `kaggle_training_dataset_formatted2`: Primary training data (11 months of mail history)\n  * Multiple agency-specific historical datasets (`kaggle_mail_dataset_formatted1a`, `1b`, `2`, `3`)\n  * Demographic data by zip code (`demo_per_formatted`)\n  * Historical performance by zip code (`Zip_perf`)\n  * `test.csv`: Evaluation data\n* **Notable Features:**\n  * Category variables: ListID, Package, Agency (must be used carefully per competition rules)\n  * Donation history and response data\n  * Demographic information by zip code\n\n**Evaluation Metrics:**\n* **Primary Metric:** AverageAmongTopP (with P=75%)\n  * For each mailing, prospects are rank-ordered by model predictions\n  * Only the top 75% of each mailing are considered in scoring\n  * The metric averages the transformed donation amount (\"Amount2\") across these top prospects\n* **Key Aspects:**\n  * Emphasizes correct ranking within mailings rather than absolute donation prediction\n  * Maximum possible score is 0.87190\n  * Uses power-transformed donation amounts to weight larger donations appropriately",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (with ranking objective)\n* **Objective:** Predict donor potential to help non-profits optimize direct mail campaigns by targeting the most receptive prospects. The goal is to rank prospects within each mailing to maximize donations from the top 75% of recipients.\n* **Key Points:**\n  * Focus on rank-ordering prospects within individual mailings rather than selecting entire mailings\n  * Target variable is transformed donation amount (\"Amount2\" = donation^1.15)\n  * Must avoid simply identifying \"good\" vs \"bad\" entire mailings (evaluation focuses on within-mailing rankings)\n  * Training data covers Nov 2011 and earlier, while test data covers Dec 2011 onward",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing direct mail campaign history and donor response data from multiple non-profit agencies\n* **Key Data Files:**\n  * `kaggle_training_dataset_formatted2`: Primary training data (11 months of mail history)\n  * Multiple agency-specific historical datasets (`kaggle_mail_dataset_formatted1a`, `1b`, `2`, `3`)\n  * Demographic data by zip code (`demo_per_formatted`)\n  * Historical performance by zip code (`Zip_perf`)\n  * `test.csv`: Evaluation data\n* **Notable Features:**\n  * Category variables: ListID, Package, Agency (must be used carefully per competition rules)\n  * Donation history and response data\n  * Demographic information by zip code",
      "Evaluation Metrics": "* **Primary Metric:** AverageAmongTopP (with P=75%)\n  * For each mailing, prospects are rank-ordered by model predictions\n  * Only the top 75% of each mailing are considered in scoring\n  * The metric averages the transformed donation amount (\"Amount2\") across these top prospects\n* **Key Aspects:**\n  * Emphasizes correct ranking within mailings rather than absolute donation prediction\n  * Maximum possible score is 0.87190\n  * Uses power-transformed donation amounts to weight larger donations appropriately"
    },
    "file_path": "kaggle_datasets/46/problem_summary.md"
  },
  "549": {
    "problem_id": "549",
    "title": "Reverse Image-to-Prompt Prediction with Stable Diffusion",
    "problem_type": "Image-to-Text (Multimodal Regression)",
    "objective": "Predict the text prompts used to generate images created by Stable Diffusion 2.0. The task involves reversing the typical text-to-image generation process by inferring the original prompt from a given synthetic image.",
    "evaluation_metric": null,
    "full_content": "# Reverse Image-to-Prompt Prediction with Stable Diffusion\n\n**Problem Description:**\n*   **Problem Type:** Image-to-Text (Multimodal Regression)\n*   **Objective:** Predict the text prompts used to generate images created by Stable Diffusion 2.0. The task involves reversing the typical text-to-image generation process by inferring the original prompt from a given synthetic image.\n    *   **Key Points:**\n        *   Focus on understanding the latent relationship between prompts and generated images.\n        *   Prompts vary in complexity, ranging from simple to multi-object/modifier descriptions.\n        *   Submissions require converting predicted prompts into 384-dimensional embeddings (either directly or via intermediate prompt prediction).\n\n**Dataset Overview:**\n*   **Data Type & Context:** \n    *   **Primary Data:** Synthetic images (512x512 px PNGs) generated from diverse text prompts using Stable Diffusion 2.0 (originally 768x768 px, downsampled).\n    *   **Context:** Images are outputs of a fixed generative process (50 steps, default parameters). Prompts were generated via undisclosed methods.\n*   **Data Files:**\n    *   `images/` - Training/generated images (example set provided; re-run test set contains ~16,000 hidden images).\n    *   `prompts.csv` - Example prompt-image pairs (for illustration only; not usable in test submissions).\n    *   `sample_submission.csv` - Format template with example embeddings.\n*   **Key Features:**\n    *   Images exhibit varied styles (e.g., \"highly detailed, sharp focus, illustration, 3D renders\").\n    *   Prompts contain semantic modifiers (e.g., \"majestic, epic\") and stylistic descriptors.\n\n**Evaluation Metrics:**\n*   **Primary Metric:** Mean cosine similarity between predicted and ground truth prompt embeddings.\n    *   **Components:**\n        *   Embeddings are 384-dimensional vectors derived from prompts (calculation method shown in [reference notebook](https://www.kaggle.com/code/inversion/calculating-stable-diffusion-prompt-embeddings)).\n        *   Similarity is robust to paraphrasing (e.g., \"epic cat\" vs. \"majestic kitten\").\n    *   **Submission Format:** Flattened rows of `imgId_eId,val` pairs for each embedding dimension.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Image-to-Text (Multimodal Regression)\n*   **Objective:** Predict the text prompts used to generate images created by Stable Diffusion 2.0. The task involves reversing the typical text-to-image generation process by inferring the original prompt from a given synthetic image.\n    *   **Key Points:**\n        *   Focus on understanding the latent relationship between prompts and generated images.\n        *   Prompts vary in complexity, ranging from simple to multi-object/modifier descriptions.\n        *   Submissions require converting predicted prompts into 384-dimensional embeddings (either directly or via intermediate prompt prediction).",
      "Dataset Overview": "*   **Data Type & Context:** \n    *   **Primary Data:** Synthetic images (512x512 px PNGs) generated from diverse text prompts using Stable Diffusion 2.0 (originally 768x768 px, downsampled).\n    *   **Context:** Images are outputs of a fixed generative process (50 steps, default parameters). Prompts were generated via undisclosed methods.\n*   **Data Files:**\n    *   `images/` - Training/generated images (example set provided; re-run test set contains ~16,000 hidden images).\n    *   `prompts.csv` - Example prompt-image pairs (for illustration only; not usable in test submissions).\n    *   `sample_submission.csv` - Format template with example embeddings.\n*   **Key Features:**\n    *   Images exhibit varied styles (e.g., \"highly detailed, sharp focus, illustration, 3D renders\").\n    *   Prompts contain semantic modifiers (e.g., \"majestic, epic\") and stylistic descriptors.",
      "Evaluation Metrics": "*   **Primary Metric:** Mean cosine similarity between predicted and ground truth prompt embeddings.\n    *   **Components:**\n        *   Embeddings are 384-dimensional vectors derived from prompts (calculation method shown in [reference notebook](https://www.kaggle.com/code/inversion/calculating-stable-diffusion-prompt-embeddings)).\n        *   Similarity is robust to paraphrasing (e.g., \"epic cat\" vs. \"majestic kitten\").\n    *   **Submission Format:** Flattened rows of `imgId_eId,val` pairs for each embedding dimension."
    },
    "file_path": "kaggle_datasets/549/problem_summary.md"
  },
  "582": {
    "problem_id": "582",
    "title": "Lux AI Season 2 - Multi-Agent Resource Management and Strategy",
    "problem_type": "Reinforcement Learning / Multi-Agent Strategy Game",
    "objective": "Design autonomous agents to compete in a 1v1 resource gathering and management simulation on Mars. The core goal is to maximize lichen coverage (victory points) by:",
    "evaluation_metric": null,
    "full_content": "# Lux AI Season 2 - Multi-Agent Resource Management and Strategy\n\n**Problem Description:**\n* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Design autonomous agents to compete in a 1v1 resource gathering and management simulation on Mars. The core goal is to maximize lichen coverage (victory points) by:\n    * Optimizing resource collection (ice/ore) and refinement (water/metal)\n    * Managing robot fleets (light/heavy units with different capabilities)\n    * Strategically expanding lichen colonies while disrupting opponents\n* **Key Points:**\n    * Large-scale decision-making with hundreds of units on 64x64 maps\n    * Real-time strategy elements with simultaneous action execution\n    * Procedurally generated maps requiring adaptive strategies\n    * Complex game mechanics including:\n        * Day/night power cycles\n        * Robot collisions and rubble mechanics\n        * Factory management and lichen growth systems\n\n**Dataset Overview:**\n* **Data Type:** Simulation environment with complete game state observation\n* **Context:** Turn-based strategy game with:\n    * Grid-based world representation (64x64 tiles)\n    * Multiple entity types (robots, factories, resources)\n    * Full observability of game state\n* **Key Features:**\n    * Map features: Ice, ore, rubble, lichen\n    * Unit attributes: Power, cargo, position, action queues\n    * Factory stats: Resource stocks, lichen connections\n* **Environment Files:**\n    * Python starter kit (agent.py, main.py)\n    * Jupyter notebook tutorial\n    * GPU-accelerated Jax version available\n\n**Evaluation Metrics:**\n* **Primary Metric:** Elo-based Skill Rating System\n    * Gaussian N(μ,σ²) model estimating agent strength\n    * μ updates based on match outcomes (win/loss/draw)\n    * σ decreases with more games played (certainty increases)\n* **Match Resolution:**\n    * Victory: Destroy all opponent factories OR have more lichen at game end (1000 turns)\n    * Draw: Equal lichen or simultaneous factory destruction\n    * No score differential considered - pure win/loss outcome",
    "sections": {
      "Problem Description": "* **Problem Type:** Reinforcement Learning / Multi-Agent Strategy Game\n* **Objective:** Design autonomous agents to compete in a 1v1 resource gathering and management simulation on Mars. The core goal is to maximize lichen coverage (victory points) by:\n    * Optimizing resource collection (ice/ore) and refinement (water/metal)\n    * Managing robot fleets (light/heavy units with different capabilities)\n    * Strategically expanding lichen colonies while disrupting opponents\n* **Key Points:**\n    * Large-scale decision-making with hundreds of units on 64x64 maps\n    * Real-time strategy elements with simultaneous action execution\n    * Procedurally generated maps requiring adaptive strategies\n    * Complex game mechanics including:\n        * Day/night power cycles\n        * Robot collisions and rubble mechanics\n        * Factory management and lichen growth systems",
      "Dataset Overview": "* **Data Type:** Simulation environment with complete game state observation\n* **Context:** Turn-based strategy game with:\n    * Grid-based world representation (64x64 tiles)\n    * Multiple entity types (robots, factories, resources)\n    * Full observability of game state\n* **Key Features:**\n    * Map features: Ice, ore, rubble, lichen\n    * Unit attributes: Power, cargo, position, action queues\n    * Factory stats: Resource stocks, lichen connections\n* **Environment Files:**\n    * Python starter kit (agent.py, main.py)\n    * Jupyter notebook tutorial\n    * GPU-accelerated Jax version available",
      "Evaluation Metrics": "* **Primary Metric:** Elo-based Skill Rating System\n    * Gaussian N(μ,σ²) model estimating agent strength\n    * μ updates based on match outcomes (win/loss/draw)\n    * σ decreases with more games played (certainty increases)\n* **Match Resolution:**\n    * Victory: Destroy all opponent factories OR have more lichen at game end (1000 turns)\n    * Draw: Equal lichen or simultaneous factory destruction\n    * No score differential considered - pure win/loss outcome"
    },
    "file_path": "kaggle_datasets/582/problem_summary.md"
  },
  "79": {
    "problem_id": "79",
    "title": "Right Whale Call Detection from Audio Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Right Whale Call Detection from Audio Recordings\n\n## Problem Description\n* **Problem Type:** Binary Classification (Audio Signal Processing)\n* **Objective:** Develop an algorithm to detect North Atlantic Right Whale calls in underwater audio recordings. Participants must classify each audio clip as either containing a whale call (1) or noise (0).\n* **Key Points:**\n  * Focus on archival data recorded using Marine Autonomous Recording Units (MARUs)\n  * Solutions will be ported to a High Performance Computing platform for large-scale data mining\n  * Real-world utility optimization suggested (processing with limited training history)\n\n## Dataset Overview\n* **Data Type:** Audio files (.aif format) from underwater hydrophone recordings\n* **Context:** Marine bioacoustics data collected over several years across multiple ocean basins\n* **Data Files:**\n  * train2.zip: Training audio clips (labeled with _1.aif for whale calls, _0.aif for noise)\n  * test2.zip: Unlabeled test audio clips requiring prediction\n  * sampleSubmission.csv: Example submission file format\n* **Features:**\n  * Raw audio waveforms\n  * Temporal patterns of whale vocalizations\n  * Background ocean noise characteristics\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n* **Metric Implementation:**\n  * Calculated using true labels vs predicted probabilities\n  * Standard implementations provided for:\n    * MATLAB (perfcurve)\n    * R (roc.area)\n    * Python (scikit-learn metrics.roc_curve + metrics.auc)",
    "sections": {},
    "file_path": "kaggle_datasets/79/problem_summary.md"
  },
  "112": {
    "problem_id": "112",
    "title": "Large-Scale Hierarchical Multi-Label Text Classification of Wikipedia Documents",
    "problem_type": "Multi-label Hierarchical Text Classification",
    "objective": "Classify Wikipedia documents into one or more of 325,056 hierarchical categories, where:",
    "evaluation_metric": null,
    "full_content": "# Large-Scale Hierarchical Multi-Label Text Classification of Wikipedia Documents\n\n**Problem Description:**\n* **Problem Type:** Multi-label Hierarchical Text Classification\n* **Objective:** Classify Wikipedia documents into one or more of 325,056 hierarchical categories, where:\n    * The hierarchy is a graph that may contain cycles\n    * Documents can belong to multiple categories (multi-label)\n    * Only leaf nodes of the hierarchy are valid classification targets\n* **Key Points:**\n    * Extremely large-scale classification problem with severe class imbalance\n    * Challenges include data sparsity despite large dataset size\n    * Statistical dependence between hierarchical classes must be considered\n\n**Dataset Overview:**\n* **Data Type & Context:** Text documents from Wikipedia in sparse vector format (libSVM style)\n* **Data Files:**\n    * `train` - Training set (2.4M documents)\n    * `test` - Test set (452,167 documents)\n    * `hierarchy` - Parent-child relationships defining category structure\n    * Remapped versions available for certain ML frameworks\n* **Features:**\n    * Documents represented as sparse feature vectors (term indices with TF weights)\n    * Each feature corresponds to a stemmed word\n    * Multi-label ground truth (multiple category IDs per document)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Macro F1-Score (MaF)\n    * Calculated as harmonic mean of Macro Precision (MaP) and Macro Recall (MaR):\n        * MaP = Average of per-class precision across all classes\n        * MaR = Average of per-class recall across all classes\n        * MaF = 2*(MaP*MaR)/(MaP+MaR)\n    * True positives, false positives and false negatives counted per class\n    * All classes weighted equally regardless of size",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-label Hierarchical Text Classification\n* **Objective:** Classify Wikipedia documents into one or more of 325,056 hierarchical categories, where:\n    * The hierarchy is a graph that may contain cycles\n    * Documents can belong to multiple categories (multi-label)\n    * Only leaf nodes of the hierarchy are valid classification targets\n* **Key Points:**\n    * Extremely large-scale classification problem with severe class imbalance\n    * Challenges include data sparsity despite large dataset size\n    * Statistical dependence between hierarchical classes must be considered",
      "Dataset Overview": "* **Data Type & Context:** Text documents from Wikipedia in sparse vector format (libSVM style)\n* **Data Files:**\n    * `train` - Training set (2.4M documents)\n    * `test` - Test set (452,167 documents)\n    * `hierarchy` - Parent-child relationships defining category structure\n    * Remapped versions available for certain ML frameworks\n* **Features:**\n    * Documents represented as sparse feature vectors (term indices with TF weights)\n    * Each feature corresponds to a stemmed word\n    * Multi-label ground truth (multiple category IDs per document)",
      "Evaluation Metrics": "* **Primary Metric:** Macro F1-Score (MaF)\n    * Calculated as harmonic mean of Macro Precision (MaP) and Macro Recall (MaR):\n        * MaP = Average of per-class precision across all classes\n        * MaR = Average of per-class recall across all classes\n        * MaF = 2*(MaP*MaR)/(MaP+MaR)\n    * True positives, false positives and false negatives counted per class\n    * All classes weighted equally regardless of size"
    },
    "file_path": "kaggle_datasets/112/problem_summary.md"
  },
  "320": {
    "problem_id": "320",
    "title": "Landmark Recognition in Images with Large-Scale Dataset",
    "problem_type": "Computer Vision - Multi-class Classification (with potential null class)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Landmark Recognition in Images with Large-Scale Dataset\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Multi-class Classification (with potential null class)\n* **Objective:**  \n    * Build models to recognize landmarks depicted in images from a large-scale dataset, predicting the correct landmark label (if any exists).\n    * Handle a massive number of classes (>200K landmarks) with potentially limited training examples per class.\n    * Address challenges like diverse image content within a single landmark category (e.g., indoor/outdoor views of a museum) and noisy data.\n* **Key Points:**\n    * Test images may contain zero, one, or multiple landmarks (though training images have exactly one label).\n    * Dataset is raw and noisy, without automated cleaning, unlike previous editions.\n    * Competition is linked to a Landmark Retrieval Challenge, sharing the same test set.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Image data depicting global landmarks (famous and obscure), with associated landmark labels.\n    * Context includes web-mined images with permissive licenses and crowdsourced labels.\n* **Data Files:**  \n    * `train.csv`: Contains image IDs (hashes) and corresponding landmark labels (integers).\n    * `test.csv`: Lists test image IDs (URLs for download in Stage 1; separate download for Stage 2).\n    * `recognition_sample_submission.csv`: Submission format example.\n* **Features:**  \n    * Images vary in content (e.g., building exteriors, indoor artifacts).  \n    * Labels represent unique landmarks (over 200K classes).  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Global Average Precision (GAP) at \\(k=1\\) (micro Average Precision).  \n* **Components:**  \n    * Predictions consist of a landmark label and confidence score per test image.  \n    * GAP is computed over all predictions, sorted by confidence:  \n        * \\(N\\) = total predictions across all queries.  \n        * \\(M\\) = total queries with at least one visible landmark.  \n        * \\(P(i)\\) = precision at rank \\(i\\).  \n        * \\(rel(i)\\) = 1 if prediction \\(i\\) is correct, else 0.  \n    * Formula: \\(GAP = \\frac{1}{M} \\sum_{i=1}^{N} P(i) \\cdot rel(i)\\).",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Multi-class Classification (with potential null class)\n* **Objective:**  \n    * Build models to recognize landmarks depicted in images from a large-scale dataset, predicting the correct landmark label (if any exists).\n    * Handle a massive number of classes (>200K landmarks) with potentially limited training examples per class.\n    * Address challenges like diverse image content within a single landmark category (e.g., indoor/outdoor views of a museum) and noisy data.\n* **Key Points:**\n    * Test images may contain zero, one, or multiple landmarks (though training images have exactly one label).\n    * Dataset is raw and noisy, without automated cleaning, unlike previous editions.\n    * Competition is linked to a Landmark Retrieval Challenge, sharing the same test set.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Image data depicting global landmarks (famous and obscure), with associated landmark labels.\n    * Context includes web-mined images with permissive licenses and crowdsourced labels.\n* **Data Files:**  \n    * `train.csv`: Contains image IDs (hashes) and corresponding landmark labels (integers).\n    * `test.csv`: Lists test image IDs (URLs for download in Stage 1; separate download for Stage 2).\n    * `recognition_sample_submission.csv`: Submission format example.\n* **Features:**  \n    * Images vary in content (e.g., building exteriors, indoor artifacts).  \n    * Labels represent unique landmarks (over 200K classes).",
      "Evaluation Metrics": "* **Primary Metric:** Global Average Precision (GAP) at \\(k=1\\) (micro Average Precision).  \n* **Components:**  \n    * Predictions consist of a landmark label and confidence score per test image.  \n    * GAP is computed over all predictions, sorted by confidence:  \n        * \\(N\\) = total predictions across all queries.  \n        * \\(M\\) = total queries with at least one visible landmark.  \n        * \\(P(i)\\) = precision at rank \\(i\\).  \n        * \\(rel(i)\\) = 1 if prediction \\(i\\) is correct, else 0.  \n    * Formula: \\(GAP = \\frac{1}{M} \\sum_{i=1}^{N} P(i) \\cdot rel(i)\\)."
    },
    "file_path": "kaggle_datasets/320/problem_summary.md"
  },
  "576": {
    "problem_id": "576",
    "title": "Bengali Speech Recognition from Out-of-Distribution Audio",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Bengali Speech Recognition from Out-of-Distribution Audio\n\n## Problem Description\n* **Problem Type**: NLP - Automatic Speech Recognition (Speech-to-Text)\n* **Objective**: \n    * Build a model to transcribe Bengali speech from audio recordings that are out-of-distribution compared to the training data\n    * Handle diverse dialects and prosodic features of Bengali speech across different domains\n* **Key Points**:\n    * Focus on generalization to unseen domains (17 test domains not present in training)\n    * Address challenges in low-resource language processing (Bengali has limited open-source speech recognition models)\n    * Handle variations in speech patterns (e.g., religious sermons vs regular speech)\n\n## Dataset Overview\n* **Data Type**: Audio recordings (MP3) with text transcriptions\n* **Context**: \n    * 1,200 hours of Bengali speech from ~24,000 speakers across India and Bangladesh\n    * Intentionally includes domain shifts to test out-of-distribution generalization\n* **Data Files**:\n    * `train/`: MP3 audio files for training\n    * `test/`: MP3 audio files for testing (out-of-distribution domains)\n    * `train.csv`: Contains `id`, `sentence` (transcription), and `split` (train/valid) columns\n    * `sample_submission.csv`: Submission template\n* **Features**:\n    * Audio files recorded at 32k sample rate, 48k bit rate, mono channel\n    * Text transcriptions normalized using bnUnicodeNormalizer\n    * Valid split contains manually reviewed/corrected transcriptions\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Word Error Rate (WER)\n* **Calculation Process**:\n    1. Compute WER for each test instance\n    2. Average WERs within domains (weighted by number of words)\n    3. Take unweighted mean of domain averages as final score\n* **Implementation**: Uses `jiwer` library for WER calculation\n* **Special Considerations**:\n    * Domain-wise evaluation ensures balanced performance across different speech types\n    * Weighting by sentence length gives more importance to longer transcriptions",
    "sections": {},
    "file_path": "kaggle_datasets/576/problem_summary.md"
  },
  "318": {
    "problem_id": "318",
    "title": "Binary Classification with High-Dimensional Tabular Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with High-Dimensional Tabular Data\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict a binary target variable using a high-dimensional dataset (20,000 rows × 300 features) while avoiding overfitting, given only 250 training samples.\n* **Key Points:**\n  * Extreme dimensionality challenge: 300 continuous features vs. only 250 training samples.\n  * Primary focus is on developing techniques to prevent overfitting.\n  * Follow-up to a previous competition with similar constraints.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data consisting of continuous variables with unknown real-world context (features are anonymized).\n* **Data Files:**\n  * `train.csv` (250 samples)\n  * `test.csv` (19,750 samples)\n  * `sample_submission.csv`\n* **Features:**\n  * `id`: Sample identifier\n  * `target`: Binary target variable (nature/meaning not specified)\n  * `0-299`: 300 continuous-valued features (anonymized)\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC-ROC)\n  * Measures model's ability to distinguish between positive and negative classes\n  * Evaluates performance across all classification thresholds\n  * Robust to class imbalance\n  * Ranges from 0 to 1 (higher is better)",
    "sections": {},
    "file_path": "kaggle_datasets/318/problem_summary.md"
  },
  "41": {
    "problem_id": "41",
    "title": "Predicting Music Track Ratings from Listener Profiles and Word Associations",
    "problem_type": "Regression",
    "objective": "Predict how much a listener will like a particular music track based on their demographics, music preferences, and word associations describing artists.",
    "evaluation_metric": null,
    "full_content": "# Predicting Music Track Ratings from Listener Profiles and Word Associations\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict how much a listener will like a particular music track based on their demographics, music preferences, and word associations describing artists.\n    * **Key Points:**\n        * Combines multiple data sources: user demographics, track/artist ratings, music preference questions, and descriptive word associations.\n        * Focuses on predicting granular track-level ratings (not just artist-level preferences).\n        * Uses UK-specific market research data from EMI's One Million Interview Dataset.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with mixed numerical/categorical features and text-derived word associations.\n* **Data Files:**\n    * `train.csv`/`test.csv`: Contains artist, track, user IDs, and ratings (train only).\n    * `words.csv`: Word associations (82 binary columns) and artist preference metrics.\n    * `users.csv`/`UserKey.csv`: Demographic and music habit data (age, region, 19 scaled music attitude questions).\n* **Key Features:**\n    * User demographics (age, gender, working status, UK region).\n    * Music engagement metrics (hours listening, importance of music).\n    * 19 scaled questions about music discovery habits and attitudes.\n    * 82 binary word associations (e.g., \"Soulful\", \"Cheesy\") describing artists.\n    * Artist/track identifiers and temporal research wave information.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Error (RMSE).\n    * Measures deviation between predicted and actual track ratings (scale: X-100).\n    * Penalizes larger errors more heavily due to squaring.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict how much a listener will like a particular music track based on their demographics, music preferences, and word associations describing artists.\n    * **Key Points:**\n        * Combines multiple data sources: user demographics, track/artist ratings, music preference questions, and descriptive word associations.\n        * Focuses on predicting granular track-level ratings (not just artist-level preferences).\n        * Uses UK-specific market research data from EMI's One Million Interview Dataset.",
      "Dataset Overview": "* **Data Type:** Tabular data with mixed numerical/categorical features and text-derived word associations.\n* **Data Files:**\n    * `train.csv`/`test.csv`: Contains artist, track, user IDs, and ratings (train only).\n    * `words.csv`: Word associations (82 binary columns) and artist preference metrics.\n    * `users.csv`/`UserKey.csv`: Demographic and music habit data (age, region, 19 scaled music attitude questions).\n* **Key Features:**\n    * User demographics (age, gender, working status, UK region).\n    * Music engagement metrics (hours listening, importance of music).\n    * 19 scaled questions about music discovery habits and attitudes.\n    * 82 binary word associations (e.g., \"Soulful\", \"Cheesy\") describing artists.\n    * Artist/track identifiers and temporal research wave information.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Error (RMSE).\n    * Measures deviation between predicted and actual track ratings (scale: X-100).\n    * Penalizes larger errors more heavily due to squaring."
    },
    "file_path": "kaggle_datasets/41/problem_summary.md"
  },
  "571": {
    "problem_id": "571",
    "title": "Data-Centric Random Forest Regression with Synthetic Tabular Data",
    "problem_type": "Regression (Data-Centric Optimization)",
    "objective": "Participants must improve a synthetic dataset that will be used to train a fixed random forest regressor. The goal is to optimize the dataset itself (rather than the model) to minimize prediction errors on a hidden test set.",
    "evaluation_metric": null,
    "full_content": "# Data-Centric Random Forest Regression with Synthetic Tabular Data\n\n**Problem Description:**\n* **Problem Type:** Regression (Data-Centric Optimization)\n* **Objective:** Participants must improve a synthetic dataset that will be used to train a fixed random forest regressor. The goal is to optimize the dataset itself (rather than the model) to minimize prediction errors on a hidden test set.\n* **Key Points:**\n  * The competition focuses on data-centric improvements rather than model architecture changes.\n  * The model architecture is fixed (Random Forest with specified hyperparameters).\n  * Submissions must be datasets (not predictions) that adhere to strict formatting rules.\n\n**Dataset Overview:**\n* **Data Type & Context:** Synthetic tabular data based on dissolved oxygen prediction in river water (derived from real-world data).\n* **Data Files:**\n  * `sample_submission.csv` (example submission file with required format)\n* **Features:**\n  * Contains anonymized numerical features (e.g., O2_1, O2_2, etc.) and a target column.\n  * Original dataset may be used for reference/improvement.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Error (RMSE)\n  * The submitted dataset is used to train a fixed random forest model.\n  * This model makes predictions on a hidden test set.\n  * RMSE is calculated between predictions and ground truth test labels.\n* **Submission Constraints:**\n  * Must maintain all columns from sample submission.\n  * No NaN values allowed.\n  * May have fewer (but not more) rows than sample submission.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Data-Centric Optimization)\n* **Objective:** Participants must improve a synthetic dataset that will be used to train a fixed random forest regressor. The goal is to optimize the dataset itself (rather than the model) to minimize prediction errors on a hidden test set.\n* **Key Points:**\n  * The competition focuses on data-centric improvements rather than model architecture changes.\n  * The model architecture is fixed (Random Forest with specified hyperparameters).\n  * Submissions must be datasets (not predictions) that adhere to strict formatting rules.",
      "Dataset Overview": "* **Data Type & Context:** Synthetic tabular data based on dissolved oxygen prediction in river water (derived from real-world data).\n* **Data Files:**\n  * `sample_submission.csv` (example submission file with required format)\n* **Features:**\n  * Contains anonymized numerical features (e.g., O2_1, O2_2, etc.) and a target column.\n  * Original dataset may be used for reference/improvement.",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Error (RMSE)\n  * The submitted dataset is used to train a fixed random forest model.\n  * This model makes predictions on a hidden test set.\n  * RMSE is calculated between predictions and ground truth test labels.\n* **Submission Constraints:**\n  * Must maintain all columns from sample submission.\n  * No NaN values allowed.\n  * May have fewer (but not more) rows than sample submission."
    },
    "file_path": "kaggle_datasets/571/problem_summary.md"
  },
  "327": {
    "problem_id": "327",
    "title": "Multi-label Audio Tagging with Noisy and Curated Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Audio Tagging with Noisy and Curated Data\n\n## Problem Description\n* **Problem Type:** Multi-label Audio Classification\n* **Objective:** Develop a general-purpose audio tagging system that can automatically recognize and tag sounds from 80 diverse categories, using a combination of manually-labeled (curated) and noisy web-sourced audio data.\n    * **Key Points:**\n        * Must handle multi-label classification (some clips have multiple sound tags)\n        * Dataset contains two distinct subsets: a small curated set with clean labels and a larger noisy set with automatically-generated labels\n        * Potential domain mismatch between training (mostly web audio) and test (manually-labeled) data\n        * Audio clips have variable lengths (0.3s to 30s)\n\n## Dataset Overview\n* **Data Type:** Audio files (uncompressed PCM 16 bit, 44.1 kHz, mono) with clip-level multi-label tags\n* **Context:** Sounds from Freesound (manually labeled) and Flickr videos (noisy labels) covering 80 categories from AudioSet Ontology\n* **Data Files:**\n    * `train_curated.csv` + `train_curated.zip` (4,970 clips, 10.5 hours)\n    * `train_noisy.csv` + `train_noisy.zip` (19,815 clips, ~80 hours)\n    * `test.zip` (public leaderboard test set)\n    * `sample_submission.csv` (format example)\n* **Features:**\n    * Audio waveforms (variable length clips)\n    * 80 possible labels per clip (e.g., \"Guitar\", \"Water\", \"Human voice\", \"Motor vehicle\")\n\n## Evaluation Metrics\n* **Primary Metric:** Label-weighted label-ranking average precision (lwlrap)\n    * **Components:**\n        * Measures precision of retrieving a ranked list of relevant labels for each test clip\n        * Generalization of mean reciprocal rank for multi-label cases\n        * Label-weighted version gives equal weight to each label (not each test item)\n        * Implemented as average of per-class metrics weighted by label frequency",
    "sections": {},
    "file_path": "kaggle_datasets/327/problem_summary.md"
  },
  "115": {
    "problem_id": "115",
    "title": "Reconstructing Neural Connectivity from Fluorescence Imaging Data",
    "problem_type": "Binary Classification (with confidence scores)",
    "objective": "Reconstruct the directed wiring (connectome) between neurons in a living neural network using time series data from fluorescence imaging of neural activity. Participants must predict the presence or absence of connections between neuron pairs, along with a confidence score.",
    "evaluation_metric": null,
    "full_content": "# Reconstructing Neural Connectivity from Fluorescence Imaging Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (with confidence scores)\n* **Objective:** Reconstruct the directed wiring (connectome) between neurons in a living neural network using time series data from fluorescence imaging of neural activity. Participants must predict the presence or absence of connections between neuron pairs, along with a confidence score.\n    * **Key Points:**\n        * Focus on recovering network structure from simulated calcium fluorescence imaging data.\n        * Connections are directional (neuron I → neuron J).\n        * Includes spatial coordinates of neurons to account for physical arrangement.\n        * Simulates real-world challenges like light scattering effects and varying signal-to-noise ratios.\n\n**Dataset Overview:**\n* **Data Type & Context:** Time series fluorescence data from simulated neuronal cultures (100-1000 neurons), with accompanying spatial coordinates.\n* **Data Files:**\n    * Primary competition files: `validation.tgz`, `test.tgz` (each containing 1000-neuron networks)\n    * Training/experimental files: Multiple archives (`small.tgz`, `normal-1.tgz` to `normal-4.tgz`, etc.) with varying network properties\n    * File types:\n        * Fluorescence (F): Time series of neural activity (20ms intervals)\n        * NetworkPosition (P): X/Y coordinates of neurons\n        * Network (N): Ground truth connections (only in training data)\n* **Key Features:**\n    * Fluorescence signals representing neural activity over time\n    * Spatial coordinates of neurons in a 1mm² area\n    * Varied network architectures (different clustering coefficients, connection densities, and noise levels)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Implementation Details:**\n        * Treated as binary classification: 1=connection exists, 0=no connection\n        * Participants submit confidence scores (0-1) for each potential connection\n        * AUC calculated using trapezoid method by:\n            1. Varying classification thresholds on confidence scores\n            2. Plotting True Positive Rate (tp/pos) vs False Positive Rate (fp/neg)\n            3. Computing area under the resulting curve\n    * **Submission Format:**\n        * Two columns: Connection ID (format `NET_neuronI_neuronJ`) and Strength (confidence score 0-1)\n        * Separate predictions required for",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (with confidence scores)\n* **Objective:** Reconstruct the directed wiring (connectome) between neurons in a living neural network using time series data from fluorescence imaging of neural activity. Participants must predict the presence or absence of connections between neuron pairs, along with a confidence score.\n    * **Key Points:**\n        * Focus on recovering network structure from simulated calcium fluorescence imaging data.\n        * Connections are directional (neuron I → neuron J).\n        * Includes spatial coordinates of neurons to account for physical arrangement.\n        * Simulates real-world challenges like light scattering effects and varying signal-to-noise ratios.",
      "Dataset Overview": "* **Data Type & Context:** Time series fluorescence data from simulated neuronal cultures (100-1000 neurons), with accompanying spatial coordinates.\n* **Data Files:**\n    * Primary competition files: `validation.tgz`, `test.tgz` (each containing 1000-neuron networks)\n    * Training/experimental files: Multiple archives (`small.tgz`, `normal-1.tgz` to `normal-4.tgz`, etc.) with varying network properties\n    * File types:\n        * Fluorescence (F): Time series of neural activity (20ms intervals)\n        * NetworkPosition (P): X/Y coordinates of neurons\n        * Network (N): Ground truth connections (only in training data)\n* **Key Features:**\n    * Fluorescence signals representing neural activity over time\n    * Spatial coordinates of neurons in a 1mm² area\n    * Varied network architectures (different clustering coefficients, connection densities, and noise levels)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under ROC Curve (AUC)\n    * **Implementation Details:**\n        * Treated as binary classification: 1=connection exists, 0=no connection\n        * Participants submit confidence scores (0-1) for each potential connection\n        * AUC calculated using trapezoid method by:\n            1. Varying classification thresholds on confidence scores\n            2. Plotting True Positive Rate (tp/pos) vs False Positive Rate (fp/neg)\n            3. Computing area under the resulting curve\n    * **Submission Format:**\n        * Two columns: Connection ID (format `NET_neuronI_neuronJ`) and Strength (confidence score 0-1)\n        * Separate predictions required for"
    },
    "file_path": "kaggle_datasets/115/problem_summary.md"
  },
  "585": {
    "problem_id": "585",
    "title": "Regression with a Mohs Hardness Dataset",
    "problem_type": "Regression",
    "objective": "Predict the Mohs hardness of a mineral based on its properties. The goal is to build a regression model that accurately estimates the continuous target variable (`Hardness`).",
    "evaluation_metric": null,
    "full_content": "# Regression with a Mohs Hardness Dataset\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the Mohs hardness of a mineral based on its properties. The goal is to build a regression model that accurately estimates the continuous target variable (`Hardness`).\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world mineral hardness data.\n  * Participants are encouraged to explore the original dataset for potential improvements in model performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing mineral properties and their corresponding Mohs hardness values.\n* **Data Files:**\n  * `train.csv`: Training dataset with the target variable `Hardness`.\n  * `test.csv`: Test dataset for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:** The dataset includes various mineral properties (27 columns in total), though specific features are not detailed. The target variable is `Hardness`, a continuous measure of mineral hardness on the Mohs scale.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Median Absolute Error (MedAE)\n* **Components:**\n  * MedAE is calculated as the median of the absolute differences between the predicted and actual values.\n  * Formula: `MedAE(y, ŷ) = median(|y₁ - ŷ₁|, ..., |yₙ - ŷₙ|)`, where `ŷ` is the predicted value and `y` is the ground truth.",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the Mohs hardness of a mineral based on its properties. The goal is to build a regression model that accurately estimates the continuous target variable (`Hardness`).\n* **Key Points:**\n  * The dataset is synthetically generated from a deep learning model trained on real-world mineral hardness data.\n  * Participants are encouraged to explore the original dataset for potential improvements in model performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing mineral properties and their corresponding Mohs hardness values.\n* **Data Files:**\n  * `train.csv`: Training dataset with the target variable `Hardness`.\n  * `test.csv`: Test dataset for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in the required format.\n* **Features:** The dataset includes various mineral properties (27 columns in total), though specific features are not detailed. The target variable is `Hardness`, a continuous measure of mineral hardness on the Mohs scale.",
      "Evaluation Metrics": "* **Evaluation Metric:** Median Absolute Error (MedAE)\n* **Components:**\n  * MedAE is calculated as the median of the absolute differences between the predicted and actual values.\n  * Formula: `MedAE(y, ŷ) = median(|y₁ - ŷ₁|, ..., |yₙ - ŷₙ|)`, where `ŷ` is the predicted value and `y` is the ground truth."
    },
    "file_path": "kaggle_datasets/585/problem_summary.md"
  },
  "83": {
    "problem_id": "83",
    "title": "Predicting Employee Access Needs Based on Job Role",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Employee Access Needs Based on Job Role\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict whether an employee should be granted access to a specific resource based on their job role attributes, aiming to minimize manual access transactions (grants/revokes) as employees' roles change over time.\n    - **Key Points:**\n        - Model must learn from historical access approval/denial data.\n        - Predictions should automate access provisioning, reducing administrative overhead.\n        - Input consists of employee role information + resource code; output is binary access decision.\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing employee role attributes and historical access decisions at Amazon (2010-2011).\n- **Data Files:**\n    - `train.csv`: Contains ACTION (target), RESOURCE ID, and 8 role-related features.\n    - `test.csv`: Same structure as train (without ACTION) for prediction.\n- **Key Features:**\n    - Hierarchical role attributes (ROLLUP_1, ROLLUP_2, DEPTNAME, FAMILY_DESC)\n    - Manager ID (MGR_ID) and unique role codes (ROLE_CODE)\n    - Resource IDs indicating specific access privileges\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the ROC Curve (AUC)\n    - **Calculation:**\n        - Measures model's ability to discriminate between approved (1) and denied (0) access.\n        - Evaluates performance across all classification thresholds.\n        - Implementations provided for Matlab (perfcurve), R (roc.area), and Python (sklearn.metrics.roc_curve + auc).",
    "sections": {},
    "file_path": "kaggle_datasets/83/problem_summary.md"
  },
  "578": {
    "problem_id": "578",
    "title": "LLM Prompt Design for Real-World Applications",
    "problem_type": "NLP - Prompt Engineering",
    "objective": "Design practical, creative, and innovative prompts for Large Language Models (LLMs) using Google's MakerSuite tool. The goal is to demonstrate real-world use-cases across diverse categories, with winning prompts integrated into MakerSuite.",
    "evaluation_metric": null,
    "full_content": "# LLM Prompt Design for Real-World Applications\n\n**Problem Description:**\n* **Problem Type:** NLP - Prompt Engineering  \n* **Objective:** Design practical, creative, and innovative prompts for Large Language Models (LLMs) using Google's MakerSuite tool. The goal is to demonstrate real-world use-cases across diverse categories, with winning prompts integrated into MakerSuite.  \n* **Key Points:**  \n  * Focus on utility and innovation across specified domains:  \n    * Developer tools (code generation, debugging, etc.)  \n    * Data science tools (project guidance, code assistance)  \n    * Education/interactive tutors (academic subjects or practical skills)  \n    * Everyday utilities (spreadsheets, presentations, tech support)  \n    * Explanatory reasoning (model justification of answers)  \n    * Storytelling/games (interactive narrative creation)  \n    * Open category for other creative applications  \n  * Submissions evaluated by Google judges (no public leaderboard)  \n\n**Dataset Overview:**  \n* **Data Type:** Not applicable (prompt engineering competition)  \n* **Data Files:**  \n  * `prompt_categories.md` (category descriptions)  \n  * `submission_instructions.md` (submission guidelines)  \n* **Features:** N/A (Participants create prompts without provided datasets)  \n\n**Evaluation Metrics:**  \n* **Evaluation Rubric (0-10pts per criterion):**  \n  * **Practicality:** Solves a real-world problem  \n  * **Usefulness:** Produces high-quality/helpful outputs  \n  * **Robustness:** Works across diverse inputs  \n  * **Novelty:** Unique/creative approach  \n  * **Delightfulness:** Impressive, share-worthy results  \n* **Judging:** Manual evaluation by Google/Kaggle panel with no quantitative metric.",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Prompt Engineering  \n* **Objective:** Design practical, creative, and innovative prompts for Large Language Models (LLMs) using Google's MakerSuite tool. The goal is to demonstrate real-world use-cases across diverse categories, with winning prompts integrated into MakerSuite.  \n* **Key Points:**  \n  * Focus on utility and innovation across specified domains:  \n    * Developer tools (code generation, debugging, etc.)  \n    * Data science tools (project guidance, code assistance)  \n    * Education/interactive tutors (academic subjects or practical skills)  \n    * Everyday utilities (spreadsheets, presentations, tech support)  \n    * Explanatory reasoning (model justification of answers)  \n    * Storytelling/games (interactive narrative creation)  \n    * Open category for other creative applications  \n  * Submissions evaluated by Google judges (no public leaderboard)  \n\n**Dataset Overview:**  \n* **Data Type:** Not applicable (prompt engineering competition)  \n* **Data Files:**  \n  * `prompt_categories.md` (category descriptions)  \n  * `submission_instructions.md` (submission guidelines)  \n* **Features:** N/A (Participants create prompts without provided datasets)  \n\n**Evaluation Metrics:**  \n* **Evaluation Rubric (0-10pts per criterion):**  \n  * **Practicality:** Solves a real-world problem  \n  * **Usefulness:** Produces high-quality/helpful outputs  \n  * **Robustness:** Works across diverse inputs  \n  * **Novelty:** Unique/creative approach  \n  * **Delightfulness:** Impressive, share-worthy results  \n* **Judging:** Manual evaluation by Google/Kaggle panel with no quantitative metric."
    },
    "file_path": "kaggle_datasets/578/problem_summary.md"
  },
  "77": {
    "problem_id": "77",
    "title": "Facial Expression Recognition from Images",
    "problem_type": "Multi-class Classification (Computer Vision)",
    "objective": "Classify facial expressions in images into one of seven emotion categories (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral).",
    "evaluation_metric": null,
    "full_content": "# Facial Expression Recognition from Images\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Computer Vision)\n* **Objective:** Classify facial expressions in images into one of seven emotion categories (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral).\n* **Key Points:**\n  * Focus on representation learning methods, though not explicitly required.\n  * Dataset is newly introduced, emphasizing generalizability to new data.\n  * Strict rules against manual labeling of test data (final test set released only 72 hours before competition close).\n\n**Dataset Overview:**\n* **Data Type & Context:** Grayscale facial images (48x48 pixels) with centered, registered faces.\n* **Data Files:**\n  * `train.csv`: Contains \"emotion\" (0-6 label) and \"pixels\" (space-separated values in row-major order).\n  * `test.csv`: Contains only \"pixels\" column for prediction.\n  * `example_submission.csv`, `fer2013.tar.gz`, `icml_face_data.csv` (auxiliary files).\n* **Features:**\n  * Single-channel pixel values (0-255) representing facial expressions.\n  * 28,709 training examples, 3,589 public test examples, and 3,589 final test examples.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Accuracy (percentage of correctly classified emotions).\n* **Scoring Protocol:**\n  * Public leaderboard initially uses a provisional test set.\n  * Final evaluation uses a separate test set released 72 hours before competition end to prevent manual labeling.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Computer Vision)\n* **Objective:** Classify facial expressions in images into one of seven emotion categories (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral).\n* **Key Points:**\n  * Focus on representation learning methods, though not explicitly required.\n  * Dataset is newly introduced, emphasizing generalizability to new data.\n  * Strict rules against manual labeling of test data (final test set released only 72 hours before competition close).",
      "Dataset Overview": "* **Data Type & Context:** Grayscale facial images (48x48 pixels) with centered, registered faces.\n* **Data Files:**\n  * `train.csv`: Contains \"emotion\" (0-6 label) and \"pixels\" (space-separated values in row-major order).\n  * `test.csv`: Contains only \"pixels\" column for prediction.\n  * `example_submission.csv`, `fer2013.tar.gz`, `icml_face_data.csv` (auxiliary files).\n* **Features:**\n  * Single-channel pixel values (0-255) representing facial expressions.\n  * 28,709 training examples, 3,589 public test examples, and 3,589 final test examples.",
      "Evaluation Metrics": "* **Primary Metric:** Accuracy (percentage of correctly classified emotions).\n* **Scoring Protocol:**\n  * Public leaderboard initially uses a provisional test set.\n  * Final evaluation uses a separate test set released 72 hours before competition end to prevent manual labeling."
    },
    "file_path": "kaggle_datasets/77/problem_summary.md"
  },
  "311": {
    "problem_id": "311",
    "title": "Predicting NCAA March Madness Outcomes with Historical Basketball Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting NCAA March Madness Outcomes with Historical Basketball Data\n\n## Problem Description\n- **Problem Type**: Binary Classification (with probabilistic outputs)\n- **Objective**: Predict the probability of one team beating another in NCAA Division I Men's Basketball Tournament (\"March Madness\") games. Participants must forecast outcomes for all possible matchups in the tournament.\n- **Key Points**:\n  - Two-stage competition: \n    - Stage 1: Model building using historical tournament data (2014-2018)\n    - Stage 2: Forecasting actual 2019 tournament matchups\n  - Must predict every possible matchup between tournament teams (2,278 predictions for 68 teams)\n  - Predictions must be bounded away from 0% and 100% to avoid infinite log loss penalties\n\n## Dataset Overview\n- **Data Type**: Tabular data with game results, team statistics, and rankings\n- **Context**: NCAA basketball games from 1985-2019, including regular season, conference tournaments, and NCAA tournaments\n- **Key Data Files**:\n  - Teams.csv (team IDs and names)\n  - Seasons.csv (season metadata)\n  - RegularSeasonCompactResults.csv (game outcomes)\n  - NCAATourneyCompactResults.csv (tournament outcomes)\n  - RegularSeasonDetailedResults.csv (team box scores)\n  - MasseyOrdinals.csv (weekly team rankings)\n  - PlayByPlay files (event logs for recent seasons)\n- **Important Features**:\n  - Game outcomes (win/loss, scores)\n  - Team statistics (field goals, rebounds, assists, etc.)\n  - Tournament seeds and brackets\n  - Team rankings from multiple systems\n  - Play-by-play data (for recent seasons)\n\n## Evaluation Metrics\n- **Primary Metric**: Log Loss (binary cross-entropy)\n- **Calculation**:\n  ```\n  LogLoss = −1/n * Σ[y_i*log(ŷ_i) + (1−y_i)*log(1−ŷ_i)]\n  ```\n  Where:\n  - n = number of games\n  - ŷ_i = predicted probability of team 1 beating team 2\n  - y_i = 1 if team 1 wins, 0 if team 2 wins\n- **Key Properties**:\n  - Heavily penalizes confident incorrect predictions\n  - Predictions are bounded away from 0 and 1 to avoid infinite penalties\n  - All games weighted equally",
    "sections": {},
    "file_path": "kaggle_datasets/311/problem_summary.md"
  },
  "123": {
    "problem_id": "123",
    "title": "Seizure Detection in Intracranial EEG Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Seizure Detection in Intracranial EEG Recordings\n\n## Problem Description\n- **Problem Type:** Binary Classification (Seizure vs. Non-Seizure) with Time-Sensitive Detection\n- **Objective:** Develop an algorithm to detect seizures in intracranial EEG recordings with two key goals:\n  - High sensitivity & specificity for automated seizure diaries\n  - Early detection capability (within first 15 seconds) for responsive neurostimulation therapy\n- **Key Points:**\n  - Must address both seizure presence classification and early onset detection\n  - Current commercial solutions have high false positive rates\n  - Solution should assist both clinical care and basic epilepsy research\n\n## Dataset Overview\n- **Data Type:** Time-series EEG recordings (intracranial)\n- **Context:** \n  - Human patients with drug-resistant epilepsy undergoing surgical evaluation\n  - Canine subjects with naturally occurring epilepsy\n  - Recordings from implanted electrodes with varying configurations\n- **Data Files:**\n  - Training: `ictal_segment_N.mat` (seizure), `interictal_segment_N.mat` (non-seizure)\n  - Testing: `test_segment_N.mat`\n- **Key Features:**\n  - Multi-electrode voltage time series (400Hz-5000Hz sampling)\n  - 1-second clips with seizure latency annotations (ictal only)\n  - Variable electrode counts and placements across subjects\n\n## Evaluation Metrics\n- **Primary Metric:** Mean of two AUC scores:\n  1. AUC for seizure detection (binary classification)\n  2. AUC for early detection (within first 15 seconds of seizure)\n- **Calculation:** \n  - Final score = 1/2 × (AUC_seizure + AUC_early)\n  - Early detection clips are double-weighted in evaluation\n  - ROC curves evaluated across all test segments",
    "sections": {},
    "file_path": "kaggle_datasets/123/problem_summary.md"
  },
  "547": {
    "problem_id": "547",
    "title": "Multiclass Classification of Vector-Borne Diseases",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multiclass Classification of Vector-Borne Diseases\n\n## Problem Description\n* **Problem Type:** Multiclass Classification\n* **Objective:** Predict the correct vector-borne disease prognosis based on tabular medical data. Participants must classify each case into one of several possible disease categories.\n    * **Key Points:**\n        * The dataset is synthetically generated from real-world data to balance realism with competition integrity.\n        * Prognosis labels in the original data containing spaces were converted to underscores (e.g., \"West Nile fever\" → \"West_Nile_fever\").\n        * Submissions require predicting up to 3 possible prognoses per case, ordered by likelihood.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing medical features related to vector-borne disease cases.\n* **Data Files:**\n    * `train.csv` - Contains features and target prognosis labels\n    * `test.csv` - Contains features for making predictions\n    * `sample_submission.csv` - Demonstrates submission format\n* **Features:** \n    * 132 anonymized features (medical/clinical measurements)\n    * Target column: `prognosis` (disease classification)\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Precision at K (MPA@3)\n    * **Components:**\n        * Each submission can contain up to 3 predicted classes per case\n        * Predictions are ordered by confidence (most likely first)\n        * Score rewards correct predictions appearing earlier in the list\n        * Calculation: Average precision across all test cases, where precision at K considers whether the correct class appears in the top K predictions",
    "sections": {},
    "file_path": "kaggle_datasets/547/problem_summary.md"
  },
  "48": {
    "problem_id": "48",
    "title": "Product Mention Identification and Disambiguation in User-Generated Web Content",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Product Mention Identification and Disambiguation in User-Generated Web Content\n\n## Problem Description\n* **Problem Type**:  \n  * NLP - Named Entity Recognition (NER) & Entity Disambiguation\n* **Objective**:  \n  * Automatically identify all mentions of consumer products in user-generated web content (blogs, forums, reviews) and correctly link them to specific products in a large catalog (15M+ products).\n  * Key sub-tasks:\n    * Token-level identification of product mentions in noisy text\n    * Cross-referencing mentions with a structured product catalog\n    * Handling ambiguity (e.g., \"iPhone\" could refer to multiple models)\n* **Key Points**:\n  * Focus on consumer electronics (CE) and automotive (AU) verticals\n  * Text data is pre-tokenized with imperfect sentence/paragraph segmentation\n  * Requires handling of colloquial product names (e.g., \"ifone\" for iPhone)\n\n## Dataset Overview\n* **Data Type**:  \n  * JSON/CSV files containing:\n    * Tokenized web text (user-generated content)\n    * Structured product catalog entries\n    * Manually annotated product mention mappings\n* **Data Files**:\n  * `training-annotated-text.json` (annotated text samples)\n  * `training-disambiguated-product-mentions.csv` (ground truth mappings)\n  * `products.json` (15M+ product catalog)\n  * `training-non-annotated-text.json` (additional unlabeled text)\n  * `leaderboard-text.json` (validation set)\n  * `evaluation-text.json` (final test set, released later)\n* **Key Features**:\n  * Text items: Tokenized web content with sentence/paragraph markers (~~, \n\n)\n  * Products: Semi-structured records with ID, name, category (CE/AU), and price\n  * Mentions: Token-span annotations linked to product IDs\n\n## Evaluation Metrics\n* **Primary Metric**:  \n  * **F1 Score** (harmonic mean of precision and recall) calculated over:\n    * Token spans of predicted vs. actual product mentions\n    * Correctness of product ID disambiguation\n* **Scoring Components**:\n  1. True Positives (TP): Correct span + correct product ID(s)\n  2. False Positives (FP): \n     * Predicted span not in ground truth\n     * Correct span but wrong product ID(s)\n  3.",
    "sections": {},
    "file_path": "kaggle_datasets/48/problem_summary.md"
  },
  "70": {
    "problem_id": "70",
    "title": "Predicting Social Media Influence from Pairwise Comparisons",
    "problem_type": "Binary Classification (Preference Learning)",
    "objective": "Predict human judgments about which individual in a pair is more influential on social media (Twitter) based on pre-computed features.",
    "evaluation_metric": null,
    "full_content": "# Predicting Social Media Influence from Pairwise Comparisons\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Preference Learning)\n* **Objective:** Predict human judgments about which individual in a pair is more influential on social media (Twitter) based on pre-computed features.\n    * **Key Points:**\n        * Each data point compares two individuals (A vs. B) using 11 standardized Twitter activity features.\n        * Binary label: '1' if A is more influential than B, '0' otherwise.\n        * Focus on replicating human judgment of influence, not defining influence algorithmically.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing pairwise comparisons of Twitter users' influence.\n    * **Data Files:**\n        * `train.csv`: Contains feature pairs and human-judged labels.\n        * `test.csv`: Feature pairs for which predictions are required.\n        * `sample_predictions.csv`: Example submission format (ID and predicted label).\n    * **Features:** 11 non-negative numeric Twitter activity metrics per individual (e.g., interaction volume, follower count), standardized but not explicitly named.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to rank influential individuals correctly across all classification thresholds.\n        * Implementations provided in R (ROCR package) and Python (scikit-learn).",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Preference Learning)\n* **Objective:** Predict human judgments about which individual in a pair is more influential on social media (Twitter) based on pre-computed features.\n    * **Key Points:**\n        * Each data point compares two individuals (A vs. B) using 11 standardized Twitter activity features.\n        * Binary label: '1' if A is more influential than B, '0' otherwise.\n        * Focus on replicating human judgment of influence, not defining influence algorithmically.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing pairwise comparisons of Twitter users' influence.\n    * **Data Files:**\n        * `train.csv`: Contains feature pairs and human-judged labels.\n        * `test.csv`: Feature pairs for which predictions are required.\n        * `sample_predictions.csv`: Example submission format (ID and predicted label).\n    * **Features:** 11 non-negative numeric Twitter activity metrics per individual (e.g., interaction volume, follower count), standardized but not explicitly named.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to rank influential individuals correctly across all classification thresholds.\n        * Implementations provided in R (ROCR package) and Python (scikit-learn)."
    },
    "file_path": "kaggle_datasets/70/problem_summary.md"
  },
  "329": {
    "problem_id": "329",
    "title": "Text Analysis for Job Bulletin Optimization in Los Angeles",
    "problem_type": "NLP - Text Analysis & Structured Data Extraction",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Text Analysis for Job Bulletin Optimization in Los Angeles\n\n**Problem Description:**\n* **Problem Type:** NLP - Text Analysis & Structured Data Extraction\n* **Objective:** \n    * Convert unstructured job bulletins into structured data to:\n        * Identify biased language affecting applicant diversity\n        * Improve applicant pool quality\n        * Enhance discoverability of promotional pathways\n* **Key Points:**\n    * Focus on public sector hiring challenges (33% workforce retiring)\n    * Requires both data transformation (text→CSV) and analytical insights\n    * Emphasizes documentation and reproducibility\n\n**Dataset Overview:**\n* **Data Type:** Unstructured text documents with supplemental structured files\n* **Context:** 683 plain-text job postings from Los Angeles municipal government\n* **Data Files:**\n    * Primary: Folder of 683 job bulletin text files\n    * Supporting:\n        * Kaggle_data_dictionary.csv\n        * Job_titles.csv\n        * Sample job class export template.csv\n        * Annotation documents (PDF/DOCX)\n* **Features:**\n    * Raw job description text\n    * Manually annotated examples\n    * Template for structured output format\n\n**Evaluation Metrics:**\n* **Composite Rubric (15 points total):**\n    * *Accuracy (5 pts):*\n        - Correctness of text-to-CSV conversion\n        - Field matching against manual validation set\n    * *Documentation (5 pts):*\n        - Code quality and comments\n        - Methodology transparency\n        - Solution evaluation documentation\n        - Preference for open-source tools\n    * *Recommendation (5 pts):*\n        - Original data-driven insights\n        - Actionable diversity/quality improvements\n        - Effective visualization of findings\n        - Addresses at least one core problem objective",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Text Analysis & Structured Data Extraction\n* **Objective:** \n    * Convert unstructured job bulletins into structured data to:\n        * Identify biased language affecting applicant diversity\n        * Improve applicant pool quality\n        * Enhance discoverability of promotional pathways\n* **Key Points:**\n    * Focus on public sector hiring challenges (33% workforce retiring)\n    * Requires both data transformation (text→CSV) and analytical insights\n    * Emphasizes documentation and reproducibility",
      "Dataset Overview": "* **Data Type:** Unstructured text documents with supplemental structured files\n* **Context:** 683 plain-text job postings from Los Angeles municipal government\n* **Data Files:**\n    * Primary: Folder of 683 job bulletin text files\n    * Supporting:\n        * Kaggle_data_dictionary.csv\n        * Job_titles.csv\n        * Sample job class export template.csv\n        * Annotation documents (PDF/DOCX)\n* **Features:**\n    * Raw job description text\n    * Manually annotated examples\n    * Template for structured output format",
      "Evaluation Metrics": "* **Composite Rubric (15 points total):**\n    * *Accuracy (5 pts):*\n        - Correctness of text-to-CSV conversion\n        - Field matching against manual validation set\n    * *Documentation (5 pts):*\n        - Code quality and comments\n        - Methodology transparency\n        - Solution evaluation documentation\n        - Preference for open-source tools\n    * *Recommendation (5 pts):*\n        - Original data-driven insights\n        - Actionable diversity/quality improvements\n        - Effective visualization of findings\n        - Addresses at least one core problem objective"
    },
    "file_path": "kaggle_datasets/329/problem_summary.md"
  },
  "84": {
    "problem_id": "84",
    "title": "MasterCard Transaction Data Cleansing Competition",
    "problem_type": "Data Cleansing / Data Quality Improvement",
    "objective": "Improve the quality of information within transaction data by identifying and correcting errors, inconsistencies, or missing values. Participants are tasked with developing methods to enhance the reliability and usability of financial transaction records.",
    "evaluation_metric": null,
    "full_content": "# MasterCard Transaction Data Cleansing Competition\n\n**Problem Description:**\n* **Problem Type:** Data Cleansing / Data Quality Improvement\n* **Objective:** Improve the quality of information within transaction data by identifying and correcting errors, inconsistencies, or missing values. Participants are tasked with developing methods to enhance the reliability and usability of financial transaction records.\n* **Key Points:**\n  * Focused on transaction data quality (likely including merchant names, transaction amounts, timestamps, etc.)\n  * Private, invitation-only competition based on prior performance\n  * Evaluation likely measures improvement in data consistency, accuracy, or completeness\n\n**Dataset Overview:**\n* **Data Type:** Tabular transaction data (specific fields not disclosed publicly)\n* **Context:** Financial transaction records from MasterCard's payment network\n* **Data Files:** Not publicly disclosed (competition was private/invitation-only)\n* **Features:** Likely includes standard transaction fields (amounts, timestamps, merchant IDs, location data) but specific columns were not made public\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Custom data quality metric (not publicly specified)\n* **Components:** \n  * Likely measured improvement in data consistency, accuracy, or completeness\n  * May have included aspects like error reduction rate or validation against ground truth\n  * Specific scoring methodology was private to participants",
    "sections": {
      "Problem Description": "* **Problem Type:** Data Cleansing / Data Quality Improvement\n* **Objective:** Improve the quality of information within transaction data by identifying and correcting errors, inconsistencies, or missing values. Participants are tasked with developing methods to enhance the reliability and usability of financial transaction records.\n* **Key Points:**\n  * Focused on transaction data quality (likely including merchant names, transaction amounts, timestamps, etc.)\n  * Private, invitation-only competition based on prior performance\n  * Evaluation likely measures improvement in data consistency, accuracy, or completeness",
      "Dataset Overview": "* **Data Type:** Tabular transaction data (specific fields not disclosed publicly)\n* **Context:** Financial transaction records from MasterCard's payment network\n* **Data Files:** Not publicly disclosed (competition was private/invitation-only)\n* **Features:** Likely includes standard transaction fields (amounts, timestamps, merchant IDs, location data) but specific columns were not made public",
      "Evaluation Metrics": "* **Evaluation Metric:** Custom data quality metric (not publicly specified)\n* **Components:** \n  * Likely measured improvement in data consistency, accuracy, or completeness\n  * May have included aspects like error reduction rate or validation against ground truth\n  * Specific scoring methodology was private to participants"
    },
    "file_path": "kaggle_datasets/84/problem_summary.md"
  },
  "540": {
    "problem_id": "540",
    "title": "Curriculum Recommendation for K-12 Educational Content",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Curriculum Recommendation for K-12 Educational Content\n\n## Problem Description\n* **Problem Type**: Information Retrieval / Recommendation System\n* **Objective**: Develop a model to accurately match K-12 educational content to specific topics in diverse curriculum taxonomies. The goal is to automate the process of curriculum alignment, which is currently manual and resource-intensive.\n* **Key Points**:\n  * Content and topics span multiple languages and STEM subjects\n  * Must handle hierarchical topic structures (topic trees)\n  * Solution should work for educational systems with limited computational resources\n  * Special consideration for refugee learners and crisis contexts\n\n## Dataset Overview\n* **Data Type**: Multilingual educational metadata with hierarchical relationships\n* **Primary Files**:\n  * `topics.csv` - Curriculum topics with hierarchy metadata\n  * `content.csv` - Educational resources with metadata\n  * `correlations.csv` - Ground truth topic-content mappings\n  * `sample_submission.csv` - Submission format template\n* **Key Features**:\n  * Topics: hierarchical structure (channel, parent, level), titles/descriptions, language codes\n  * Content: titles/descriptions, content type (video/document/exercise/etc.), extracted text (when available)\n  * Multilingual support (language codes for both topics and content)\n\n## Evaluation Metrics\n* **Primary Metric**: Mean F2 Score (sample-wise calculation)\n  * F2 score emphasizes recall over precision (β=2)\n  * Calculated for each topic-content recommendation, then averaged\n* **Efficiency Prize Metric** (optional track):\n  * Combined score considering both F2 performance and runtime\n  * Formula: `1/(Benchmark - maxF2)*F2 + (1/32400)*RuntimeSeconds`\n  * CPU-only submissions eligible\n  * Rewards models that balance accuracy with computational efficiency",
    "sections": {},
    "file_path": "kaggle_datasets/540/problem_summary.md"
  },
  "124": {
    "problem_id": "124",
    "title": "Binary Classification of Illicit Content in Online Ads",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Illicit Content in Online Ads\n\n## Problem Description\n* **Problem Type:** Binary Classification (Text & Tabular Data)\n* **Objective:** Predict whether online advertisements on Avito.ru contain prohibited/illicit content based on moderators' historical decisions.\n    * Key challenge is automating content moderation at scale for Russia's largest classifieds platform.\n    * Models must learn from human moderation patterns to flag ads violating Russian legislation or platform rules.\n* **Key Points:**\n    * Focus on Russian-language text analysis (titles, descriptions) combined with structured ad metadata.\n    * Must handle heterogeneous data: JSON attributes, categorical features, and text fields.\n    * Subtle distinction between outright illegal items vs. platform-prohibited content.\n\n## Dataset Overview\n* **Data Type:** Multimodal (Text + Tabular)\n    * Russian-language ad text (titles/descriptions) with tabular metadata about listings.\n    * Historical ads sampled from Dec 2013 - Apr 2014, already closed/moderated.\n* **Data Files:**\n    * `avito_train.zip`: Contains ad features + target labels (`is_blocked` binary flag)\n    * `avito_test.zip`: Ad features without labels for prediction\n    * `sampleSubmission.tsv`: Submission format template\n* **Key Features:**\n    * Text: `title`, `description` (with masked PII like @@PHONE@@)\n    * Categorical: `category`, `subcategory`\n    * Structured: `attrs` (JSON of item-specific attributes)\n    * Numerical: `price`, `phones_cnt`, `emails_cnt`, `urls_cnt`\n    * Target: `is_blocked` (binary moderation decision)\n\n## Evaluation Metrics\n* **Primary Metric:** Average Precision at K (AP@K)\n    * Formula: \n        ```\n        AP@K = (Σ from i=1 to K of P@i) / K\n        ```\n        Where `P@i` = (# relevant forbidden ads in top i predictions) / i\n* **Implementation Notes:**\n    * Requires ranking all test ads by predicted likelihood of being prohibited.\n    * Submission is an ordered list of ad IDs from most to least likely illicit.\n    * Emphasizes correct prioritization of high-risk ads for moderation review.",
    "sections": {},
    "file_path": "kaggle_datasets/124/problem_summary.md"
  },
  "316": {
    "problem_id": "316",
    "title": "Recommending Career Advice Questions to Professionals",
    "problem_type": "Recommendation System / Information Retrieval",
    "objective": "Develop a method to recommend relevant career advice questions to professionals who are most likely to answer them, improving the matching efficiency on CareerVillage.org's platform.",
    "evaluation_metric": null,
    "full_content": "# Recommending Career Advice Questions to Professionals\n\n**Problem Description:**\n* **Problem Type:** Recommendation System / Information Retrieval\n* **Objective:** Develop a method to recommend relevant career advice questions to professionals who are most likely to answer them, improving the matching efficiency on CareerVillage.org's platform.\n    * **Key Points:**\n        * Focus on volunteer motivation and question relevance rather than traditional predictive modeling.\n        * Solution should effectively increase the number of questions answered by better matching.\n        * Must consider practical implementation constraints for immediate deployment.\n        * Should be extensible to accommodate future data features and objectives.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular and text data from a career advice Q&A platform, including user profiles, questions, answers, interactions, and engagement metrics.\n* **Key Data Files:**\n    * `professionals.csv`: Volunteer profiles (the target recommenders).\n    * `questions.csv`: Student questions (the items to recommend).\n    * `answers.csv`: Historical answer records (for modeling engagement patterns).\n    * `matches.csv`: Question-email mappings showing past recommendations.\n    * `emails.csv`: Email metadata including frequency and timing.\n    * Additional files: User interactions (`comments.csv`), engagement scores (`answer_scores.csv`, `question_scores.csv`), and tagging data (`tags.csv`, `tag_questions.csv`, `tag_users.csv`).\n\n**Evaluation Metrics:**\n* **Primary Evaluation:** Qualitative scoring by judges across four criteria (0-5 points each, summed to 20 max):\n    * **Documentation & Implementation Feasibility** (clarity and production-readiness).\n    * **Data Insights & Communication** (value of derived insights and presentation).\n    * **Model & Feature Quality** (effectiveness of the recommendation approach).\n    * **Future Recommendations** (extensibility and scalability suggestions).\n* **No quantitative metric** was specified due to the lack of live testing; solutions were assessed based on their theoretical justification and practical design.",
    "sections": {
      "Problem Description": "* **Problem Type:** Recommendation System / Information Retrieval\n* **Objective:** Develop a method to recommend relevant career advice questions to professionals who are most likely to answer them, improving the matching efficiency on CareerVillage.org's platform.\n    * **Key Points:**\n        * Focus on volunteer motivation and question relevance rather than traditional predictive modeling.\n        * Solution should effectively increase the number of questions answered by better matching.\n        * Must consider practical implementation constraints for immediate deployment.\n        * Should be extensible to accommodate future data features and objectives.",
      "Dataset Overview": "* **Data Type & Context:** Tabular and text data from a career advice Q&A platform, including user profiles, questions, answers, interactions, and engagement metrics.\n* **Key Data Files:**\n    * `professionals.csv`: Volunteer profiles (the target recommenders).\n    * `questions.csv`: Student questions (the items to recommend).\n    * `answers.csv`: Historical answer records (for modeling engagement patterns).\n    * `matches.csv`: Question-email mappings showing past recommendations.\n    * `emails.csv`: Email metadata including frequency and timing.\n    * Additional files: User interactions (`comments.csv`), engagement scores (`answer_scores.csv`, `question_scores.csv`), and tagging data (`tags.csv`, `tag_questions.csv`, `tag_users.csv`).",
      "Evaluation Metrics": "* **Primary Evaluation:** Qualitative scoring by judges across four criteria (0-5 points each, summed to 20 max):\n    * **Documentation & Implementation Feasibility** (clarity and production-readiness).\n    * **Data Insights & Communication** (value of derived insights and presentation).\n    * **Model & Feature Quality** (effectiveness of the recommendation approach).\n    * **Future Recommendations** (extensibility and scalability suggestions).\n* **No quantitative metric** was specified due to the lack of live testing; solutions were assessed based on their theoretical justification and practical design."
    },
    "file_path": "kaggle_datasets/316/problem_summary.md"
  },
  "184": {
    "problem_id": "184",
    "title": "Multi-class Classification for 8th Grade Science Questions",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Classification for 8th Grade Science Questions\n\n## Problem Description\n- **Problem Type:** Multi-class Classification (Text-based Question Answering)\n- **Objective:** Develop a model that can correctly answer multiple-choice questions from a standardized 8th grade science exam. The model must select the correct answer (A, B, C, or D) for each question based on the question text and answer options.\n- **Key Points:**\n  - The challenge focuses on AI's ability to understand and reason through general science questions.\n  - Questions are designed to test fundamental world knowledge at an 8th grade level.\n  - The competition has two stages: model development on training data, followed by final evaluation on a held-out test set.\n  - Models must demonstrate automated answer generation without manual intervention.\n\n## Dataset Overview\n- **Data Type:** Text data (multiple-choice science questions and answers)\n- **Context:** Questions from US 8th grade science curriculum, with four answer options per question.\n- **Data Files:**\n  - `training_set.tsv` - 2,500 labeled questions (for model development)\n  - `validation_set.tsv` - 8,132 unlabeled questions (for public leaderboard)\n  - `test_set.tsv` - 21,298 unlabeled questions (for final evaluation)\n  - `sample_submission.csv` - submission format example\n- **Features:**\n  - `question`: Text of the science question\n  - `answerA`-`answerD`: Text of each answer option\n  - `correctAnswer`: Correct option (A-D) for training data only\n\n## Evaluation Metrics\n- **Evaluation Metric:** Categorization Accuracy (Fraction of correctly answered questions)\n- **Components:**\n  - Each question has exactly one correct answer among four options.\n  - Random guessing would yield ~25% accuracy (baseline).\n  - Final score is calculated as: (Number of correct answers) / (Total questions).\n  - Public leaderboard uses validation set questions; private leaderboard uses new test set questions.",
    "sections": {},
    "file_path": "kaggle_datasets/184/problem_summary.md"
  },
  "514": {
    "problem_id": "514",
    "title": "Predicting Rocket League Goal Probabilities from Gameplay Snapshots",
    "problem_type": "Binary Classification (Multi-output)",
    "objective": "Predict the probability of each team scoring a goal within the next 10 seconds of gameplay, given a snapshot of a Rocket League match.",
    "evaluation_metric": null,
    "full_content": "# Predicting Rocket League Goal Probabilities from Gameplay Snapshots\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Multi-output)\n* **Objective:** Predict the probability of each team scoring a goal within the next 10 seconds of gameplay, given a snapshot of a Rocket League match.\n    * **Key Points:**\n        * The dataset is large and requires efficient handling (e.g., scaling down, online learning, feature engineering).\n        * The training data consists of time series, but predictions are made pointwise (temporal information could be leveraged).\n        * The task involves predicting probabilities for two targets simultaneously (Team A and Team B scoring).\n\n**Dataset Overview:**\n* **Data Type:** Tabular data (time series snapshots of Rocket League matches).\n* **Context:** Professional Rocket League gameplay data, including player/ball positions, velocities, and boost levels.\n* **Data Files:**\n    * `train_[0-9].csv`: Training data split into 10 files (sorted by game, event, and time).\n    * `test.csv`: Test data (rows scrambled).\n    * `[train|test]_dtypes.csv`: Data types for efficient memory usage.\n    * `sample_submission.csv`: Example submission file.\n* **Key Features:**\n    * Ball and player positions/velocities (3D vectors).\n    * Player boost levels and boost orb timers.\n    * Team and player identifiers (train only).\n    * Target columns: `team_[A|B]_scoring_within_10sec` (binary labels).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Log Loss (Multi-column)\n    * **Components:**\n        * For each test observation, predict probabilities for both teams (Team A and Team B).\n        * The log loss is averaged across both targets and all observations.\n        * Formula: \n          ```\n          score = -1/2 * sum_over_teams(1/N * sum_over_observations[y_i,m * log(y_hat_i,m) + (1-y_i,m) * log(1-y_hat_i,m)])\n          ```\n        * Predicted probabilities are clipped to `[1e-15, 1-1e-15]` to avoid infinite penalties.\n        * Lower log loss values indicate better performance.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Multi-output)\n* **Objective:** Predict the probability of each team scoring a goal within the next 10 seconds of gameplay, given a snapshot of a Rocket League match.\n    * **Key Points:**\n        * The dataset is large and requires efficient handling (e.g., scaling down, online learning, feature engineering).\n        * The training data consists of time series, but predictions are made pointwise (temporal information could be leveraged).\n        * The task involves predicting probabilities for two targets simultaneously (Team A and Team B scoring).",
      "Dataset Overview": "* **Data Type:** Tabular data (time series snapshots of Rocket League matches).\n* **Context:** Professional Rocket League gameplay data, including player/ball positions, velocities, and boost levels.\n* **Data Files:**\n    * `train_[0-9].csv`: Training data split into 10 files (sorted by game, event, and time).\n    * `test.csv`: Test data (rows scrambled).\n    * `[train|test]_dtypes.csv`: Data types for efficient memory usage.\n    * `sample_submission.csv`: Example submission file.\n* **Key Features:**\n    * Ball and player positions/velocities (3D vectors).\n    * Player boost levels and boost orb timers.\n    * Team and player identifiers (train only).\n    * Target columns: `team_[A|B]_scoring_within_10sec` (binary labels).",
      "Evaluation Metrics": "* **Evaluation Metric:** Log Loss (Multi-column)\n    * **Components:**\n        * For each test observation, predict probabilities for both teams (Team A and Team B).\n        * The log loss is averaged across both targets and all observations.\n        * Formula: \n          ```\n          score = -1/2 * sum_over_teams(1/N * sum_over_observations[y_i,m * log(y_hat_i,m) + (1-y_i,m) * log(1-y_hat_i,m)])\n          ```\n        * Predicted probabilities are clipped to `[1e-15, 1-1e-15]` to avoid infinite penalties.\n        * Lower log loss values indicate better performance."
    },
    "file_path": "kaggle_datasets/514/problem_summary.md"
  },
  "342": {
    "problem_id": "342",
    "title": "Instance Segmentation on Open Images Dataset",
    "problem_type": "Computer Vision - Instance Segmentation",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Instance Segmentation on Open Images Dataset\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Instance Segmentation\n* **Objective:**  \n  Participants are tasked with generating precise segmentation masks for object instances in images, covering 300 distinct categories. The goal is to accurately outline object boundaries at the pixel level.\n  * **Key Points:**\n    * Focus on segmenting diverse objects in complex scenes (e.g., animals, vehicles, household items).\n    * Masks must account for occlusions and varying object scales.\n    * Submissions require confidence scores for each predicted mask.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  RGB images with pixel-level instance segmentation annotations. The dataset includes:\n  * **Training set:** 2.1 million masks (generated via human-in-the-loop AI-assisted annotation).\n  * **Validation set:** 23k manually-annotated high-quality masks.\n  * **Test set:** 99,999 independent images (same as other Open Images Challenge tracks).\n* **Key Files:**\n  * `test.zip` - Challenge test images\n  * Sample submission files demonstrating RLE-encoded mask format\n* **Notable Features:**\n  * Images depict real-world scenes with CC-licensed content.\n  * 300 segmentable classes with imbalanced distributions.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision (mAP) over all 300 classes.\n  * **Calculation Details:**\n    * Mask-to-mask IoU matching (unlike bbox-based detection metrics).\n    * Precision-recall curves evaluated at multiple IoU thresholds.\n    * Final score averages AP across all classes.\n* **Submission Format:**\n  * CSV with RLE-encoded masks (zlib compressed + base64 encoded).\n  * Required columns: `ImageID`, `ImageWidth`, `ImageHeight`, `PredictionString` (containing label-confidence-mask tuples).\n  * Strict 5GB file size limit (~50-100 detections/image average).",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Instance Segmentation\n* **Objective:**  \n  Participants are tasked with generating precise segmentation masks for object instances in images, covering 300 distinct categories. The goal is to accurately outline object boundaries at the pixel level.\n  * **Key Points:**\n    * Focus on segmenting diverse objects in complex scenes (e.g., animals, vehicles, household items).\n    * Masks must account for occlusions and varying object scales.\n    * Submissions require confidence scores for each predicted mask.",
      "Dataset Overview": "* **Data Type & Context:**  \n  RGB images with pixel-level instance segmentation annotations. The dataset includes:\n  * **Training set:** 2.1 million masks (generated via human-in-the-loop AI-assisted annotation).\n  * **Validation set:** 23k manually-annotated high-quality masks.\n  * **Test set:** 99,999 independent images (same as other Open Images Challenge tracks).\n* **Key Files:**\n  * `test.zip` - Challenge test images\n  * Sample submission files demonstrating RLE-encoded mask format\n* **Notable Features:**\n  * Images depict real-world scenes with CC-licensed content.\n  * 300 segmentable classes with imbalanced distributions.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision (mAP) over all 300 classes.\n  * **Calculation Details:**\n    * Mask-to-mask IoU matching (unlike bbox-based detection metrics).\n    * Precision-recall curves evaluated at multiple IoU thresholds.\n    * Final score averages AP across all classes.\n* **Submission Format:**\n  * CSV with RLE-encoded masks (zlib compressed + base64 encoded).\n  * Required columns: `ImageID`, `ImageWidth`, `ImageHeight`, `PredictionString` (containing label-confidence-mask tuples).\n  * Strict 5GB file size limit (~50-100 detections/image average)."
    },
    "file_path": "kaggle_datasets/342/problem_summary.md"
  },
  "170": {
    "problem_id": "170",
    "title": "Binary Classification of Rare Particle Decay τ → μμμ",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Rare Particle Decay τ → μμμ\n\n## Problem Description\n* **Problem Type:** Binary Classification (Physics - Particle Detection)\n* **Objective:**  \n    * Detect the rare lepton flavor-violating decay τ → μμμ (tau to three muons) in Large Hadron Collider (LHC) data, which would indicate \"new physics\" beyond the Standard Model.  \n    * Distinguish between signal (simulated τ → μμμ events) and background (real LHC collision data where this decay cannot occur).  \n* **Key Points:**  \n    * Focus on identifying a phenomenon *not yet observed* in particle physics.  \n    * Must pass two validation tests before evaluation:  \n        * **Agreement Test:** Ensures model performs similarly on real vs. simulated control channel data (Ds → φπ).  \n        * **Correlation Test:** Ensures predictions are uncorrelated with hidden τ mass (to avoid bias).  \n\n## Dataset Overview\n* **Data Type & Context:**  \n    * Tabular data from the LHCb experiment, containing kinematic and geometric properties of particle collision events.  \n* **Data Files:**  \n    * `training.csv`: Labeled data (signal=1 for simulated τ → μμμ, background=0 for real LHC data).  \n    * `test.csv`: Unlabeled data (mixed signal/background/control events; excludes `mass`, `signal` columns).  \n    * `check_agreement.csv`: Control channel data (Ds → φπ) for agreement validation.  \n    * `check_correlation.csv`: Background events with `mass` for correlation checks.  \n* **Key Features:**  \n    * Particle trajectory metrics (`FlightDistance`, `IP`, `DOCA`).  \n    * Momentum/energy measures (`pt`, `eta`, `p`).  \n    * Detector-specific variables (`SPDhits`, isolation parameters).  \n    * **Excluded in test data:** `mass`, `production`, `min_ANNmuon` (to prevent leakage).  \n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Area Under the ROC Curve (AUC).  \n    * **Weighting Scheme:**  \n        * TPR [0.0, 0.2]: Weight = 2.0  \n        * TPR [0.2, 0.4]: Weight = 1.5  \n        * T",
    "sections": {},
    "file_path": "kaggle_datasets/170/problem_summary.md"
  },
  "24": {
    "problem_id": "24",
    "title": "Predicting Car Auction Lemons (Binary Classification)",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Car Auction Lemons (Binary Classification)\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict whether a car purchased at auction is a \"kick\" (bad buy) based on vehicle and auction features.  \n  * **Key Points:**  \n    - Identify high-risk vehicles that may have undisclosed mechanical issues, title problems, or odometer tampering.  \n    - Help dealerships avoid costly purchases that cannot be resold profitably.  \n    - Dataset contains real-world auction data with missing values.  \n\n## Dataset Overview  \n* **Data Type & Context:** Tabular data representing used car auction transactions.  \n* **Data Files:**  \n  - `training.csv` (60% of data)  \n  - `test.csv` (40% of data)  \n  - `Carvana_Data_Dictionary.txt` (variable definitions)  \n  - Example/sample files  \n* **Key Features:**  \n  - 32 anonymized independent variables (columns C3-C34) covering vehicle/auction attributes  \n  - Binary target variable `IsBadBuy` (C2) indicating kick status  \n\n## Evaluation Metrics  \n* **Primary Metric:** Gini Index  \n  * **Interpretation:**  \n    - Normalized variant of the AUC (Gini = 2*AUC - 1)  \n    - Measures model's ability to rank bad buys higher than good buys  \n    - Higher values indicate better separation between classes (max = 1)",
    "sections": {},
    "file_path": "kaggle_datasets/24/problem_summary.md"
  },
  "389": {
    "problem_id": "389",
    "title": "M5 Forecasting - Uncertainty: Quantile Regression for Walmart Sales",
    "problem_type": "Time Series Forecasting with Uncertainty Estimation (Quantile Regression)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# M5 Forecasting - Uncertainty: Quantile Regression for Walmart Sales\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting with Uncertainty Estimation (Quantile Regression)\n* **Objective:** \n    * Predict daily unit sales for Walmart products across multiple stores for a 28-day horizon.\n    * Estimate uncertainty distributions by forecasting 9 quantiles (0.005 to 0.995) for each prediction.\n    * Focus on hierarchical data structure (item-store-state levels) with explanatory variables.\n* **Key Points:**\n    * Companion to M5 Forecasting - Accuracy (point forecasts).\n    * First-of-its-kind competition emphasizing uncertainty quantification.\n    * Data spans 3 US states (CA, TX, WI) with store-item hierarchies.\n\n**Dataset Overview:**\n* **Data Type & Context:** \n    * Tabular time series data of Walmart retail sales with:\n        * Daily sales records (`d_1` to `d_1941`)\n        * Product/store metadata and pricing data\n        * Calendar features (weekday, holidays, etc.)\n* **Data Files:**\n    * `sales_train_validation.csv` (historical sales: d_1-d_1913)\n    * `sales_train_evaluation.csv` (extended sales: d_1-d_1941)\n    * `calendar.csv` (date metadata)\n    * `sell_prices.csv` (product pricing by store/date)\n    * `sample_submission.csv` (quantile forecast template)\n* **Key Features:**\n    * Hierarchical structure (item → department → category → store → state)\n    * Temporal features (day-of-week, holidays, promotions)\n    * Price history and product identifiers\n\n**Evaluation Metrics:**\n* **Primary Metric:** Weighted Scaled Pinball Loss (WSPL)\n    * **Components:**\n        * Pinball loss calculated for each quantile forecast (0.005 to 0.995)\n        * Scaling: Normalized by actual sales to handle magnitude differences\n        * Weighting: Aggregated across hierarchical levels (item/store/state/etc.)\n    * **Goal:** Minimize WSPL across all quantiles and aggregation levels\n    * **Interpretation:** Lower values indicate better probabilistic calibration",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting with Uncertainty Estimation (Quantile Regression)\n* **Objective:** \n    * Predict daily unit sales for Walmart products across multiple stores for a 28-day horizon.\n    * Estimate uncertainty distributions by forecasting 9 quantiles (0.005 to 0.995) for each prediction.\n    * Focus on hierarchical data structure (item-store-state levels) with explanatory variables.\n* **Key Points:**\n    * Companion to M5 Forecasting - Accuracy (point forecasts).\n    * First-of-its-kind competition emphasizing uncertainty quantification.\n    * Data spans 3 US states (CA, TX, WI) with store-item hierarchies.",
      "Dataset Overview": "* **Data Type & Context:** \n    * Tabular time series data of Walmart retail sales with:\n        * Daily sales records (`d_1` to `d_1941`)\n        * Product/store metadata and pricing data\n        * Calendar features (weekday, holidays, etc.)\n* **Data Files:**\n    * `sales_train_validation.csv` (historical sales: d_1-d_1913)\n    * `sales_train_evaluation.csv` (extended sales: d_1-d_1941)\n    * `calendar.csv` (date metadata)\n    * `sell_prices.csv` (product pricing by store/date)\n    * `sample_submission.csv` (quantile forecast template)\n* **Key Features:**\n    * Hierarchical structure (item → department → category → store → state)\n    * Temporal features (day-of-week, holidays, promotions)\n    * Price history and product identifiers",
      "Evaluation Metrics": "* **Primary Metric:** Weighted Scaled Pinball Loss (WSPL)\n    * **Components:**\n        * Pinball loss calculated for each quantile forecast (0.005 to 0.995)\n        * Scaling: Normalized by actual sales to handle magnitude differences\n        * Weighting: Aggregated across hierarchical levels (item/store/state/etc.)\n    * **Goal:** Minimize WSPL across all quantiles and aggregation levels\n    * **Interpretation:** Lower values indicate better probabilistic calibration"
    },
    "file_path": "kaggle_datasets/389/problem_summary.md"
  },
  "177": {
    "problem_id": "177",
    "title": "Walmart Shopping Trip Type Classification",
    "problem_type": "Multi-class Classification",
    "objective": "Classify customer shopping trips into predefined trip types based solely on transactional purchase data. The goal is to replicate Walmart's proprietary trip categorization method using limited features.",
    "evaluation_metric": null,
    "full_content": "# Walmart Shopping Trip Type Classification\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification\n* **Objective:** Classify customer shopping trips into predefined trip types based solely on transactional purchase data. The goal is to replicate Walmart's proprietary trip categorization method using limited features.\n    * **Key Points:**\n        * Focuses on market basket analysis (using purchased items to infer trip purpose)\n        * Must predict among 38 distinct trip types (including an \"other\" category)\n        * Uses only transactional data without additional context about trip types\n        * Requires handling product returns (negative ScanCount values)\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data of customer transactions with product purchase records\n* **Data Files:**\n    * train.csv (labeled data with TripType)\n    * test.csv (encrypted file requiring survey completion)\n    * sample_submission.csv\n* **Key Features:**\n    * VisitNumber (trip identifier)\n    * Weekday (temporal feature)\n    * Upc/DepartmentDescription/FinelineNumber (product categorization)\n    * ScanCount (purchase quantity, with negatives indicating returns)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * For each visit: predicted probabilities for all 38 trip types\n        * Probabilities rescaled to sum to 1 before scoring\n        * Clipped to [10^-15, 1-10^-15] to avoid log extremes\n        * Formula: -(1/N)ΣΣ y_ij*log(p_ij) where:\n            * N = number of visits\n            * y_ij = 1 if visit i is class j, else 0\n            * p_ij = predicted probability for class j",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification\n* **Objective:** Classify customer shopping trips into predefined trip types based solely on transactional purchase data. The goal is to replicate Walmart's proprietary trip categorization method using limited features.\n    * **Key Points:**\n        * Focuses on market basket analysis (using purchased items to infer trip purpose)\n        * Must predict among 38 distinct trip types (including an \"other\" category)\n        * Uses only transactional data without additional context about trip types\n        * Requires handling product returns (negative ScanCount values)",
      "Dataset Overview": "* **Data Type & Context:** Tabular data of customer transactions with product purchase records\n* **Data Files:**\n    * train.csv (labeled data with TripType)\n    * test.csv (encrypted file requiring survey completion)\n    * sample_submission.csv\n* **Key Features:**\n    * VisitNumber (trip identifier)\n    * Weekday (temporal feature)\n    * Upc/DepartmentDescription/FinelineNumber (product categorization)\n    * ScanCount (purchase quantity, with negatives indicating returns)",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class logarithmic loss (log loss)\n    * **Components:**\n        * For each visit: predicted probabilities for all 38 trip types\n        * Probabilities rescaled to sum to 1 before scoring\n        * Clipped to [10^-15, 1-10^-15] to avoid log extremes\n        * Formula: -(1/N)ΣΣ y_ij*log(p_ij) where:\n            * N = number of visits\n            * y_ij = 1 if visit i is class j, else 0\n            * p_ij = predicted probability for class j"
    },
    "file_path": "kaggle_datasets/177/problem_summary.md"
  },
  "345": {
    "problem_id": "345",
    "title": "Kuzushiji Character Recognition in Historical Japanese Documents",
    "problem_type": "Computer Vision - Object Detection and Multiclass Classification",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Kuzushiji Character Recognition in Historical Japanese Documents\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Object Detection and Multiclass Classification\n* **Objective:** \n    * Develop a model to locate and classify ancient Kuzushiji characters in historical Japanese documents, transcribing them into contemporary Japanese Unicode characters.\n    * The task involves both detecting character positions (via center points within bounding boxes) and correctly identifying character types from over 4,300 possible classes.\n* **Key Points:**\n    * Must handle unique challenges of Kuzushiji script:\n        * Large character set (4,300+ classes) with long-tailed frequency distribution\n        * Character variations (Hentaigana) where single modern characters have multiple historical forms\n        * Visual similarity between distinct characters\n        * Connected/overlapping cursive characters\n        * Non-standard page layouts (wrapped text, diagonal arrangements)\n    * Practical constraints:\n        * Max 1,200 predictions per page\n        * Must ignore annotation characters and bleed-through text from opposite pages\n\n**Dataset Overview:**\n* **Data Type:** Image data (scanned historical documents) with spatial annotations\n* **Context:** High-resolution scans of Japanese books/documents from 8th-19th centuries containing Kuzushiji script\n* **Data Files:**\n    * `train_images.zip` - Training document images\n    * `test_images.zip` - Test document images\n    * `train.csv` - Training labels with bounding boxes (format: `image_id`, `labels` as space-separated `Unicode X Y Width Height` sequences)\n    * `unicode_translation.csv` - Mapping between Unicode IDs and Japanese characters\n    * `sample_submission.csv` - Submission format example (requires `Unicode X Y` triplets)\n* **Key Features:**\n    * Images contain complex layouts with:\n        * Main text columns\n        * Small annotation characters (to be ignored)\n        * Potential illustrations\n    * Each character annotation includes:\n        * Unicode representation\n        * Precise spatial coordinates\n        * Bounding box dimensions (training only)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Modified F1 Score\n    * True Positive requires:\n        1. Correct Unicode character prediction\n        2. Predicted center point (X,Y) must fall within ground truth bounding box\n    * Standard precision/recall calculation based on above criteria\n* **Implementation Notes:**\n    *",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Object Detection and Multiclass Classification\n* **Objective:** \n    * Develop a model to locate and classify ancient Kuzushiji characters in historical Japanese documents, transcribing them into contemporary Japanese Unicode characters.\n    * The task involves both detecting character positions (via center points within bounding boxes) and correctly identifying character types from over 4,300 possible classes.\n* **Key Points:**\n    * Must handle unique challenges of Kuzushiji script:\n        * Large character set (4,300+ classes) with long-tailed frequency distribution\n        * Character variations (Hentaigana) where single modern characters have multiple historical forms\n        * Visual similarity between distinct characters\n        * Connected/overlapping cursive characters\n        * Non-standard page layouts (wrapped text, diagonal arrangements)\n    * Practical constraints:\n        * Max 1,200 predictions per page\n        * Must ignore annotation characters and bleed-through text from opposite pages",
      "Dataset Overview": "* **Data Type:** Image data (scanned historical documents) with spatial annotations\n* **Context:** High-resolution scans of Japanese books/documents from 8th-19th centuries containing Kuzushiji script\n* **Data Files:**\n    * `train_images.zip` - Training document images\n    * `test_images.zip` - Test document images\n    * `train.csv` - Training labels with bounding boxes (format: `image_id`, `labels` as space-separated `Unicode X Y Width Height` sequences)\n    * `unicode_translation.csv` - Mapping between Unicode IDs and Japanese characters\n    * `sample_submission.csv` - Submission format example (requires `Unicode X Y` triplets)\n* **Key Features:**\n    * Images contain complex layouts with:\n        * Main text columns\n        * Small annotation characters (to be ignored)\n        * Potential illustrations\n    * Each character annotation includes:\n        * Unicode representation\n        * Precise spatial coordinates\n        * Bounding box dimensions (training only)",
      "Evaluation Metrics": "* **Primary Metric:** Modified F1 Score\n    * True Positive requires:\n        1. Correct Unicode character prediction\n        2. Predicted center point (X,Y) must fall within ground truth bounding box\n    * Standard precision/recall calculation based on above criteria\n* **Implementation Notes:**\n    *"
    },
    "file_path": "kaggle_datasets/345/problem_summary.md"
  },
  "513": {
    "problem_id": "513",
    "title": "Cervical Spine Fracture Detection from CT Scans",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Cervical Spine Fracture Detection from CT Scans\n\n## Problem Description\n* **Problem Type:** Binary Classification (Computer Vision - Medical Imaging)\n* **Objective:** Develop a machine learning model to detect and localize cervical spine fractures in CT scans at two levels:\n  * Patient-level: Predict presence of *any* fracture (`patient_overall`)\n  * Vertebra-level: Predict fractures for each of the 7 cervical vertebrae (C1-C7)\n* **Key Points:**\n  * Focus on cervical spine (neck) fractures, which are critical to detect quickly to prevent paralysis\n  * Dataset includes challenging cases with degenerative disease/osteoporosis\n  * Requires multi-label prediction (8 outputs per scan)\n\n## Dataset Overview\n* **Data Type:** 3D Medical Imaging (CT Scans) + Tabular Metadata\n* **Context:** ~3,000 de-identified CT studies from 12 global sites, annotated by radiologists\n* **Data Files:**\n  * `train.csv`/`test.csv`: Study IDs and fracture labels\n  * `[train/test]_images/`: DICOM files (axial slices, ≤1mm thickness)\n  * `train_bounding_boxes.csv`: Bounding box annotations (subset)\n  * `segmentations/`: NIFTI files with vertebra-level segmentations (subset)\n* **Key Features:**\n  * DICOM images (some JPEG-compressed)\n  * 7 vertebral-level binary labels + patient-level label per scan\n  * Additional segmentation masks for vertebrae (C1-T12) in subset\n\n## Evaluation Metrics\n* **Primary Metric:** Weighted Multi-Label Logarithmic Loss\n  * Calculates binary log loss separately for each fracture subtype (C1-C7 + patient_overall)\n  * Weighting: `patient_overall` receives higher weight than individual vertebrae\n  * Final score: Average across all rows (8 predictions per scan)\n* **Submission Format:**\n  * Requires probability predictions for all 8 labels per test scan\n  * Format: `row_id,fractured` where row_id = `[StudyID]_[C1-C7 or patient_overall]`",
    "sections": {},
    "file_path": "kaggle_datasets/513/problem_summary.md"
  },
  "183": {
    "problem_id": "183",
    "title": "Predicting Airbnb New User Booking Destinations",
    "problem_type": "Multi-class Classification (with ranking component)",
    "objective": "Predict the country where a new Airbnb user will make their first booking from a list of 12 possible destinations (including 'NDF' for no booking).",
    "evaluation_metric": null,
    "full_content": "# Predicting Airbnb New User Booking Destinations\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (with ranking component)\n* **Objective:** Predict the country where a new Airbnb user will make their first booking from a list of 12 possible destinations (including 'NDF' for no booking).\n    * **Key Points:**\n        * Focuses on **new users** from the USA, requiring analysis of early behavioral signals.\n        * Predictions must be **ranked** (top 5 most probable countries per user) for evaluation.\n        * Includes nuanced classes: 'NDF' (no booking) vs. 'other' (booking to unlisted country).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data combining user demographics, web session logs, and booking history.\n    * **Primary Files:**\n        * `train_users.csv`, `test_users.csv`: User attributes (account creation date, demographics, device/browser info, etc.) with `country_destination` as the target.\n        * `sessions.csv`: Web session logs (actions, timestamps) linked to users via `user_id`.\n        * Auxiliary files (`countries.csv`, `age_gender_bkts.csv`) provide summary statistics.\n    * **Key Features:**\n        * User metadata: signup method, language, affiliate marketing channels.\n        * Temporal features: account creation date, first activity timestamp.\n        * Behavioral data: aggregated from session logs (action types, durations).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Normalized Discounted Cumulative Gain (NDCG@5)\n    * **Components:**\n        * Predictions are **ranked lists** of up to 5 countries per user.\n        * Only the correct country (if present in predictions) contributes to the score, with higher weight for top ranks.\n        * Formula: \n            * \\( DCG_k = \\sum_{i=1}^k \\frac{2^{rel_i} - 1}{\\log_2(i+1)} \\) (where \\( rel_i = 1 \\) for correct country, else 0).\n            * Normalized by the ideal DCG (IDCG) to scale scores between 0.0 and 1.0.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (with ranking component)\n* **Objective:** Predict the country where a new Airbnb user will make their first booking from a list of 12 possible destinations (including 'NDF' for no booking).\n    * **Key Points:**\n        * Focuses on **new users** from the USA, requiring analysis of early behavioral signals.\n        * Predictions must be **ranked** (top 5 most probable countries per user) for evaluation.\n        * Includes nuanced classes: 'NDF' (no booking) vs. 'other' (booking to unlisted country).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data combining user demographics, web session logs, and booking history.\n    * **Primary Files:**\n        * `train_users.csv`, `test_users.csv`: User attributes (account creation date, demographics, device/browser info, etc.) with `country_destination` as the target.\n        * `sessions.csv`: Web session logs (actions, timestamps) linked to users via `user_id`.\n        * Auxiliary files (`countries.csv`, `age_gender_bkts.csv`) provide summary statistics.\n    * **Key Features:**\n        * User metadata: signup method, language, affiliate marketing channels.\n        * Temporal features: account creation date, first activity timestamp.\n        * Behavioral data: aggregated from session logs (action types, durations).",
      "Evaluation Metrics": "* **Primary Metric:** Normalized Discounted Cumulative Gain (NDCG@5)\n    * **Components:**\n        * Predictions are **ranked lists** of up to 5 countries per user.\n        * Only the correct country (if present in predictions) contributes to the score, with higher weight for top ranks.\n        * Formula: \n            * \\( DCG_k = \\sum_{i=1}^k \\frac{2^{rel_i} - 1}{\\log_2(i+1)} \\) (where \\( rel_i = 1 \\) for correct country, else 0).\n            * Normalized by the ideal DCG (IDCG) to scale scores between 0.0 and 1.0."
    },
    "file_path": "kaggle_datasets/183/problem_summary.md"
  },
  "148": {
    "problem_id": "148",
    "title": "Probabilistic Rainfall Estimation from Polarimetric Radar Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Probabilistic Rainfall Estimation from Polarimetric Radar Data\n\n## Problem Description\n* **Problem Type**: Probabilistic Regression (Rainfall Distribution Prediction)\n* **Objective**: Predict the full probabilistic distribution of hourly rainfall accumulation (0-69mm) at gauge locations using polarimetric radar measurements. The goal is to estimate the cumulative distribution function (CDF) P(y≤Y) for each hour rather than a single point estimate.\n* **Key Points**:\n  * Addresses inherent mismatch between radar observations (spatial averages) and rain gauge measurements (point observations)\n  * Requires non-decreasing CDF outputs (P(y≤k) ≤ P(y≤k+1) for all k)\n  * Focuses on agricultural applications where probabilistic estimates are more valuable than deterministic ones\n\n## Dataset Overview\n* **Data Type**: Tabular time-series data from polarimetric radar systems paired with rain gauge measurements\n* **Context**: Weather monitoring in midwestern US corn-growing regions (2013-2014), with anonymized locations/times\n* **Data Files**:\n  * train_2013.csv - Training set (first 8 days of Apr-Nov 2013)\n  * test_2014.csv - Test set (same months in 2014)\n  * sampleSubmission.csv - Submission format template\n* **Key Features**:\n  * Radar measurements: Reflectivity, differential phase (Kdp), correlation coefficient (RhoHV), hydrometeor type\n  * Derived quantities: Multiple rain rate estimates (RR1-RR3), water volume, drop size statistics\n  * Metadata: TimeToEnd (minutes remaining in hour), DistanceToRadar (anonymized)\n  * Target: Expected (actual gauge measurement in mm)\n\n## Evaluation Metrics\n* **Primary Metric**: Continuous Ranked Probability Score (CRPS)\n  * Formula: C = (1/70N) Σ_N Σ_{n=0}^69 (P(y≤n) - H(n-z))²\n    * N = number of test samples\n    * z = actual gauge value (mm)\n    * H(x) = Heaviside step function (1 if x≥0, else 0)\n  * Measures how close the predicted CDF matches the observed rainfall\n  * Penalizes both bias and variance in predictions\n* **Validation Rule**: Submissions with non-monotonic CDFs (P(y≤k) > P(y≤",
    "sections": {},
    "file_path": "kaggle_datasets/148/problem_summary.md"
  },
  "23": {
    "problem_id": "23",
    "title": "Credit Scoring for Financial Distress Prediction",
    "problem_type": "Binary Classification (Probability Prediction)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Credit Scoring for Financial Distress Prediction\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Probability Prediction)\n* **Objective:**  \n    * Predict the probability that a borrower will experience financial distress in the next two years.  \n    * Improve upon existing credit scoring methods to help banks make better lending decisions and assist borrowers in financial planning.  \n* **Key Points:**  \n    * Focuses on real-world applicability in banking/finance sector.  \n    * Uses historical borrower data to assess default risk.  \n\n**Dataset Overview:**\n* **Data Type & Context:**  \n    * Tabular data containing anonymized financial and demographic information about 250,000 borrowers.  \n* **Data Files:**  \n    * `cs-training.csv` (training data)  \n    * `cs-test.csv` (test data)  \n    * `sampleEntry.csv` (sample submission format)  \n    * `Data Dictionary.xls` (feature descriptions)  \n* **Features:**  \n    * Likely includes financial history metrics (e.g., debt ratios, credit lines), payment behavior, and demographic factors.  \n    * Specific features not detailed in overview, but data dictionary would contain full descriptions.  \n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)  \n    * Measures model's ability to distinguish between borrowers who will/won't experience financial distress.  \n    * Higher values indicate better ranking of risky borrowers above safe ones.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Probability Prediction)\n* **Objective:**  \n    * Predict the probability that a borrower will experience financial distress in the next two years.  \n    * Improve upon existing credit scoring methods to help banks make better lending decisions and assist borrowers in financial planning.  \n* **Key Points:**  \n    * Focuses on real-world applicability in banking/finance sector.  \n    * Uses historical borrower data to assess default risk.",
      "Dataset Overview": "* **Data Type & Context:**  \n    * Tabular data containing anonymized financial and demographic information about 250,000 borrowers.  \n* **Data Files:**  \n    * `cs-training.csv` (training data)  \n    * `cs-test.csv` (test data)  \n    * `sampleEntry.csv` (sample submission format)  \n    * `Data Dictionary.xls` (feature descriptions)  \n* **Features:**  \n    * Likely includes financial history metrics (e.g., debt ratios, credit lines), payment behavior, and demographic factors.  \n    * Specific features not detailed in overview, but data dictionary would contain full descriptions.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)  \n    * Measures model's ability to distinguish between borrowers who will/won't experience financial distress.  \n    * Higher values indicate better ranking of risky borrowers above safe ones."
    },
    "file_path": "kaggle_datasets/23/problem_summary.md"
  },
  "525": {
    "problem_id": "525",
    "title": "Predicting Enzyme Thermostability from Protein Sequences",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Enzyme Thermostability from Protein Sequences\n\n## Problem Description\n* **Problem Type**: Regression (with ranking focus)\n* **Objective**: Predict the thermostability (melting temperature, `tm`) of enzyme variants based on their protein sequences and pH conditions. The primary goal is to accurately rank the stability of single-point mutation variants, with higher `tm` values indicating greater stability.\n* **Key Points**:\n  * Focus on predicting stability impact of single amino acid mutations/deletions\n  * Must consider pH conditions which affect protein stability\n  * Relative ranking (Spearman correlation) more important than absolute `tm` values\n  * Applications in biotechnology for designing more stable enzymes\n\n## Dataset Overview\n* **Data Type**: Tabular data with protein sequences and associated experimental measurements\n* **Context**: Enzyme variants with measured melting temperatures from published studies and Novozymes' lab\n* **Data Files**:\n  * `train.csv`: Contains protein sequences, pH values, data source, and target `tm` values\n  * `test.csv`: Contains protein sequences to predict (single-mutation variants of a specific enzyme)\n  * `wildtype_structure_prediction_af2.pdb`: AlphaFold2-predicted 3D structure of wild-type enzyme\n* **Key Features**:\n  * `protein_sequence`: Amino acid sequence (most 221 residues, some 220 due to deletions)\n  * `pH`: Acidity level during stability measurement\n  * `tm`: Target melting temperature (higher = more stable)\n\n## Evaluation Metrics\n* **Primary Metric**: Spearman's rank correlation coefficient\n  * Measures how well predicted rankings match true stability rankings\n  * Focuses on ordinal relationship rather than absolute values\n  * Range: -1 (perfect inverse correlation) to +1 (perfect agreement)\n  * Formula: ρ = 1 - (6∑d²)/(n(n²-1)), where d is rank difference per observation",
    "sections": {},
    "file_path": "kaggle_datasets/525/problem_summary.md"
  },
  "141": {
    "problem_id": "141",
    "title": "Predicting Popularity of New York Times Blog Articles",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Popularity of New York Times Blog Articles\n\n## Problem Description\n* **Problem Type:** Binary Classification  \n* **Objective:** Predict whether a New York Times blog article will receive 25 or more comments (binary indicator of popularity).  \n  * **Key Points:**  \n    * Focus on identifying features that drive article popularity for editorial prioritization.  \n    * Popularity is defined as a binary threshold (≥25 comments).  \n    * Data is sourced from NYTimes blog articles published between September-December 2014.  \n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing metadata and text features of NYTimes blog articles.  \n* **Data Files:**  \n  * `NYTimesBlogTrain.csv` (6,532 articles)  \n  * `NYTimesBlogTest.csv` (1,870 articles)  \n  * `SampleSubmission.csv` (example submission format)  \n* **Key Features:**  \n  * **Target:** `Popular` (1 if ≥25 comments, 0 otherwise).  \n  * **Features:**  \n    * Categorical: `NewsDesk`, `SectionName`, `SubsectionName`.  \n    * Text: `Headline`, `Snippet`, `Abstract`.  \n    * Numerical: `WordCount`.  \n    * Temporal: `PubDate` (publication timestamp).  \n    * Identifier: `UniqueID`.  \n\n## Evaluation Metrics\n* **Primary Metric:** AUC (Area Under the ROC Curve).  \n  * **Interpretation:**  \n    * Measures the model’s ability to distinguish between popular (1) and non-popular (0) articles.  \n    * AUC = 1 indicates perfect classification; 0.5 implies random guessing.  \n    * Robust to class imbalance compared to accuracy.  \n* **Submission Format:** CSV with columns `UniqueID` and `Probability1` (predicted probability of class 1).",
    "sections": {},
    "file_path": "kaggle_datasets/141/problem_summary.md"
  },
  "373": {
    "problem_id": "373",
    "title": "COVID-19 Global Forecasting (Time Series Prediction)",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict the cumulative number of confirmed COVID-19 cases and fatalities for future dates (April 16 - May 14, 2020) across global regions.",
    "evaluation_metric": null,
    "full_content": "# COVID-19 Global Forecasting (Time Series Prediction)\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict the cumulative number of confirmed COVID-19 cases and fatalities for future dates (April 16 - May 14, 2020) across global regions. \n    * **Key Points:**\n        * Secondary goal: Identify factors impacting COVID-19 transmission rates (e.g., policy interventions, environmental variables).\n        * Encourages incorporation of external datasets (e.g., temperature, policy actions, healthcare capacity).\n        * Focus on providing actionable insights for medical/governmental institutions.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data tracking COVID-19 cases and fatalities by region (country/state) over time.\n* **Data Files:**\n    * `train.csv`: Historical cumulative cases/fatalities by date and region.\n    * `test.csv`: Dates and regions for forecasting (with `ForecastId` for submission).\n    * `sample_submission.csv`: Submission format template.\n* **Features:** \n    * Key columns include date, region identifiers (e.g., country, state), and cumulative counts (confirmed cases, fatalities).\n    * Data sourced from Johns Hopkins CSSE, updated daily during the competition.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n    * **Calculation:**\n        * RMSLE per target column (ConfirmedCases, Fatalities):  \n          $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}$$\n        * Final score: Average of RMSLE across both columns.\n    * **Leaderboard Split:**\n        * Public: Evaluated on 2020-04-01 to 2020-04-15.\n        * Private: Evaluated on 2020-04-16 to 2020-05-14.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict the cumulative number of confirmed COVID-19 cases and fatalities for future dates (April 16 - May 14, 2020) across global regions. \n    * **Key Points:**\n        * Secondary goal: Identify factors impacting COVID-19 transmission rates (e.g., policy interventions, environmental variables).\n        * Encourages incorporation of external datasets (e.g., temperature, policy actions, healthcare capacity).\n        * Focus on providing actionable insights for medical/governmental institutions.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data tracking COVID-19 cases and fatalities by region (country/state) over time.\n* **Data Files:**\n    * `train.csv`: Historical cumulative cases/fatalities by date and region.\n    * `test.csv`: Dates and regions for forecasting (with `ForecastId` for submission).\n    * `sample_submission.csv`: Submission format template.\n* **Features:** \n    * Key columns include date, region identifiers (e.g., country, state), and cumulative counts (confirmed cases, fatalities).\n    * Data sourced from Johns Hopkins CSSE, updated daily during the competition.",
      "Evaluation Metrics": "* **Primary Metric:** Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n    * **Calculation:**\n        * RMSLE per target column (ConfirmedCases, Fatalities):  \n          $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}$$\n        * Final score: Average of RMSLE across both columns.\n    * **Leaderboard Split:**\n        * Public: Evaluated on 2020-04-01 to 2020-04-15.\n        * Private: Evaluated on 2020-04-16 to 2020-05-14."
    },
    "file_path": "kaggle_datasets/373/problem_summary.md"
  },
  "4": {
    "problem_id": "4",
    "title": "Binary Classification of HIV Progression Using Genetic and Clinical Data",
    "problem_type": "Binary Classification",
    "objective": "Predict whether an HIV patient's infection will improve (responder status = 1) or not (responder status = 0) after 16 weeks of therapy, based on genetic sequences and clinical indicators.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of HIV Progression Using Genetic and Clinical Data\n\n**Problem Description:**\n*   **Problem Type:** Binary Classification\n*   **Objective:** Predict whether an HIV patient's infection will improve (responder status = 1) or not (responder status = 0) after 16 weeks of therapy, based on genetic sequences and clinical indicators.\n    *   Improvement is defined as a 100-fold decrease in HIV-1 viral load.\n    *   Focuses on identifying genetic markers in HIV sequences that correlate with treatment response.\n*   **Key Points:**\n    *   Targets short-term progression prediction (16 weeks).\n    *   Uses nucleotide sequences of two HIV proteins (Reverse Transcriptase and Protease) along with clinical data.\n    *   No prior biological knowledge is required to participate.\n\n**Dataset Overview:**\n*   **Data Type & Context:** Tabular data containing HIV patient records with genetic sequences and clinical measurements.\n*   **Data Files:**\n    *   `training_data.csv`: Contains 1,000 patient records for model training.\n    *   `test_data.csv`: Contains 692 patient records for generating competition submissions.\n    *   `hivprogression_solution.csv`: Example submission file.\n*   **Features:**\n    *   Patient ID\n    *   Responder status (binary target variable)\n    *   Protease nucleotide sequence (if available)\n    *   Reverse Transcriptase nucleotide sequence (if available)\n    *   Viral load at therapy start (log-10 scale)\n    *   CD4 count at therapy start\n\n**Evaluation Metrics:**\n*   **Primary Metric:** Misclassification Error Rate (proportion of incorrect predictions)\n    *   Equal penalty for false positives and false negatives.\n    *   Final leaderboard score is calculated as `1 - misclassification rate`, so higher scores indicate better performance.\n*   **Validation Approach:**\n    *   Public leaderboard uses 30% of test data to prevent overfitting.\n    *   Full evaluation occurs after competition deadline.",
    "sections": {
      "Problem Description": "*   **Problem Type:** Binary Classification\n*   **Objective:** Predict whether an HIV patient's infection will improve (responder status = 1) or not (responder status = 0) after 16 weeks of therapy, based on genetic sequences and clinical indicators.\n    *   Improvement is defined as a 100-fold decrease in HIV-1 viral load.\n    *   Focuses on identifying genetic markers in HIV sequences that correlate with treatment response.\n*   **Key Points:**\n    *   Targets short-term progression prediction (16 weeks).\n    *   Uses nucleotide sequences of two HIV proteins (Reverse Transcriptase and Protease) along with clinical data.\n    *   No prior biological knowledge is required to participate.",
      "Dataset Overview": "*   **Data Type & Context:** Tabular data containing HIV patient records with genetic sequences and clinical measurements.\n*   **Data Files:**\n    *   `training_data.csv`: Contains 1,000 patient records for model training.\n    *   `test_data.csv`: Contains 692 patient records for generating competition submissions.\n    *   `hivprogression_solution.csv`: Example submission file.\n*   **Features:**\n    *   Patient ID\n    *   Responder status (binary target variable)\n    *   Protease nucleotide sequence (if available)\n    *   Reverse Transcriptase nucleotide sequence (if available)\n    *   Viral load at therapy start (log-10 scale)\n    *   CD4 count at therapy start",
      "Evaluation Metrics": "*   **Primary Metric:** Misclassification Error Rate (proportion of incorrect predictions)\n    *   Equal penalty for false positives and false negatives.\n    *   Final leaderboard score is calculated as `1 - misclassification rate`, so higher scores indicate better performance.\n*   **Validation Approach:**\n    *   Public leaderboard uses 30% of test data to prevent overfitting.\n    *   Full evaluation occurs after competition deadline."
    },
    "file_path": "kaggle_datasets/4/problem_summary.md"
  },
  "387": {
    "problem_id": "387",
    "title": "Multilingual Toxic Comment Classification",
    "problem_type": "Binary Classification (Text/NLP)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Multilingual Toxic Comment Classification\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Text/NLP)\n* **Objective:**  \n  * Predict the probability that a Wikipedia talk page comment is toxic (defined as *rude, disrespectful, or likely to make someone leave a discussion*).  \n  * Key challenge: Train models using **English-only training data** but evaluate on **multilingual test data** (non-English comments).  \n* **Key Points:**  \n  * Focus on **multilingual generalization** (zero-shot/few-shot learning for toxicity detection).  \n  * Builds on prior Jigsaw competitions (2018 toxicity classification, 2019 bias mitigation).  \n  * Dataset contains **explicit/profane content**.  \n\n**Dataset Overview:**  \n* **Data Type:** Text data (Wikipedia talk page comments) with toxicity labels.  \n* **Data Files:**  \n  * **Training:**  \n    * `jigsaw-toxic-comment-train.csv` (English Wikipedia comments).  \n    * `jigsaw-unintended-bias-train.csv` (English Civil Comments data with additional bias labels).  \n  * **Evaluation:**  \n    * `validation.csv` (multilingual comments for validation).  \n    * `test.csv` (multilingual comments for testing).  \n    * Preprocessed BERT-ready files (`*-processed-seqlen128.csv`).  \n* **Key Features:**  \n  * `comment_text`: Raw text of the comment.  \n  * `toxic`: Binary label (0/1) indicating toxicity.  \n  * `lang`: Language code (for test/validation data).  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n* **Submission Format:**  \n  * CSV with `id` and predicted probability (`toxic`) for each test comment.  \n  * Example:  \n    ```csv\n    id,toxic\n    0,0.5\n    1,0.1\n    ",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Text/NLP)\n* **Objective:**  \n  * Predict the probability that a Wikipedia talk page comment is toxic (defined as *rude, disrespectful, or likely to make someone leave a discussion*).  \n  * Key challenge: Train models using **English-only training data** but evaluate on **multilingual test data** (non-English comments).  \n* **Key Points:**  \n  * Focus on **multilingual generalization** (zero-shot/few-shot learning for toxicity detection).  \n  * Builds on prior Jigsaw competitions (2018 toxicity classification, 2019 bias mitigation).  \n  * Dataset contains **explicit/profane content**.  \n\n**Dataset Overview:**  \n* **Data Type:** Text data (Wikipedia talk page comments) with toxicity labels.  \n* **Data Files:**  \n  * **Training:**  \n    * `jigsaw-toxic-comment-train.csv` (English Wikipedia comments).  \n    * `jigsaw-unintended-bias-train.csv` (English Civil Comments data with additional bias labels).  \n  * **Evaluation:**  \n    * `validation.csv` (multilingual comments for validation).  \n    * `test.csv` (multilingual comments for testing).  \n    * Preprocessed BERT-ready files (`*-processed-seqlen128.csv`).  \n* **Key Features:**  \n  * `comment_text`: Raw text of the comment.  \n  * `toxic`: Binary label (0/1) indicating toxicity.  \n  * `lang`: Language code (for test/validation data).  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n* **Submission Format:**  \n  * CSV with `id` and predicted probability (`toxic`) for each test comment.  \n  * Example:  \n    ```csv\n    id,toxic\n    0,0.5\n    1,0.1"
    },
    "file_path": "kaggle_datasets/387/problem_summary.md"
  },
  "15": {
    "problem_id": "15",
    "title": "Predicting Chess Game Outcomes with Improved Rating Systems",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Chess Game Outcomes with Improved Rating Systems\n\n## Problem Description\n- **Problem Type**: Probability Prediction (Regression-like task predicting win/draw/loss probabilities)\n- **Objective**: Develop a more accurate chess rating system than the traditional Elo system to predict game outcomes (White win, draw, or Black win) between professional chess players. The system should:\n  * Track player strength over time using historical game results\n  * Account for the advantage of playing White\n  * Handle players with limited historical data\n- **Key Points**:\n  * Direct comparison against Elo and other established rating systems (Glicko, Chessmetrics)\n  * Special \"FIDE prize\" category for practical systems meeting strict transparency/updatability requirements\n  * Must work with incomplete historical data (some games lack color information)\n\n## Dataset Overview\n- **Data Type**: Tabular data of chess game results with player metadata\n- **Data Files**:\n  * Primary training set (3 parts, 1.84M games with complete game/color info)\n  * Secondary training set (312K games with results but missing color info)\n  * Tertiary training set (265K \"imputed\" games satisfying tournament statistics)\n  * Test set (100K games from months 133-135)\n  * Initial ratings list (14K players)\n- **Key Features**:\n  * Player IDs (anonymized)\n  * Month of game\n  * Game outcome (White score: 1.0, 0.5, or 0.0)\n  * Piece color assignments (when available)\n  * Previous game counts for stability filtering\n\n## Evaluation Metrics\n- **Primary Metric**: Binomial Deviance (Log Likelihood)\n  * Calculated as: -[Y*LOG10(E) + (1-Y)*LOG10(1-E)] per game\n  * Where Y = actual outcome (0, 0.5, or 1) and E = predicted White score\n  * Predictions capped at [0.01, 0.99] to avoid undefined log values\n  * Spurious test games (fake matchups) are excluded from scoring\n- **Interpretation**:\n  * Lower scores indicate better predictions\n  * Penalizes both overconfident incorrect predictions and underconfident correct predictions\n  * Naturally handles the three possible game outcomes (White win/draw/Black win)",
    "sections": {},
    "file_path": "kaggle_datasets/15/problem_summary.md"
  },
  "380": {
    "problem_id": "380",
    "title": "Wildlife Animal Species Classification in Camera Trap Images",
    "problem_type": "Multiclass Classification (Computer Vision - Fine-Grained Visual Categorization)",
    "objective": "Classify animal species in camera trap images where training and test data come from different geographic locations. The key challenge is generalizing to unseen camera locations with partially overlapping species distributions.",
    "evaluation_metric": null,
    "full_content": "# Wildlife Animal Species Classification in Camera Trap Images\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective:** Classify animal species in camera trap images where training and test data come from different geographic locations. The key challenge is generalizing to unseen camera locations with partially overlapping species distributions.\n* **Key Points:**\n  * Domain adaptation problem: Test cameras have different distributions from training cameras\n  * Multimodal data is allowed: Can leverage (i) primary camera trap data, (ii) iNaturalist citizen science data, and (iii) Landsat-8 multispectral imagery of locations\n  * Includes \"empty\" class (0) for images without animals\n  * Provides pre-trained animal detection model (Faster-RCNN) to assist with localization\n\n**Dataset Overview:**\n* **Data Type:** Image data (camera trap photos) with geographic metadata and optional multispectral satellite data\n* **Context:** Wildlife conservation monitoring through automated species identification\n* **Data Files:**\n  * Training set: 217,959 images from 441 global locations (WCS data)\n  * Test set: 62,894 images from 111 locations\n  * JSON files with annotations, metadata, and detection results\n  * Optional: iNaturalist datasets (2017-2019) and Landsat-8 multispectral patches\n* **Features:**\n  * Camera trap images (various species and empty scenes)\n  * Geographic location metadata\n  * For satellite data: 200x200x9 pixel patches across 9 spectral bands\n  * Provided animal detection bounding boxes and confidences\n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy\n  * Simple classification accuracy across all test images\n  * Must correctly predict species ID or \"empty\" class (0)\n* **Submission Format:**\n  * CSV with image IDs and predicted class integers\n  * Class 0 represents no animal present",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification (Computer Vision - Fine-Grained Visual Categorization)\n* **Objective:** Classify animal species in camera trap images where training and test data come from different geographic locations. The key challenge is generalizing to unseen camera locations with partially overlapping species distributions.\n* **Key Points:**\n  * Domain adaptation problem: Test cameras have different distributions from training cameras\n  * Multimodal data is allowed: Can leverage (i) primary camera trap data, (ii) iNaturalist citizen science data, and (iii) Landsat-8 multispectral imagery of locations\n  * Includes \"empty\" class (0) for images without animals\n  * Provides pre-trained animal detection model (Faster-RCNN) to assist with localization",
      "Dataset Overview": "* **Data Type:** Image data (camera trap photos) with geographic metadata and optional multispectral satellite data\n* **Context:** Wildlife conservation monitoring through automated species identification\n* **Data Files:**\n  * Training set: 217,959 images from 441 global locations (WCS data)\n  * Test set: 62,894 images from 111 locations\n  * JSON files with annotations, metadata, and detection results\n  * Optional: iNaturalist datasets (2017-2019) and Landsat-8 multispectral patches\n* **Features:**\n  * Camera trap images (various species and empty scenes)\n  * Geographic location metadata\n  * For satellite data: 200x200x9 pixel patches across 9 spectral bands\n  * Provided animal detection bounding boxes and confidences",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy\n  * Simple classification accuracy across all test images\n  * Must correctly predict species ID or \"empty\" class (0)\n* **Submission Format:**\n  * CSV with image IDs and predicted class integers\n  * Class 0 represents no animal present"
    },
    "file_path": "kaggle_datasets/380/problem_summary.md"
  },
  "3": {
    "problem_id": "3",
    "title": "Predicting World Cup 2010 Team Progression",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting World Cup 2010 Team Progression\n\n## Problem Description\n* **Problem Type**: Multi-class Classification (Ordinal Outcomes)\n* **Objective**: Predict how far each country will progress in the 2010 FIFA World Cup tournament, competing against quantitative models from major investment banks.\n* **Key Points**:\n  * Participants must predict tournament outcomes for all teams (winner, runner-up, quarter-finalists, etc.)\n  * Two parallel challenges:\n    * **Take on the Quants Challenge**: Basic prediction of team progression stages\n    * **Confidence Challenge**: Same predictions but weighted by confidence levels\n  * Predictions are \"or better\" (e.g., predicting \"quarter-final or better\" includes teams that reach semi-finals or finals)\n\n## Dataset Overview\n* **Data Type**: Tabular data with team statistics and historical performance\n* **Context**: Soccer/football team performance data for World Cup predictions\n* **Data Files**:\n  * `FIFA_country_codes.csv` (country code mappings)\n  * `world_cup_data.csv` (team statistics and historical data)\n* **Features**:\n  * FIFA rankings (1994-2010)\n  * Countries' historical World Cup records\n  * Continent and host country information\n  * Suggested external data sources include betting markets, Elo ratings, player salaries, and socioeconomic data\n\n## Evaluation Metrics\n* **Primary Evaluation Metrics**:\n  * **Take on the Quants Challenge**:\n    * Points awarded based on correct stage predictions (higher stages earn more points)\n    * Scoring table:\n      | Prediction Tier | Points |\n      |----------------|--------|\n      | Winner | 32 |\n      | Runner up or better | 16 |\n      | Third or better | 10.67 |\n      | Fourth or better | 8 |\n      | Quarter final or better | 4 |\n      | Round of 16 or better | 2 |\n      | Third in group or better | 1.33 |\n      | Last in group or better | 1 |\n  * **Confidence Challenge**:\n    * Confidence-weighted version of same predictions\n    * Correct predictions: + (confidence × tier points)\n    * Incorrect predictions: - (confidence × tier points)\n    * Different point values:\n      | Prediction Tier | Points |\n      |----------------|--------|\n      | Winner | 3.47 |\n      | Runner up or better | 2.",
    "sections": {},
    "file_path": "kaggle_datasets/3/problem_summary.md"
  },
  "374": {
    "problem_id": "374",
    "title": "Deepfake Detection in Video Data",
    "problem_type": "Binary Classification (Computer Vision - Video Analysis)",
    "objective": "Detect manipulated media (deepfakes) in video content, distinguishing between REAL videos and FAKE videos containing AI-generated facial/voice manipulations.",
    "evaluation_metric": null,
    "full_content": "# Deepfake Detection in Video Data\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Computer Vision - Video Analysis)\n* **Objective:** Detect manipulated media (deepfakes) in video content, distinguishing between REAL videos and FAKE videos containing AI-generated facial/voice manipulations.\n    * **Key Points:**\n        * Focus on detecting both facial swaps and voice manipulations\n        * Addresses real-world misinformation risks from deepfake technology\n        * Requires robust models that generalize to unseen organic videos\n        * Competition uses a unique black-box submission system where code is re-run on private test sets\n\n**Dataset Overview:**\n* **Data Type & Context:** Video files (.mp4) with metadata, containing both authentic and manipulated content\n* **Data Files:**\n    * Training Set (470GB total, split into 50 zip files)\n    * Public Validation Set (`test_videos.zip`)\n    * Public/Private Test Sets (withheld for evaluation)\n    * Sample files (`train_sample_videos.zip`, `sample_submission.csv`)\n* **Features:**\n    * Video frames (visual content)\n    * Audio tracks (for voice manipulation detection)\n    * Metadata including:\n        * Filename\n        * Label (REAL/FAKE)\n        * Original video reference (for FAKE samples)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss (Binary Cross-Entropy)\n    * **Calculation:**  \n        LogLoss = −1/𝑛 ∑[𝑦ᵢlog(𝑦̂ᵢ) + (1−𝑦ᵢ)log(1−𝑦̂ᵢ)]\n        * Where:\n            * n = number of videos\n            * 𝑦̂ᵢ = predicted probability of being FAKE\n            * 𝑦ᵢ = 1 if FAKE, 0 if REAL\n    * **Properties:**\n        * Penalizes confident wrong predictions severely\n        * Predictions are bounded away from 0/1 extremes\n        * Lower values indicate better performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Computer Vision - Video Analysis)\n* **Objective:** Detect manipulated media (deepfakes) in video content, distinguishing between REAL videos and FAKE videos containing AI-generated facial/voice manipulations.\n    * **Key Points:**\n        * Focus on detecting both facial swaps and voice manipulations\n        * Addresses real-world misinformation risks from deepfake technology\n        * Requires robust models that generalize to unseen organic videos\n        * Competition uses a unique black-box submission system where code is re-run on private test sets",
      "Dataset Overview": "* **Data Type & Context:** Video files (.mp4) with metadata, containing both authentic and manipulated content\n* **Data Files:**\n    * Training Set (470GB total, split into 50 zip files)\n    * Public Validation Set (`test_videos.zip`)\n    * Public/Private Test Sets (withheld for evaluation)\n    * Sample files (`train_sample_videos.zip`, `sample_submission.csv`)\n* **Features:**\n    * Video frames (visual content)\n    * Audio tracks (for voice manipulation detection)\n    * Metadata including:\n        * Filename\n        * Label (REAL/FAKE)\n        * Original video reference (for FAKE samples)",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss (Binary Cross-Entropy)\n    * **Calculation:**  \n        LogLoss = −1/𝑛 ∑[𝑦ᵢlog(𝑦̂ᵢ) + (1−𝑦ᵢ)log(1−𝑦̂ᵢ)]\n        * Where:\n            * n = number of videos\n            * 𝑦̂ᵢ = predicted probability of being FAKE\n            * 𝑦ᵢ = 1 if FAKE, 0 if REAL\n    * **Properties:**\n        * Penalizes confident wrong predictions severely\n        * Predictions are bounded away from 0/1 extremes\n        * Lower values indicate better performance"
    },
    "file_path": "kaggle_datasets/374/problem_summary.md"
  },
  "146": {
    "problem_id": "146",
    "title": "Restaurant Revenue Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Restaurant Revenue Prediction\n\n## Problem Description\n* **Problem Type**: Regression\n* **Objective**: Predict the annual revenue of restaurants based on demographic, real estate, and commercial data. The goal is to help TFI (the company behind brands like Burger King and Popeyes) make data-driven decisions about new restaurant locations, reducing the risk of closures and financial losses.\n    * **Key Points**:\n        * The current process for selecting new restaurant sites is subjective and based on personal judgment.\n        * Incorrect location choices can lead to closures within 18 months and operating losses.\n        * The competition aims to replace subjective decisions with a mathematical model.\n\n## Dataset Overview\n* **Data Type**: Tabular data\n* **Context**: Data includes restaurant opening dates, location details, and obfuscated features related to demographics, real estate, and commercial points of interest.\n* **Data Files**:\n    * `train.csv`: Training set with 137 restaurants (includes the target variable `Revenue`).\n    * `test.csv`: Test set with 100,000 restaurants (includes additional ignored data to deter manual guessing).\n    * `sampleSubmission.csv`: Example submission file in the correct format.\n* **Features**:\n    * **Id**: Restaurant identifier.\n    * **Open Date**: Opening date of the restaurant.\n    * **City**: City where the restaurant is located (includes Unicode characters).\n    * **City Group**: Categorization of cities (e.g., \"Big cities\" or \"Other\").\n    * **Type**: Restaurant type (e.g., Food Court (FC), Inline (IL), Drive Thru (DT), Mobile (MB)).\n    * **P1-P37**: Obfuscated features grouped into:\n        * **Demographic data**: Population, age/gender distribution, development scales.\n        * **Real estate data**: Property size (m²), facade details, parking availability.\n        * **Commercial data**: Proximity to points of interest (e.g., schools, banks, competitors).\n    * **Revenue**: Transformed annual revenue (target variable).\n\n## Evaluation Metrics\n* **Primary Metric**: Root Mean Squared Error (RMSE)\n    * **Components**:\n        * RMSE penalizes larger errors more heavily than smaller ones.\n        * Formula:  \n          \\[\n          \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y",
    "sections": {},
    "file_path": "kaggle_datasets/146/problem_summary.md"
  },
  "522": {
    "problem_id": "522",
    "title": "Video-Based Event Detection in Football Matches",
    "problem_type": "Computer Vision - Event Detection in Videos",
    "objective": "Develop a computer vision model to automatically classify three types of football (soccer) events in long video recordings:",
    "evaluation_metric": null,
    "full_content": "# Video-Based Event Detection in Football Matches\n\n**Problem Description:**\n* **Problem Type:** Computer Vision - Event Detection in Videos\n* **Objective:** Develop a computer vision model to automatically classify three types of football (soccer) events in long video recordings:\n    * **Plays** (passes, crosses)\n    * **Throw-ins**\n    * **Challenges**\n* **Key Points:**\n    * Goal is to scale data collection for youth/semi-pro leagues currently lacking resources\n    * Must detect both event type and precise timestamp\n    * Events have specific definitions (e.g., crosses require certain player positions)\n    * Competition runs in two stages: initial training phase and forecasting phase with future matches\n\n**Dataset Overview:**\n* **Data Type:** Video recordings of Bundesliga matches + annotated event timestamps\n* **Data Files:**\n    * `train/`: Videos from 8 games (some full matches, some halves)\n    * `test/`: Videos for public leaderboard (1 full + 4 half games)\n    * `clips/`: Additional unannotated videos for generalization\n    * `train.csv`: Event annotations with:\n        * `video_id`, `event` type, `time` (seconds), `event_attributes`\n* **Key Features:**\n    * Long video recordings (~50 minutes per half)\n    * Precise event timestamps required\n    * Scoring intervals defined where events should be detected\n\n**Evaluation Metrics:**\n* **Primary Metric:** Average Precision (AP) averaged over:\n    * Timestamp error thresholds (different tolerances per event class)\n    * Event classes\n* **Evaluation Process:**\n    1. **Selection:** Only predictions within scoring intervals are evaluated\n    2. **Assignment:** Predictions matched to ground-truth by:\n        * Event class\n        * Timestamp tolerance\n        * Confidence score\n    3. **Scoring:** AP calculated per (event × tolerance × video) group\n    4. **Reduction:** Final score = mean AP across tolerances, then mean across events\n* **Timestamp Tolerances:**\n    * Challenge: [0.30, 0.40, 0.50, 0.60, 0.70] seconds\n    * Play/Throw-in: [0.15, 0.20, 0.25, 0.30, 0.35] seconds",
    "sections": {
      "Problem Description": "* **Problem Type:** Computer Vision - Event Detection in Videos\n* **Objective:** Develop a computer vision model to automatically classify three types of football (soccer) events in long video recordings:\n    * **Plays** (passes, crosses)\n    * **Throw-ins**\n    * **Challenges**\n* **Key Points:**\n    * Goal is to scale data collection for youth/semi-pro leagues currently lacking resources\n    * Must detect both event type and precise timestamp\n    * Events have specific definitions (e.g., crosses require certain player positions)\n    * Competition runs in two stages: initial training phase and forecasting phase with future matches",
      "Dataset Overview": "* **Data Type:** Video recordings of Bundesliga matches + annotated event timestamps\n* **Data Files:**\n    * `train/`: Videos from 8 games (some full matches, some halves)\n    * `test/`: Videos for public leaderboard (1 full + 4 half games)\n    * `clips/`: Additional unannotated videos for generalization\n    * `train.csv`: Event annotations with:\n        * `video_id`, `event` type, `time` (seconds), `event_attributes`\n* **Key Features:**\n    * Long video recordings (~50 minutes per half)\n    * Precise event timestamps required\n    * Scoring intervals defined where events should be detected",
      "Evaluation Metrics": "* **Primary Metric:** Average Precision (AP) averaged over:\n    * Timestamp error thresholds (different tolerances per event class)\n    * Event classes\n* **Evaluation Process:**\n    1. **Selection:** Only predictions within scoring intervals are evaluated\n    2. **Assignment:** Predictions matched to ground-truth by:\n        * Event class\n        * Timestamp tolerance\n        * Confidence score\n    3. **Scoring:** AP calculated per (event × tolerance × video) group\n    4. **Reduction:** Final score = mean AP across tolerances, then mean across events\n* **Timestamp Tolerances:**\n    * Challenge: [0.30, 0.40, 0.50, 0.60, 0.70] seconds\n    * Play/Throw-in: [0.15, 0.20, 0.25, 0.30, 0.35] seconds"
    },
    "file_path": "kaggle_datasets/522/problem_summary.md"
  },
  "12": {
    "problem_id": "12",
    "title": "Predicting Grant Application Success",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a grant application to the University of Melbourne will be successful (1) or unsuccessful (0) based on historical application data. The goal is to help the university prioritize applications with higher success probability and identify key factors influencing grant approval.",
    "evaluation_metric": null,
    "full_content": "# Predicting Grant Application Success\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a grant application to the University of Melbourne will be successful (1) or unsuccessful (0) based on historical application data. The goal is to help the university prioritize applications with higher success probability and identify key factors influencing grant approval.\n    * **Key Points:**\n        * Focus on reducing wasted effort by academics on low-probability applications\n        * Requires interpretability to identify important success factors\n        * Model must be implementable by the university\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing grant application records with investigator profiles and application details\n* **Data Files:**\n    * `unimelb_training.csv` (8,707 applications from 2005-2008)\n    * `unimelb_test.csv` (2,176 applications from 2009-mid 2010)\n    * `unimelb_example.csv` (submission format example)\n* **Key Features:**\n    * Application details: Sponsor code, grant category, contract value band, start date\n    * Research classification: RFCD codes (research fields), SEO codes (socio-economic objectives)\n    * Investigator profiles: Role, seniority, publication history (A*/A/B/C journal counts)\n    * Historical performance: Number of previous successful/unsuccessful grants\n    * Demographic information: Birth country (aggregated by continent), home language\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Advantages:**\n        * Robust to class imbalance (20-25% success rate)\n        * Evaluates performance across all classification thresholds\n    * **Calculation Components:**\n        * True Positive Rate (Recall): TP / (TP + FN)\n        * False Positive Rate: FP / (FP + TN)\n        * AUC represents the area under the curve plotting TPR vs FPR at all thresholds\n        * Random guessing yields AUC = 0.5, perfect classifier yields AUC = 1",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a grant application to the University of Melbourne will be successful (1) or unsuccessful (0) based on historical application data. The goal is to help the university prioritize applications with higher success probability and identify key factors influencing grant approval.\n    * **Key Points:**\n        * Focus on reducing wasted effort by academics on low-probability applications\n        * Requires interpretability to identify important success factors\n        * Model must be implementable by the university",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing grant application records with investigator profiles and application details\n* **Data Files:**\n    * `unimelb_training.csv` (8,707 applications from 2005-2008)\n    * `unimelb_test.csv` (2,176 applications from 2009-mid 2010)\n    * `unimelb_example.csv` (submission format example)\n* **Key Features:**\n    * Application details: Sponsor code, grant category, contract value band, start date\n    * Research classification: RFCD codes (research fields), SEO codes (socio-economic objectives)\n    * Investigator profiles: Role, seniority, publication history (A*/A/B/C journal counts)\n    * Historical performance: Number of previous successful/unsuccessful grants\n    * Demographic information: Birth country (aggregated by continent), home language",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Advantages:**\n        * Robust to class imbalance (20-25% success rate)\n        * Evaluates performance across all classification thresholds\n    * **Calculation Components:**\n        * True Positive Rate (Recall): TP / (TP + FN)\n        * False Positive Rate: FP / (FP + TN)\n        * AUC represents the area under the curve plotting TPR vs FPR at all thresholds\n        * Random guessing yields AUC = 0.5, perfect classifier yields AUC = 1"
    },
    "file_path": "kaggle_datasets/12/problem_summary.md"
  },
  "179": {
    "problem_id": "179",
    "title": "Optimizing Santa's Sleigh Routes to Minimize Reindeer Weariness",
    "problem_type": "Combinatorial Optimization (Route and Load Optimization)",
    "objective": "Minimize the total weighted reindeer weariness (WRW) by optimizing the routes and loads of Santa's sleigh trips to deliver all gifts worldwide.",
    "evaluation_metric": null,
    "full_content": "# Optimizing Santa's Sleigh Routes to Minimize Reindeer Weariness\n\n**Problem Description:**\n* **Problem Type:** Combinatorial Optimization (Route and Load Optimization)\n* **Objective:** Minimize the total weighted reindeer weariness (WRW) by optimizing the routes and loads of Santa's sleigh trips to deliver all gifts worldwide. \n    * **Key Points:**\n        * All sleighs start and end at the North Pole (Lat=90, Long=0).\n        * Sleighs have a base weight of 10 and a weight limit of 1000 (excluding the base weight).\n        * Gifts must be delivered in the assigned order and cannot be dropped off early.\n        * Unlimited sleigh trips are allowed, but all gifts must be delivered.\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with geographic coordinates and weights.\n* **Data Files:**\n    * `gifts.csv`: Contains GiftId, Latitude, Longitude, and Weight for each gift.\n    * `sample_submission.csv`: Example submission with GiftId and TripId columns.\n* **Features:**\n    * `GiftId`: Unique identifier for each gift.\n    * `Latitude/Longitude`: Destination coordinates (used with Haversine distance).\n    * `Weight`: Weight of the gift (critical for load optimization).\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Weighted Reindeer Weariness (WRW), a custom metric.\n    * **Components of WRW:**\n        * Calculated as the sum of distances traveled multiplied by the cumulative weight carried at each segment.\n        * Formula:  \n          𝑊𝑅𝑊 = ∑(distance between stops) × (sum of weights carried after each stop).\n        * Example for a trip with gifts A and B:  \n          WRW = (NP→A) × (A+B+10) + (A→B) × (B+10) + (B→NP) × (10).\n    * **Distance Calculation:** Uses Haversine formula for spherical Earth distances.",
    "sections": {
      "Problem Description": "* **Problem Type:** Combinatorial Optimization (Route and Load Optimization)\n* **Objective:** Minimize the total weighted reindeer weariness (WRW) by optimizing the routes and loads of Santa's sleigh trips to deliver all gifts worldwide. \n    * **Key Points:**\n        * All sleighs start and end at the North Pole (Lat=90, Long=0).\n        * Sleighs have a base weight of 10 and a weight limit of 1000 (excluding the base weight).\n        * Gifts must be delivered in the assigned order and cannot be dropped off early.\n        * Unlimited sleigh trips are allowed, but all gifts must be delivered.",
      "Dataset Overview": "* **Data Type:** Tabular data with geographic coordinates and weights.\n* **Data Files:**\n    * `gifts.csv`: Contains GiftId, Latitude, Longitude, and Weight for each gift.\n    * `sample_submission.csv`: Example submission with GiftId and TripId columns.\n* **Features:**\n    * `GiftId`: Unique identifier for each gift.\n    * `Latitude/Longitude`: Destination coordinates (used with Haversine distance).\n    * `Weight`: Weight of the gift (critical for load optimization).",
      "Evaluation Metrics": "* **Evaluation Metric:** Weighted Reindeer Weariness (WRW), a custom metric.\n    * **Components of WRW:**\n        * Calculated as the sum of distances traveled multiplied by the cumulative weight carried at each segment.\n        * Formula:  \n          𝑊𝑅𝑊 = ∑(distance between stops) × (sum of weights carried after each stop).\n        * Example for a trip with gifts A and B:  \n          WRW = (NP→A) × (A+B+10) + (A→B) × (B+10) + (B→NP) × (10).\n    * **Distance Calculation:** Uses Haversine formula for spherical Earth distances."
    },
    "file_path": "kaggle_datasets/179/problem_summary.md"
  },
  "328": {
    "problem_id": "328",
    "title": "Binary Classification with an Anonymized Tabular Dataset",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability of the binary `target` variable for each instance in the test set. The dataset is anonymized with no provided data dictionary, requiring participants to rely on feature engineering and model selection without explicit feature context.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with an Anonymized Tabular Dataset\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of the binary `target` variable for each instance in the test set. The dataset is anonymized with no provided data dictionary, requiring participants to rely on feature engineering and model selection without explicit feature context.\n* **Key Points:**\n  * The competition serves as a trial for Kaggle's new synchronous Kernels-only format, where submissions are run against both public and private test sets in real time.\n  * Participants must ensure their code generalizes, as the private test set may differ in size, IDs, or other aspects from the public test set.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with anonymized features, presented as a binary classification problem. The dataset's origin and feature meanings are intentionally obscured (accompanied only by a cryptic poem hinting at careful feature selection).\n* **Data Files:**\n  * `train.csv`: Training set with features and binary target.\n  * `test.csv`: Test set (features only; requires target prediction).\n  * `sample_submission.csv`: Example submission file with `id` and predicted `target` probability.\n* **Features:** 517 anonymized columns (no descriptions provided). The dataset emphasizes robustness to unknown test-set variations.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC).\n  * Measures the model's ability to distinguish between the two classes.\n  * Predictions must be probabilities (values between 0 and 1) for each test instance.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of the binary `target` variable for each instance in the test set. The dataset is anonymized with no provided data dictionary, requiring participants to rely on feature engineering and model selection without explicit feature context.\n* **Key Points:**\n  * The competition serves as a trial for Kaggle's new synchronous Kernels-only format, where submissions are run against both public and private test sets in real time.\n  * Participants must ensure their code generalizes, as the private test set may differ in size, IDs, or other aspects from the public test set.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with anonymized features, presented as a binary classification problem. The dataset's origin and feature meanings are intentionally obscured (accompanied only by a cryptic poem hinting at careful feature selection).\n* **Data Files:**\n  * `train.csv`: Training set with features and binary target.\n  * `test.csv`: Test set (features only; requires target prediction).\n  * `sample_submission.csv`: Example submission file with `id` and predicted `target` probability.\n* **Features:** 517 anonymized columns (no descriptions provided). The dataset emphasizes robustness to unknown test-set variations.",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC).\n  * Measures the model's ability to distinguish between the two classes.\n  * Predictions must be probabilities (values between 0 and 1) for each test instance."
    },
    "file_path": "kaggle_datasets/328/problem_summary.md"
  },
  "85": {
    "problem_id": "85",
    "title": "Multi-label Bird Species Classification from Audio Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-label Bird Species Classification from Audio Recordings\n\n## Problem Description\n* **Problem Type:** Multi-label Classification (Audio Signal Processing)\n* **Objective:** Predict the set of bird species present in 10-second field recordings collected in natural environments. The task involves identifying multiple simultaneous bird vocalizations amidst environmental noise.\n* **Key Points:**\n  * Real-world challenges include overlapping bird vocalizations, non-bird sounds (insects), and background noise (wind/rain/vehicles)\n  * Dataset represents more difficult conditions than prior research (includes recordings with rain/wind)\n  * Requires processing raw audio into meaningful features for classification\n\n## Dataset Overview\n* **Data Type:** Audio recordings (WAV files) with associated spectrogram representations\n* **Context:** Field recordings from H.J. Andrews Experimental Forest in Oregon, collected over 2 years at 13 locations\n* **Data Files:**\n  * `src_wavs/`: Raw 10-second mono WAV files (16kHz, 16-bit)\n  * `rec_id2filename.txt`: Mapping between recording IDs and filenames\n  * `species_list.txt`: 19 target bird species (0-18)\n  * `CVfolds_2.txt`: Train/test split indicators\n  * `rec_labels_test_hidden.txt`: Training labels (test labels hidden)\n* **Key Features:**\n  * Provided in multiple representations:\n    * Raw WAV files\n    * Spectrograms (BMP images)\n    * Noise-filtered spectrograms\n    * 38-dimensional segment features\n    * Histogram of segments features\n  * Supplemental segmentation data available (bounding boxes, pixel-level annotations)\n\n## Evaluation Metrics\n* **Primary Metric:** Area Under ROC Curve (AUC)\n  * Calculated separately for each of the 19 species\n  * Final score is the average AUC across all species\n* **Evaluation Protocol:**\n  * Test set split into public (1/3) and private (2/3) portions\n  * Public leaderboard shows performance on public portion\n  * Final ranking determined by private test performance\n* **Submission Format:**\n  * Probability predictions for all 19 species per test recording\n  * Combined ID format: `rec_id * 100 + species_number`\n  * Requires exactly 6138 prediction lines (323 test recordings × 19 species)",
    "sections": {},
    "file_path": "kaggle_datasets/85/problem_summary.md"
  },
  "71": {
    "problem_id": "71",
    "title": "Gender Prediction from Handwritten Documents",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Gender Prediction from Handwritten Documents\n\n## Problem Description\n* **Problem Type**: Binary Classification (Gender Prediction)\n* **Objective**: Predict whether a handwritten document was produced by a male or female writer using handwriting analysis. The task has forensic applications and aims to advance research in document analysis.\n    * Participants must predict gender probabilities for 193 test-set writers using either:\n        * Raw handwritten document images (Arabic/English)\n        * Pre-extracted feature sets for those unfamiliar with image processing\n    * **Key Points**:\n        * Dataset includes variable and fixed text samples in both Arabic and English\n        * Handwriting samples come from 475 writers (282 for training, 193 for testing)\n        * Competition part of ICDAR2013 conference to benchmark state-of-the-art methods\n\n## Dataset Overview\n* **Data Type**: \n    * Primary: Handwritten document images (600dpi/300dpi scans)\n    * Alternative: Tabular feature representations for non-image approaches\n* **Data Files**:\n    * `images_gender.zip` - Full image dataset (600dpi)\n    * `images_subset.zip` - Sample subset (5 writers)\n    * `train.csv`/`test.csv` - Feature-based datasets\n    * `train_answers.csv` - Training labels (writer ID + gender)\n* **Key Features**:\n    * Per-writer samples: 4 pages (variable/fixed text in Arabic/English)\n    * Feature columns include:\n        - Writer ID\n        - Page ID (1-4)\n        - Language (Arabic/English)\n        - Same_text flag (for standardized content)\n        - Handcrafted features (similar to prior writer identification contests)\n\n## Evaluation Metrics\n* **Primary Metric**: Logarithmic Loss (LogLoss)\n    * Measures accuracy of predicted probabilities (0-1) for male gender classification\n    * Formula: $-\\frac{1}{N}\\sum_{i=1}^N [y_i\\log(p_i) + (1-y_i)\\log(1-p_i)]$\n        * $y_i$ = true label (1 for male, 0 for female)\n        * $p_i$ = predicted probability of being male\n        * N = number of test samples\n* **Submission Format**: CSV with writer IDs and male probability predictions",
    "sections": {},
    "file_path": "kaggle_datasets/71/problem_summary.md"
  },
  "541": {
    "problem_id": "541",
    "title": "Binary Classification with a Tabular Pulsar Dataset",
    "problem_type": "Binary Classification",
    "objective": "Predict whether an observation is a pulsar (binary target `Class`) based on tabular features. The goal is to generate predicted probabilities for the positive class (pulsar).",
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Tabular Pulsar Dataset\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether an observation is a pulsar (binary target `Class`) based on tabular features. The goal is to generate predicted probabilities for the positive class (pulsar).\n* **Key Points:**\n  * Dataset is synthetically generated from a real-world pulsar classification dataset, with similar but not identical feature distributions.\n  * Participants are encouraged to explore differences between synthetic and original data, and potentially incorporate the original dataset for improved performance.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data representing pulsar observations, synthetically generated from a deep learning model trained on real pulsar classification data.\n* **Data Files:**\n  * `train.csv`: Training data with binary target `Class`.\n  * `test.csv`: Test data for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in required format.\n* **Features:** Features are anonymized but derived from real pulsar classification data (original dataset includes characteristics like pulse profile statistics).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Log Loss (Logarithmic Loss)\n* **Metric Components:**\n  * Formula: `LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n    * `n`: Number of test set observations\n    * `ŷ_i`: Predicted probability of being a pulsar\n    * `y_i`: Actual label (1 for pulsar, 0 otherwise)\n  * Key characteristics:\n    * Heavily penalizes confident but wrong predictions\n    * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n    * Uses natural logarithm",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether an observation is a pulsar (binary target `Class`) based on tabular features. The goal is to generate predicted probabilities for the positive class (pulsar).\n* **Key Points:**\n  * Dataset is synthetically generated from a real-world pulsar classification dataset, with similar but not identical feature distributions.\n  * Participants are encouraged to explore differences between synthetic and original data, and potentially incorporate the original dataset for improved performance.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data representing pulsar observations, synthetically generated from a deep learning model trained on real pulsar classification data.\n* **Data Files:**\n  * `train.csv`: Training data with binary target `Class`.\n  * `test.csv`: Test data for which predictions must be made.\n  * `sample_submission.csv`: Example submission file in required format.\n* **Features:** Features are anonymized but derived from real pulsar classification data (original dataset includes characteristics like pulse profile statistics).",
      "Evaluation Metrics": "* **Primary Metric:** Log Loss (Logarithmic Loss)\n* **Metric Components:**\n  * Formula: `LogLoss = -1/n * Σ[y_i*log(ŷ_i) + (1-y_i)*log(1-ŷ_i)]`\n    * `n`: Number of test set observations\n    * `ŷ_i`: Predicted probability of being a pulsar\n    * `y_i`: Actual label (1 for pulsar, 0 otherwise)\n  * Key characteristics:\n    * Heavily penalizes confident but wrong predictions\n    * Predictions are bounded away from 0 and 1 to avoid infinite penalties\n    * Uses natural logarithm"
    },
    "file_path": "kaggle_datasets/541/problem_summary.md"
  },
  "317": {
    "problem_id": "317",
    "title": "Ciphertext Matching Challenge with Classic Encryption",
    "problem_type": "Text Matching / Cryptanalysis",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Ciphertext Matching Challenge with Classic Encryption\n\n**Problem Description:**\n* **Problem Type:** Text Matching / Cryptanalysis\n* **Objective:**  \n  Participants must match encrypted ciphertexts (from a movie review dataset) to their corresponding plaintexts by predicting the correct plaintext index for each ciphertext. The challenge involves decrypting text that has been transformed using up to 4 layered classic ciphers.\n* **Key Points:**\n  * Each ciphertext is encrypted with 1-4 sequential ciphers (indicated by a `difficulty` level).\n  * Texts were padded to the next hundred characters before encryption.\n  * The competition includes meta-puzzling elements to identify cipher types and keys.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  Text data consisting of encrypted movie reviews from the 2011 Stanford Movie Review Dataset.\n* **Data Files:**\n  * `training.csv`: Contains plaintext reviews (`text`), unique IDs (`plaintext_id`), and target indices (`index`).\n  * `test.csv`: Contains ciphertext IDs (`ciphertext_id`), encrypted text (`ciphertext`), and encryption difficulty levels (`difficulty`).\n  * `sample_submission.csv`: Example submission file with the required format.\n* **Features:**\n  * Plaintext: Raw movie review text (in training data).\n  * Ciphertext: Encrypted versions of reviews (in test data).\n  * Difficulty: Integer (1-4) indicating the number/layers of ciphers applied.\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Accuracy  \n  * Submissions are scored based on the percentage of correctly matched ciphertext-to-plaintext indices.\n  * Metric Calculation: `sklearn.metrics.accuracy_score` compares predicted indices against ground truth.",
    "sections": {
      "Problem Description": "* **Problem Type:** Text Matching / Cryptanalysis\n* **Objective:**  \n  Participants must match encrypted ciphertexts (from a movie review dataset) to their corresponding plaintexts by predicting the correct plaintext index for each ciphertext. The challenge involves decrypting text that has been transformed using up to 4 layered classic ciphers.\n* **Key Points:**\n  * Each ciphertext is encrypted with 1-4 sequential ciphers (indicated by a `difficulty` level).\n  * Texts were padded to the next hundred characters before encryption.\n  * The competition includes meta-puzzling elements to identify cipher types and keys.",
      "Dataset Overview": "* **Data Type & Context:**  \n  Text data consisting of encrypted movie reviews from the 2011 Stanford Movie Review Dataset.\n* **Data Files:**\n  * `training.csv`: Contains plaintext reviews (`text`), unique IDs (`plaintext_id`), and target indices (`index`).\n  * `test.csv`: Contains ciphertext IDs (`ciphertext_id`), encrypted text (`ciphertext`), and encryption difficulty levels (`difficulty`).\n  * `sample_submission.csv`: Example submission file with the required format.\n* **Features:**\n  * Plaintext: Raw movie review text (in training data).\n  * Ciphertext: Encrypted versions of reviews (in test data).\n  * Difficulty: Integer (1-4) indicating the number/layers of ciphers applied.",
      "Evaluation Metrics": "* **Evaluation Metric:** Accuracy  \n  * Submissions are scored based on the percentage of correctly matched ciphertext-to-plaintext indices.\n  * Metric Calculation: `sklearn.metrics.accuracy_score` compares predicted indices against ground truth."
    },
    "file_path": "kaggle_datasets/317/problem_summary.md"
  },
  "125": {
    "problem_id": "125",
    "title": "Predicting Fire Loss Cost for Insurance Policies",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Fire Loss Cost for Insurance Policies\n\n## Problem Description\n* **Problem Type:** Regression (predicting a continuous target variable)\n* **Objective:** Predict the transformed ratio of loss to total insured value (target variable) for insurance policies, enabling more accurate risk assessment and tailored insurance coverage.\n    * Focuses on modeling fire losses, which are high-severity, low-frequency events with inherent volatility.\n    * Goal is to improve identification of policyholders' risk exposure.\n* **Key Points:**\n    * The target variable represents a transformed loss-to-value ratio.\n    * Competition emphasizes innovation in modeling challenging insurance risk scenarios.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data representing insurance policy records with:\n    * Policy characteristics (17 normalized variables)\n    * Crime rate data (9 variables)\n    * Geodemographic data (37 variables)\n    * Weather data (236 variables)\n* **Data Files:**\n    * train.csv - Training data with target values\n    * test.csv - Test data for predictions\n    * sampleSubmission.csv - Example submission format\n* **Key Features:**\n    * Mixed feature types: ordinal, nominal, and continuous variables\n    * Special handling for missing values (\"Z\" level in categoricals, NA elsewhere)\n    * var11 serves as the weight for evaluation metric calculation\n    * Hierarchical structure in var4 (letter-number combinations)\n\n## Evaluation Metrics\n* **Primary Metric:** Normalized, weighted Gini coefficient\n    * **Calculation Process:**\n        1. Predictions are sorted from largest to smallest (only order matters)\n        2. For each percentile x%, calculate accumulated weighted loss (target × var11 weight)\n        3. Compare against \"null model\" straight line (10% loss in 10% data)\n        4. Gini coefficient = Area between model curve and straight line\n        5. Normalized by dividing by perfect model's Gini coefficient\n    * **Key Characteristics:**\n        * Emphasizes ranking quality rather than absolute prediction values\n        * Incorporates weighting through var11 in the dataset",
    "sections": {},
    "file_path": "kaggle_datasets/125/problem_summary.md"
  },
  "76": {
    "problem_id": "76",
    "title": "Multi-class Classification on Obfuscated Black Box Data",
    "problem_type": "Multi-class Classification",
    "objective": "Train a classifier on a dataset where the input features are intentionally obfuscated (not human-readable), without any prior knowledge of what the data represents. The goal is to achieve high classification accuracy on a private test set.",
    "evaluation_metric": null,
    "full_content": "# Multi-class Classification on Obfuscated Black Box Data\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification\n* **Objective:** Train a classifier on a dataset where the input features are intentionally obfuscated (not human-readable), without any prior knowledge of what the data represents. The goal is to achieve high classification accuracy on a private test set.\n    * **Key Points:**\n        * Designed to minimize human intervention in the training process (reducing \"human-in-the-loop\" advantages).\n        * Includes additional unsupervised data (135,735 examples) from a similar distribution to improve models.\n        * Classes are labeled from 1 to 9 (originally required as \"1.0\" to \"9.0\" but later updated to integer format).\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data with obfuscated numerical features (1,875 features per example). No semantic meaning is provided for the features or classes.\n* **Data Files:**\n    * `train.csv`: 1,000 labeled examples (features + class labels).\n    * `test.csv`: 10,000 unlabeled examples (5,000 public, 5,000 private test).\n    * `extra_unsupervised_data.npy/.tgz`: 135,735 unlabeled examples for unsupervised learning.\n    * `sample_submission.csv`: Example submission file.\n* **Features:** All 1,875 features are anonymized and non-interpretable (no metadata provided).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class Classification Accuracy (percentage of correctly classified test examples).\n    * **Submission Format Requirements:**\n        * CSV file with columns: `Id` (1 to 10,000) and `Class` (integers 1 through 9).\n        * Earlier submissions required class labels as strings (e.g., \"1.0\"), but this was later updated to integer format.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification\n* **Objective:** Train a classifier on a dataset where the input features are intentionally obfuscated (not human-readable), without any prior knowledge of what the data represents. The goal is to achieve high classification accuracy on a private test set.\n    * **Key Points:**\n        * Designed to minimize human intervention in the training process (reducing \"human-in-the-loop\" advantages).\n        * Includes additional unsupervised data (135,735 examples) from a similar distribution to improve models.\n        * Classes are labeled from 1 to 9 (originally required as \"1.0\" to \"9.0\" but later updated to integer format).",
      "Dataset Overview": "* **Data Type & Context:** Tabular data with obfuscated numerical features (1,875 features per example). No semantic meaning is provided for the features or classes.\n* **Data Files:**\n    * `train.csv`: 1,000 labeled examples (features + class labels).\n    * `test.csv`: 10,000 unlabeled examples (5,000 public, 5,000 private test).\n    * `extra_unsupervised_data.npy/.tgz`: 135,735 unlabeled examples for unsupervised learning.\n    * `sample_submission.csv`: Example submission file.\n* **Features:** All 1,875 features are anonymized and non-interpretable (no metadata provided).",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class Classification Accuracy (percentage of correctly classified test examples).\n    * **Submission Format Requirements:**\n        * CSV file with columns: `Id` (1 to 10,000) and `Class` (integers 1 through 9).\n        * Earlier submissions required class labels as strings (e.g., \"1.0\"), but this was later updated to integer format."
    },
    "file_path": "kaggle_datasets/76/problem_summary.md"
  },
  "579": {
    "problem_id": "579",
    "title": "AI Security Capture the Flag Challenges",
    "problem_type": "Adversarial Learning / Security Challenges (CTF-style)",
    "objective": "Solve 27 unique machine learning security challenges to capture digital flags by exploiting vulnerabilities in AI/ML systems. Challenges involve techniques like:",
    "evaluation_metric": null,
    "full_content": "# AI Security Capture the Flag Challenges\n\n**Problem Description:**\n* **Problem Type:** Adversarial Learning / Security Challenges (CTF-style)\n* **Objective:** Solve 27 unique machine learning security challenges to capture digital flags by exploiting vulnerabilities in AI/ML systems. Challenges involve techniques like:\n    * Model evasion\n    * Data poisoning\n    * Model stealing\n    * Adversarial fooling\n* **Key Points:**\n    * Each solved challenge yields a unique flag string (worth 1 point)\n    * Challenges vary in difficulty but are equally weighted\n    * Participants interact with external API endpoints to solve challenges\n    * Strict prohibition against sharing flags or infrastructure hacking\n\n**Dataset Overview:**\n* **Data Type:** Mixed-format challenge data (API interactions, not traditional datasets)\n* **Context:** Security-focused challenges requiring creative exploitation of ML systems\n* **Data Files:**\n    * Cluster1/ (folder)\n    * Cluster2/ (folder)\n    * Granny/ (folder)\n    * Hush/ (folder)\n    * Pixelated/ (folder)\n    * Terrance/ (folder)\n    * sample_submission.csv (template for flag submissions)\n* **Features:**\n    * Challenges involve interacting with ML systems through a provided query function\n    * Each challenge has unique prompts and required solution approaches\n    * Flags follow specific pattern (beginning with `gAAAAABl`)\n\n**Evaluation Metrics:**\n* **Evaluation Metric:** Cumulative flag count (1 point per solved challenge)\n    * Perfect score = 27 points (all challenges solved)\n* **Scoring Components:**\n    * Binary scoring per challenge (solved/not solved)\n    * Ties broken by earliest submission time\n    * Flags must be submitted in specific CSV format with Name/Flag columns",
    "sections": {
      "Problem Description": "* **Problem Type:** Adversarial Learning / Security Challenges (CTF-style)\n* **Objective:** Solve 27 unique machine learning security challenges to capture digital flags by exploiting vulnerabilities in AI/ML systems. Challenges involve techniques like:\n    * Model evasion\n    * Data poisoning\n    * Model stealing\n    * Adversarial fooling\n* **Key Points:**\n    * Each solved challenge yields a unique flag string (worth 1 point)\n    * Challenges vary in difficulty but are equally weighted\n    * Participants interact with external API endpoints to solve challenges\n    * Strict prohibition against sharing flags or infrastructure hacking",
      "Dataset Overview": "* **Data Type:** Mixed-format challenge data (API interactions, not traditional datasets)\n* **Context:** Security-focused challenges requiring creative exploitation of ML systems\n* **Data Files:**\n    * Cluster1/ (folder)\n    * Cluster2/ (folder)\n    * Granny/ (folder)\n    * Hush/ (folder)\n    * Pixelated/ (folder)\n    * Terrance/ (folder)\n    * sample_submission.csv (template for flag submissions)\n* **Features:**\n    * Challenges involve interacting with ML systems through a provided query function\n    * Each challenge has unique prompts and required solution approaches\n    * Flags follow specific pattern (beginning with `gAAAAABl`)",
      "Evaluation Metrics": "* **Evaluation Metric:** Cumulative flag count (1 point per solved challenge)\n    * Perfect score = 27 points (all challenges solved)\n* **Scoring Components:**\n    * Binary scoring per challenge (solved/not solved)\n    * Ties broken by earliest submission time\n    * Flags must be submitted in specific CSV format with Name/Flag columns"
    },
    "file_path": "kaggle_datasets/579/problem_summary.md"
  },
  "82": {
    "problem_id": "82",
    "title": "Predicting Useful Votes for Yelp Reviews",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Useful Votes for Yelp Reviews\n\n## Problem Description\n* **Problem Type:** Regression (with count prediction)\n* **Objective:** Predict the number of \"useful\" votes a Yelp review will receive at a future point in time, using review text, user behavior, and business data.\n    * Key Points:\n        * Focuses on predicting review quality metric (useful votes) without waiting for community feedback\n        * Requires handling temporal aspects (training data from 2013-01-19 vs. test data from 2013-03-12)\n        * Solution should be production-ready (competition tied to Yelp recruitment)\n\n## Dataset Overview\n* **Data Type:** JSON-structured text data with tabular relationships\n* **Context:** Yelp review ecosystem including businesses, users, check-ins, and review text\n* **Data Files:**\n    * `yelp_training_set.tgz/zip` (primary training data)\n    * `yelp_test_set.tgz/zip` (test data)\n    * `sample_submission.csv` (submission format)\n* **Key Features:**\n    * Review-level: text content, star rating, date, initial vote counts\n    * User-level: review history, average rating, voting patterns\n    * Business-level: location, categories, star rating, check-in patterns\n    * Temporal check-in data by hour/day of week\n\n## Evaluation Metrics\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * Components:\n        * `ϵ = sqrt(1/n * Σ(log(p_i + 1) - log(a_i + 1))^2)`\n        * Where:\n            * `p_i` = predicted useful votes\n            * `a_i` = actual useful votes\n            * `log` = natural logarithm\n        * Designed to properly scale errors relative to vote counts\n        * Predictions can be non-integer despite actual votes being whole numbers",
    "sections": {},
    "file_path": "kaggle_datasets/82/problem_summary.md"
  },
  "49": {
    "problem_id": "49",
    "title": "Predicting Xbox Game Interest from Search Queries",
    "problem_type": "Recommendation System / Multi-class Classification",
    "objective": "Predict which Xbox game (SKU) a visitor will be most interested in based on their search query and click behavior. Participants must recommend the top 5 most likely games for each user query.",
    "evaluation_metric": null,
    "full_content": "# Predicting Xbox Game Interest from Search Queries\n\n**Problem Description:**\n* **Problem Type:** Recommendation System / Multi-class Classification\n* **Objective:** Predict which Xbox game (SKU) a visitor will be most interested in based on their search query and click behavior. Participants must recommend the top 5 most likely games for each user query.\n* **Key Points:**\n  * Uses mobile web behavior data from Best Buy\n  * Focuses specifically on Xbox game products\n  * Accounts for temporal relationship between searches and clicks (within 5 minutes)\n  * Note: Some clicks may not be directly from search results\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing user search queries and product click behavior on Best Buy's mobile website\n* **Data Files:**\n  * train.csv - Contains user clicks with associated queries and timestamps\n  * test.csv - Contains user queries without click information (prediction target)\n  * small_product_data.xml - Product information for Xbox games\n  * popular_skus.py/popular_skus.csv - Sample benchmark submission\n* **Key Features:**\n  * User IDs\n  * Product SKUs and categories\n  * Search query terms\n  * Query and click timestamps (within 5-minute window)\n\n**Evaluation Metrics:**\n* **Primary Metric:** MAP@5 (Mean Average Precision at 5)\n  * Measures quality of top 5 recommendations\n  * Rewards putting correct items higher in recommendation list\n  * Calculates average precision for each query, then takes mean across all queries\n  * Specifically evaluates how well the model ranks relevant products in top positions",
    "sections": {
      "Problem Description": "* **Problem Type:** Recommendation System / Multi-class Classification\n* **Objective:** Predict which Xbox game (SKU) a visitor will be most interested in based on their search query and click behavior. Participants must recommend the top 5 most likely games for each user query.\n* **Key Points:**\n  * Uses mobile web behavior data from Best Buy\n  * Focuses specifically on Xbox game products\n  * Accounts for temporal relationship between searches and clicks (within 5 minutes)\n  * Note: Some clicks may not be directly from search results",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing user search queries and product click behavior on Best Buy's mobile website\n* **Data Files:**\n  * train.csv - Contains user clicks with associated queries and timestamps\n  * test.csv - Contains user queries without click information (prediction target)\n  * small_product_data.xml - Product information for Xbox games\n  * popular_skus.py/popular_skus.csv - Sample benchmark submission\n* **Key Features:**\n  * User IDs\n  * Product SKUs and categories\n  * Search query terms\n  * Query and click timestamps (within 5-minute window)",
      "Evaluation Metrics": "* **Primary Metric:** MAP@5 (Mean Average Precision at 5)\n  * Measures quality of top 5 recommendations\n  * Rewards putting correct items higher in recommendation list\n  * Calculates average precision for each query, then takes mean across all queries\n  * Specifically evaluates how well the model ranks relevant products in top positions"
    },
    "file_path": "kaggle_datasets/49/problem_summary.md"
  },
  "122": {
    "problem_id": "122",
    "title": "Binary Classification of Visual Stimuli from MEG Brain Recordings",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Visual Stimuli from MEG Brain Recordings\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the category of a visual stimulus (face or scrambled face) presented to a subject based on concurrent MEG (magnetoencephalography) recordings of their brain activity.\n    * **Key Points:**\n        * The task involves decoding brain activity patterns associated with visual stimuli.\n        * Data comes from multiple subjects, with train and test sets containing recordings from different subjects.\n        * Subject variability in brain anatomy and activity patterns is expected, making cross-subject generalization challenging.\n\n## Dataset Overview\n* **Data Type & Context:** Time series data from MEG recordings (306 channels at 250Hz) capturing brain activity during visual stimulus presentation.\n    * **Data Files:**\n        * `train_subjectXX.mat`: Contains 3D data matrices (trial × channel × time) and labels for 16 subjects.\n        * `test_subjectXX.mat`: Contains 3D data matrices for 7 subjects (no labels provided).\n        * Sample submission file (`random_submission.csv`).\n    * **Features:**\n        * Each trial consists of 1.5 seconds of MEG data (375 time points at 250Hz).\n        * 306 channels per trial (102 locations with 3 sensors each: 2 gradiometers and 1 magnetometer).\n        * Labels: 0 (Scrambled face) or 1 (Face).\n\n## Evaluation Metrics\n* **Evaluation Metric:** Prediction Accuracy (total correct predictions / total test trials).\n    * **Important Notes:**\n        * Public (leaderboard) and private (final) scores are computed on trials from different sets of subjects.\n        * This cross-subject evaluation emphasizes generalization across individuals.",
    "sections": {},
    "file_path": "kaggle_datasets/122/problem_summary.md"
  },
  "310": {
    "problem_id": "310",
    "title": "NCAA Women's Basketball Tournament Outcome Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NCAA Women's Basketball Tournament Outcome Prediction\n\n## Problem Description\n* **Problem Type:** Binary Classification (with probabilistic outputs)\n* **Objective:** Predict the probability of one team defeating another in NCAA Division I Women's Basketball Tournament (March Madness) matchups. The competition has two stages:\n  * Stage 1: Predict outcomes for historical tournaments (2014-2018 seasons)\n  * Stage 2: Forecast outcomes for the 2019 tournament before it begins\n* **Key Points:**\n  * Must predict probabilities for all possible pairwise matchups among the 64 tournament teams (2,016 predictions per season)\n  * Teams are uniquely identified by consistent 4-digit IDs (3000-3999 range)\n  * Competition emphasizes probabilistic predictions rather than binary outcomes\n  * Historical data spans multiple seasons with varying tournament scheduling formats\n\n## Dataset Overview\n* **Data Type:** Tabular data with basketball game results, team statistics, and tournament structure\n* **Primary Data Files:**\n  * `WTeams.csv` - Team IDs and names (366 unique teams)\n  * `WSeasons.csv` - Season metadata including tournament region assignments\n  * `WNCAATourneySeeds.csv` - Tournament seeds for all teams (1998-2019)\n  * `WRegularSeasonCompactResults.csv` - Game results from regular seasons (1998-2019)\n  * `WNCAATourneyCompactResults.csv` - NCAA tournament game results (1998-2019)\n  * Detailed results files with team-level box scores (2010-2019)\n  * Geographic data files with game locations (2010-2019)\n* **Key Features:**\n  * Game outcomes (win/loss, scores, overtime periods)\n  * Team performance statistics (field goals, rebounds, assists, turnovers, etc.)\n  * Tournament structure and seeding information\n  * Temporal features using standardized \"day numbers\" aligned across seasons\n  * Geographic location data for games\n\n## Evaluation Metrics\n* **Primary Metric:** Logarithmic Loss (Log Loss)\n  * Formula: \n    ```\n    LogLoss = −1/n * Σ[y_i*log(ŷ_i) + (1−y_i)*log(1−ŷ_i)]\n    ```\n  * Where:\n    * n = number of games\n    * ŷ_i = predicted probability of team 1 winning\n    * y_i = 1 if team 1",
    "sections": {},
    "file_path": "kaggle_datasets/310/problem_summary.md"
  },
  "546": {
    "problem_id": "546",
    "title": "Isolated American Sign Language Recognition with TensorFlow Lite",
    "problem_type": "Multiclass Classification (Video Classification)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Isolated American Sign Language Recognition with TensorFlow Lite\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification (Video Classification)\n* **Objective:**  \n    * Develop a TensorFlow Lite model to classify isolated American Sign Language (ASL) signs from landmark data extracted using MediaPipe Holistic Solution.  \n    * The model will be integrated into the PopSign educational game to enable players to practice signing by recognizing their gestures in real-time.  \n* **Key Points:**  \n    * Focus on **isolated signs** (250-sign vocabulary) rather than continuous signing.  \n    * Model must run **on-device** (mobile) with strict constraints:  \n        * <100ms latency per video  \n        * <40MB storage space  \n    * Must handle variations in signing (regional accents, left/right-handed signers, timing inconsistencies).  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Processed **landmark data** (x,y,z coordinates) extracted from ~100k videos of ASL signs using MediaPipe Holistic.  \n    * Landmarks represent facial features, hand poses (left/right), and body poses.  \n* **Data Files:**  \n    * `train_landmark_files/[participant_id]/[sequence_id].parquet` - Landmark sequences (543 landmarks per frame).  \n    * `train.csv` - Metadata mapping sequences to labels (`sign` column).  \n    * `sign_to_prediction_index_map.json` - Label encoding.  \n* **Key Features:**  \n    * Normalized spatial coordinates (`x`, `y`, `z`) for each landmark.  \n    * Landmark types: `face`, `left_hand`, `right_hand`, `pose`.  \n    * Note: `z` values (depth) are less reliable due to MediaPipe limitations.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Classification Accuracy  \n    * Simple ratio of correctly predicted signs to total predictions.  \n* **Constraints:**  \n    * Submissions must be **TensorFlow Lite models** (v2.9.1 compatible).  \n    * Inference must meet latency/storage limits (tested on ~40k videos).  \n    * Model inputs/outputs must adhere to specified signatures (e.g., `serving_default`).",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification (Video Classification)\n* **Objective:**  \n    * Develop a TensorFlow Lite model to classify isolated American Sign Language (ASL) signs from landmark data extracted using MediaPipe Holistic Solution.  \n    * The model will be integrated into the PopSign educational game to enable players to practice signing by recognizing their gestures in real-time.  \n* **Key Points:**  \n    * Focus on **isolated signs** (250-sign vocabulary) rather than continuous signing.  \n    * Model must run **on-device** (mobile) with strict constraints:  \n        * <100ms latency per video  \n        * <40MB storage space  \n    * Must handle variations in signing (regional accents, left/right-handed signers, timing inconsistencies).  \n\n**Dataset Overview:**  \n* **Data Type & Context:**  \n    * Processed **landmark data** (x,y,z coordinates) extracted from ~100k videos of ASL signs using MediaPipe Holistic.  \n    * Landmarks represent facial features, hand poses (left/right), and body poses.  \n* **Data Files:**  \n    * `train_landmark_files/[participant_id]/[sequence_id].parquet` - Landmark sequences (543 landmarks per frame).  \n    * `train.csv` - Metadata mapping sequences to labels (`sign` column).  \n    * `sign_to_prediction_index_map.json` - Label encoding.  \n* **Key Features:**  \n    * Normalized spatial coordinates (`x`, `y`, `z`) for each landmark.  \n    * Landmark types: `face`, `left_hand`, `right_hand`, `pose`.  \n    * Note: `z` values (depth) are less reliable due to MediaPipe limitations.  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Classification Accuracy  \n    * Simple ratio of correctly predicted signs to total predictions.  \n* **Constraints:**  \n    * Submissions must be **TensorFlow Lite models** (v2.9.1 compatible).  \n    * Inference must meet latency/storage limits (tested on ~40k videos).  \n    * Model inputs/outputs must adhere to specified signatures (e.g., `serving_default`)."
    },
    "file_path": "kaggle_datasets/546/problem_summary.md"
  },
  "40": {
    "problem_id": "40",
    "title": "Social Network Link Prediction Challenge",
    "problem_type": "Graph-Based Recommendation (Link Prediction)",
    "objective": "Predict missing directed edges in an anonymized social graph by recommending up to 10 potential connections for each test user. The goal is to rank these recommendations such that true missing links appear as high as possible in the predicted list.",
    "evaluation_metric": null,
    "full_content": "# Social Network Link Prediction Challenge\n\n**Problem Description:**\n* **Problem Type:** Graph-Based Recommendation (Link Prediction)\n* **Objective:** Predict missing directed edges in an anonymized social graph by recommending up to 10 potential connections for each test user. The goal is to rank these recommendations such that true missing links appear as high as possible in the predicted list.\n* **Key Points:**\n  * Focus on precision of top recommendations rather than binary classification\n  * Directed graph structure (source → destination nodes)\n  * Participants must work individually and use only provided data\n  * Top submissions undergo code review by Facebook for recruitment consideration\n\n**Dataset Overview:**\n* **Data Type & Context:** Directed social graph represented as edge pairs (tabular format)\n* **Data Files:**\n  * `train.csv`: Contains source-destination node pairs representing existing edges\n  * `test.csv`: Contains source nodes requiring destination recommendations\n  * Benchmark files (bfs_benchmark.csv, top_k_benchmark.csv, random_benchmark.csv)\n* **Features:**\n  * Source node ID (integer)\n  * Destination node ID (integer)\n  * Implicit graph structure features must be derived from edge connections\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Average Precision @ 10 (MAP@10)\n* **Components:**\n  * For each user:\n    * Calculate precision at each rank position k (P(k)) up to 10\n    * Sum P(k) values where recommended nodes are correct\n    * Divide by minimum of actual missing edges or 10\n  * Final score averages AP@10 across all test users\n  * Example Scoring:\n    * Correct recommendations at positions 1 & 3: (1/1 + 2/3)/min(m,10)\n    * Higher scores when correct predictions appear earlier in recommendation list",
    "sections": {
      "Problem Description": "* **Problem Type:** Graph-Based Recommendation (Link Prediction)\n* **Objective:** Predict missing directed edges in an anonymized social graph by recommending up to 10 potential connections for each test user. The goal is to rank these recommendations such that true missing links appear as high as possible in the predicted list.\n* **Key Points:**\n  * Focus on precision of top recommendations rather than binary classification\n  * Directed graph structure (source → destination nodes)\n  * Participants must work individually and use only provided data\n  * Top submissions undergo code review by Facebook for recruitment consideration",
      "Dataset Overview": "* **Data Type & Context:** Directed social graph represented as edge pairs (tabular format)\n* **Data Files:**\n  * `train.csv`: Contains source-destination node pairs representing existing edges\n  * `test.csv`: Contains source nodes requiring destination recommendations\n  * Benchmark files (bfs_benchmark.csv, top_k_benchmark.csv, random_benchmark.csv)\n* **Features:**\n  * Source node ID (integer)\n  * Destination node ID (integer)\n  * Implicit graph structure features must be derived from edge connections",
      "Evaluation Metrics": "* **Primary Metric:** Mean Average Precision @ 10 (MAP@10)\n* **Components:**\n  * For each user:\n    * Calculate precision at each rank position k (P(k)) up to 10\n    * Sum P(k) values where recommended nodes are correct\n    * Divide by minimum of actual missing edges or 10\n  * Final score averages AP@10 across all test users\n  * Example Scoring:\n    * Correct recommendations at positions 1 & 3: (1/1 + 2/3)/min(m,10)\n    * Higher scores when correct predictions appear earlier in recommendation list"
    },
    "file_path": "kaggle_datasets/40/problem_summary.md"
  },
  "319": {
    "problem_id": "319",
    "title": "TMDB Box Office Revenue Prediction",
    "problem_type": "Regression",
    "objective": "Predict the worldwide box office revenue for movies based on metadata features. The goal is to build a model that accurately estimates a movie's financial success using pre-release available data.",
    "evaluation_metric": null,
    "full_content": "# TMDB Box Office Revenue Prediction\n\n**Problem Description:**\n* **Problem Type:** Regression\n* **Objective:** Predict the worldwide box office revenue for movies based on metadata features. The goal is to build a model that accurately estimates a movie's financial success using pre-release available data.\n    * **Key Points:**\n        * Must use only data available before a movie's release (no post-release metrics).\n        * Dataset includes remakes and unrelated movies with identical titles (must treat as separate instances).\n        * Encourages collection of additional publicly available pre-release data.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing movie metadata from The Movie Database (TMDB), including:\n    * Cast/crew information\n    * Plot keywords\n    * Budget\n    * Release dates\n    * Production companies/countries\n    * Languages\n    * Poster images\n* **Data Files:**\n    * train.csv (7,398 movies)\n    * test.csv (4,398 movies)\n    * sample_submission.csv\n* **Notable Features:**\n    * Mixed data types (numerical, categorical, text, image URLs)\n    * High variance in revenue values (from flops to blockbusters)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * **Key Components:**\n        * Uses logarithmic transformation to prevent over-weighting blockbuster movies\n        * Calculated as: sqrt(mean((log(pred + 1) - log(actual + 1))^2))\n        * +1 terms handle zero-revenue edge cases",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression\n* **Objective:** Predict the worldwide box office revenue for movies based on metadata features. The goal is to build a model that accurately estimates a movie's financial success using pre-release available data.\n    * **Key Points:**\n        * Must use only data available before a movie's release (no post-release metrics).\n        * Dataset includes remakes and unrelated movies with identical titles (must treat as separate instances).\n        * Encourages collection of additional publicly available pre-release data.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing movie metadata from The Movie Database (TMDB), including:\n    * Cast/crew information\n    * Plot keywords\n    * Budget\n    * Release dates\n    * Production companies/countries\n    * Languages\n    * Poster images\n* **Data Files:**\n    * train.csv (7,398 movies)\n    * test.csv (4,398 movies)\n    * sample_submission.csv\n* **Notable Features:**\n    * Mixed data types (numerical, categorical, text, image URLs)\n    * High variance in revenue values (from flops to blockbusters)",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Squared Logarithmic Error (RMSLE)\n    * **Key Components:**\n        * Uses logarithmic transformation to prevent over-weighting blockbuster movies\n        * Calculated as: sqrt(mean((log(pred + 1) - log(actual + 1))^2))\n        * +1 terms handle zero-revenue edge cases"
    },
    "file_path": "kaggle_datasets/319/problem_summary.md"
  },
  "584": {
    "problem_id": "584",
    "title": "Predicting Gene Expression Changes from Single-Cell Perturbations",
    "problem_type": "Regression (Multi-output)",
    "objective": "Predict how small molecules (drug compounds) change gene expression across different cell types. The goal is to model differential expression values (-log10(p-value) * sign(LFC)) for 18,211 genes given a cell type and compound pair.",
    "evaluation_metric": null,
    "full_content": "# Predicting Gene Expression Changes from Single-Cell Perturbations\n\n**Problem Description:**\n* **Problem Type:** Regression (Multi-output)\n* **Objective:** Predict how small molecules (drug compounds) change gene expression across different cell types. The goal is to model differential expression values (-log10(p-value) * sign(LFC)) for 18,211 genes given a cell type and compound pair.\n* **Key Points:**\n  * Focus on generalizing perturbation responses across cell types (train on some cell types, predict on others)\n  * Applications in drug discovery by accelerating prediction of cellular responses\n  * Biological priors (multi-omics data) are provided to inform predictions\n  * Challenge involves predicting into new cell types with limited training data for those types\n\n**Dataset Overview:**\n* **Data Type:** Tabular data with biological multi-omics features\n* **Context:** Single-cell RNA sequencing data from human PBMCs (peripheral blood mononuclear cells) treated with 144 compounds\n* **Key Files:**\n  * `de_train.parquet` - Main training data with differential expression values\n  * `adata_train.parquet` - Raw single-cell count data\n  * `multiome_train.parquet` - Additional multi-omics data (RNA + chromatin accessibility)\n  * `id_map.csv` - Cell type/compound pairs to predict\n* **Important Features:**\n  * 18,211 gene expression features\n  * Cell type annotations (T cells, NK cells, B cells, Myeloid cells)\n  * Compound information (SMILES strings, LINCS IDs)\n  * Multi-omics features (gene expression + ATAC-seq peaks)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Rowwise Root Mean Squared Error (MRRMSE)\n* **Calculation:**\n  * Computed separately for each row (cell type/compound pair)\n  * Averages RMSE across all 18,211 genes per row\n  * Then averages these row-wise RMSE values across all test samples\n  * Formula: MRRMSE = (1/R) * Σ[ (1/n) * Σ(y_ij - ŷ_ij)^2 ]^(1/2) where R=rows, n=genes",
    "sections": {
      "Problem Description": "* **Problem Type:** Regression (Multi-output)\n* **Objective:** Predict how small molecules (drug compounds) change gene expression across different cell types. The goal is to model differential expression values (-log10(p-value) * sign(LFC)) for 18,211 genes given a cell type and compound pair.\n* **Key Points:**\n  * Focus on generalizing perturbation responses across cell types (train on some cell types, predict on others)\n  * Applications in drug discovery by accelerating prediction of cellular responses\n  * Biological priors (multi-omics data) are provided to inform predictions\n  * Challenge involves predicting into new cell types with limited training data for those types",
      "Dataset Overview": "* **Data Type:** Tabular data with biological multi-omics features\n* **Context:** Single-cell RNA sequencing data from human PBMCs (peripheral blood mononuclear cells) treated with 144 compounds\n* **Key Files:**\n  * `de_train.parquet` - Main training data with differential expression values\n  * `adata_train.parquet` - Raw single-cell count data\n  * `multiome_train.parquet` - Additional multi-omics data (RNA + chromatin accessibility)\n  * `id_map.csv` - Cell type/compound pairs to predict\n* **Important Features:**\n  * 18,211 gene expression features\n  * Cell type annotations (T cells, NK cells, B cells, Myeloid cells)\n  * Compound information (SMILES strings, LINCS IDs)\n  * Multi-omics features (gene expression + ATAC-seq peaks)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Rowwise Root Mean Squared Error (MRRMSE)\n* **Calculation:**\n  * Computed separately for each row (cell type/compound pair)\n  * Averages RMSE across all 18,211 genes per row\n  * Then averages these row-wise RMSE values across all test samples\n  * Formula: MRRMSE = (1/R) * Σ[ (1/n) * Σ(y_ij - ŷ_ij)^2 ]^(1/2) where R=rows, n=genes"
    },
    "file_path": "kaggle_datasets/584/problem_summary.md"
  },
  "570": {
    "problem_id": "570",
    "title": "American Sign Language Fingerspelling Recognition",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# American Sign Language Fingerspelling Recognition\n\n## Problem Description\n* **Problem Type:** Sequence-to-Sequence Prediction (Video Classification)\n* **Objective:** Develop a model to translate American Sign Language (ASL) fingerspelling videos into text. The task involves:\n  * Processing hand and facial landmark sequences from videos\n  * Recognizing fingerspelled characters (letters, numbers, symbols) with co-articulation effects\n  * Handling varied signing styles, backgrounds, and lighting conditions\n* **Key Points:**\n  * Focuses on fingerspelling (not full ASL grammar)\n  * Must handle real-world signing speeds (average 57 words/minute)\n  * Dataset includes co-articulation effects where handshapes modify based on adjacent letters\n  * Models must be efficient (≤40MB storage, ≤5 hours inference time for test set)\n\n## Dataset Overview\n* **Data Type:** Time-series landmark coordinates extracted from videos\n* **Context:** Smartphone videos of 100+ Deaf signers fingerspelling phrases, addresses, and URLs\n* **Data Files:**\n  * `train.csv`/`supplemental_metadata.csv`: Metadata with file paths and phrases\n  * `*_landmarks/`: Parquet files containing landmark sequences\n  * `character_to_prediction_index.json`: Character mapping dictionary\n* **Features:**\n  * 1,629 spatial coordinates (x,y,z) for 543 landmarks per frame\n  * Landmark types: face, left_hand, pose, right_hand\n  * Normalized coordinates from MediaPipe holistic model\n  * Sequences contain ~1,000 phrases each\n\n## Evaluation Metrics\n* **Primary Metric:** Normalized total Levenshtein distance\n  * Calculation: `(N - D) / N` where:\n    * `N` = Total characters in all labels\n    * `D` = Total Levenshtein distance (edit distance) between predictions and labels\n  * Ranges from 0 (worst) to 1 (perfect prediction)\n* **Implementation Constraints:**\n  * Submissions must be TensorFlow Lite models\n  * Must use provided inference pipeline format\n  * Optional `inference_args.json` for column subset selection",
    "sections": {},
    "file_path": "kaggle_datasets/570/problem_summary.md"
  },
  "114": {
    "problem_id": "114",
    "title": "Predicting Happiness from Polling Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Happiness from Polling Data\n\n## Problem Description\n- **Problem Type:** Binary Classification\n- **Objective:** Predict whether a user is happy (1) or not happy/neutral (0) based on their demographic information and responses to informal polling questions.\n- **Key Points:**\n  - Uses data from Show of Hands, a mobile/web polling platform with over 300,000 downloads and 75 million votes.\n  - Focuses on identifying which life aspects and characteristics predict happiness.\n  - Limited to students of the MITx 15.071x course on edX.\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data containing user demographics and responses to 101 polling questions.\n- **Data Files:**\n  - `train.csv`: Training set with features and target variable (Happy).\n  - `test.csv`: Test set with features only.\n  - `sampleSubmission.csv`: Example submission file format.\n  - `Questions.pdf`: Question text and possible answers for each question code.\n- **Key Features:**\n  - **Demographics:** YOB (Year of Birth), Gender, Income, HouseholdStatus, EducationLevel, Party.\n  - **Poll Responses:** 101 anonymized question columns (Q124742 to Q96024) with blank entries for unanswered questions.\n  - **Derived Feature:** `votes` (count of questions answered by user).\n  - **Target Variable:** `Happy` (binary: 1=happy, 0=neutral/not happy).\n\n## Evaluation Metrics\n- **Primary Metric:** Area Under the Curve (AUC)\n  - **Interpretation:** Measures the model's ability to distinguish between happy and unhappy users. \n    - AUC = 1: Perfect prediction\n    - AUC ≈ 0.5: Random guessing\n  - **Advantage:** Less sensitive to class imbalance than accuracy.\n- **Submission Format:** CSV with `UserID` and predicted probability of happiness (`Probability1`).",
    "sections": {},
    "file_path": "kaggle_datasets/114/problem_summary.md"
  },
  "326": {
    "problem_id": "326",
    "title": "Fine-Grained Segmentation and Classification of Fashion Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Segmentation and Classification of Fashion Images\n\n## Problem Description\n* **Problem Type**: Computer Vision - Instance Segmentation & Multi-label Classification\n* **Objective**: Develop algorithms to accurately segment and classify fashion images by:\n    * Performing fine-grained instance segmentation of apparel items and parts\n    * Assigning correct category labels and associated attributes to each segmented region\n* **Key Points**:\n    * Combines segmentation with fine-grained attribute classification\n    * Uses a specialized fashion taxonomy with 46 apparel objects and 92 attributes\n    * Must handle complex fashion structures and ambiguous descriptions\n    * Focuses on real-world applications for fashion professionals and consumers\n\n## Dataset Overview\n* **Data Type**: Image data with segmentation masks and multi-label annotations\n* **Context**: Fashion images from daily-life, celebrity events, and online shopping\n* **Data Files**:\n    * `train/` - Training images (40K with instance segmentation, 10K with additional fine-grained attributes)\n    * `test/` - Test images to segment/classify\n    * `train.csv` - Training annotations with segmentation masks and labels\n    * `label_descriptions.json` - Taxonomy of apparel categories and attributes\n* **Features**:\n    * `ImageId` - Unique image identifier\n    * `EncodedPixels` - Run-length encoded segmentation masks\n    * `ClassId` - Concatenated category and attribute labels (e.g., \"35_24_51_69_88_195_210_306\")\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Average Precision at different IoU thresholds (mAP@[0.5:0.95])\n* **Components**:\n    * Calculates IoU (Intersection over Union) for each predicted mask\n    * Evaluates at 10 IoU thresholds from 0.5 to 0.95 (step 0.05)\n    * For each threshold:\n        * Computes precision using true positives (TP), false positives (FP), and false negatives (FN)\n        * TP requires correct mask (IoU > threshold) AND correct class prediction\n    * Final score is mean of average precisions across all images and classes\n    * Requires mask resizing to (512, 512) before submission",
    "sections": {},
    "file_path": "kaggle_datasets/326/problem_summary.md"
  },
  "548": {
    "problem_id": "548",
    "title": "Multi-Agent Reinforcement Learning for Mars Terraforming Strategy",
    "problem_type": "Multi-Agent Reinforcement Learning (Competitive Strategy Game)",
    "objective": "Design autonomous agents that compete in a 1v1 scenario to terraform Mars by optimizing resource gathering, allocation, and lichen growth strategies. The core challenge involves:",
    "evaluation_metric": null,
    "full_content": "# Multi-Agent Reinforcement Learning for Mars Terraforming Strategy\n\n**Problem Description:**\n* **Problem Type:** Multi-Agent Reinforcement Learning (Competitive Strategy Game)\n* **Objective:** Design autonomous agents that compete in a 1v1 scenario to terraform Mars by optimizing resource gathering, allocation, and lichen growth strategies. The core challenge involves:\n    * Multi-variable optimization under resource constraints\n    * Real-time opponent analysis and adaptive policy development\n    * Strategic planning across 1000-turn games with day/night cycles\n* **Key Points:**\n    * Complete information game state (perfect opponent visibility)\n    * Simultaneous action execution with 3-second decision window\n    * Complex game mechanics including:\n        - Resource conversion chains (ice→water, ore→metal)\n        - Robot management (light/heavy units with different capabilities)\n        - Lichen growth dynamics (primary scoring mechanism)\n        - Collision physics and rubble mechanics\n\n**Dataset Overview:**\n* **Data Type:** Simulation environment with programmatic API (no static dataset)\n* **Context:** Mars terraforming scenario with 48×48 grid maps featuring:\n    * Raw resources (ice, ore)\n    * Refined resources (water, metal)\n    * Dynamic entities (robots, factories)\n    * Environmental factors (rubble, lichen)\n* **Key Components:**\n    * Python starter kit with core classes:\n        - `Board` (game state representation)\n        - `Unit` (robot/factory base class)\n        - `Game` (simulation controller)\n    * Example agents (`agent.py`) and tutorial notebooks\n    * Visualization tools for debugging\n\n**Evaluation Metrics:**\n* **Primary Metric:** Skill Rating System (Gaussian N(μ,σ²))\n    * μ (skill estimate) initialized at 600 for new submissions\n    * σ (uncertainty) decreases with more matches played\n* **Rating Update Rules:**\n    * Wins increase μ, losses decrease μ (magnitude depends on opponent strength)\n    * Draws move ratings toward mutual mean\n    * Updates consider deviation from expected outcome\n* **Final Ranking:** Based on lichen coverage after 1000 turns:\n    * Direct comparison of total lichen values between opponents\n    * Automatic loss if all factories are destroyed\n    * Draw if equal lichen or simultaneous factory destruction",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-Agent Reinforcement Learning (Competitive Strategy Game)\n* **Objective:** Design autonomous agents that compete in a 1v1 scenario to terraform Mars by optimizing resource gathering, allocation, and lichen growth strategies. The core challenge involves:\n    * Multi-variable optimization under resource constraints\n    * Real-time opponent analysis and adaptive policy development\n    * Strategic planning across 1000-turn games with day/night cycles\n* **Key Points:**\n    * Complete information game state (perfect opponent visibility)\n    * Simultaneous action execution with 3-second decision window\n    * Complex game mechanics including:\n        - Resource conversion chains (ice→water, ore→metal)\n        - Robot management (light/heavy units with different capabilities)\n        - Lichen growth dynamics (primary scoring mechanism)\n        - Collision physics and rubble mechanics",
      "Dataset Overview": "* **Data Type:** Simulation environment with programmatic API (no static dataset)\n* **Context:** Mars terraforming scenario with 48×48 grid maps featuring:\n    * Raw resources (ice, ore)\n    * Refined resources (water, metal)\n    * Dynamic entities (robots, factories)\n    * Environmental factors (rubble, lichen)\n* **Key Components:**\n    * Python starter kit with core classes:\n        - `Board` (game state representation)\n        - `Unit` (robot/factory base class)\n        - `Game` (simulation controller)\n    * Example agents (`agent.py`) and tutorial notebooks\n    * Visualization tools for debugging",
      "Evaluation Metrics": "* **Primary Metric:** Skill Rating System (Gaussian N(μ,σ²))\n    * μ (skill estimate) initialized at 600 for new submissions\n    * σ (uncertainty) decreases with more matches played\n* **Rating Update Rules:**\n    * Wins increase μ, losses decrease μ (magnitude depends on opponent strength)\n    * Draws move ratings toward mutual mean\n    * Updates consider deviation from expected outcome\n* **Final Ranking:** Based on lichen coverage after 1000 turns:\n    * Direct comparison of total lichen values between opponents\n    * Automatic loss if all factories are destroyed\n    * Draw if equal lichen or simultaneous factory destruction"
    },
    "file_path": "kaggle_datasets/548/problem_summary.md"
  },
  "47": {
    "problem_id": "47",
    "title": "Binary Classification of Insulting Social Media Comments",
    "problem_type": "Binary Classification (Text Classification)",
    "objective": "Predict whether a comment from an online conversation (e.g., news comments, blogs, forums) is insulting to another participant in the discussion. The goal is to build a generalizable single-class classifier capable of operating in near real-time to filter abusive content.",
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Insulting Social Media Comments\n\n**Problem Description:**\n* **Problem Type:** Binary Classification (Text Classification)\n* **Objective:** Predict whether a comment from an online conversation (e.g., news comments, blogs, forums) is insulting to another participant in the discussion. The goal is to build a generalizable single-class classifier capable of operating in near real-time to filter abusive content.\n    * **Key Points:**\n        * Focuses on insults directed at conversation participants, not third parties (e.g., celebrities).\n        * Insults may or may not contain explicit profanity or slurs.\n        * Requires handling subtlety in language (non-obvious insults are not labeled as positive).\n        * Training data may contain minimal label noise (<1% error).\n\n**Dataset Overview:**\n* **Data Type:** Text data (social media comments) with timestamps.\n* **Data Files:**\n    * `train.csv`: Labeled training data (comment text + binary insult label).\n    * `test.csv`: Unlabeled test data for submissions.\n    * `sample_submission_null.csv`: Submission format example.\n    * Additional verification files (`impermium_verification_set.csv`, `impermium_verification_labels.csv`) for final evaluation.\n* **Features:**\n    * **Label:** Binary (0 = neutral, 1 = insulting).\n    * **Timestamp:** Format \"YYYYMMDDhhmmssZ\" (optional, may be blank).\n    * **Comment Text:** Unicode-escaped text in double-quotes (mostly English).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC).\n    * **Key Properties:**\n        * Measures ranking quality of predictions (probability scores between 0 and 1).\n        * Penalizes high-confidence incorrect predictions more severely.\n    * **Final Ranking:** Based on a separate verification set released after code submission.",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification (Text Classification)\n* **Objective:** Predict whether a comment from an online conversation (e.g., news comments, blogs, forums) is insulting to another participant in the discussion. The goal is to build a generalizable single-class classifier capable of operating in near real-time to filter abusive content.\n    * **Key Points:**\n        * Focuses on insults directed at conversation participants, not third parties (e.g., celebrities).\n        * Insults may or may not contain explicit profanity or slurs.\n        * Requires handling subtlety in language (non-obvious insults are not labeled as positive).\n        * Training data may contain minimal label noise (<1% error).",
      "Dataset Overview": "* **Data Type:** Text data (social media comments) with timestamps.\n* **Data Files:**\n    * `train.csv`: Labeled training data (comment text + binary insult label).\n    * `test.csv`: Unlabeled test data for submissions.\n    * `sample_submission_null.csv`: Submission format example.\n    * Additional verification files (`impermium_verification_set.csv`, `impermium_verification_labels.csv`) for final evaluation.\n* **Features:**\n    * **Label:** Binary (0 = neutral, 1 = insulting).\n    * **Timestamp:** Format \"YYYYMMDDhhmmssZ\" (optional, may be blank).\n    * **Comment Text:** Unicode-escaped text in double-quotes (mostly English).",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC).\n    * **Key Properties:**\n        * Measures ranking quality of predictions (probability scores between 0 and 1).\n        * Penalizes high-confidence incorrect predictions more severely.\n    * **Final Ranking:** Based on a separate verification set released after code submission."
    },
    "file_path": "kaggle_datasets/47/problem_summary.md"
  },
  "321": {
    "problem_id": "321",
    "title": "Predicting Laboratory Earthquake Timing from Seismic Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Laboratory Earthquake Timing from Seismic Data\n\n## Problem Description\n* **Problem Type:** Time Series Regression\n* **Objective:** Predict the time remaining before the next laboratory earthquake occurs using real-time seismic data. The goal is to forecast \"when\" the earthquake will happen based on acoustic signals from a controlled experiment.\n    * **Key Points:**\n        * Focuses exclusively on temporal prediction (time-to-failure) rather than location or magnitude\n        * Uses data from a double direct shear geometry experiment mimicking tectonic fault behavior\n        * Features aperiodic earthquake failures, making prediction more challenging than periodic cases\n\n## Dataset Overview\n* **Data Type & Context:** Time series data from a laboratory earthquake simulation experiment with:\n    * Continuous seismic signals (`acoustic_data`)\n    * Corresponding time-to-failure labels (`time_to_failure`)\n* **Data Files:**\n    * `train.csv` - Single continuous segment of training data\n    * `test/` - Folder containing many discontinuous test segments\n    * `sample_submission.csv` - Submission template\n* **Key Features:**\n    * `acoustic_data`: Seismic signal measurements (int16)\n    * `time_to_failure`: Target variable - seconds until next earthquake (float64)\n    * `seg_id`: Test segment identifiers (one prediction per segment)\n\n## Evaluation Metrics\n* **Primary Metric:** Mean Absolute Error (MAE)\n    * Calculated between predicted and actual time remaining before next earthquake\n    * Lower values indicate better performance (perfect prediction would yield MAE=0)",
    "sections": {},
    "file_path": "kaggle_datasets/321/problem_summary.md"
  },
  "113": {
    "problem_id": "113",
    "title": "Walmart Store Sales Forecasting with Holiday Markdowns",
    "problem_type": "Time Series Forecasting (Regression)",
    "objective": "Predict weekly sales for each department in 45 Walmart stores, incorporating the impact of holiday-related markdown events.",
    "evaluation_metric": null,
    "full_content": "# Walmart Store Sales Forecasting with Holiday Markdowns\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (Regression)\n* **Objective:** Predict weekly sales for each department in 45 Walmart stores, incorporating the impact of holiday-related markdown events.\n    * **Key Points:**\n        * Must handle limited historical data for holiday periods (only one occurrence per year for each major holiday).\n        * Requires modeling the effect of promotional markdowns, which are inconsistently available in the data (only after Nov 2011 and not for all stores).\n        * Must account for varying impacts across different departments and stores.\n        * Holiday weeks are weighted more heavily in evaluation.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing historical sales, store/department metadata, and regional economic indicators.\n* **Data Files:**\n    * `train.csv`: Historical sales data (2010-02-05 to 2012-11-01) with Store, Dept, Date, Weekly_Sales, and IsHoliday columns.\n    * `test.csv`: Same structure as train.csv but without Weekly_Sales (target for prediction).\n    * `stores.csv`: Store metadata (type and size).\n    * `features.csv`: Additional temporal features per store (temperature, fuel price, markdowns, CPI, unemployment).\n* **Key Features:**\n    * Temporal: Date, IsHoliday (Super Bowl, Labor Day, Thanksgiving, Christmas weeks).\n    * Store/Department: Store number, Department number.\n    * Economic: MarkDown1-5 (anonymized promotional data), CPI, Unemployment.\n    * Environmental: Temperature, Fuel_Price.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Weighted Mean Absolute Error (WMAE)\n    * **Components:**\n        * Standard MAE calculation: \\(\\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\\)\n        * Holiday weighting: Errors for holiday weeks (IsHoliday=True) are multiplied by 5.\n        * Final formula: \\(WMAE = \\frac{1}{\\sum w_i} \\sum_{i=1}^n w_i|y_i - \\hat{y}_i|\\) where \\(w_i = 5\\) for holiday weeks, \\(1\\) otherwise.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (Regression)\n* **Objective:** Predict weekly sales for each department in 45 Walmart stores, incorporating the impact of holiday-related markdown events.\n    * **Key Points:**\n        * Must handle limited historical data for holiday periods (only one occurrence per year for each major holiday).\n        * Requires modeling the effect of promotional markdowns, which are inconsistently available in the data (only after Nov 2011 and not for all stores).\n        * Must account for varying impacts across different departments and stores.\n        * Holiday weeks are weighted more heavily in evaluation.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing historical sales, store/department metadata, and regional economic indicators.\n* **Data Files:**\n    * `train.csv`: Historical sales data (2010-02-05 to 2012-11-01) with Store, Dept, Date, Weekly_Sales, and IsHoliday columns.\n    * `test.csv`: Same structure as train.csv but without Weekly_Sales (target for prediction).\n    * `stores.csv`: Store metadata (type and size).\n    * `features.csv`: Additional temporal features per store (temperature, fuel price, markdowns, CPI, unemployment).\n* **Key Features:**\n    * Temporal: Date, IsHoliday (Super Bowl, Labor Day, Thanksgiving, Christmas weeks).\n    * Store/Department: Store number, Department number.\n    * Economic: MarkDown1-5 (anonymized promotional data), CPI, Unemployment.\n    * Environmental: Temperature, Fuel_Price.",
      "Evaluation Metrics": "* **Primary Metric:** Weighted Mean Absolute Error (WMAE)\n    * **Components:**\n        * Standard MAE calculation: \\(\\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\\)\n        * Holiday weighting: Errors for holiday weeks (IsHoliday=True) are multiplied by 5.\n        * Final formula: \\(WMAE = \\frac{1}{\\sum w_i} \\sum_{i=1}^n w_i|y_i - \\hat{y}_i|\\) where \\(w_i = 5\\) for holiday weeks, \\(1\\) otherwise."
    },
    "file_path": "kaggle_datasets/113/problem_summary.md"
  },
  "577": {
    "problem_id": "577",
    "title": "Binary Classification with a Software Defects Dataset",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification with a Software Defects Dataset\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability of software defects in C programs based on various code attributes. The target variable is binary (`defects`: 0=False, 1=True).\n* **Key Points:**\n  * The dataset is synthetically generated from a real-world Software Defect Dataset, with feature distributions close to the original.\n  * Participants are encouraged to explore differences between synthetic and original data and test whether incorporating the original dataset improves performance.\n\n## Dataset Overview\n* **Data Type & Context:** Tabular data containing features derived from C program code attributes, synthetically generated to mimic real-world software defect patterns.\n* **Data Files:**\n  * `train.csv`: Contains the target variable `defects` (binary) and features for training.\n  * `test.csv`: Contains features for testing (target `defects` must be predicted).\n  * `sample_submission.csv`: Example submission file with `id` and predicted probability for `defects`.\n* **Features:** \n  * The dataset includes 47 columns (features anonymized or derived from code metrics). Specific features are not named, but they represent attributes related to code quality and defect likelihood.\n\n## Evaluation Metrics\n* **Evaluation Metric:** Area Under the ROC Curve (ROC AUC)\n  * **Components:** \n    * Measures the ability of the model to distinguish between defective (1) and non-defective (0) code.\n    * Submissions must predict probabilities (not hard labels) for the `defects` target.",
    "sections": {},
    "file_path": "kaggle_datasets/577/problem_summary.md"
  },
  "78": {
    "problem_id": "78",
    "title": "Author Disambiguation in Academic Publications",
    "problem_type": "Entity Resolution / Clustering (Unsupervised Learning)",
    "objective": "Identify which author records in a scholarly database refer to the same real-world person, resolving name ambiguity and variations.",
    "evaluation_metric": null,
    "full_content": "# Author Disambiguation in Academic Publications\n\n**Problem Description:**\n* **Problem Type:** Entity Resolution / Clustering (Unsupervised Learning)\n* **Objective:** Identify which author records in a scholarly database refer to the same real-world person, resolving name ambiguity and variations.\n    * **Key Points:**\n        * Cold start problem: No labeled training data provided for duplicates\n        * Must handle name variations (e.g., \"J. Doe\" vs \"Jane A. Doe\")\n        * Must infer duplicates from publication patterns and metadata\n        * Solution must be symmetric (if A=B, then B=A in results)\n\n**Dataset Overview:**\n* **Data Type:** Relational/Tabular data from academic publications\n* **Data Files:**\n    * Author.csv (250K author profiles with names/affiliations)\n    * Paper.csv (2.5M papers with titles/years/venues/keywords)\n    * PaperAuthor.csv (noisy paper-author associations)\n    * Conference.csv/Journal.csv (venue metadata)\n* **Key Features:**\n    * Author names and affiliations (with variations)\n    * Paper titles, publication years, and keywords\n    * Co-authorship networks (derivable from PaperAuthor)\n    * Venue information (conferences/journals)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean F1-Score\n    * **Components:**\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * F1 = 2 * (Precision * Recall) / (Precision + Recall)\n    * **Implementation Notes:**\n        * Each author must include themselves in their duplicate list\n        * All duplicates must be reciprocally listed (symmetric relations)\n        * Metric is sensitive to both false positives and false negatives\n        * Baseline F1 is high (~1) since most authors are unique",
    "sections": {
      "Problem Description": "* **Problem Type:** Entity Resolution / Clustering (Unsupervised Learning)\n* **Objective:** Identify which author records in a scholarly database refer to the same real-world person, resolving name ambiguity and variations.\n    * **Key Points:**\n        * Cold start problem: No labeled training data provided for duplicates\n        * Must handle name variations (e.g., \"J. Doe\" vs \"Jane A. Doe\")\n        * Must infer duplicates from publication patterns and metadata\n        * Solution must be symmetric (if A=B, then B=A in results)",
      "Dataset Overview": "* **Data Type:** Relational/Tabular data from academic publications\n* **Data Files:**\n    * Author.csv (250K author profiles with names/affiliations)\n    * Paper.csv (2.5M papers with titles/years/venues/keywords)\n    * PaperAuthor.csv (noisy paper-author associations)\n    * Conference.csv/Journal.csv (venue metadata)\n* **Key Features:**\n    * Author names and affiliations (with variations)\n    * Paper titles, publication years, and keywords\n    * Co-authorship networks (derivable from PaperAuthor)\n    * Venue information (conferences/journals)",
      "Evaluation Metrics": "* **Primary Metric:** Mean F1-Score\n    * **Components:**\n        * Precision = TP / (TP + FP)\n        * Recall = TP / (TP + FN)\n        * F1 = 2 * (Precision * Recall) / (Precision + Recall)\n    * **Implementation Notes:**\n        * Each author must include themselves in their duplicate list\n        * All duplicates must be reciprocally listed (symmetric relations)\n        * Metric is sensitive to both false positives and false negatives\n        * Baseline F1 is high (~1) since most authors are unique"
    },
    "file_path": "kaggle_datasets/78/problem_summary.md"
  },
  "583": {
    "problem_id": "583",
    "title": "Machine Unlearning for Age Prediction on Face Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Machine Unlearning for Age Prediction on Face Images\n\n## Problem Description\n* **Problem Type**: Machine Unlearning (Privacy-Preserving Deep Learning)\n* **Objective**: Develop an algorithm to remove the influence of specific training examples (the \"forget set\") from a pre-trained age prediction model while maintaining model accuracy on retained data. The goal is to protect privacy by making it impossible to determine via membership inference attacks whether forgotten examples were used in training.\n* **Key Points**:\n  * Must unlearn entire subjects (all images of 15 individuals) rather than individual images\n  * Solution must maintain model utility on retained data and validation set\n  * Hard computational constraint: Unlearning must be faster than retraining from scratch\n  * Requires introducing randomness in unlearning process to properly estimate distribution\n\n## Dataset Overview\n* **Data Type**: Image data (face photos) with tabular metadata\n* **Context**: Age prediction from facial images with privacy removal requirements\n* **Data Files**:\n  * `retain.csv`, `forget.csv`, `validation.csv` - Metadata splits with person/image IDs and age labels\n  * `images/[person_id]/[image_id].png` - 32x32px face images (~30,000 total)\n  * `original_model.pth` - Pre-trained ResNet-18 PyTorch model\n  * `age_class_weights.json` - Class weights for age groups\n* **Key Features**:\n  * Images organized by `person_id` with unique `image_id`\n  * Age prediction targets (`age` and binned `age_group`)\n  * Note: 2% of images have duplicate perceptual hashes (known limitation)\n\n## Evaluation Metrics\n* **Primary Metric**: Custom metric balancing:\n  * **Forgetting Quality**: Measures indistinguishability between unlearned models and models retrained from scratch (without forget set)\n  * **Model Utility**: Maintained accuracy on retained and validation data\n* **Implementation Notes**:\n  * Requires submitting 512 model checkpoints to estimate distributions\n  * Randomness in unlearning process (e.g., data shuffling) is critical for proper evaluation\n  * Metric details fully disclosed post-competition\n  * Public/private leaderboards use same dataset with different splits",
    "sections": {},
    "file_path": "kaggle_datasets/583/problem_summary.md"
  },
  "147": {
    "problem_id": "147",
    "title": "Forest Cover Type Prediction",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Forest Cover Type Prediction\n\n## Problem Description\n- **Problem Type:** Multi-class Classification\n- **Objective:** Predict the predominant forest cover type (1 of 7 classes) for 30x30 meter patches in the Roosevelt National Forest using cartographic variables. The task involves classifying the forest category based on ecological features rather than remotely sensed data.\n- **Key Points:**\n  - Focuses on wilderness areas with minimal human disturbance, making the cover types ecologically driven.\n  - Uses raw, unscaled data with binary columns for qualitative variables (wilderness areas, soil types).\n\n## Dataset Overview\n- **Data Type & Context:** Tabular data representing cartographic features of forest patches in Colorado.\n- **Data Files:**\n  - `train.csv` (15,120 observations with features and `Cover_Type` labels)\n  - `test.csv` (565,892 observations with features only)\n  - `sampleSubmission.csv` (example submission format)\n- **Key Features:**\n  - **Continuous/Numerical:** Elevation, Aspect, Slope, Distances (hydrology, roadways, fire points), Hillshade indices.\n  - **Binary:** 4 wilderness area columns (presence/absence), 40 soil type columns (presence/absence).\n  - **Target:** `Cover_Type` (7 classes: Spruce/Fir, Lodgepole Pine, etc.).\n\n## Evaluation Metrics\n- **Primary Metric:** Multi-class Classification Accuracy.\n  - Submissions require predicting the integer class (1-7) for each test observation.\n  - Format: `Id` and `Cover_Type` columns in a CSV file.",
    "sections": {},
    "file_path": "kaggle_datasets/147/problem_summary.md"
  },
  "375": {
    "problem_id": "375",
    "title": "NCAA March Madness Analytics Competition",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# NCAA March Madness Analytics Competition\n\n## Problem Description\n- **Problem Type**: Exploratory Data Analysis (EDA) / Data Storytelling\n- **Objective**: Participants are tasked with creating a data-driven narrative exploring the concept of \"competitiveness\" in NCAA March Madness basketball tournaments. The goal is to quantify or characterize aspects like:\n    * Team competitiveness dynamics (e.g., underdog performances, late-game resilience)\n    * Factors influencing game outcomes (e.g., player performance, travel effects, effort metrics)\n    * Comparative analysis between regular season and tournament performance\n- **Key Points**:\n    * Focus on creative interpretation of \"madness\" - unexpected outcomes and competitive dynamics\n    * Requires combining narrative storytelling with data exploration\n    * Encourages novel metrics for competitiveness (scalar, clustered, or other representations)\n    * Macro and micro perspectives are both valid (season-level trends or individual player impacts)\n\n## Dataset Overview\n- **Data Type**: Multi-modal basketball data covering:\n    * Tabular game results (scores, locations, dates)\n    * Team statistics (box scores, rankings)\n    * Play-by-play event logs\n    * Geographical game locations\n- **Data Files** (Key Files):\n    * `MTeams.csv`/`WTeams.csv`: Team IDs and metadata\n    * `MRegularSeasonCompactResults.csv`: Historical regular season results\n    * `MNCAATourneyCompactResults.csv`: Tournament game results\n    * `MSeasons.csv`: Season metadata including date alignments\n    * `MMasseyOrdinals.csv`: Weekly team rankings from multiple systems\n    * `MEvents[YYYY].csv`: Play-by-play data (2015-2019)\n    * Supplementary files for conferences, cities, coaches, etc.\n- **Features**:\n    * Game outcomes with scores, locations, and dates\n    * Team performance metrics (FG%, rebounds, turnovers, etc.)\n    * Player-level event data (shots, fouls, turnovers with court coordinates)\n    * Multiple ranking systems over time\n    * Tournament seed information and bracket structure\n\n## Evaluation Metrics\n- **Evaluation Components**:\n    * **Documentation (4 pts)**: \n        * Clear research question and methodology\n        * Reproducible analysis with proper citations\n    * **Creativity/Originality (5 pts)**:\n        * Novel insights or approaches\n        * Innovative use of data/techniques\n    *",
    "sections": {},
    "file_path": "kaggle_datasets/375/problem_summary.md"
  },
  "523": {
    "problem_id": "523",
    "title": "Lux AI 2022 - Beta: Multi-Agent Reinforcement Learning for Terraforming Mars",
    "problem_type": "Multi-Agent Reinforcement Learning (Competitive Strategy Game)",
    "objective": "Design an autonomous agent to control robots and factories in a 1v1 terraforming simulation, optimizing resource gathering, allocation, and lichen growth to outscore the opponent.",
    "evaluation_metric": null,
    "full_content": "# Lux AI 2022 - Beta: Multi-Agent Reinforcement Learning for Terraforming Mars\n\n**Problem Description:**\n* **Problem Type:** Multi-Agent Reinforcement Learning (Competitive Strategy Game)\n* **Objective:** Design an autonomous agent to control robots and factories in a 1v1 terraforming simulation, optimizing resource gathering, allocation, and lichen growth to outscore the opponent.\n* **Key Points:**\n  * Real-time strategy with simultaneous action execution (3-second turn limit)\n  * Complete information game state with opponent observation\n  * Multi-variable optimization involving:\n    * Resource management (ice/ore → water/metal conversion)\n    * Robot control (light/heavy units with different capabilities)\n    * Lichen cultivation (primary scoring mechanism)\n  * Dynamic environmental factors:\n    * Day/night cycle affecting power generation\n    * Weather events (marsquakes, cold snaps, dust storms, solar flares)\n    * Rubble accumulation affecting movement\n\n**Dataset Overview:**\n* **Data Type:** Simulation environment with programmatic API (no traditional dataset files)\n* **Key Components:**\n  * Python starter kit (`agent.py`, `main.py`)\n  * Game engine specifications (GitHub repository)\n  * Symmetric 48x48 grid maps with:\n    * Resource tiles (ice/ore)\n    * Factory locations\n    * Dynamic rubble/lichen states\n* **Core Features:**\n  * Robot units with cargo/power constraints\n  * Factory production pipelines\n  * Weather event schedules\n  * Lichen growth mechanics\n\n**Evaluation Metrics:**\n* **Primary Metric:** Skill Rating System (Gaussian N(μ,σ²))\n  * μ: Estimated skill (initialized at 600)\n  * σ: Rating uncertainty (decreases with more matches)\n* **Match Resolution:**\n  * Win/Loss/Draw determined by:\n    * Lichen count after 1000 turns (primary)\n    * Factory destruction (instant loss)\n  * Rating updates based on:\n    * Outcome deviation from expected result\n    * Current uncertainty values\n  * No effect from victory margin (only binary outcome matters)\n* **Tournament Structure:**\n  * Continuous ladder system\n  * Bots play against similarly-rated opponents\n  * Final ranking based on highest μ at deadline",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-Agent Reinforcement Learning (Competitive Strategy Game)\n* **Objective:** Design an autonomous agent to control robots and factories in a 1v1 terraforming simulation, optimizing resource gathering, allocation, and lichen growth to outscore the opponent.\n* **Key Points:**\n  * Real-time strategy with simultaneous action execution (3-second turn limit)\n  * Complete information game state with opponent observation\n  * Multi-variable optimization involving:\n    * Resource management (ice/ore → water/metal conversion)\n    * Robot control (light/heavy units with different capabilities)\n    * Lichen cultivation (primary scoring mechanism)\n  * Dynamic environmental factors:\n    * Day/night cycle affecting power generation\n    * Weather events (marsquakes, cold snaps, dust storms, solar flares)\n    * Rubble accumulation affecting movement",
      "Dataset Overview": "* **Data Type:** Simulation environment with programmatic API (no traditional dataset files)\n* **Key Components:**\n  * Python starter kit (`agent.py`, `main.py`)\n  * Game engine specifications (GitHub repository)\n  * Symmetric 48x48 grid maps with:\n    * Resource tiles (ice/ore)\n    * Factory locations\n    * Dynamic rubble/lichen states\n* **Core Features:**\n  * Robot units with cargo/power constraints\n  * Factory production pipelines\n  * Weather event schedules\n  * Lichen growth mechanics",
      "Evaluation Metrics": "* **Primary Metric:** Skill Rating System (Gaussian N(μ,σ²))\n  * μ: Estimated skill (initialized at 600)\n  * σ: Rating uncertainty (decreases with more matches)\n* **Match Resolution:**\n  * Win/Loss/Draw determined by:\n    * Lichen count after 1000 turns (primary)\n    * Factory destruction (instant loss)\n  * Rating updates based on:\n    * Outcome deviation from expected result\n    * Current uncertainty values\n  * No effect from victory margin (only binary outcome matters)\n* **Tournament Structure:**\n  * Continuous ladder system\n  * Bots play against similarly-rated opponents\n  * Final ranking based on highest μ at deadline"
    },
    "file_path": "kaggle_datasets/523/problem_summary.md"
  },
  "2": {
    "problem_id": "2",
    "title": "World Cup 2010 - Confidence Challenge",
    "problem_type": "Confidence-Weighted Prediction (Custom Evaluation Metric)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# World Cup 2010 - Confidence Challenge\n\n**Problem Description:**\n* **Problem Type:** Confidence-Weighted Prediction (Custom Evaluation Metric)\n* **Objective:**  \n  * Predict how far each country will progress in the 2010 FIFA World Cup (e.g., winner, runner-up, quarter-finals, etc.).  \n  * Assign a confidence score (0-100) to each prediction, where higher confidence amplifies both gains (if correct) and losses (if incorrect).  \n* **Key Points:**  \n  * Predictions are framed as \"**or better**\" (e.g., \"runner-up or better\" means the team finishes 1st or 2nd).  \n  * Submissions must include: predicted outcome, country code, and confidence level.  \n  * Incentivizes well-calibrated confidence (overconfidence penalized heavily if wrong).  \n\n**Dataset Overview:**  \n* **Data Type:** Tabular data (historical sports performance, rankings, and metadata).  \n* **Data Files:**  \n  * `2010_country_codes.csv`: Three-letter country codes for submission formatting.  \n  * `submission_template.csv`: Template for predictions and confidence scores.  \n  * `example_submission.csv`: Sample submission file.  \n* **Features:**  \n  * Provided: Historical FIFA rankings (1994 onwards), team performance records, continent/host-country data.  \n  * Suggested external data: Betting odds, Elo ratings, player salaries, socioeconomic indicators (participants encouraged to augment).  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom Confidence-Weighted Score  \n  * **Scoring Logic:**  \n    * Correct predictions: `Points × (Confidence/100)` (e.g., 3.47 × 0.5 = 1.73 for 50% confidence on \"winner\").  \n    * Incorrect predictions: `-Points × (Confidence/100)` (same penalty magnitude).  \n  * **Point Values:**  \n    | Outcome Tier               | Points |  \n    |----------------------------|--------|  \n    | Winner                     | 3.47   |  \n    | Runner-up or better        | 2.77   |  \n    | Third or better            | 2.37   |  \n    | ... (see table in summary) | ...    |  \n  * Tiebreaker: Earlier submissions rank higher.",
    "sections": {
      "Problem Description": "* **Problem Type:** Confidence-Weighted Prediction (Custom Evaluation Metric)\n* **Objective:**  \n  * Predict how far each country will progress in the 2010 FIFA World Cup (e.g., winner, runner-up, quarter-finals, etc.).  \n  * Assign a confidence score (0-100) to each prediction, where higher confidence amplifies both gains (if correct) and losses (if incorrect).  \n* **Key Points:**  \n  * Predictions are framed as \"**or better**\" (e.g., \"runner-up or better\" means the team finishes 1st or 2nd).  \n  * Submissions must include: predicted outcome, country code, and confidence level.  \n  * Incentivizes well-calibrated confidence (overconfidence penalized heavily if wrong).  \n\n**Dataset Overview:**  \n* **Data Type:** Tabular data (historical sports performance, rankings, and metadata).  \n* **Data Files:**  \n  * `2010_country_codes.csv`: Three-letter country codes for submission formatting.  \n  * `submission_template.csv`: Template for predictions and confidence scores.  \n  * `example_submission.csv`: Sample submission file.  \n* **Features:**  \n  * Provided: Historical FIFA rankings (1994 onwards), team performance records, continent/host-country data.  \n  * Suggested external data: Betting odds, Elo ratings, player salaries, socioeconomic indicators (participants encouraged to augment).  \n\n**Evaluation Metrics:**  \n* **Primary Metric:** Custom Confidence-Weighted Score  \n  * **Scoring Logic:**  \n    * Correct predictions: `Points × (Confidence/100)` (e.g., 3.47 × 0.5 = 1.73 for 50% confidence on \"winner\").  \n    * Incorrect predictions: `-Points × (Confidence/100)` (same penalty magnitude).  \n  * **Point Values:**  \n    | Outcome Tier               | Points |  \n    |----------------------------|--------|  \n    | Winner                     | 3.47   |  \n    | Runner-up or better        | 2.77   |  \n    | Third or better            | 2.37   |  \n    | ... (see table in summary) | ...    |  \n  * Tiebreaker: Earlier submissions rank higher."
    },
    "file_path": "kaggle_datasets/2/problem_summary.md"
  },
  "381": {
    "problem_id": "381",
    "title": "Fine-Grained Segmentation and Attribute Classification for Fashion Images",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Fine-Grained Segmentation and Attribute Classification for Fashion Images\n\n## Problem Description\n- **Problem Type**: Computer Vision - Instance Segmentation with Multi-Label Attribute Classification\n- **Objective**: Develop algorithms to accurately segment fashion items in images and classify their fine-grained attributes. The task combines:\n  - Pixel-level segmentation of 46 apparel categories (27 main items + 19 parts)\n  - Multi-label classification of 294 associated attributes for each segmented object\n- **Key Points**:\n  - Requires handling complex fashion taxonomy designed by domain experts\n  - Must address real-world challenges like occlusions, varied poses, and ambiguous descriptions\n  - Solutions should simultaneously optimize segmentation quality and attribute prediction accuracy\n\n## Dataset Overview\n- **Data Type**: Image data with segmentation masks and attribute annotations\n- **Context**: 50K fashion images from daily life, celebrity events, and online shopping\n- **Data Files**:\n  - `train/`: Training images\n  - `test/`: Test images for prediction\n  - `train.csv`: Contains ImageId, EncodedPixels (RLE masks), ClassId, and AttributesIds\n  - `label_descriptions.json`: Mapping of class/attribute IDs to descriptions\n- **Key Features**:\n  - High-resolution images with complex fashion items\n  - Run-length encoded segmentation masks\n  - Hierarchical label structure (categories + attributes)\n  - Attributes are multi-label (each object can have multiple attributes)\n\n## Evaluation Metrics\n- **Primary Metric**: Mean Average Precision (mAP) across:\n  - **IoU Thresholds**: 0.5 to 0.95 (step 0.05) for segmentation quality\n  - **F1 Thresholds**: 0.5 to 0.95 (step 0.05) for attribute prediction accuracy\n- **Scoring Components**:\n  1. True Positive requires:\n     - IoU > threshold with ground truth mask\n     - F1 score > threshold for predicted vs true attributes\n  2. Final score is mean of:\n     - Individual average precisions per image\n     - Individual average precisions per ClassId\n- **Critical Considerations**:\n  - Both segmentation AND classification must be correct\n  - Predictions must be resized to longest side = 1024 pixels\n  - No overlapping masks allowed for same class in same image",
    "sections": {},
    "file_path": "kaggle_datasets/381/problem_summary.md"
  },
  "178": {
    "problem_id": "178",
    "title": "Right Whale Recognition in Aerial Photographs",
    "problem_type": "Multi-class Classification (Computer Vision - Object Recognition)",
    "objective": "Automate the identification of individual North Atlantic right whales from aerial photographs to aid conservation efforts. The task requires matching each image to one of ~500 known whale IDs in the catalog.",
    "evaluation_metric": null,
    "full_content": "# Right Whale Recognition in Aerial Photographs\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Computer Vision - Object Recognition)\n* **Objective:** Automate the identification of individual North Atlantic right whales from aerial photographs to aid conservation efforts. The task requires matching each image to one of ~500 known whale IDs in the catalog.\n* **Key Points:**\n  * Real-world conservation application with direct impact on whale protection efforts\n  * Analogous to facial recognition, requiring detection + classification pipeline\n  * Test set includes transformed images (resized/cropped/flipped) to prevent manual labeling\n  * Dataset intentionally excludes metadata (dates, geo-tags) to focus on visual recognition\n\n**Dataset Overview:**\n* **Data Type:** Aerial photograph images (JPG) containing single whale specimens\n* **Context:** 10 years of NOAA-collected images from helicopter surveys, hand-labeled by marine biologists\n* **Data Files:**\n  * `imgs.zip`: All training/test images\n  * `train.csv`: Image filenames with corresponding whale IDs\n  * `sample_submission.csv`: Submission format example\n  * `imgs_subset.zip`: 500-image subset for experimentation\n* **Features:**\n  * Raw pixel data from aerial photos\n  * Whale IDs represent individual animals (~500 classes)\n  * Images vary in composition/angle due to real-world survey conditions\n\n**Evaluation Metrics:**\n* **Primary Metric:** Multi-class Logarithmic Loss (Log Loss)\n* **Calculation Details:**\n  * For each image, requires predicted probability distribution across all whale IDs\n  * Formula: \n    ```\n    logloss = -1/N Σ[i=1→N] Σ[j=1→M] y_ij * log(p_ij)\n    ```\n    Where:\n    * N = number of test images\n    * M = number of whale classes\n    * y_ij = 1 if image i belongs to whale j, else 0\n    * p_ij = predicted probability for image i and whale j\n  * Probabilities are rescaled (row-normalized) before scoring\n  * Clipped to [10^-15, 1-10^-15] to avoid log(0) extremes",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Computer Vision - Object Recognition)\n* **Objective:** Automate the identification of individual North Atlantic right whales from aerial photographs to aid conservation efforts. The task requires matching each image to one of ~500 known whale IDs in the catalog.\n* **Key Points:**\n  * Real-world conservation application with direct impact on whale protection efforts\n  * Analogous to facial recognition, requiring detection + classification pipeline\n  * Test set includes transformed images (resized/cropped/flipped) to prevent manual labeling\n  * Dataset intentionally excludes metadata (dates, geo-tags) to focus on visual recognition",
      "Dataset Overview": "* **Data Type:** Aerial photograph images (JPG) containing single whale specimens\n* **Context:** 10 years of NOAA-collected images from helicopter surveys, hand-labeled by marine biologists\n* **Data Files:**\n  * `imgs.zip`: All training/test images\n  * `train.csv`: Image filenames with corresponding whale IDs\n  * `sample_submission.csv`: Submission format example\n  * `imgs_subset.zip`: 500-image subset for experimentation\n* **Features:**\n  * Raw pixel data from aerial photos\n  * Whale IDs represent individual animals (~500 classes)\n  * Images vary in composition/angle due to real-world survey conditions",
      "Evaluation Metrics": "* **Primary Metric:** Multi-class Logarithmic Loss (Log Loss)\n* **Calculation Details:**\n  * For each image, requires predicted probability distribution across all whale IDs\n  * Formula: \n    ```\n    logloss = -1/N Σ[i=1→N] Σ[j=1→M] y_ij * log(p_ij)\n    ```\n    Where:\n    * N = number of test images\n    * M = number of whale classes\n    * y_ij = 1 if image i belongs to whale j, else 0\n    * p_ij = predicted probability for image i and whale j\n  * Probabilities are rescaled (row-normalized) before scoring\n  * Clipped to [10^-15, 1-10^-15] to avoid log(0) extremes"
    },
    "file_path": "kaggle_datasets/178/problem_summary.md"
  },
  "13": {
    "problem_id": "13",
    "title": "Binary Classification of Driver Alertness Using Physiological, Environmental, and Vehicular Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Binary Classification of Driver Alertness Using Physiological, Environmental, and Vehicular Data\n\n## Problem Description\n* **Problem Type:** Binary Classification\n* **Objective:**  \n  The goal is to detect whether a driver is alert (1) or not alert (0) using data collected during driving sessions. The task involves analyzing physiological, environmental, and vehicular data to predict driver alertness, which is critical for preventing accidents caused by distractions, fatigue, or drowsiness.\n* **Key Points:**\n  * The competition emphasizes models that use fewer physiological variables (columns starting with 'P') due to practical deployment constraints.\n  * Participants must select 5 submissions for final evaluation (or the last 5 entries are chosen by default).\n\n## Dataset Overview\n* **Data Type & Context:**  \n  Tabular data representing sequential observations recorded every 100ms during driving trials (~2 minutes per trial). Data includes:\n  * **Physiological metrics (8 columns: P1-P8):** Driver-specific biometrics (e.g., heart rate, skin conductance).  \n  * **Environmental metrics (11 columns: E1-E11):** External conditions (e.g., weather, road type).  \n  * **Vehicular metrics (11 columns: V1-V11):** Vehicle state (e.g., speed, steering angle).  \n* **Data Files:**  \n  * `fordTrain.csv`: Training data with labeled alertness (column 3: 1=alert, 0=not alert).  \n  * `fordTest.csv`: Test data without labels.  \n  * `example_submission.csv`: Submission template requiring real-valued predictions (0 to 1).  \n* **Key Features:**  \n  * Each row represents a 100ms snapshot within a trial (unique `Trial ID`).  \n  * Features are anonymized (units/names not disclosed).  \n\n## Evaluation Metrics\n* **Primary Metric:** Area Under the ROC Curve (AUC).  \n* **Metric Details:**  \n  * AUC evaluates binary classifiers across all decision thresholds, measuring the ability to distinguish between alert (positive class) and not-alert (negative class) states.  \n  * Advantages:  \n    * Robust to class imbalance.  \n    * No need to predefine a classification threshold.  \n  * Interpretation:  \n    * **AUC = 1:** Perfect classifier.  \n    * **AUC = 0.5:** Random guessing.",
    "sections": {},
    "file_path": "kaggle_datasets/13/problem_summary.md"
  },
  "386": {
    "problem_id": "386",
    "title": "Tweet Sentiment Phrase Extraction",
    "problem_type": "NLP - Text Extraction (Sequence Labeling/Phrase Selection)",
    "objective": "",
    "evaluation_metric": null,
    "full_content": "# Tweet Sentiment Phrase Extraction\n\n**Problem Description:**\n* **Problem Type:** NLP - Text Extraction (Sequence Labeling/Phrase Selection)\n* **Objective:**  \n  * Extract the specific word or phrase from a tweet that best supports its pre-labeled sentiment (positive, negative, or neutral).  \n  * The extracted text must be a contiguous substring of the original tweet, including punctuation and spacing.\n* **Key Points:**  \n  * Focuses on interpretability by identifying sentiment-supporting text spans rather than predicting sentiment itself.  \n  * Requires precise substring matching (quoted text in submissions must exactly match ground truth).  \n  * Dataset may contain offensive/profane language.\n\n**Dataset Overview:**\n* **Data Type & Context:**  \n  * Tabular data containing tweets (`text`), sentiment labels (`sentiment`), and (for training) the annotated support phrase (`selected_text`).  \n  * Sourced from Figure Eight's \"Sentiment Analysis: Emotion in Text\" dataset (CC BY 4.0 license).\n* **Data Files:**  \n  * `train.csv`: Contains `textID`, `text`, `sentiment`, and `selected_text` (ground truth phrase).  \n  * `test.csv`: Contains `textID`, `text`, and `sentiment` (no `selected_text`).  \n  * `sample_submission.csv`: Submission template with `textID` and `selected_text` columns.\n* **Key Features:**  \n  * `text`: Raw tweet content (may include special characters, emojis, etc.).  \n  * `sentiment`: Categorical label (positive/negative/neutral).  \n  * `selected_text`: Substring of `text` justifying the sentiment (training only).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Word-level Jaccard Score (similarity between predicted and ground truth phrases).  \n  * **Calculation:**  \n    ```python\n    Jaccard(str1, str2) = len(intersection of word sets) / len(union of word sets)\n    ```\n  * **Aggregation:** Mean Jaccard score across all test samples.  \n* **Submission Format:**  \n  * CSV with `textID` and `selected_text` columns.  \n  * Predicted phrases must be exact substrings of the original tweet (including punctuation and whitespace).",
    "sections": {
      "Problem Description": "* **Problem Type:** NLP - Text Extraction (Sequence Labeling/Phrase Selection)\n* **Objective:**  \n  * Extract the specific word or phrase from a tweet that best supports its pre-labeled sentiment (positive, negative, or neutral).  \n  * The extracted text must be a contiguous substring of the original tweet, including punctuation and spacing.\n* **Key Points:**  \n  * Focuses on interpretability by identifying sentiment-supporting text spans rather than predicting sentiment itself.  \n  * Requires precise substring matching (quoted text in submissions must exactly match ground truth).  \n  * Dataset may contain offensive/profane language.",
      "Dataset Overview": "* **Data Type & Context:**  \n  * Tabular data containing tweets (`text`), sentiment labels (`sentiment`), and (for training) the annotated support phrase (`selected_text`).  \n  * Sourced from Figure Eight's \"Sentiment Analysis: Emotion in Text\" dataset (CC BY 4.0 license).\n* **Data Files:**  \n  * `train.csv`: Contains `textID`, `text`, `sentiment`, and `selected_text` (ground truth phrase).  \n  * `test.csv`: Contains `textID`, `text`, and `sentiment` (no `selected_text`).  \n  * `sample_submission.csv`: Submission template with `textID` and `selected_text` columns.\n* **Key Features:**  \n  * `text`: Raw tweet content (may include special characters, emojis, etc.).  \n  * `sentiment`: Categorical label (positive/negative/neutral).  \n  * `selected_text`: Substring of `text` justifying the sentiment (training only).",
      "Evaluation Metrics": "* **Primary Metric:** Word-level Jaccard Score (similarity between predicted and ground truth phrases).  \n  * **Calculation:**  \n    ```python\n    Jaccard(str1, str2) = len(intersection of word sets) / len(union of word sets)\n    ```\n  * **Aggregation:** Mean Jaccard score across all test samples.  \n* **Submission Format:**  \n  * CSV with `textID` and `selected_text` columns.  \n  * Predicted phrases must be exact substrings of the original tweet (including punctuation and whitespace)."
    },
    "file_path": "kaggle_datasets/386/problem_summary.md"
  },
  "5": {
    "problem_id": "5",
    "title": "Tourism Time Series Forecasting",
    "problem_type": "Time Series Forecasting",
    "objective": "Predict the next four yearly observations for 518 tourism-related time series. The competition aims to improve forecasting accuracy for tourism industry planning and management.",
    "evaluation_metric": null,
    "full_content": "# Tourism Time Series Forecasting\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict the next four yearly observations for 518 tourism-related time series. The competition aims to improve forecasting accuracy for tourism industry planning and management.\n    * **Key Points:**\n        * Part one focuses on yearly time series (part two would involve monthly/quarterly series).\n        * Winning methodology must outperform thresholds from a referenced paper (MASE < 2.28 for yearly series).\n        * Winner contributes a discussion paper to the International Journal of Forecasting.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular time series data containing yearly tourism metrics (518 unique series).\n* **Data Files:**\n    * `tourism_data.csv` (main dataset with series identifiers and observations)\n    * `template.csv` (submission template)\n    * `example.csv` (sample submission file)\n* **Features:** Each series contains:\n    * Identifier (first row)\n    * Sequential yearly observations (subsequent rows)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Scaled Error (MASE) averaged across all series.\n    * **MASE Calculation:**\n        * Numerator: Mean absolute error of forecasts vs actuals\n        * Denominator: Mean absolute difference of consecutive training observations (scaling factor)\n        * Formula:  \n          `MASE = [(n-1)/h] * Σ|y_{n+i} - f_{n+i}| / Σ|y_j - y_{j-1}|`  \n          (where n = training length, h = forecast horizon)\n    * Final score: Average MASE across all 518 series.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting\n* **Objective:** Predict the next four yearly observations for 518 tourism-related time series. The competition aims to improve forecasting accuracy for tourism industry planning and management.\n    * **Key Points:**\n        * Part one focuses on yearly time series (part two would involve monthly/quarterly series).\n        * Winning methodology must outperform thresholds from a referenced paper (MASE < 2.28 for yearly series).\n        * Winner contributes a discussion paper to the International Journal of Forecasting.",
      "Dataset Overview": "* **Data Type & Context:** Tabular time series data containing yearly tourism metrics (518 unique series).\n* **Data Files:**\n    * `tourism_data.csv` (main dataset with series identifiers and observations)\n    * `template.csv` (submission template)\n    * `example.csv` (sample submission file)\n* **Features:** Each series contains:\n    * Identifier (first row)\n    * Sequential yearly observations (subsequent rows)",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Scaled Error (MASE) averaged across all series.\n    * **MASE Calculation:**\n        * Numerator: Mean absolute error of forecasts vs actuals\n        * Denominator: Mean absolute difference of consecutive training observations (scaling factor)\n        * Formula:  \n          `MASE = [(n-1)/h] * Σ|y_{n+i} - f_{n+i}| / Σ|y_j - y_{j-1}|`  \n          (where n = training length, h = forecast horizon)\n    * Final score: Average MASE across all 518 series."
    },
    "file_path": "kaggle_datasets/5/problem_summary.md"
  },
  "524": {
    "problem_id": "524",
    "title": "Detecting Continuous Gravitational Waves in Noisy Interferometer Data",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Detecting Continuous Gravitational Waves in Noisy Interferometer Data\n\n## Problem Description\n* **Problem Type**: Binary Classification (Signal Detection)\n* **Objective**: \n    * Detect weak, long-lasting continuous gravitational-wave (CW) signals from rapidly-spinning neutron stars within noisy interferometer data.\n    * Distinguish between data containing simulated CW signals (target=1) and pure noise or indeterminate cases (target=0/-1).\n* **Key Points**:\n    * Signals are 1-2 orders of magnitude weaker than detector noise.\n    * Traditional matched-filtering approaches are computationally infeasible due to the need for quintillions of waveform templates.\n    * Successful detection would enable discovery of a new class of gravitational waves beyond merging black holes/neutron stars.\n\n## Dataset Overview\n* **Data Type**: Time-frequency data (Short-time Fourier Transforms) from gravitational-wave interferometers.\n* **Context**: \n    * Simulated and real noise data from LIGO Hanford & Livingston detectors.\n    * Each sample contains non-contiguous SFTs with GPS timestamps (interferometers are not always online).\n* **Data Files**:\n    * `train/`, `test/` folders (HDF5 format)\n    * `train_labels.csv` (target labels: 1=CW present, 0=noise, -1=indeterminate)\n    * `sample_submission.csv` (submission template)\n* **Features**:\n    * Time-frequency representations of detector data.\n    * Signal parameters randomized (8 total, including frequency/spin-down) but not provided in data.\n\n## Evaluation Metrics\n* **Primary Metric**: Area Under the ROC Curve (AUC)\n    * Measures model's ability to rank true signals higher than noise.\n    * Evaluates predicted probabilities against binary targets (excluding indeterminate -1 cases).",
    "sections": {},
    "file_path": "kaggle_datasets/524/problem_summary.md"
  },
  "372": {
    "problem_id": "372",
    "title": "COVID-19 Global Forecasting (Week 3)",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# COVID-19 Global Forecasting (Week 3)\n\n## Problem Description\n* **Problem Type**: Time Series Forecasting\n* **Objective**: \n    * Forecast the cumulative number of confirmed COVID-19 cases and fatalities for future dates (April 1 to April 30) across global regions.\n    * Secondary goal: Identify factors impacting COVID-19 transmission rates through external data integration and analysis.\n* **Key Points**:\n    * Participants are encouraged to incorporate external datasets (e.g., policy interventions, environmental factors) to improve forecasts and insights.\n    * Focus on both accuracy and interpretability of factors influencing transmission.\n\n## Dataset Overview\n* **Data Type & Context**: \n    * Tabular time-series data tracking COVID-19 cases and fatalities by region (country/state) over time.\n    * Sourced from Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).\n* **Data Files**:\n    * `train.csv`: Historical cumulative cases and fatalities by date and region.\n    * `test.csv`: Dates and regions for which forecasts are required.\n    * `submission.csv`: Sample submission format (cumulative predictions).\n* **Features**:\n    * Key columns: `Date`, `Country/Region`, `Province/State`, `ConfirmedCases`, `Fatalities`.\n    * Additional external data (e.g., policy actions, environmental variables) may be integrated.\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Columnwise Root Mean Squared Logarithmic Error (RMSLE).\n* **Calculation**:\n    * For each column (ConfirmedCases, Fatalities):\n        \\[\n        \\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\log(p_i + 1) - \\log(a_i + 1))^2}\n        \\]\n        where \\(p_i\\) = predicted value, \\(a_i\\) = actual value, \\(n\\) = number of observations.\n    * Final score: Mean of RMSLE across both columns.\n* **Leaderboard Periods**:\n    * Public: Predictions for March 26 - April 8 (evaluated on data up to March 26).\n    * Private: Predictions for April 9 - May 7 (evaluated on most recent data).",
    "sections": {},
    "file_path": "kaggle_datasets/372/problem_summary.md"
  },
  "140": {
    "problem_id": "140",
    "title": "Predicting Chess Player Elo Ratings from Single Game Moves",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Chess Player Elo Ratings from Single Game Moves\n\n## Problem Description\n* **Problem Type**: Regression (predicting continuous Elo ratings).\n* **Objective**: Predict the FIDE Elo ratings of both chess players (white and black) based solely on the moves from a single game. The challenge focuses on intrinsic skill assessment rather than game outcomes.\n    * **Key Points**:\n        * Uses move quality analysis (via chess engine evaluations) as primary features.\n        * Requires predicting ratings for both players in each game.\n        * Dataset includes games from elite, ranked players with anonymized identities.\n        * No prior chess knowledge is required; computational analysis of move sequences is central.\n\n## Dataset Overview\n* **Data Type**: Chess game data in two formats:\n    * Portable Game Notation (PGN) with Standard Algebraic Notation (SAN) moves.\n    * Universal Chess Interface (UCI) formatted move sequences.\n* **Context**: 50,000 games between elite players, split into training (25,000 games with Elo labels) and test sets (25,000 games without labels).\n* **Data Files**:\n    * `data.pgn.zip` / `data_uci.pgn.zip`: Raw game data in PGN/UCI formats.\n    * `stockfish.csv.zip`: Precomputed move-by-move centipawn (cp) advantage scores for white, generated by the Stockfish engine (1-second analysis per move).\n    * `sampleSubmission.csv.zip`: Submission template with `Event, WhiteElo, BlackElo` columns.\n* **Features**:\n    * Move sequences (SAN or UCI) for each game.\n    * Engine-derived centipawn scores reflecting positional advantage at each move.\n    * Anonymized metadata (e.g., event ID, result) without player identities.\n\n## Evaluation Metrics\n* **Primary Metric**: Mean Absolute Error (MAE).\n    * **Calculation**: Average absolute difference between predicted and actual Elo ratings for both white and black players across all test games.\n    * **Submission Format**: Requires separate predictions for white and black Elo in each test game.",
    "sections": {},
    "file_path": "kaggle_datasets/140/problem_summary.md"
  },
  "14": {
    "problem_id": "14",
    "title": "Arabic Writer Identification from Handwritten Documents",
    "problem_type": "Multi-class Classification (Writer Identification) with Unknown Class Detection",
    "objective": "Develop an algorithm to identify the writer of Arabic handwritten documents by calculating probability scores that:",
    "evaluation_metric": null,
    "full_content": "# Arabic Writer Identification from Handwritten Documents\n\n**Problem Description:**\n* **Problem Type:** Multi-class Classification (Writer Identification) with Unknown Class Detection\n* **Objective:** Develop an algorithm to identify the writer of Arabic handwritten documents by calculating probability scores that:\n    * A test document matches a known writer from the training set\n    * A test document belongs to an unknown writer not in the training set\n* **Key Points:**\n    * Focus on forensic document analysis applications\n    * Handwriting variability makes exact character reproduction impossible\n    * Two-stage approach expected:\n        * Feature extraction from document images\n        * Classification based on feature similarity\n    * Special consideration for detecting writers not present in training data\n\n**Dataset Overview:**\n* **Data Type:** Handwritten Arabic document images with pre-extracted features\n* **Context:** Collected from 50+ writers at Qatar University, each writing 3 paragraphs\n* **Data Files:**\n    * Image sets (color, grayscale, binary formats):\n        * `train/`: XXX_Y.png (writer ID_paragraph number)\n        * `test/`: ZZ.png (anonymous test documents)\n    * Feature files:\n        * `train.csv`, `test.csv` with 70+ pre-extracted features\n    * `sample_entry.csv`: Submission format example\n* **Key Features:**\n    * Geometric features (connected components, holes, moments)\n    * Projection histograms (X/Y axes)\n    * Fourier descriptors\n    * Chain code histograms\n    * Curvature measurements\n    * Luminance distributions\n\n**Evaluation Metrics:**\n* **Primary Metric:** Mean Absolute Error (MAE)\n* **Submission Format:**\n    * Matrix of probabilities where:\n        * Rows: Test documents (ZZ images)\n        * Columns: Known writers (XXX) + \"unknown\" class\n        * Each cell contains P(test_doc ∈ writer|unknown)\n    * Probability sums don't need to normalize to 1\n* **Scoring:**\n    * Measures deviation between predicted probabilities and ground truth\n    * Lower MAE indicates better performance",
    "sections": {
      "Problem Description": "* **Problem Type:** Multi-class Classification (Writer Identification) with Unknown Class Detection\n* **Objective:** Develop an algorithm to identify the writer of Arabic handwritten documents by calculating probability scores that:\n    * A test document matches a known writer from the training set\n    * A test document belongs to an unknown writer not in the training set\n* **Key Points:**\n    * Focus on forensic document analysis applications\n    * Handwriting variability makes exact character reproduction impossible\n    * Two-stage approach expected:\n        * Feature extraction from document images\n        * Classification based on feature similarity\n    * Special consideration for detecting writers not present in training data",
      "Dataset Overview": "* **Data Type:** Handwritten Arabic document images with pre-extracted features\n* **Context:** Collected from 50+ writers at Qatar University, each writing 3 paragraphs\n* **Data Files:**\n    * Image sets (color, grayscale, binary formats):\n        * `train/`: XXX_Y.png (writer ID_paragraph number)\n        * `test/`: ZZ.png (anonymous test documents)\n    * Feature files:\n        * `train.csv`, `test.csv` with 70+ pre-extracted features\n    * `sample_entry.csv`: Submission format example\n* **Key Features:**\n    * Geometric features (connected components, holes, moments)\n    * Projection histograms (X/Y axes)\n    * Fourier descriptors\n    * Chain code histograms\n    * Curvature measurements\n    * Luminance distributions",
      "Evaluation Metrics": "* **Primary Metric:** Mean Absolute Error (MAE)\n* **Submission Format:**\n    * Matrix of probabilities where:\n        * Rows: Test documents (ZZ images)\n        * Columns: Known writers (XXX) + \"unknown\" class\n        * Each cell contains P(test_doc ∈ writer|unknown)\n    * Probability sums don't need to normalize to 1\n* **Scoring:**\n    * Measures deviation between predicted probabilities and ground truth\n    * Lower MAE indicates better performance"
    },
    "file_path": "kaggle_datasets/14/problem_summary.md"
  },
  "182": {
    "problem_id": "182",
    "title": "Predicting Insurance Quote Conversion",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a potential customer will purchase a quoted homeowners insurance policy based on anonymized customer and sales activity data.",
    "evaluation_metric": null,
    "full_content": "# Predicting Insurance Quote Conversion\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a potential customer will purchase a quoted homeowners insurance policy based on anonymized customer and sales activity data.\n    * **Key Points:**\n        * The goal is to help Homesite understand the impact of pricing changes and maintain an ideal customer portfolio.\n        * The competition focuses on predicting conversion likelihood (purchase/non-purchase) rather than pricing optimization.\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing anonymized customer and policy information for homeowners insurance quotes.\n    * **Data Files:**\n        * `train.csv` - Contains features and target variable (`QuoteConversion_Flag`)\n        * `test.csv` - Contains features for making predictions\n        * `sample_submission.csv` - Example submission format\n    * **Features:** Include (but are not limited to):\n        * Coverage information\n        * Sales information\n        * Personal customer information\n        * Property characteristics\n        * Geographic data\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the model's ability to distinguish between positive (conversion) and negative (non-conversion) classes\n        * Evaluates the entire range of classification thresholds\n        * Higher values indicate better predictive performance (1.0 = perfect prediction)",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a potential customer will purchase a quoted homeowners insurance policy based on anonymized customer and sales activity data.\n    * **Key Points:**\n        * The goal is to help Homesite understand the impact of pricing changes and maintain an ideal customer portfolio.\n        * The competition focuses on predicting conversion likelihood (purchase/non-purchase) rather than pricing optimization.",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing anonymized customer and policy information for homeowners insurance quotes.\n    * **Data Files:**\n        * `train.csv` - Contains features and target variable (`QuoteConversion_Flag`)\n        * `test.csv` - Contains features for making predictions\n        * `sample_submission.csv` - Example submission format\n    * **Features:** Include (but are not limited to):\n        * Coverage information\n        * Sales information\n        * Personal customer information\n        * Property characteristics\n        * Geographic data",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures the model's ability to distinguish between positive (conversion) and negative (non-conversion) classes\n        * Evaluates the entire range of classification thresholds\n        * Higher values indicate better predictive performance (1.0 = perfect prediction)"
    },
    "file_path": "kaggle_datasets/182/problem_summary.md"
  },
  "344": {
    "problem_id": "344",
    "title": "Temporal Localization of Topics in YouTube Videos",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Temporal Localization of Topics in YouTube Videos\n\n## Problem Description\n- **Problem Type**: Video Understanding - Temporal Concept Localization (Multi-label Classification at Segment Level)\n- **Objective**: \n  - Identify precise moments (segments) within YouTube videos where specific topics/actions occur, given only video-level labels.\n  - Predict which 5-second segments contain each of 1,000 possible classes (e.g., \"blowing out candles\", \"cat sneezing\").\n- **Key Points**:\n  - Segments are fixed at 5-second intervals (aligned to 5n-second timestamps).\n  - Requires modeling both visual (RGB) and audio features simultaneously.\n  - Focus on large-scale prediction (~100K segments per class in submissions).\n\n## Dataset Overview\n- **Data Type**: \n  - Processed video data (frame-level features, not raw pixels)\n  - Extracted features from YouTube videos with temporal segments\n- **Data Files**:\n  - Frame-level TFRecord files (train/validate/test splits)\n  - `vocabulary.csv` (label ID to description mapping)\n  - `sample_submission.csv` (submission format template)\n- **Features**:\n  - Per-frame: \n    - `rgb`: 1024-dim float vector (visual features)\n    - `audio`: 128-dim float vector (audio features)\n  - Per-segment (validation only):\n    - Start/end times\n    - Positive/negative labels with confidence scores\n\n## Evaluation Metrics\n- **Evaluation Metric**: Mean Average Precision @ K (MAP@100,000)\n- **Components**:\n  - Calculated per-class, then averaged across 1,000 classes\n  - Only human-rated segments are scored (unrated segments filtered out)\n  - Precision@k considers:\n    - Correctness of predicted segments (rel(k))\n    - Ranking confidence of predictions\n    - Normalized by true positive count per class (Nc)\n  - Public/Private split at segment level (not class level)",
    "sections": {},
    "file_path": "kaggle_datasets/344/problem_summary.md"
  },
  "176": {
    "problem_id": "176",
    "title": "Multiclass Cuisine Classification from Recipe Ingredients",
    "problem_type": "Multiclass Classification (Text Data)",
    "objective": "Predict the cuisine category of a dish based solely on its list of ingredients. The task involves analyzing variable-length ingredient lists and mapping them to specific cuisine types (e.g., Indian, Italian, Korean).",
    "evaluation_metric": null,
    "full_content": "# Multiclass Cuisine Classification from Recipe Ingredients\n\n**Problem Description:**\n* **Problem Type:** Multiclass Classification (Text Data)\n* **Objective:** Predict the cuisine category of a dish based solely on its list of ingredients. The task involves analyzing variable-length ingredient lists and mapping them to specific cuisine types (e.g., Indian, Italian, Korean).\n* **Key Points:**\n  * Cultural/geographic associations of ingredients are central to the classification task.\n  * Ingredient lists are of variable length and require text processing.\n  * Dataset is derived from real-world recipe data provided by Yummly.\n\n**Dataset Overview:**\n* **Data Type & Context:** JSON-formatted recipe data containing:\n  * Tabular structure with recipe IDs and cuisine labels (training set only).\n  * Text data (ingredient lists) as variable-length arrays of strings.\n* **Data Files:**\n  * `train.json`: Contains recipe IDs, cuisine labels, and ingredient lists.\n  * `test.json`: Contains recipe IDs and ingredient lists (no cuisine labels).\n  * `sample_submission.csv`: Demonstrates submission format (ID + predicted cuisine).\n* **Features:**\n  * Primary feature: Unstructured ingredient lists (e.g., [\"turmeric\", \"garam masala\", \"naan\"]).\n  * Each recipe's ingredients appear as raw text strings requiring preprocessing.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Categorization Accuracy (percentage of correctly classified cuisines).\n* **Components:**\n  * Simple proportion of correct predictions: `(Correct Predictions / Total Predictions) * 100`.\n  * No class weighting or complex scoring rules (standard multiclass accuracy).",
    "sections": {
      "Problem Description": "* **Problem Type:** Multiclass Classification (Text Data)\n* **Objective:** Predict the cuisine category of a dish based solely on its list of ingredients. The task involves analyzing variable-length ingredient lists and mapping them to specific cuisine types (e.g., Indian, Italian, Korean).\n* **Key Points:**\n  * Cultural/geographic associations of ingredients are central to the classification task.\n  * Ingredient lists are of variable length and require text processing.\n  * Dataset is derived from real-world recipe data provided by Yummly.",
      "Dataset Overview": "* **Data Type & Context:** JSON-formatted recipe data containing:\n  * Tabular structure with recipe IDs and cuisine labels (training set only).\n  * Text data (ingredient lists) as variable-length arrays of strings.\n* **Data Files:**\n  * `train.json`: Contains recipe IDs, cuisine labels, and ingredient lists.\n  * `test.json`: Contains recipe IDs and ingredient lists (no cuisine labels).\n  * `sample_submission.csv`: Demonstrates submission format (ID + predicted cuisine).\n* **Features:**\n  * Primary feature: Unstructured ingredient lists (e.g., [\"turmeric\", \"garam masala\", \"naan\"]).\n  * Each recipe's ingredients appear as raw text strings requiring preprocessing.",
      "Evaluation Metrics": "* **Primary Metric:** Categorization Accuracy (percentage of correctly classified cuisines).\n* **Components:**\n  * Simple proportion of correct predictions: `(Correct Predictions / Total Predictions) * 100`.\n  * No class weighting or complex scoring rules (standard multiclass accuracy)."
    },
    "file_path": "kaggle_datasets/176/problem_summary.md"
  },
  "512": {
    "problem_id": "512",
    "title": "Universal Image Embedding for Cross-Domain Retrieval",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Universal Image Embedding for Cross-Domain Retrieval\n\n## Problem Description\n* **Problem Type**: Computer Vision - Image Embedding & Retrieval\n* **Objective**: Develop a universal image embedding model that can retrieve relevant database images (containing the same object) across diverse visual domains from a query image. The model must generalize across multiple object types not seen during training.\n* **Key Points**:\n  * Focus on cross-domain generalization (unlike traditional per-domain embedding models)\n  * Retrieval task requires finding images with the same instance-level object (e.g., same T-shirt, same building)\n  * Supported domains include apparel, artwork, landmarks, furniture, packaged goods, storefronts, dishes, toys, memes, illustrations, and cars\n  * Model must output compact embeddings (≤64 dimensions)\n\n## Dataset Overview\n* **Data Type**: Image data across multiple visual domains\n* **Context**: No provided training set - participants may use any publicly available datasets (must be disclosed)\n* **Evaluation Data Characteristics** (private test set):\n  * Contains 11 diverse object categories\n  * Instance-level labels (same physical object = match)\n  * Sample domains shown in competition header image\n* **Key Constraint**: Model must handle unseen domain types during evaluation\n\n## Evaluation Metrics\n* **Primary Metric**: Modified mean Precision @ 5 (mP@5)\n  * Calculated as:  \n    𝑚𝑃@5 = (1/𝑄) ∑𝑞 (1/min(𝑛𝑞,5)) ∑𝑗 𝑟𝑒𝑙𝑞(𝑗)  \n    Where:\n    * 𝑄 = number of query images\n    * 𝑛𝑞 = relevant index images for query q\n    * 𝑟𝑒𝑙𝑞(𝑗) = 1 if j-th prediction is correct, else 0\n* **Key Metric Properties**:\n  * Accounts for queries with <5 relevant images\n  * Uses Euclidean distance in embedding space for kNN (k=5)\n  * Evaluates retrieval quality across all domains",
    "sections": {},
    "file_path": "kaggle_datasets/512/problem_summary.md"
  },
  "22": {
    "problem_id": "22",
    "title": "Photo Quality Prediction via Metadata Analysis",
    "problem_type": "Binary Classification",
    "objective": "Predict whether a human evaluator would rate a photo album as 'good' (1) or not (0) based solely on anonymized metadata features. The goal is to automate the identification of high-quality albums for highlighting, particularly for travel-related content.",
    "evaluation_metric": null,
    "full_content": "# Photo Quality Prediction via Metadata Analysis\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a human evaluator would rate a photo album as 'good' (1) or not (0) based solely on anonymized metadata features. The goal is to automate the identification of high-quality albums for highlighting, particularly for travel-related content.\n    * **Key Points:**\n        * Focuses on metadata only (no image pixel data available)\n        * Requires handling anonymized text features (tokenized words)\n        * Aims to combine multiple signals (location, dimensions, text tokens) for prediction\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing metadata features from user-generated photo albums\n* **Data Files:**\n    * training.csv (contains target 'good' labels)\n    * test.csv\n    * example_entry.csv\n* **Key Features:**\n    * Geographic: latitude, longitude (integrally-rounded)\n    * Image characteristics: width, height (pixels), size (photo count)\n    * Tokenized text: name, description, caption (common non-stop words encoded as numbers)\n    * Target: binary 'good' label (human evaluation)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Capped binomial deviance\n    * **Components:**\n        * Based on the evaluation method from Chess Ratings 2 competition\n        * Measures the deviance between predicted probabilities and actual binary outcomes\n        * Includes a capping mechanism to limit extreme values' impact",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether a human evaluator would rate a photo album as 'good' (1) or not (0) based solely on anonymized metadata features. The goal is to automate the identification of high-quality albums for highlighting, particularly for travel-related content.\n    * **Key Points:**\n        * Focuses on metadata only (no image pixel data available)\n        * Requires handling anonymized text features (tokenized words)\n        * Aims to combine multiple signals (location, dimensions, text tokens) for prediction",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing metadata features from user-generated photo albums\n* **Data Files:**\n    * training.csv (contains target 'good' labels)\n    * test.csv\n    * example_entry.csv\n* **Key Features:**\n    * Geographic: latitude, longitude (integrally-rounded)\n    * Image characteristics: width, height (pixels), size (photo count)\n    * Tokenized text: name, description, caption (common non-stop words encoded as numbers)\n    * Target: binary 'good' label (human evaluation)",
      "Evaluation Metrics": "* **Primary Metric:** Capped binomial deviance\n    * **Components:**\n        * Based on the evaluation method from Chess Ratings 2 competition\n        * Measures the deviance between predicted probabilities and actual binary outcomes\n        * Includes a capping mechanism to limit extreme values' impact"
    },
    "file_path": "kaggle_datasets/22/problem_summary.md"
  },
  "149": {
    "problem_id": "149",
    "title": "Multi-class Product Classification with Obfuscated Features",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Multi-class Product Classification with Obfuscated Features\n\n## Problem Description\n* **Problem Type:** Multi-class Classification\n* **Objective:** \n    * Build a predictive model to classify e-commerce products into one of nine main categories.\n    * Address the challenge of inconsistent product classification across Otto Group's global subsidiaries.\n    * Generate accurate product performance analysis through better classification.\n* **Key Points:**\n    * Focus on clustering similar products despite diverse global infrastructure.\n    * All features are anonymized and obfuscated (counts of different events).\n    * Winning models will be open-sourced.\n\n## Dataset Overview\n* **Data Type & Context:** \n    * Tabular data representing product features in Otto Group's e-commerce ecosystem.\n    * Contains obfuscated numerical features (event counts) for over 200,000 products.\n* **Data Files:**\n    * train.csv (training set with target labels)\n    * test.csv (test set for predictions)\n    * sampleSubmission.csv (example submission format)\n* **Key Features:**\n    * 93 anonymized numerical features (feat_1 to feat_93)\n    * Product ID (anonymous identifier)\n    * Target variable with 9 class labels (main product categories)\n\n## Evaluation Metrics\n* **Primary Metric:** Multi-class Logarithmic Loss (Log Loss)\n* **Metric Components:**\n    * Formula: \n        ```\n        logloss = -1/N * Σ(i=1 to N) Σ(j=1 to M) y_ij * log(p_ij)\n        ```\n        Where:\n        * N = number of products\n        * M = number of classes (9)\n        * y_ij = 1 if product i is in class j, else 0\n        * p_ij = predicted probability of product i belonging to class j\n    * Technical Notes:\n        * Probabilities are rescaled (each row divided by row sum)\n        * Predicted probabilities clipped to [10^-15, 1-10^-15] to avoid log extremes",
    "sections": {},
    "file_path": "kaggle_datasets/149/problem_summary.md"
  },
  "515": {
    "problem_id": "515",
    "title": "Predicting Markdown Cell Order in Python Notebooks",
    "problem_type": "Sequence Prediction / Ranking (NLP + Code Understanding)",
    "objective": "Reconstruct the correct order of markdown cells in Jupyter notebooks based on the sequence of code cells, demonstrating an understanding of how natural language comments relate to code segments.",
    "evaluation_metric": null,
    "full_content": "# Predicting Markdown Cell Order in Python Notebooks\n\n**Problem Description:**\n* **Problem Type:** Sequence Prediction / Ranking (NLP + Code Understanding)\n* **Objective:** Reconstruct the correct order of markdown cells in Jupyter notebooks based on the sequence of code cells, demonstrating an understanding of how natural language comments relate to code segments.\n    * **Key Points:**\n        * Focuses on narrative structure of Python notebooks (code + markdown cells)\n        * Requires modeling relationships between code cells and shuffled markdown cells\n        * Notebooks are processed to remove empty cells and ensure Python-only code\n        * Test set includes future notebooks to prevent lookup-based solutions\n\n**Dataset Overview:**\n* **Data Type:** JSON files containing notebook cell content + CSV metadata\n    * **Context:** 160,000 public Kaggle notebooks with code/markdown cells\n* **Data Files:**\n    * `train/` - 140K JSON files with code cells (ordered) + markdown cells (shuffled)\n    * `train_orders.csv` - Ground truth cell ordering\n    * `train_ancestors.csv` - Notebook forking history (for validation)\n    * `test/` - 20K notebooks in same format as training\n* **Features:**\n    * Code cell content (Python)\n    * Markdown cell content (natural language)\n    * Cell IDs and ordering metadata\n\n**Evaluation Metrics:**\n* **Primary Metric:** Kendall Tau Correlation\n    * Measures ranking similarity between predicted and true cell orders\n    * **Calculation:**\n        * S = Number of adjacent swaps needed to sort prediction\n        * Worst-case swaps = n(n-1)/2 for n cells\n        * K = 1 - 4*(∑S_i)/(∑n_i(n_i-1)) across all test notebooks\n    * Implementation provided in competition notebook",
    "sections": {
      "Problem Description": "* **Problem Type:** Sequence Prediction / Ranking (NLP + Code Understanding)\n* **Objective:** Reconstruct the correct order of markdown cells in Jupyter notebooks based on the sequence of code cells, demonstrating an understanding of how natural language comments relate to code segments.\n    * **Key Points:**\n        * Focuses on narrative structure of Python notebooks (code + markdown cells)\n        * Requires modeling relationships between code cells and shuffled markdown cells\n        * Notebooks are processed to remove empty cells and ensure Python-only code\n        * Test set includes future notebooks to prevent lookup-based solutions",
      "Dataset Overview": "* **Data Type:** JSON files containing notebook cell content + CSV metadata\n    * **Context:** 160,000 public Kaggle notebooks with code/markdown cells\n* **Data Files:**\n    * `train/` - 140K JSON files with code cells (ordered) + markdown cells (shuffled)\n    * `train_orders.csv` - Ground truth cell ordering\n    * `train_ancestors.csv` - Notebook forking history (for validation)\n    * `test/` - 20K notebooks in same format as training\n* **Features:**\n    * Code cell content (Python)\n    * Markdown cell content (natural language)\n    * Cell IDs and ordering metadata",
      "Evaluation Metrics": "* **Primary Metric:** Kendall Tau Correlation\n    * Measures ranking similarity between predicted and true cell orders\n    * **Calculation:**\n        * S = Number of adjacent swaps needed to sort prediction\n        * Worst-case swaps = n(n-1)/2 for n cells\n        * K = 1 - 4*(∑S_i)/(∑n_i(n_i-1)) across all test notebooks\n    * Implementation provided in competition notebook"
    },
    "file_path": "kaggle_datasets/515/problem_summary.md"
  },
  "171": {
    "problem_id": "171",
    "title": "Sponsored Web Page Classification",
    "problem_type": "Binary Classification",
    "objective": "Predict whether web pages served by StumbleUpon are sponsored (native ads) or organic content based on their raw HTML files.",
    "evaluation_metric": null,
    "full_content": "# Sponsored Web Page Classification\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict whether web pages served by StumbleUpon are sponsored (native ads) or organic content based on their raw HTML files.\n    * **Key Points:**\n        * Focus on identifying paid content disguised as regular web pages\n        * Goal is to help media companies filter out poorly designed native ads\n        * Challenge involves analyzing raw HTML content including text, links, and downloadable images\n\n**Dataset Overview:**\n* **Data Type & Context:** Raw HTML files of web pages (over 300,000 files)\n* **Data Files:**\n    * Training files: {0,1,2,3,4}.zip (later consolidated into train_v2.csv)\n    * Test files: 5.zip\n    * Metadata files: train.csv/train_v2.csv (contains file names and labels), sampleSubmission_v2.csv\n* **Features:**\n    * Raw HTML content including text, links, and images\n    * Binary labels (0 = organic, 1 = sponsored)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between sponsored vs organic content\n        * Evaluates performance across all classification thresholds\n        * Submission requires probability predictions for each test instance",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict whether web pages served by StumbleUpon are sponsored (native ads) or organic content based on their raw HTML files.\n    * **Key Points:**\n        * Focus on identifying paid content disguised as regular web pages\n        * Goal is to help media companies filter out poorly designed native ads\n        * Challenge involves analyzing raw HTML content including text, links, and downloadable images",
      "Dataset Overview": "* **Data Type & Context:** Raw HTML files of web pages (over 300,000 files)\n* **Data Files:**\n    * Training files: {0,1,2,3,4}.zip (later consolidated into train_v2.csv)\n    * Test files: 5.zip\n    * Metadata files: train.csv/train_v2.csv (contains file names and labels), sampleSubmission_v2.csv\n* **Features:**\n    * Raw HTML content including text, links, and images\n    * Binary labels (0 = organic, 1 = sponsored)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between sponsored vs organic content\n        * Evaluates performance across all classification thresholds\n        * Submission requires probability predictions for each test instance"
    },
    "file_path": "kaggle_datasets/171/problem_summary.md"
  },
  "343": {
    "problem_id": "343",
    "title": "Fraud Detection in E-Commerce Transactions",
    "problem_type": "Binary Classification",
    "objective": "Predict the probability that an online transaction is fraudulent (denoted by the binary target `isFraud`). The goal is to improve fraud detection accuracy to reduce false positives and enhance customer experience.",
    "evaluation_metric": null,
    "full_content": "# Fraud Detection in E-Commerce Transactions\n\n**Problem Description:**\n* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that an online transaction is fraudulent (denoted by the binary target `isFraud`). The goal is to improve fraud detection accuracy to reduce false positives and enhance customer experience.\n    * **Key Points:**\n        * Real-world e-commerce transaction data with potential class imbalance (fraud cases being rare)\n        * Requires joining transaction and identity data via `TransactionID` (not all transactions have identity info)\n        * Opportunity for feature engineering to improve results\n\n**Dataset Overview:**\n* **Data Type & Context:** Tabular data containing e-commerce transaction records with identity information\n    * **Data Files:**\n        * `train_transaction.csv`, `train_identity.csv` - training set\n        * `test_transaction.csv`, `test_identity.csv` - test set\n        * `sample_submission.csv` - submission format example\n    * **Key Features:**\n        * Transaction features: ProductCD, card1-card6, addr1-addr2, email domains, M1-M9\n        * Identity features: DeviceType, DeviceInfo, id_12-id_38\n        * `TransactionDT`: timedelta from reference datetime (not actual timestamp)\n\n**Evaluation Metrics:**\n* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between fraudulent and legitimate transactions\n        * Evaluates predicted probabilities against binary ground truth\n        * Higher AUC indicates better discrimination between classes",
    "sections": {
      "Problem Description": "* **Problem Type:** Binary Classification\n* **Objective:** Predict the probability that an online transaction is fraudulent (denoted by the binary target `isFraud`). The goal is to improve fraud detection accuracy to reduce false positives and enhance customer experience.\n    * **Key Points:**\n        * Real-world e-commerce transaction data with potential class imbalance (fraud cases being rare)\n        * Requires joining transaction and identity data via `TransactionID` (not all transactions have identity info)\n        * Opportunity for feature engineering to improve results",
      "Dataset Overview": "* **Data Type & Context:** Tabular data containing e-commerce transaction records with identity information\n    * **Data Files:**\n        * `train_transaction.csv`, `train_identity.csv` - training set\n        * `test_transaction.csv`, `test_identity.csv` - test set\n        * `sample_submission.csv` - submission format example\n    * **Key Features:**\n        * Transaction features: ProductCD, card1-card6, addr1-addr2, email domains, M1-M9\n        * Identity features: DeviceType, DeviceInfo, id_12-id_38\n        * `TransactionDT`: timedelta from reference datetime (not actual timestamp)",
      "Evaluation Metrics": "* **Primary Metric:** Area Under the ROC Curve (AUC)\n    * **Components:**\n        * Measures model's ability to distinguish between fraudulent and legitimate transactions\n        * Evaluates predicted probabilities against binary ground truth\n        * Higher AUC indicates better discrimination between classes"
    },
    "file_path": "kaggle_datasets/343/problem_summary.md"
  },
  "185": {
    "problem_id": "185",
    "title": "Predicting Life Insurance Risk Classification",
    "problem_type": null,
    "objective": null,
    "evaluation_metric": null,
    "full_content": "# Predicting Life Insurance Risk Classification\n\n## Problem Description\n* **Problem Type**: Ordinal Classification (8-level risk rating)\n* **Objective**: \n    * Develop a predictive model to classify life insurance applicants' risk levels automatically\n    * Streamline the traditional 30-day manual assessment process\n    * Maintain privacy while improving customer experience in insurance applications\n* **Key Points**:\n    * Target variable \"Response\" has 8 ordinal levels (higher = higher risk)\n    * Goal is to replicate/improve upon manual risk assessment accuracy\n    * Solution impacts business efficiency and customer acquisition\n\n## Dataset Overview\n* **Data Type**: Tabular data (insurance application features)\n* **Context**: \n    * 100+ anonymized/normalized variables about applicants\n    * Includes product, demographic, employment, medical, and insurance history data\n* **Data Files**:\n    * train.csv (with Response values)\n    * test.csv (requires Response prediction)\n    * sample_submission.csv\n* **Key Features**:\n    * Mixed data types:\n        * Categorical: Product/employment/insurance history features\n        * Continuous: Age, height, weight, BMI\n        * Discrete: Medical history counts\n        * Binary: 48 medical keyword flags\n    * All features normalized/anonymized\n\n## Evaluation Metrics\n* **Primary Metric**: Quadratic Weighted Kappa\n* **Metric Components**:\n    * Measures agreement between predicted and actual ordinal ratings\n    * Calculation process:\n        1. Construct NxN histogram matrix O (actual vs predicted)\n        2. Compute weight matrix w where w(i,j) = (i-j)²/(N-1)²\n        3. Create expected ratings matrix E (no-correlation assumption)\n        4. Calculate: κ = 1 - (sum(w(i,j)*O(i,j)) / sum(w(i,j)*E(i,j)))\n    * Ranges from -1 to 1 (1 = perfect agreement)\n    * Penalizes larger classification errors more severely",
    "sections": {},
    "file_path": "kaggle_datasets/185/problem_summary.md"
  },
  "25": {
    "problem_id": "25",
    "title": "Predicting Stock Market Resiliency After Liquidity Shocks",
    "problem_type": "Time Series Forecasting (Financial Markets)",
    "objective": "Develop models to predict short-term bid/ask price movements following liquidity shocks (large trades that impact order book liquidity). The goal is to forecast mean reversion behavior (resiliency) in stock prices after such shocks.",
    "evaluation_metric": null,
    "full_content": "# Predicting Stock Market Resiliency After Liquidity Shocks\n\n**Problem Description:**\n* **Problem Type:** Time Series Forecasting (Financial Markets)\n* **Objective:** Develop models to predict short-term bid/ask price movements following liquidity shocks (large trades that impact order book liquidity). The goal is to forecast mean reversion behavior (resiliency) in stock prices after such shocks.\n    * **Key Points:**\n        * Focus on modeling how markets recover after liquidity depletion (spread widening or permanent price shifts).\n        * Improve backtesting realism by accounting for market resiliency, which is often assumed to be zero in existing simulations.\n        * Factors to consider include order arrival rates, bid-ask spread, transaction size, timing, and trading volume.\n\n**Dataset Overview:**\n* **Data Type:** High-frequency financial time series data (Trade and Quote (TAQ) data from the London Stock Exchange).\n* **Context:** Limit order book events (quotes and trades) before/after liquidity shocks in medium-to-low liquidity stocks.\n* **Data Files:**\n    * `training.zip` (primary training data)\n    * `testing.zip` (test data with 50,000 rows for submission)\n    * Example submission files (`example_entry_*.zip`)\n* **Features:**\n    * Preprocessed observations of order book state before/after shocks.\n    * Includes variables related to order flow, spread dynamics, transaction characteristics, and timing (specific schema shown in competition's data fields table).\n\n**Evaluation Metrics:**\n* **Primary Metric:** Root Mean Square Error (RMSE)\n    * **Calculation:**\n        * RMSE computed separately for bid and ask prices at each time step after a shock.\n        * Final score is the cumulative RMSE across all predictions in the test set.\n    * **Submission Format:** Requires predicting 50 future bid/ask prices (100 values total) for each of 50,000 test observations.",
    "sections": {
      "Problem Description": "* **Problem Type:** Time Series Forecasting (Financial Markets)\n* **Objective:** Develop models to predict short-term bid/ask price movements following liquidity shocks (large trades that impact order book liquidity). The goal is to forecast mean reversion behavior (resiliency) in stock prices after such shocks.\n    * **Key Points:**\n        * Focus on modeling how markets recover after liquidity depletion (spread widening or permanent price shifts).\n        * Improve backtesting realism by accounting for market resiliency, which is often assumed to be zero in existing simulations.\n        * Factors to consider include order arrival rates, bid-ask spread, transaction size, timing, and trading volume.",
      "Dataset Overview": "* **Data Type:** High-frequency financial time series data (Trade and Quote (TAQ) data from the London Stock Exchange).\n* **Context:** Limit order book events (quotes and trades) before/after liquidity shocks in medium-to-low liquidity stocks.\n* **Data Files:**\n    * `training.zip` (primary training data)\n    * `testing.zip` (test data with 50,000 rows for submission)\n    * Example submission files (`example_entry_*.zip`)\n* **Features:**\n    * Preprocessed observations of order book state before/after shocks.\n    * Includes variables related to order flow, spread dynamics, transaction characteristics, and timing (specific schema shown in competition's data fields table).",
      "Evaluation Metrics": "* **Primary Metric:** Root Mean Square Error (RMSE)\n    * **Calculation:**\n        * RMSE computed separately for bid and ask prices at each time step after a shock.\n        * Final score is the cumulative RMSE across all predictions in the test set.\n    * **Submission Format:** Requires predicting 50 future bid/ask prices (100 values total) for each of 50,000 test observations."
    },
    "file_path": "kaggle_datasets/25/problem_summary.md"
  },
  "388": {
    "problem_id": "388",
    "title": "Predicting Age and Assessments from Multimodal Brain MRI Features",
    "problem_type": "Multivariate Regression (predicting multiple continuous targets).",
    "objective": "Predict age and four assessment values (domain1_var1, domain1_var2, domain2_var1, domain2_var2) from multimodal brain MRI features, including:",
    "evaluation_metric": null,
    "full_content": "# Predicting Age and Assessments from Multimodal Brain MRI Features\n\n**Problem Description:**\n* **Problem Type:** Multivariate Regression (predicting multiple continuous targets).\n* **Objective:** Predict age and four assessment values (domain1_var1, domain1_var2, domain2_var1, domain2_var2) from multimodal brain MRI features, including:\n  * Functional MRI (fMRI) spatial maps\n  * Functional network connectivity (FNC) matrices\n  * Structural MRI (sMRI) source-based morphometry (SBM) loadings.\n* **Key Points:**\n  * Focus on generalizing predictions across different MRI scanners/sites (addressing \"site effects\" bias).\n  * Dataset includes nearly 12K unaffected subjects (healthy population) for normative modeling.\n  * Special emphasis on performance for data from a second scanner (hidden in test set).\n\n**Dataset Overview:**\n* **Data Type:** Multimodal neuroimaging features (3D spatial maps, tabular connectivity/loading values) + tabular targets.\n* **Data Files:**\n  * `fMRI_train/`, `fMRI_test/`: 3D spatial maps (.mat format)\n  * `fnc.csv`: Static FNC correlation features\n  * `loading.csv`: sMRI SBM loadings\n  * `train_scores.csv`: Target variables (age + 4 assessments)\n  * `reveal_ID_site2.csv`: IDs from different scanner (for bias mitigation).\n* **Key Features:**\n  * 53 ICA-derived functional networks (spatial maps + connectivity)\n  * Structural MRI loadings (gray matter concentration patterns)\n  * Scanner/site metadata for bias detection.\n\n**Evaluation Metrics:**\n* **Primary Metric:** Feature-weighted, normalized absolute error:\n  * `score = ∑[w_f * (∑|y_f,i - ŷ_f,i| / ∑ŷ_f,i)]`  \n* **Components:**\n  * Weights: `[0.3 (age), 0.175 (domain1_var1), 0.175 (domain1_var2), 0.175 (domain2_var1), 0.175 (domain2_var2)]`\n  * Missing values skipped in calculation.\n  * Normalization by sum of ground truth values per feature.",
    "sections": {
      "Problem Description": "* **Problem Type:** Multivariate Regression (predicting multiple continuous targets).\n* **Objective:** Predict age and four assessment values (domain1_var1, domain1_var2, domain2_var1, domain2_var2) from multimodal brain MRI features, including:\n  * Functional MRI (fMRI) spatial maps\n  * Functional network connectivity (FNC) matrices\n  * Structural MRI (sMRI) source-based morphometry (SBM) loadings.\n* **Key Points:**\n  * Focus on generalizing predictions across different MRI scanners/sites (addressing \"site effects\" bias).\n  * Dataset includes nearly 12K unaffected subjects (healthy population) for normative modeling.\n  * Special emphasis on performance for data from a second scanner (hidden in test set).",
      "Dataset Overview": "* **Data Type:** Multimodal neuroimaging features (3D spatial maps, tabular connectivity/loading values) + tabular targets.\n* **Data Files:**\n  * `fMRI_train/`, `fMRI_test/`: 3D spatial maps (.mat format)\n  * `fnc.csv`: Static FNC correlation features\n  * `loading.csv`: sMRI SBM loadings\n  * `train_scores.csv`: Target variables (age + 4 assessments)\n  * `reveal_ID_site2.csv`: IDs from different scanner (for bias mitigation).\n* **Key Features:**\n  * 53 ICA-derived functional networks (spatial maps + connectivity)\n  * Structural MRI loadings (gray matter concentration patterns)\n  * Scanner/site metadata for bias detection.",
      "Evaluation Metrics": "* **Primary Metric:** Feature-weighted, normalized absolute error:\n  * `score = ∑[w_f * (∑|y_f,i - ŷ_f,i| / ∑ŷ_f,i)]`  \n* **Components:**\n  * Weights: `[0.3 (age), 0.175 (domain1_var1), 0.175 (domain1_var2), 0.175 (domain2_var1), 0.175 (domain2_var2)]`\n  * Missing values skipped in calculation.\n  * Normalization by sum of ground truth values per feature."
    },
    "file_path": "kaggle_datasets/388/problem_summary.md"
  }
}